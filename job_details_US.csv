job_link,job_title,job_tipo,tiempo,job_location,job_description,aptitudes,tipo_solicitud
https://www.linkedin.com/jobs/view/3984549050/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=7w4zWr9aricC0Vm6SOcjCw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 3 días,Estados Unidos,"Acerca del empleo
We are looking for a skilled Data Engineer to join our team on a short-term contract with the potential for full-time employment in the future. The ideal candidate will be excellent at transforming business requirements into actionable insights using the data collected from our systems. The role involves reading code (Rust and potentially other programming languages), writing SQL, having some coding skills, but also interacting with the business and understanding business requirements.

Responsibilities:
Collaborate with stakeholders to understand business requirements and translate them into data-driven solutions.
Read and understand Rust code, and potentially other languages, to comprehend data structures and workflows.
Write efficient SQL queries to extract, manipulate, and analyse data.
Mine and analyse data from databases to drive optimisation and improvement of product development, finance, marketing techniques, and business strategies.
Interpret complex data sets, analyse trends, and provide recommendations based on findings.
Develop custom data models and algorithms to apply to data sets.
Utilise predictive modelling to increase and optimise customer experiences, revenue generation, and other business outcomes.
Present findings and recommendations in a clear and concise manner to both technical and non-technical stakeholders.
Requirements:
Proven experience as a Data Engineer or similar role.
Proficiency in SQL and ability to write complex queries.
Experience in reading and understanding source code written in languages such as Rust, Python, Java and others.
Strong knowledge of statistical techniques and concepts (regression, properties of distributions, statistical tests, etc).
Familiarity with data visualisation tools (e.g., Metabase, Tableau, Power BI etc).
Excellent verbal and written communication skills, with the ability to present complex information clearly to non-technical stakeholders.
Ability to work independently and manage multiple projects simultaneously.
Strong problem-solving skills and attention to detail.
Nice to have:
Experience writing Bash and Rust code.
Experience working with DBT and Kubernetes.
Any other Data Engineering experience is a big plus.
Benefits:
Salary: We offer a competitive salary package, which includes stock options, based on your skills, experience, and expertise in the field.
Unlimited vacation: We believe in a healthy work-life balance, so we offer our employees the flexibility to take as much paid vacation time as they need to recharge and come back refreshed.

Join us! 
If you're passionate about leveraging data to level up humanity and have a proven track record of developing and implementing data solutions in a collaborative and professional environment, we want to hear from you. Please email your CV to engineeringjobs@fedi.xyz. We look forward to receiving your application for this exciting position.","Ciencia de datos y Ingeniería de datos, Atención al detalle, Bases de datos, Comunicación, Comunicación escrita, Modelo de datos, Necesidades empresariales, Optimización y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3977318867/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=XOfMRWTMPTNN3MKVzeTT%2FQ%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 1 semana,"Nueva York, Estados Unidos","Acerca del empleo
Who We Are

Same Seats. Better Prices.

Among the fastest growing technology companies, TickPick is reshaping the ticketing industry, putting money back in the wallets of live event-goers. Since inception, we have saved our customers over $100 million in service fees. Our BestPrice Guarantee backs up our promise to deliver better prices than our competition.
For the last five years, TickPick has been named a Deloitte Technology Fast 500 award winner and has landed on lists of Inc. 5000’s and Crain's New York Business’ Fast 50.
If you are passionate about concerts, sports, theater or all of them, and want to see your skills and experience have a direct impact on a fast growing company, TickPick is the place for you. We are building a diverse team, committed to providing the most innovative, transparent, and cost-effective ticket marketplace in the industry.

Who You Are

You are: a data engineer, backend software engineer, or other data specialist with strong desire and ability to implement high-impact data movement and management within a growing, technology-first team. 
In this core role, you will be responsible for building and maintaining data pipelines touching a wide array of tools from the modern data stack, including but not limited to Snowflake, Spark, Dagster, dbt, and Azure Cloud offerings. You’ll be working closely with our current Data Engineer and other Analytics-focussed team members, as well as stakeholders from across the whole business. 
If you value continual learning and are looking to join a high-visibility and high-impact team at a growing company that’s making data a priority, then this role is for you.
Core Responsibilities
Discover opportunities for data acquisition and implementation solutions
Develop production processes and solutions to model, mine and surface data
Improve and ensure data reliability, quality and efficiency

Requirements

BS or above in Computer Science or a related field, or equivalent personal or professional experience
Communication ability is paramount: you understand the value of open communication and have a track record of interacting effectively with stakeholders and team members
Experience and competency with at least one cloud services provider (Azure preferred; AWS or GCP also worthwhile)
Strong Python and SQL skills, as well as at least general competency with web languages (HTML/Javascript)
Python data competency – knowledge of and experience with, eg, Pandas or other Dataframe libraries. Polars, PySpark, or DuckDB are a big plus
Experience and competency with at least one general data orchestration tool, eg Airflow, Dagster, Prefect, or similar other experience. Dagster specifically is a big plus
dbt or other data modeling experience is a plus 
Kafka experience is a plus
Experience with web scraping tools is a plus, eg BeautifulSoup, Playwright, requests, or Selenium
Experience with a dashboarding tool (eg Looker, Tableau, Superset, Metabase, or Streamlit) is a plus, but not most centrally important
Experience with managed ETL tools (eg Fivetran, Hightouch) is worth mentioning if you have it, but not most centrally important 

Benefits

Per the NYC pay transparency law, the hiring range for this position is $90,000 to $110,000.
As a candidate for this position, your salary will be contingent upon your work experience, education, skills and any other factors TickPick considers relevant to the hiring decision. In addition to your salary, TickPick believes in providing a competitive benefits package for its employees. TickPick offers:

A hybrid in-office approach, enabling remote work a portion of each week
Health Care Plan (Medical, Dental & Vision)
Retirement Plan Contribution (401k, IRA)
Life Insurance (Basic, Voluntary & AD&D)
Paid Time Off (Vacation, Sick & Holidays)
Family Leave (Maternity, Paternity)
Training & Development
$100 Monthly Stipend to Attend Live Events
Employee Outings
Free Lunch & Snacks

Diversity at TickPick
At TickPick, we know that diversity of all types, in an environment that pursues equity and inclusion, strengthens our organization’s culture. When our employees are representative of the communities we serve, with diversity in demographics and a broad set of backgrounds, we provide a superior experience for both our customers and our employees. Fostering an open and supportive environment where our employees are empowered and encouraged to bring their whole selves to the table enables TickPick to thrive. The diverse approaches and collaborative problem solving that result enable us to provide an innovative, nimble, and creative marketplace for our customers and sellers. This belief is central to who we are and what we do, and we are proud of it.
TickPick, LLC is proud to be an equal opportunity employer open to all qualified candidates regardless of race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, marital status, citizenship status, military status, protected veteran status or any other category protected.
Apply for this job","Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos , Python y SQL, Ciencias de la computación, Comunicación, HTML, Lenguajes web y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3968660224/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=3gZj%2BPVowwIxWSQ8wHNoZA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer I,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 días,Estados Unidos,"Acerca del empleo
Overview

Now is the time to join us!

We’re Personify Health. We’re the first and only personalized health platform company to bring health, wellbeing, and navigation solutions together. Helping businesses optimize investments in their members while empowering people to meaningfully engage with their health. At Personify Health, we believe in offering total rewards, flexible opportunities, and a diverse inclusive community, where every voice matters. Together, we’re shaping a healthier, more engaged future.

Responsibilities

 Who are you? 

Data Engineer I perform development activities with the guidance of another member of the data engineering team. You will work closely with account management, ETL, data warehouse, business intelligence, and reporting teams as you develop data pipelines and enhancements and investigate and troubleshoot issues.

 In this role you will wear many hats, but your knowledge will be essential in the following: 

 Extracting, cleansing, and loading data. 
 Building data pipelines using SQL, Kafka, and other technologies. 
 Triage incoming bugs and incidents. 
 Perform technical operation tasks. 
 Investigate and troubleshoot issues with data and data pipelines. 
 Participation in sprint refinement, planning, and kick-off to help estimate stories, raise awareness and additional implementation details. 
 Help monitor areas of the data pipeline and raise awareness to team when issues arise. 
 Performing quality assurance work to verify the accuracy of code and data results 

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Qualifications

What you bring to the Personify Health team:

In Order To Represent The Best Of What We Have To Offer You Come To Us With a Multitude Of Positive Attributes Including

 1 – 2 years or less experience in data engineering 
 SW certification or degree in IT related field 
 Solid grasp of modern relational and non-relational models and differences between them. 
 Proficiency in writing SQL, the use of Excel, and some analytical tools. 
 Understanding of REST API. 
 Understanding of JSON. 
 Detail oriented and able to examine data and code for quality and accuracy. 
 Knowledge of Agile environments, including Scrum and Kanban methodologies 
 Python / R / programming language experience preferred 
 ETL experience preferred 
 AWS Lambda / Console experience preferred 
 Git experience preferred 

No candidate will meet every single desired qualification. If your experience looks a little different from what we’ve identified and you think you can bring value to the role, we’d love to learn more about you!

Personify Health is an equal opportunity organization and is committed to diversity, inclusion, equity, and social justice.

In compliance with all states and cities that require transparency of pay, the base compensation for this position ranges from $65,000 to $76,000. Note that salary may vary based on location, skills, and experience. This position is eligible for 10% target bonus/variable compensation as well as health, dental, vision, mental health and other benefits.

We strive to cultivate a work environment where differences are celebrated, and employees of all backgrounds are empowered to thrive. Personify Health is committed to driving Diversity, Equity, Inclusion and Belonging (DEIB) for all stakeholders: employees (at each organization level), members, clients and the communities in which we operate. Diversity is core to who we are and critical to our work in health and wellbeing.","Canalizaciones de datos, Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , JSON y SQL, Aseguramiento de la calidad, Lenguaje de hojas de estilo LESS y Transferencia de Estado Representacional (REST)",Solicitar
https://www.linkedin.com/jobs/view/3981834994/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=wPRg1Ei%2FK0KJ9IX4sHE7Zg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Location: Remote (For Non-Local) or Hybrid (Local to NYC area)

Position Summary

This high-performing individual will be responsible for collaborating with other team members and vendors to build accurate data management workflows. The ideal candidate has a background in healthcare data management or PBM (Pharmacy Benefit Management) data.

Position Responsibilities

Consulting with operational and analytics teams to understand the company’s data needs.
Designing ETL workflows using ETLWORKS and our in-house ETL tools.
Conducting preliminary testing of workflows built before releasing to production.
Troubleshooting any issues that may arise.
Providing maintenance support.

Required Qualifications

Bachelor’s degree in computer science, information technology, or a related field.
Proficient knowledge of coding languages, including Java, XML, and SQL and Python.
Proficiency in warehousing architecture techniques.
Proven work experience as an ETL developer.
Strong project management skills.
Ability to analyze a company’s big-picture data needs.
Clear communication skills.
Ability to troubleshoot and solve complex technical problems.

Preferred Qualifications

1-3 years ETL Experience
Snowflake Task function proficiency
3+ years of SQL experience
3+ years of Python experience
Healthcare and or PBM data experience

Base Salary: $95,000 - $115,000

Nothing in this position description restricts management’s right to assign or reassign duties and responsibilities to this job at any time.

About Capital Rx

Capital Rx is a full-service pharmacy benefit manager (PBM) and pharmacy benefit administrator (PBA), advancing our nation’s electronic healthcare infrastructure to improve drug price visibility and patient outcomes. As a Certified B Corp™, Capital Rx is executing its mission through the deployment of JUDI®, the company’s cloud-native enterprise health platform, and a Single-Ledger Model™, which increases visibility and reduces variability in drug prices. JUDI connects every aspect of the pharmacy ecosystem in one efficient, scalable platform, servicing millions of members for Medicare, Medicaid, and commercial plans. Together with its clients, Capital Rx is reimagining the administration of pharmacy benefits and rebuilding trust in healthcare.

Capital Rx values a diverse workplace and celebrates the diversity that each employee brings to the table. We are proud to provide equal employment opportunities to all employees and applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, medical condition, genetic information, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, Bases de datos, Ciencias de la computación, Comunicación, Gestión de proyectos y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3935306117/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=freaNP1lzgsNz8oU4Zfp3g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer / Analyst - Remote - Contract,"120 US$K/año - 170 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Nueva York, NY","Acerca del empleo
This position is 100% remote* (This role will primarily operate on East Coast hours)

HappyFunCorp helps businesses develop great products and deliver powerful technical solutions with a blend of deep engineering skills, human-focused design, and a culture that makes working with us enjoyable. We've partnered with startups, small and mid-size businesses, and enterprise clients, including those in the Fortune 500, to innovate and modernize across a variety of verticals. Though we're a distributed team, our roots are in Brooklyn, NY, where HappyFunCorp began.

 Our team of over 60 sharp and talented software engineers, designers, and product architects hail from diverse backgrounds, bringing empathy and business savvy to every digital project we take on. Whether you're looking to take an idea from conception to production, expand an existing web or mobile application, or lead a complicated digital transformation project, we can help.

Responsibilities:

We are looking for an experienced technologist with a blend of Data Engineering and Data Analysis skills who is comfortable working in a large cloud-based data environment
You will need to become familiar with and extract data from a large data lake which will inform the development of new software product features 
You will synthesize data from multiple third party software platforms such as Salesforce Health Cloud and other EHR / EMR systems
It is important to have a product delivery mindset as you will work closely with internal and external Design, Product, and Engineering teams
We are a software product development agency, so prior experience in a startup environment or software agency / consultancy would be preferred


Requirements

Knowledge of technologies including Snowflake, Mulesoft, Salesforce Health Cloud, and at least one modern BI tool (Tableau preferred)
Knowledge of the the pharmaceutical or healthcare industry is highly desirable
Experience working with EHR / EMR systems is preferred
Understanding of HIPAA, data privacy, and healthcare regulations
Experience working in cloud environments is required
Solid understanding of data-access SQL, and API integration and development. 
Proficiency in one or more modern programming languages such as Java or Python


Benefits

Company Values:

Creative Optimism. We are problem-solvers who use a lens of opportunity to make the world a bit better with all that we do
Dignity. Our best work can only be done in an environment in which HFCers and our partners treat each other - and themselves - with dignity
Equity. We strive for diversity across many dimensions and we believe our team is strongest when it is fairest
Entrepreneurial Spirit. Our ""engine"" is the energy that comes with ownership, agency, and responsibility for what we produce
Trust The bedrock of any organization is the growth of trust in our leaders, peers, and partners - the most rewarding work demands it

HFC is a good fit for entrepreneurially-minded doers that learn and adapt quickly and have a passion for what they do. What we look for is someone with experience turning ideas into fully-fledged products. We offer competitive pay, a fully remote company culture, and the opportunity to work on cool projects with great people. If this sounds like you, send us your application!

Salary range: $120k - $170k (Exact compensation may vary based on skills, experience, and location.)","Analítica de datos, Análisis de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Lenguajes de programación, Microsoft Power BI y Python, Java y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3977862271/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=ajw1%2FkRUYg0PIW6qK9Nwrg%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Columbus y alrededores, Ohio","Acerca del empleo
**MUST BE LOCAL TO COLUMBUS, OH (preferred) OR MINNEAPOLIS, MN**

Agility Partners is seeking qualified applicants to fill an open Junior Data Engineer position with one of our Banking clients. In this dynamic role you will be optimizing data pipelines, proficient in Python, SQL and experienced with AWS cloud services including S3, Glue, Athena, and Lambda.
Design, develop, and optimize data processing pipelines and workflows.
Implement and manage data solutions using AWS services (S3, Glue, Athena, Lambda).
Perform data wrangling and exploration to support business needs.
Develop and maintain robust ETL processes.
Collaborate with cross-functional teams to understand data requirements and deliver effective solutions.
Provide production support, identify and automate areas of improvement.

The Ideal Candidate
1-2 years of data engineering experience
The ideal candidate will have a strong background in optimizing data pipelines, proficient in Python, SQL and experienced with AWS cloud services including S3, Glue, Athena, and Lambda.
Skilled in data wrangling, data exploration, and developing robust ETL processes.
Possess strong scripting and automation skills, along with expertise in version control with Git.
We value individuals who proactively embrace emerging technologies to streamline workloads and enhance operational efficiency.
Nice to have: Snowflake, SQL Server, Machine Learning experience

Reasons to Love It
This is a great opportunity to work for a mid-sized financial institution that is striving to be the bank of choice; one that focuses on its customers, not its competition. An organization that provides a dynamic, fulfilling work environment that is productive, collaborative and innovative.
Highly visible team with a regional financial services company where your work matters and your accomplishments are recognized!
Amazing opportunity for growth, healthy work/life balance and a community focused environment
Working for an organization that focuses on company culture, inclusion and diversity
On a team whose Core values that include: Can-Do Attitude, Service at Heart and Forward Thinking
50% medical coverage for you and your entire family
401(k) 
Life Insurance 
Disability coverage","AWS Lambda, Amazon Web Services (AWS), Canalizaciones de datos, Ingeniería de datos y Microsoft SQL Server, AWS Glue, Amazon Athena, Amazon S3 y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983535619/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=kHeh2oZdvcE8nh0NelgkXw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Chicago, IL","Acerca del empleo
Are you interested in building the future of healthcare and transforming the patient experience? Are you hopeful about what data and medical research can do to improve medicine? We're looking for a Data Engineer to ensure PatientIQ remains at the forefront of using data to drive positive healthcare outcomes.

As a core member of the Data Engineering department, you will be in a dynamic environment that works cross-functionally with all other departments, such as Engineering, Product, Customer Success, and Marketing. You will work on a broad array of problems that rely on data to derive insights for our business and customers. We heavily value diligence, curiosity, and initiative, which are key to unlocking the value of PatientIQ's data for our users and decision-making. Your work will be impactful across the entire organization.

Role Responsibilities

Clean and process large, complex datasets using tools such as SQL and Python
Migrate client data into PatientIQ's platform per established service level agreements (SLA)
Develop rigorous data quality assurance checks throughout the ETL process
Design, develop, maintain, and streamline scalable data pipelines to support healthcare data analysis
Work closely with data analysts to understand their data needs and develop solutions to meet those needs
Monitor and optimize the performance of data pipelines and systems
Collaborate with other teams to integrate data from multiple sources


Requirements

Ideal Qualifications

BS/MS in Computer Science, Engineering, Mathematics, or a related field
2+ years of experience as a Data Engineer
Experience designing, building, and maintaining ETL infrastructure in a production setting
Experience working with large, complex datasets
Deep knowledge of SQL and at least one programming language (e.g., Python, Java, Ruby, etc.)
Experience with version control systems (e.g., Git) and writing reusable and extensible code
Highly self-motivated with strong analytical problem-solving skills and attention to detail


Nice to Haves

Experience working in Healthcare, Finance, or another regulated industry
Experience with workflow management systems such as Airflow
Experience in machine learning and/or business intelligence
Experience with cloud technologies such as AWS, Google Cloud Platform, or Azure


Benefits

 Great Benefits - top-notch health, dental and vision insurance. Additional perks available including 401K
 We are Mission Driven - our team is motivated to solve complex problems, drive medicine forward, and ultimately improve patient outcomes
 True Idea Meritocracy - great ideas win out. We encourage all team members to challenge the status quo because our mission demands this
 Flexible Time Off - we trust you to take the time you need when you feel it is appropriate, given your workload and responsibilities. No need to track it or save up
 World-Class Team - we're at the top of our industry because of our employees. They're the best investment we can make, and we never forget that
 Fast Growing - we are building the largest platform for healthcare providers, industry partners, researchers, and others to collaborate on the mission to improve patient outcomes","Airflow, Analítica de datos y Extraer, transformar y cargar (ETL), Aseguramiento de la calidad de los datos, Calidad de datos, Ciencias de la computación, Datasets, Gestión de flujos de trabajo, Resolución de problemas y Sistemas de gestión de flujos de trabajo",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984562067/?eBP=BUDGET_EXHAUSTED_JOB&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=impr2nWX2AnpX749c%2Fhwag%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (GCP),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,Estados Unidos,"Acerca del empleo
🚀 Join Our Data Revolution as a Data Engineer! 🌐✨
Are you a data enthusiast who thrives on turning raw data into actionable insights? 📊✨ We're on the lookout for a passionate Data Engineer to join our dynamic team and play a key role in shaping the future of our data infrastructure! 🚀

Why Us?
🌟 Cutting-edge Tech: Work with the latest technologies, including Google Cloud Platform (BigQuery, Dataflow, Dataproc), and bring your expertise to build scalable and reliable data pipelines.
🌐 Impactful Insights: Develop analytics tools that provide real-time insights into customer acquisition, operational efficiency, and critical business metrics, driving our success!
🌈 Collaborative Culture: Join a collaborative environment where your ideas matter. Work closely with stakeholders – from Executives to Design teams – and make a tangible impact on our data-driven decision-making.

What You'll Do:
💡 Use your SQL expertise to work with relational databases, Big Query, and author queries that make a difference.
🛠️ Design and implement scalable data pipelines on GCP, driving innovation in data engineering.
🌍 Migrate and create data pipelines, transforming our data infrastructure from AWS or Azure to GCP.
🐍 Write efficient Python scripts for data processing and automation, showcasing your coding prowess.
🤝 Collaborate with cross-functional teams, solving data-related challenges and supporting their data infrastructure needs.

What You Bring:
🎓 Bachelor’s Degree or equivalent experience.
💼 4+ years of experience in the software industry.
🚀 Proven ability to turn technical requirements into functional code.
🐍 Proficiency in Python and SQL.
🔗 Experience with Git and specified technologies.
🌐 Strong expertise in data engineering, with a focus on GCP.
🛠️ Experience migrating data pipelines and infrastructure from AWS to GCP.
💡 Solid understanding of data modeling, ETL processes, and data warehousing principles.
🔄 Familiarity with data pipeline orchestration tools like Pub/Sub and Cloud Functions.
💬 Excellent problem-solving and analytical skills.
🤝 Strong communication and teamwork abilities.
Ready to revolutionize data with us? 🚀 Apply now and be part of a journey where your skills make a direct impact! 🔗✨ #DataEngineer #TechInnovation #JoinUs

Are We the Right Fit For You?
The best way to get the scoop on whether Nerdery is the right place for you is to chat with current Nerds. We would be delighted to have a conversation with you and share insight into what it’s really like to work at our organization and if it’s a place where you can thrive. Our interview process will provide you ample opportunity to talk with other team members and assess whether the role is a good fit for your next chapter. Take the first step and apply today – our Talent Advocates will then reach out to you to get the ball rolling!

Must be legally authorized to work within the country of employment without sponsorship for employment visa status.

 Nerdery is an equal opportunity employer and complies with all applicable federal, state and local fair employment practice laws. Nerdery strictly prohibits and does not tolerate discrimination against employees, applicants or any other covered persons because of race, color, religion, creed, national origin or ancestry, ethnicity, sex, sexual orientation, gender (including gender nonconformity and status as a transgender or transsexual individual), pregnancy, marital status, familial status, age, physical or mental disability, citizenship, past, current or prospective service in the uniformed services, genetic information, membership or activity in a local human rights commission, status with regard to public assistance or any other characteristic protected under applicable federal, state or local law. All employees, other workers and representatives of Nerdery are prohibited from engaging in unlawful discrimination. Nerdery will ensure that all employment practices are free of such discrimination. Such employment practices include, but are not limited to: hiring, promotion, demotion, transfer, recruitment or recruitment advertising, selection, layoff, disciplinary action, termination, compensation, benefits, selection for training, including apprenticeship and other terms and conditions of employment. Nerdery will also provide reasonable accommodation to applicants and employees with disabilities pursuant to all applicable laws.","Almacenamiento de datos, Amazon Web Services (AWS), Canalizaciones de datos, Capacidad de análisis, Extraer, transformar y cargar (ETL), Git, Google Cloud , Ingeniería de datos , Python y SQL, Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983292451/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=XjxQ0IYERAdZwjm%2FV9TuPg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"100 US$K/año - 130 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,Estados Unidos,"Acerca del empleo
Create The Circular Economy With Us
At Reebelo.com, we empower customers to buy their favorite tech devices in a more sustainable way. Our mission is to refresh the way we all consume tech, delivered through a platform built on sustainable values and quality-assured devices. We are looking for motivated team members like yourself with an innovative mindset. In 2 years we expanded to 7 countries, secured $50M from top investors, and scaled to 8-digit gross sales. Change the world, and supercharge your career with Reebelo!

Find out more: https://reebelo.com/about-us

Keen to join one of the fastest growing consumer tech companies in the Bay Area while we’re still scaling? 🦄 Want to work for a mission-driven organization that values growth + sustainability like you? ♻️ Passionate about the opportunity to play a vital role in the creation of a fully fledged, scalable and global marketplace where you get to see the impact of your work every day? 🚀

About the Job
As a Data Engineer at Reebelo, you will partner with the BI team to understand what gaps exist in our data warehouse and fill those gaps by leveraging our ETL tooling. Additionally, you will develop data-driven tests to ensure all data sources match up and adhere to sufficient data governance. You will work closely with cross-functional teams to develop robust and scalable solutions for our data infrastructure, leveraging modern data engineering tools and frameworks.

About our Team
Our global team of engineers are hungry, motivated and always ready to challenge the status quo. Our team is working on solving cutting-edge problems with the latest tech stack and we move fast, learn along the way, and iterate. We come from a diverse set of backgrounds and each member brings different skills to the group. You're encouraged to apply even if your experience doesn't precisely match the job description.

Key Responsibilities:
Data Warehousing and ETL: Leverage modern data warehousing and ETL tooling to build and design a robust data warehouse for our BI team to consume.
Data Governance: Build and maintain data governance by automating regular tests and reconciliation between different data sources to ensure our data is 100% accurate.
Collaboration with Product and Engineering: Partner with the product and engineering teams to develop modern recommendation engines powered by AI, ML, and advanced data science techniques.

What we’re looking for:
Experience: 2-3 years of hands-on experience building and managing large data sets and data warehouses.
Technical Skills: Experience with data warehousing and modern ETL ELT tooling. Experience with AI/ML, Redshift and/or DBT is a plus.
Analytical Skills: Strong problem-solving skills with the ability to analyze and interpret complex data sets.
Startup Mindset: You thrive in fast-paced environments, are energized by limited direction, and are fearless in experimenting and learning from failures.

Bonus points:
Hands-on experience with DBT, Redshift, Fivetran and/or Segment.
Experience with ecommerce marketplaces.
Experience with Shopify.

What you can expect from us:
Empowerment: We believe in providing you with the autonomy to make decisions and drive your own projects.
Growth: A culture that values personal and professional development. With us, you’ll grow in leaps and bounds.
Impact: Every role at Reebelo is pivotal. Here, your work will make ripples.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Amazon Redshift, Bases de datos, Resolución de problemas y Shopify",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3980212628/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=fZ992DBCp8fLh3q17xM4hg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"San Francisco, CA","Acerca del empleo
SEARGIN IS HIRING!

As a dynamic multinational tech company operating in 50 countries, we drive innovation and create projects that shape the future and greatly enhance the quality of life. You will find our solutions in the space industry, supporting scientists in the development of cancer drugs, and implementing innovative technological solutions for industrial clients worldwide. These are just some of the areas in which we operate!

Currently, for the new Seargin project, we are looking for a Data Engineer!

Overview:

The Data Engineer in OneEnablement Team is responsible for supporting the DataOne platform users by introducing the platform and DataMesh supporting technologies to them (so called Technical Onboarding) and supporting already onboarded projects by resolving the support requests (L2 support) or answering the ad-hoc technical questions or issues. Sometimes it is required to create the solution proposition or prepare the small PoC.

Key Responsibilities:

Technical Onboarding: Provide the initial Data Product Journey session and tailored service offering. Offer scheduled consultancy sessions regarding infrastructure provisioning and hands-on support.
Problem Resolution: Resolve ServiceNow requests forwarded by OneSupport (L1) team to L2 support (among others OneEnablement team). Resolve ad-hoc issues and answer questions raised by Google chat space, emails or weekly DIGI DataOne Office Hours.
Documentation and Support: Co-maintain WIKI pages with documentation
Collaboration: Work closely with OneSupport (L1) team and other DataOne development teams among others OneSnowflake responsible for Snowflake platform and DataOps responsible for DataOps@Roche tool, DBT, Orchestration tools.


Required Skills:

Good knowledge of Snowflake. 
Nice to know data concepts: data warehouse, data mesh & data vault as well as other technologies like: DBT, GitLab, MonteCarlo, Immuta, Collibra. Familiarity with ServiceNow, Google Workspace, and other relevant IT tools is highly preferred.
Problem-Solving: Ability to identify issues, analyze problems, and devise efficient solutions
Collaboration and Teamwork: Experience working with cross-functional teams and the ability to facilitate collaborative efforts
Strong Communication: ability to run onboarding & consultancy sessions with customers as well as explaining technology aspects in accessible language



Discover the Power of IT Excellence. Apply!
To learn more about Seargin, please visit our web page: www.seargin.com","Extraer, transformar y cargar (ETL), Ingeniería de datos y PL/SQL, Modelado de datos, Razonamiento, Snowflake y Transformación de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3983584628/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=UACBHhh0kRUprEfcQpE7yQ%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"81,9 US$K/año - 106,5 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 4 días,"Tampa, FL","Acerca del empleo
HOW MIGHT YOU DEFY IMAGINATION?

You’ve worked hard to become the professional you are today and are now ready to take the next step in your career. How will you put your skills, experience and passion to work toward your goals? At Amgen, our shared mission—to serve patients—drives all that we do. It is key to our becoming one of the world’s leading biotechnology companies, reaching over 10 million patients worldwide. Come do your best work alongside other innovative, driven professionals in this meaningful role.

Junior Data Engineer

Live

What You Will Do

Let’s do this. Let’s change the world. In this vital role you will be part of the newly established technical/engineering team, develop data flow pipelines to extract, transform, and load data from various data sources in various data format to enterprise data lake and data warehouse system in three regions in AWS. Provide data analytics and predictive analysis to business users.

Be a key team member assisting in development of the data pipeline for Global Data and Analytics team
Collaborate with Data Architects, Business SME’s, and Data Scientists to design and develop end-to-end data pipeline to meet fast paced business need across geographic regions
Build data products and service processes which perform data transformation, metadata extraction, workload management and error processing management.
Implement standardized, automated operational and quality control processes to deliver accurate and timely data and reporting to meet or exceed SLAs.
Adhere to best practices for coding, testing and designing reusable code/component
Able to learn quickly, and explore new tools, technologies that will help to improve platform performance
Participate in sprint planning meetings and provide estimations on technical implementation; Collaborate and communicate effectively with the product teams

Win

What We Expect Of You

We are all different, yet we all use our unique contributions to serve patients. The engineering professional we seek will have these qualifications.

Basic Qualifications:

Bachelor’s degree

OR

Associate’s degree and 4 years of software engineering experience

Or

High school diploma / GED and 6 years of software engineering experience

Preferred Qualifications:

BS or MS in Computer Science or Engineering related fields
Experience with software development (Java, Python preferred ), end-to-end system design
Familiar with Python libraries related to web/API development and machine learning
Experience with data modeling for both OLAP and OLTP databases, Hands-on experience with SQL, for example Oracle, PostgreSQL, Hive SQL; SQL performance tuning
Experience with software DevOps CI/CD tools, such Git, Jenkins
Experience on AWS, familiar with EC2, S3, Redshift/Spectrum, Unit Catalog, Athena, RDS, Lambda, DynamoDB and API gateway
Experience with Apache Airflow and Apache Spark, Spark performance turning
Experience with Pharmaceutical industry, commercial operations
Ability to learn quickly, be organized and detail oriented

Thrive

What You Can Expect Of Us

As we work to develop treatments that take care of others, we also work to care for our teammates’ professional and personal growth and well-being.

The expected annual salary range for this role in the U.S. (excluding Puerto Rico) is posted. Actual salary will vary based on several factors including but not limited to, relevant skills, experience, and qualifications.

Amgen offers a Total Rewards Plan comprising health and welfare plans for staff and eligible dependents, financial plans with opportunities to save towards retirement or other goals, work/life balance, and career development opportunities including:

Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts.
A discretionary annual bonus program, or for field sales representatives, a sales-based incentive plan
Stock-based long-term incentives
Award-winning time-off plans and bi-annual company-wide shutdowns
Flexible work models, including remote work arrangements, where possible

Apply now

for a career that defies imagination

Objects in your future are closer than they appear. Join us.

careers.amgen.com

Application deadline

Amgen does not have an application deadline for this position; we will continue accepting applications until we receive a sufficient number or select a candidate for the position.

Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.","Airflow, Apache Spark y Hive, Amazon Redshift, Bases de datos, Ciencias de la computación, Modelado de datos, PostgreSQL, Procesamiento analítico en línea (OLAP) y Procesamiento de transacciones online",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3983823131/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=kGXGrqgf3Tb4fVHU5mI2kA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 4 días,"Beverly Hills, CA","Acerca del empleo
 


Data Engineers 

Beverly Hills, CA

Contract

work is in Beverly Hills and is required onsite work from day work




 Job Description:


Top 3: SQL, Tableau, AWS RedShift - the tech landscape


Functional - analytics engineering, has been pretty small since the beginning,


Been trying to bring data in from different places - not enough was good was coming in, things are built on the data warehouse - data modeling work, changing the structure of data


Need people with 7+ years of experience, more experienced than that.. Not a lot planning time. Did end up hiring in the past, hiring for personality v. skilled didn’t work in the past..

Aptitudes y experiencia deseables
SQL , TABLEAU","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Bases de datos y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3983818476/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=wRWjkumDFE4%2FwLP%2Fjh6NMg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,Estados Unidos,"Acerca del empleo
Responsibilities:

Extract data from multiple applications via the API and connect to DOMO
Work with large amounts of data to draw meaningful conclusions
Develop and implement data collection systems and other strategies that optimize statistical efficiency and data quality
Provide regular reporting and analysis to team
Develop and maintain dashboards and reports to track key metrics and performance indicators
Ability to communicate with customers to analyze historical data and identify KPIs
Ability to design report wireframes, data models, create technical designs based on. functional specifications and business requirements
Leverage advanced DOMO functionality (parameters, actions, tooltip modifications, API, etc.) to create analytical dashboards
Ability to communicate with customers to analyze historical data and identify KPIs
Analyze data to identify trends and share insights
Implement Domo dashboards

Qualifications: 

Experience in Java, Python, R or Scala required
Minimum two years of experience with DOMO 
3+ years experience in data analysis, reporting, business intelligence or financial analysis
2 years experience in providing people analytics reporting to organizations
Demonstrated experience using SQL, and DOMO
Deep understanding in relational and non-relational databases and data warehouse
Analyze, evaluate, improve, and document processes and workflows. Identify and take advantage of opportunities for process automation and simplification. 
Proven ability to design and implement new processes and facilitate user adoption
Strong understanding of Salesforce.com best practices and functionality
Strong data management abilities
A demonstrated ability to understand and articulate complex requirements
Excellent project management skills and a positive attitude
Must demonstrate exceptional verbal and written communication skills
Must demonstrate ability to communicate effectively at all levels of the organization
Must send resume in English

Powered by JazzHR

vr78yqNuby","Almacenamiento de datos, Analítica de datos, Análisis de datos, SQL y Scala, Bases de datos, Comunicación, Comunicación escrita, Java y Modelo de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3959958069/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=hklIdIQaQzFfauj4Ehl3cQ%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 mes,"Huntsville, AL","Acerca del empleo
Who We’re Looking For (Position Overview):

Spry Methods is on the search for a Junior Data Engineer to join our team in Huntsville, AL.

What Your Day-To-Day Looks Like (Position Responsibilities):

Assist in the development and maintenance of data pipelines and ETL processes
Support data integration and data warehousing activities
Collaborate with data scientists and analysts to ensure data accessibility
Monitor data systems for performance and reliability


What You Need to Succeed (Minimum Requirements):

Top Secret Clearance is Required
Bachelor’s degree in Computer Science, Data Science, or related field
1-3 years of experience in data engineering
Proficiency with SQL and data processing tools
Strong problem-solving and analytical skills
AWS Associate level certification highly desired","Almacenamiento de datos, Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Ciencias de la computación, Resolución de problemas y SC Clearance",Solicitar
https://www.linkedin.com/jobs/view/3984382071/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=d7z%2Fws53SW4dIbCSHwp9Vw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 3 días,Estados Unidos,"Acerca del empleo
Role: Data Engineer
Location: Remote
Mode: Full-time

The key responsibilities for this role are:
Lead on the work of data analysis to ensure business needs are translated into solutions against identified timelines, and work with technical leads to ensure business needs are translated into the technical solutions. Understand how to apply basic techniques for the analysis of data and synthesis of findings. Effectively involve your team in analysis and synthesis, and present clear findings that colleagues can understand and use. 
Manipulate and link different data sets. Identify, collect, and migrate data to and from a range of systems. Manage, clean, abstract, and aggregate data alongside a range of analytical studies on that data. 
Use Big Query data warehouse to acquire data with SQL transformation to calculate or join data sets if required 
Programming BI-Tools (VBA, SQL, Excel, Access) 
Drive projects with the IT-Finance

The experience/skills required for this role are:
A Level (or equivalent) statistic, computer science, mathematics, economics
Knowledge of BI-Tools, SQL-Development and QV-reporting 
Extensive experience in a Data Analyst role within a commercial organization 
Advanced level MS-Office (Excel und Access) 

Note: Applicants for employment in the U.S. must possess work authorization which does not require sponsorship by the employer for a visa (H1B or otherwise).

The job entails sitting as well as working at a computer for extended periods of time. Should be able to communicate by telephone, email or face to face.

About Us
 Infosys BPM, the business process management subsidiary of Infosys (NYSE: INFY), provides end-to-end transformative services for its clients across the globe. The company’s integrated IT and BPM solutions approach enables it to unlock business value across industries and service lines, and address business challenges for its clients. Utilizing innovative business excellence frameworks, ongoing productivity improvements, process reengineering, automation, and cutting-edge technology platforms, Infosys BPM enables its clients to achieve their cost reduction objectives, improve process efficiencies, enhance effectiveness, and deliver superior customer experience.
  Infosys BPM has 32 delivery centers in 16 countries spread across 6 continents, with more than 38000 employees from over 80 nationalities, as of Nov 2019.
  EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity/National Origin EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity","Microsoft Excel, SQL y Visual Basic for Applications (VBA)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3880670902/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=nDn8qN05mc2iGO1%2FpyqAiA%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 meses,"Fort Worth, TX","Acerca del empleo
About the role:

The reason for hiring is they have expanded and restructured- taken the old BI team and moved into Data Engineering as well as brought on a director of engineering.
This candidate will report to the Director of Data Engineering and will be working on a team of 2 that has been approved to grow into a team of 6.

Job Responsibilities:

Design and implement data collection components to ingest different data sources into big data systems.
Design and implement ETL pipelines.
Identify ways to improve data reliability, efficiency and quality.
Collaborate effectively in cross-functional teams.
Develop, integrate, deploy and test any Big Data tools and frameworks required by the customer projects.
Contribute to team effort by accomplishing related results as needed.
Close interaction with stakeholders in order to identify project scope, functional requirements, and timelines.

Required experience:

Bachelor’s degree in Electrical Engineering, Computer Science, Applied Mathematics or any other technology related field.
1-2 years of experience with SQL, python
Understanding ETL and data movement
Databricks, azure, AWS, cloud
Open to fresh graduates from college- SQL and python experience, Degrees, STEM preferred- software engineering, computer science, MIS, business analytics, etc
Understanding of cloud data warehouses and etl
Understanding of data modelling, algorithms, and data transformation techniques are the basics to work with data platforms.
Hands-on experience with ETL tools.
financial services sector experience, FinTech is a plus
start up experience is a plus

Required skills:

SQL and Python.
Cloud technologies/warehouses such as AWS and
Snowflake.
Experience with git.
Looker experience a plus.

Compensation:

The pay rate range above is the base hourly pay range that Aditi Consulting reasonably expects to pay someone for this position (compensation may vary outside of this range depending on a number of factors, including but not limited to, a candidate’s qualifications, skills, competencies, experience, location and end client requirements).

Benefits and Ancillaries:

Medical, dental, vision, PTO benefits and ancillaries may be available for eligible Aditi Consulting employees and vary based on the plan options selected by the employee.","Almacenamiento de datos, Big data, Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos , Python y SQL, Ciencias de la computación, Modelado de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3970605177/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=jTkliXGRJc356l3GdJTK3Q%3D%3D&trk=flagship3_search_srp_jobs,BI Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
Who is Credible?

We are a marketplace where users can compare personalized, prequalified rates and quotes from multiple lenders and carriers, for student loans, mortgages, personal loans, and insurance.

We’re challenging the status quo by giving power to the consumer. We believe in a world where ‘ethical’, ‘lending’, and ‘insurance’ can coexist, so we set out to build innovative platforms that actually work for customers. Our mission is to help people find the best loan or insurance policy possible.

We believe researching and buying loans or insurance shouldn’t be confusing or complex, so we’ve focused on simplicity. We’ve created the only unbiased loan and insurance buying process out there, which makes finding options straightforward and clear.

About the Role:

Our Business Intelligence team is looking for a BI Data Engineer who is passionate about data, analytics, and business strategy. You will help the team learn more about our business, teach others in the company about analytics, and improve the use of our data. You’ll be an integral part of providing data-driven insights that inform significant company decisions.

Responsibilities:


Build data pipelines and python-based ETL tools for getting, processing, and delivering data.
Develop and optimize data models/schemas in our data warehouse that enable performant, intuitive analysis.
Generate end to end reporting solutions (from data ingestion to dashboards).
Work with business leaders to define key metrics and reporting requirements.
Conduct in-depth data analyses that improve infrastructure and product performance.
Become an expert on all aspects of Credible’s data and analytics infrastructure.
Be the driving force behind the adoption and effective use of our BI tool within every team at Credible.


Education and Experience:


BA/BS in a quantitative field.
3-5+ years of work experience as a data engineer, or in a highly analytical role.
Experience writing SQL queries and using a BI tool.
Experience with programmatic scripting using Python to develop ETL pipelines and tooling.
Experience using the command line and git.
Strong grasp of statistics and experience conducting rigorous data analyses.
Experience developing models and visualizations in Looker a plus.
Experience at an e-commerce or fintech company is a plus.


Nice to Have:


The capacity to juggle multiple priorities effectively within a fast-paced environment is critical.
You’re a highly motivated self-starter with the ability to work efficiently with minimal supervision.
Anticipate business needs and think with a business owner mindset – think critically about analyses, don’t just complete them.
Passion for spreading the value of data throughout the company and communicating insights to a broad audience with varying levels of technical expertise.


Credible is open to hiring candidates in the following locations: California, Colorado, Connecticut, Florida, Georgia, Illinois, Indiana, Kentucky, Massachusetts, Michigan, Missouri, Nevada, New Jersey, New Mexico, New York, North Carolina, South Carolina, Texas, Tennessee, Utah, Virginia, Washington

Pursuant to state and local pay disclosure requirements, the pay ranges for this role, with final offer amount dependent on education, skills, experience, and location, are listed below. This role is also eligible for an annual discretionary bonus, various benefits, including medical/dental/vision, insurance, a 401(k) plan, paid time off, and other benefits in accordance with applicable plan documents.

View More Details About Credible Benefits

For high cost of labor markets such as but not limited to New York City and San Francisco:

$100,800—$133,200 USD

For all other US locations:

$84,000—$111,000 USD

Why work at Credible?

We combine the intelligence, expertise, and confidence of a financial advisor with the approachability and honesty of a friend. In other words, we’re the friend you always wish you had in finance.

We are optimistic, challengers, trustworthy, clever, and smart. We are open and transparent. We strive to act as advisors by being friendly, objective, and open in our communication. We use language that is intelligent yet approachable. When appropriate, we’ll drop in a bit of wit to position ourselves as a fresh, reliable voice in the financial world.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories consistent with applicable law.","Analítica, Capacidad de análisis, Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos y Pensamiento crítico, Indicadores clave, Modelado de datos, Panel de control y Requisitos de información",Solicitar
https://www.linkedin.com/jobs/view/3983714825/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=Of%2FwjRJG9ydhMVYePBAG%2FA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer 1,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Waller, TX","Acerca del empleo
The Data Engineer is responsible to put in place the framework for a Modern, Simple, Accurate and Secure Data Environment that connects data across the company and sets data up as an asset to the company. Data Engineers will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Data engineers implement methods to improve data reliability and quality.

Position Responsibilities May Include

Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. 

Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies 

Work with stakeholders including the Executive, Manufacturing, Sales and Marketing teams to assist with data-related technical issues and support their data infrastructure needs 

Work with data and analytics experts to strive for greater functionality in our data systems 

Develop ways to improve data quality, reliability, and efficiency 

Perform additional projects/duties to support ongoing business needs.

Nature & Scope

Possesses a broad theoretical job knowledge typically obtained through advanced education

Has no discretion to deviate from established procedures by performing structured work assignments

Work is closely supervised

Problems faced are not typically difficult nor complex

Explains facts, policies and practices related to job area

Knowledge & Skills::

Knowledge of programming languages and applications & database apps and tools 

Demonstrated analytical, quantitative & creative problem solving skills 

Effective written & verbal communication skills 

Effective organizational & time management skills including prioritization 

Solid collaboration abilities; professional & diplomatic team builder 

Ability to work independently on multiple tasks and projects, with various teams including Engineering, Sales, IT, Finance, Marketing, Manufacturing, Logistics, etc. 

Ability to apply good judgment, strong work ethic, and integrity on the job.

Competencies

Experience:

Entry level

Education/Certification

Bachelor’s degree in Engineering, Data Science, Computer Science or may consider equivalent & relevant work experience with formal training and certifications 

People Management: No

Physical Requirements / Work Environment

Must be able to perform essential responsibilities with or without reasonable accommodations

Reports To

Director, Data Management & Strategy

The Company provides equal employment opportunity to all employees and applicants regardless of a person’s race, color, religion (including religious dress or grooming practices), creed, national origin (including language use restrictions), citizenship, uniform service member or veteran status, ancestry, disability, physical or mental disability (including HIV/AIDS), medical condition (including cancer and genetic characteristics), genetic information, request for protected leave, marital status, sex, pregnancy, age (over 40), sexual orientation, gender, gender identity or expression, political affiliation, or any other characteristic protected by law. The Company will comply with all federal and state regulations and statutes about individuals with disabilities.","Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Calidad de datos, Ciencias de la computación, Comunicación, Comunicación oral, Resolución creativa de problemas y Resolución de problemas",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3969275713/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=poOK5d8WhMACEE1XRSKUwQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Contract),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,"Austin, TX","Acerca del empleo
About RevOpsforce:

RevOpsforce is a revenue operations workforce management firm supporting clients to unlock revenue potential through solutions that better align your people, processes, data, and technology.

We empower organizations with cutting-edge revenue operations management systems, seamlessly aligning sales, marketing, and customer service teams to unlock increased revenue and elevate overall company value. Our team is powered by the RevOpsforce Expert Network, composed of the highest skilled and certified professionals in revenue operations. We leverage this network to solve our clients' most complex operational challenges.

Type: Contract

Job Description:

We are seeking a talented and experienced Data Engineer to join our growing team. As a Data Engineer at our company, you will be responsible for designing, building, and maintaining the data pipelines that power our data-driven applications. In this role, you will work closely with our data science and engineering teams to develop high-quality, efficient data pipelines that enable us to derive insights from vast amounts of data. You should be comfortable working with large volumes of structured and unstructured data, and have a strong understanding of SQL, Python, and other programming languages.

Responsibilities:

Design, build, and maintain data pipelines to support data-driven applications and analytics
Analyze data to identify trends and patterns
Collaborate with data scientists and engineers to develop data-driven solutions
Write and maintain documentation for data pipelines
Monitor and optimize data pipelines for performance and efficiency

Qualifications:

Bachelor's or Master's degree in a related field (e.g. Computer Science, Data Science, Engineering)
3+ years of experience in a data engineering role
Strong skills in SQL and at least one programming language (e.g. Python, Java, C++, etc.)
Experience with big data technologies (e.g. Hadoop, Spark, etc.) and cloud platforms (e.g. AWS, Azure, GCP)
Strong problem-solving and analytical skills
Excellent communication skills and ability to work in a team environment

Join our dynamic and forward-thinking team to make a significant impact on our clients' revenue growth journey!

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

www.revopsforce.com","Análisis de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Ciencias de la computación, Comunicación, Datos no estructurados y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3984654873/?eBP=BUDGET_EXHAUSTED_JOB&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=j2AGpwVd97l%2Fsbmx9QLXnA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"San Francisco, CA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Interview : F2F interview

Visa : USC, GC, GC EAD, H4, L2

This is hybrid from day -1. Must be local.

Description

It is with Client Loyalty. They are building out a scoring engine for their merchants. Very similar to Rewards for issuers.

Data Engineer (2)

Downtown San Francisco - Three days in office. T/Th mandatory.

12 Month Contract

Experience dealing with large volumes of data, from various sources, both structured and unstructured.

Ability to triage and talk through performance / scaling issues of dealing with data at scale.

Good understanding of how data will be read (file formats, partitioning, bucketing).

Extensive experience writing testable jobs using Spark (or equivalent) framework.

Programming & Scripting Languages: Java EE, Scala, Spark, SQL, Bash.

Software Architectures (micro-services, event driven, peer-to-peer).

Application Security.

Asynchronous Pub-Sub and Point to Point Messaging Systems.

Streaming within the Hadoop ecosystem is a plus.","Almacenamiento de datos, Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3984181609/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=L0bdovoW4ry5ziPuz22Cfw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Arkansas, Estados Unidos","Acerca del empleo
Company Overview

Voted #1 EHR by PC Mag, WRS Health delivers a fully integrated cloud based EMR and practice management solutions to its clients. We bring solutions to physicians by providing constant enhancement of our products and services including EHR, practice management, marketing, patient coordination and billing.

Job Purpose and Role:

We are seeking a highly skilled Data Engineer to join our team and play a pivotal role in building robust data infrastructure to support reporting across multiple departments. The ideal candidate will have a strong background in data engineering, data analytics, and data science, with the ability to transform data into actionable insights.

Design, develop, and implement data architectures and pipelines to efficiently collect and process data
Create complex data models and structures to accommodate diverse data sources and support advanced analytics
Develop and maintain comprehensive reports, analysis and dashboards to provide actionable insights for various departments, including but not limited to finance, operations, clinical, and marketing
Leverage AWS services (e.g., Redshift, S3, EMR, Lambda) to build scalable and cost-effective data solutions
Establish policies and standards for data management
Ensure seamless integration of data across systems
Effectively handle and process JSON data for various reporting and analytical needs
Work closely with stakeholders to understand their reporting requirements, translate them into technical specifications, and deliver solutions that meet their needs
Ensure data integrity, accuracy, and consistency through rigorous quality control measures
Continuously explore new data technologies and methodologies to enhance reporting capabilities and drive business value

Qualifications

Master's degree in Computer Science, Statistics, Mathematics, or a related field
Proven experience as a Data Engineer with a strong focus on data architecture
Expert-level proficiency in data structures, algorithms, and database design
In-depth knowledge of AWScloud platform and its data-related services
Strong proficiency in Python or similar programming languages; andSQL /NoSQL databases
Experience with data visualization tools (e.g., Tableau, Power BI)
Excellent analytical and problem-solving skills
Strong communication and interpersonal skills to effectively collaborate with stakeholders
Experience in the Healthcare industry and Hubspot is a plus

If you are a passionate Data Engineer with a strong foundation in data structures and AWS, and you are eager to contribute to improving healthcare outcomes, we encourage you to apply.

Hours: Available during standard US business hours (9am-5pm EST or 8:30am-4:30pm EST)

Location: Full time - Remote

This job description is intended to describe the general requirements for the position. It is not a complete statement of duties, responsibilities or requirements. Other duties not listed here may be assigned as necessary to ensure proper operations of the department.

WRS Health is an equal opportunity employer.

Powered by JazzHR

OwGSBcDP68","Analítica de datos, Arquitectura de datos y Ciencia de datos, Bases de datos, Ciencias de la computación, Comunicación, Diseño de bases de datos, Modelado de datos, Requisitos de información y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979200043/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=U6ZlrvhIk9wnqqFlayufDw%3D%3D&trk=flagship3_search_srp_jobs,,,,,,,
https://www.linkedin.com/jobs/view/3984605898/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=uLvOYHrfBTWR%2BhYAn6MBzg%3D%3D&trk=flagship3_search_srp_jobs,Junior BI Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, TalTeam, is seeking the following. Apply via Dice today!

BE/B.Tech in Computer Science or related field

5+ years of enterprise software design and development experience

Proficiency in SQL, with experience in writing and optimizing complex queries

Professional, practical programming using Spark, SQL, Scala/Python

Strong technical expertise in Azure, Big Data, Apache Nifi, Hadoop, HDInsights, ADF, ADW etc.

Experience working with reporting and visualization tools like Power BI, Tableau, or similar.

Hands-on experience with scripting languages like Powershell/Bash for automation and data manipulation tasks.

Extensive knowledge and experience in data warehousing, Streaming data processing (ETL), e-metrics/measurement, business intelligence, information retrieval, parallel and distributed computation

Experience in implementation of Cloud Computing concepts and platforms

Experience in analyzing very large real world datasets and hands-on approach in data analytics

Experience with Test Driven Development, Continuous Integration, Continuous Deployment, Telemetry etc.

Great design and problem-solving skills, with a strong bias for quality and engineering excellence

Excellent verbal and written communications skills

Excellent problem-solving and debugging skills with a solid understanding of testing practices

Proven sense of high accountability and self-drive to take on and see through big challenges

Experience working in a global delivery model

Experience with SCRUM, Devops or similar Agile development/implementation methodologies

Familiarity with automation tools Visual Studio etc

Key skills required are Power BI or Tableau or similar visualization tools + Spark, SQL, Scala (or) Python + Apache & Hadoop.

Junior BI Data Engineer","Analítica de datos y SQL, Buenas prácticas de pruebas, Comunicación, Comunicación escrita, Depuración de programas, Desarrollo de software, Diseño de software, Manipulación de datos y Resolución de problemas",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982249950/?eBP=BUDGET_EXHAUSTED_JOB&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=lYVcrDfKRygcwASI0NxjRw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 días,"Austin, TX","Acerca del empleo
Location: Remote
Duration: 1- year contract with long term extensions

Data Engineer

ABOUT THIS FEATURED OPPORTUNITY

The Data Engineer will contribute to building a new reporting system, including an API layer and a robust data store for their Vision Management Portal. This role will focus heavily on developing and optimizing the data store to support our analytical reporting needs.

THE OPPORTUNITY FOR YOU 

You will design, implement, and maintain a new reporting system with a strong emphasis on the data store component. You will develop and enhance features in the API using Java. You will work extensively on distributed data systems to ensure efficient data storage, retrieval, and processing. You will utilize analytical databases for data modeling, performance optimization, and scaling. You will partition data to support more performant queries, aggregate data, and build roll-ups. You will ensure data consistency under high load scenarios. You will build and maintain analytical reports with capabilities to aggregate data effectively.

KEY SUCCESS FACTORS
5+ years of Data Engineering experience
Proven experience in data modeling, performance optimization, and scaling data stores to support large volumes of data
In-depth knowledge of data partitioning strategies to enhance query performance
Experience in data aggregation and building roll-ups for analytical reporting
Ability to ensure data consistency under high load conditions
Experience with Analytical Databases i.e. Snowflake, AWS Redshift, Google BigQuery, CockroachDB, Pinot, etc. 
Java development experience 
NICE TO HAVES
Experience with Druid

Our benefits package includes: (EXCLUDE on perm placements)
Comprehensive medical benefits
Competitive pay
401(k) retirement plan
…and much more!
About INSPYR Solutions

Technology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients' business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com.

INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.
 
Aptitudes y experiencia deseables
DATA ENGINEER","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ajuste de rendimiento, Amazon Redshift, Bases de datos, Lenguaje de consulta (query), Modelado de datos y Optimización",Solicitud sencilla
https://www.linkedin.com/jobs/view/3976339754/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=kcTCjd9zdfU0aUMgxvQ7Dw%3D%3D&trackingId=Kjb60DRhW%2Bai%2FtTSvarX2w%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"25 US$/h - 35 US$/h Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",hace 1 semana,"North Palm Beach, FL","Acerca del empleo
Responsibilities

Kforce has a client that is seeking a Junior Data Engineer in Juno Beach, FL. Duties Include:

 Junior Data Engineer will engage in the creation and maintenance of data pipelines, ETL workflows, and visualization suites
 Leverage Python for data extraction, transformation, and loading tasks
 Utilize Salesforce for CRM-related data needs and operations
 As a Junior Data Engineer, you will ensure the accuracy and integrity of data across various systems
 Collaborate cross-functionally with data analysts, data scientists, and business units to fulfill data needs as well as understanding the business impacts
 Participate in data modeling and architecture design
 Graph, chart, and present statistical findings for stakeholders

Requirements

 Bachelor's degree in Computer Science, Engineering, Statistics, or a related field
 1-2 years of relevant professional experience
 Proficiency in Python, SQL, and other programming or scripting languages
 Knowledge of Salesforce CRM, including customizing objects, profiles, roles, permissions, and workflows
 Familiarity with data visualization tools (Tableau, Power BI/etc.) is a plus
 Excellent problem-solving skills and attention to detail
 Strong written and verbal communication skills in English

The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.

We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.

Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.

This job is not eligible for bonuses, incentives or commissions.

Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.","Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, Atención al detalle, Ciencias de la computación, Comunicación, Comunicación oral, Modelado de datos y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984220789/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=VbgkeUwmuNzGdIPzyG7y4w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"120 US$K/año - 150 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,Estados Unidos,"Acerca del empleo
Enjoy being part of a team of motivated engineers focused on helping the world’s largest organizations innovate and execute in the cloud.

Fully paid continuing education and cloud certifications
Collaboration and mentorship with leading engineers
Work-life balance and flexibility
Upward mobility and opportunities for personal growth
Competitive salary + annual bonus opportunity


The Role:
Reporting to the Team Lead, you will be responsible for working on a team of four to five Engineers to tackle the platform needs of large organizations. Every week, you'll have the opportunity to expand your technical skills and will be presented with opportunities for continuous learning. Working in multiple environments and continually switching context from client to client, we are looking for people who thrive in an innovative and fast-paced environment. This is a standout opportunity to collaborate with an elite group of engineers, build up your skillset and, in turn, become a distinguished expert in the industry.

Responsibilities
Design and implement continuous integration and continuous deployment frameworks from code to deploy
Manage and optimize data pipelines for performance, scalability, and reliability
Develop, implement, and maintain scalable data pipelines and processes
Create and manage automated provisioning and configuration systems for data infrastructure using infrastructure-as-code principles
Design, implement, configure and manage system monitoring solutions that alert teams to problems before customers are impacted
Support developers in code deployment and troubleshooting
Work closely with customers and other team members to understand complex requirements and translate them into automated solutions
Provide support to ensure mission critical applications and components are being monitored and meet security, reporting and retention requirements as well as disaster recovery requirements of clients
Support team members

Required Qualifications:
3+ years of data infrastructure experience
3+ years of AWS experience
3+ years of Kubernetes experience
2+ years of caching and data experience
Terraform experience
Cloud certifications
Excellent communication skills
Strong multi-tasker
Self starter
Team player
Preferred Qualifications:
Azure, AWS and GCP Professional level certifications
Kubernetes certifications (CKA, CKAD, CKS)


Benefits + Perks
Benefits of full-time remote work but with a close team environment. BSC was built from the beginning to be remote. That means we work extra hard to support a culture of engagement with regular opportunities for in-person interaction.
Flexible work schedule and time off
Home office support
401k with up to 4% company match
Medical, Dental, Vision and Life Insurance


About Us:
Blue Sentry Cloud works with clients whose cloud initiatives simply cannot fail. We have performed over 300 such cloud initiatives and never had one fail. Our densely-certified team specializes in cloud and nothing else. From planning and preparation to cloud native deployments and nextgen managed services, we provide objective advice and strategic guidance to help you navigate your cloud journey.


Salary: $120,000 to $150,000 + bonus potential","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación y Resolución de incidencias",Solicitar
https://www.linkedin.com/jobs/view/3982753822/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=dWwcBpTCtk%2FYIaI9zGiKdQ%3D%3D&trk=flagship3_search_srp_jobs,Data Wrangler / Data Engineer,"80 US$K/año - 120 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 días,"Washington, DC","Acerca del empleo
Overview

Black Canyon Consulting (BCC) is searching for a Data Wrangler to support National Center for Biotechnology Information (NCBI). This opportunity is full time at the NIH-NCBI in Bethesda, MD and/or remote work.

The National Center for Biotechnology Information (NCBI) is part of the National Library of Medicine (NLM) at the National Institutes of Health (NIH). NCBI is the world’s premier biomedical center hosting over six million daily users that seek research, clinical, genetic, and other information that directly impacts biomedical research and public health.

Job Description

The candidate should have extensive Python experience, including scripting and data processing design. The individual will collaborate with the lead curator to support the Human Variation team in assessing external variation resources for acceptability for submission to dbSNP and dbVar, as well as developing appropriate data import methodologies and pipelines. This work aims to improve the breadth and accuracy of variation data while also minimizing the amount of manual curation required in the process. Other bioinformatic tasks and analyses are performed to guarantee that the data is well-prepared, consistent, and suitable for downstream applications, resulting in relevant biological insights from genetic variation data and subsequent analysis.

The overall goal of the Data Wrangler position is to enhance the efficiency, accuracy, and reliability of genetic variation data within the Human Variation team's databases, specifically dbSNP and dbVar. The candidate is expected to leverage their extensive Python experience to perform a range of bioinformatic tasks and analyses.

Required Skills

Data collection, integration and cleaning
Data transformation, normalization, and preprocessing, 
Scripting automation, Scripting in Bash, Python, or other shell scripting languages 
Implement custom analysis
SQL queries for data extraction, transformation, and loading (ETL) 
Experience running operations in a large and complex environment, preferably in data operations
Ability to troubleshoot an operational pipeline to identify highest priority problems and identify solutions
Team collaboration, onboarding, and documentation

Other Desired Experience/Expertise

Knowledge of existing workflow languages and frameworks
Work experience with production-level bioinformatics databases and pipelines
Familiarity with technical environments, complex databases, and process flows
Experience with XML schemas
Familiarity with Jira and Confluence
Experience with Agile processes, especially scrum

Educational Requirements

B.S. in a STEM field (Engineering, Computer Science, Mathematics, Physics)
Alternatively, equivalent industry experience in bioinformatics or a related field

Bonus Skills

Strong presentation skills","Python y XML, Aptitudes para hacer presentaciones, Bases de datos, Bioinformática, Ciencias de la computación, Confluence, Fiabilidad, Guiones shell y Schemas",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3891910769/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=KlueW2RSHYPZKx24WGZPiw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer(Remote)-W2,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,Estados Unidos,"Acerca del empleo
Title- Data Engineer

Location: Remote but occasional onsite meeting in

Boston/MA(MA local highly preferred) will accept EST/CST candidate as well

Visa: Only USC

We are seeking a talented Data Engineer with extensive expertise in SQL, Python, and GCP (Google Cloud Platform) to join our team. As a Data Engineer specializing in insurance, you will play a crucial role in designing, implementing, and maintaining scalable data pipelines and analytical solutions. Your focus will be on leveraging data to drive insights, optimize processes, and enhance decision-making within the insurance domain.

Responsibilities

Design and develop robust, scalable data pipelines for collecting, ingesting, and processing insurance-related data using SQL and Python.
Collaborate with cross-functional teams to understand data requirements, identify opportunities for data-driven solutions, and implement analytics solutions that support business objectives.
Optimize and tune SQL queries for performance and efficiency to handle large datasets effectively.
Implement data quality checks and validation processes to ensure data accuracy, completeness, and consistency.
Build and maintain data models, data schemas, and database architectures that align with insurance industry standards and best practices.
Utilize GCP services (BigQuery, Dataflow, Pub/Sub, etc.) to build, deploy, and manage data processing workflows and pipelines.
Work closely with stakeholders to define data integration strategies and develop ETL processes to support analytics, reporting, and machine learning initiatives.
Monitor data infrastructure and proactively identify areas for optimization, enhancement, and automation.
Stay informed about emerging technologies and industry trends related to data engineering, cloud platforms, and insurance analytics.

Requirements

Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
Proven experience as a Data Engineer or similar role, preferably within the insurance or financial services sector.
Strong proficiency in SQL and Python for data manipulation, analysis, and scripting.
Hands-on experience with cloud platforms, particularly GCP (Google Cloud Platform) services like BigQuery, Dataflow, Pub/Sub, Cloud Storage, etc.
Solid understanding of data warehousing concepts, ETL processes, and data modeling techniques.
Experience with building and optimizing data pipelines and workflows using modern data technologies and tools.
Familiarity with data governance, security, and compliance standards relevant to the insurance industry.
Strong analytical skills with the ability to translate business requirements into technical solutions.
Excellent communication and collaboration skills to work effectively with diverse teams and stakeholders.

Preferred Qualifications

Certification in Google Cloud (e.g., Professional Data Engineer, Associate Cloud Engineer).
Experience with other cloud platforms (AWS, Azure) and related technologies.
Knowledge of machine learning concepts and frameworks (e.g., TensorFlow, scikit-learn).
Familiarity with agile development methodologies and version control systems (e.g., Git).","Almacenamiento de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Ciencias de la computación, Manipulación de datos, Modelado de datos y Necesidades empresariales",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982656190/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=TNwTFUEC8ttgiP4ELTUc6g%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, People Analytics","96 US$K/año - 180 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 6 días,"Cottonwood Heights, UT","Acerca del empleo
Employee Applicant Privacy Notice

Who we are:

Shape a brighter financial future with us.

Together with our members, we’re changing the way people think about and interact with personal finance.

We’re a next-generation financial services company and national bank using innovative, mobile-first technology to help our millions of members reach their goals. The industry is going through an unprecedented transformation, and we’re at the forefront. We’re proud to come to work every day knowing that what we do has a direct impact on people’s lives, with our core values guiding us every step of the way. Join us to invest in yourself, your career, and the financial world.

The role:

We are looking for a talented and detail-oriented Data Engineer to tackle big data challenges. You will design, build, and maintain critical data pipelines and datasets, supporting areas like recruiting, compensation, talent management, and learning and development. Your work will enhance data accessibility and empower the People Team and business leaders to make informed decisions with high-quality, reliable data.

Key Responsibilities:


Develop and maintain robust data pipelines and datasets.
Build foundational data products for key business areas.
Enhance self-service data capabilities for the People Team.
Ensure high standards in ETL operations and big data pipeline management.


Join us to drive impactful change and support SoFi's mission of fostering a thriving workplace through data excellence.

What you’ll do:


Design and develop robust data architectures and data pipelines to support data ingestion, processing, storage, and retrieval. Evaluate and select appropriate technologies, frameworks, and tools to build scalable and reliable data infrastructure.
Optimize data engineering systems and processes to handle large-scale data sets efficiently. Design solutions that can scale horizontally and vertically.
Collaborate with cross-functional teams, such as other data engineers, people analysts, data scientists,, and business stakeholders, to understand data requirements and deliver solutions that meet business needs. Effectively communicate complex technical concepts to non-technical stakeholders.
Optimize data engineering systems and processes to handle large-scale data sets efficiently. Design solutions that can scale horizontally and vertically,
Enforce data governance policies and practices to maintain data integrity, security, and compliance with relevant regulations. Collaborate with data governance and security teams to implement robust data protection mechanisms and access controls.


What you’ll need:


A bachelor's degree in Computer Science, Data Science, Engineering, or a related field;
2+ years of experience in data engineering and analytics technical strategy. 
Strong Knowledge in data engineering tools and frameworks; Python / SQL / Orchestration Tools / Containers / etc..
Proficiency in relational database platforms and cloud database platforms such as Snowflake, Redshift, or GCP
Thorough knowledge of data modeling, database design, data architecture principles, data operations, OOP, and CI/CD.
Strong analytical and problem-solving abilities, with the capability to simplify complex issues into actionable plans.
Experience in the HR / People function is advantageous.


Compensation And Benefits

The base pay range for this role is listed below. Final base pay offer will be determined based on individual factors such as the candidate’s experience, skills, and location.

To view all of our comprehensive and competitive benefits, visit our Benefits at SoFi page!

SoFi provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion (including religious dress and grooming practices), sex (including pregnancy, childbirth and related medical conditions, breastfeeding, and conditions related to breastfeeding), gender, gender identity, gender expression, national origin, ancestry, age (40 or over), physical or medical disability, medical condition, marital status, registered domestic partner status, sexual orientation, genetic information, military and/or veteran status, or any other basis prohibited by applicable state or federal law.

The Company hires the best qualified candidate for the job, without regard to protected characteristics.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

New York applicants: Notice of Employee Rights

SoFi is committed to embracing diversity. As part of this commitment, SoFi offers reasonable accommodations to candidates with physical or mental disabilities. If you need accommodations to participate in the job application or interview process, please let your recruiter know or email accommodations@sofi.com.

Due to insurance coverage issues, we are unable to accommodate remote work from Hawaii or Alaska at this time.

Internal Employees

If you are a current employee, do not apply here - please navigate to our Internal Job Board in Greenhouse to apply to our open roles.","Arquitectura de datos, Extraer, transformar y cargar (ETL), Google Cloud y Ingeniería de datos, Amazon Redshift, Ciencias de la computación, Modelado de datos, Productos de datos, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984719894/?eBP=BUDGET_EXHAUSTED_JOB&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=tT9cH1m3NcnpkQDWXFsU6A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Plano, TX","Acerca del empleo
Costco IT is responsible for the technical future of Costco Wholesale, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks  eighth in Forbes “World’s Best Employers”  .

This is an environment unlike anything in the high-tech world and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others.

Come join the Costco Wholesale IT family. Costco IT is a dynamic, fast-paced environment, working through exciting transformation efforts. We are building the next generation retail environment where you will be surrounded by dedicated and highly professional employees.

Data Engineers are responsible for developing and operationalizing data pipelines/integrations to make data available for consumption (i.e. Reporting, Data Science/Machine Learning, Data APIs, etc.). This includes data ingestion, data transformation, data validation/quality, data pipeline optimization, orchestration; and deploying code to production via CI/CD. The Data Engineer role requires knowledge of software development/programming methodologies, various data sources (Relational Databases, flat files (csv, delimited), APIs, XML, JSON, etc.), data access (SQL, Python, etc.), followed by expertise in data modeling, cloud architectures/platforms, data warehousing, and data lakes. This role also will partner closely with Product Owners, Data Architects, Platform/DevOps Engineers, etc. to design, build, test, implement, and maintain data pipelines.

The Data Engineer is responsible for developing data pipelines and/or data integrations of test data for Costco’s enterprise certified data sets that are used for business-critical data consumption use cases (i.e. Reporting, Data Science/Machine Learning, Data APIs, etc.). At Costco, we are on a mission to significantly leverage data to provide better products and services for our members. This role is focused on data engineering to build and deliver data pipelines that will deliver data securely for use by Costco business groups. The Data Engineer will partner with product owners, data architects, and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth.

If you want to be a part of one of the worldwide BEST companies “to work for”, simply apply and let your career be reimagined.

 ROLE 

Implements streaming data pipelines using event/message-based architectures.
Works in tandem with Data Architects to align on data architecture requirements provided by the requestor.
Defines and maintains optimal data pipeline architecture.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery/orchestration.
Analyzes data to spot anomalies, trends, and correlate data to ensure Data Quality.
Performs peer review for another Data Engineer’s work.
Develops and operationalizes data pipelines to bring data into the various test environments used for the development of our certified data sets.
Works in tandem with Data Architects, Data Stewards, and Data Quality Engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration.
Designs, develops, and implements ETL/ELT/CDC processes using Informatica Intelligent Cloud Services (IICS), Azure Data Factory, AWS Glue, dbt, and other ETL products.
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Cosmos, Databricks, and Delta-Lake to improve and speed delivery of our data products and services.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Identifies ways to improve data reliability, efficiency, and quality of data management.
Communicates technical concepts to non-technical audiences both in written and verbal form.

Required

5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
2+ years’ hands-on experience with Informatica IICS, Azure Data Factory, AWS Glue, or other ETL tools.
3+ years’ experience working with Cloud technologies such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies.
5+ years’ experience with Data Pipeline, ETL, and Data Warehousing.
Extensive experience working with various data sources (DB2, SQL,Oracle, flat files (csv, delimited), APIs, XML, JSON.
Experience implementing data integration techniques such as event/message-based integration (Kafka, Azure Event Hub), ETL.
Advanced SQL skills; solid understanding of relational databases and business data; ability to write complex SQL queries against a variety of data sources.
Strong understanding of database storage concepts (data lake, relational databases, NoSQL, Graph, data warehousing).
Experience with Git/Azure DevOps.

Required Documents

Cover Letter
Resume

California applicants, please click to review the Costco Applicant Privacy Notice.

Pay Ranges

Level 2 - $105,000 - $135,000

Level 3 - $130,000 - $160,000

We offer a comprehensive package of benefits including paid time off, health benefits - medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan to eligible employees.

Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com

If hired, you will be required to provide proof of authorization to work in the United States. In some cases, applicants and employees for selected positions will not be sponsored for work authorization, including, but not limited to H1-B visas.","Almacenamiento de datos, Azure Data Factory, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Validación del sistemas informáticos (CSV), Bases de datos, Calidad de datos, Datasets y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3985006179/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=%2Bu%2Fff3SPo3rhD8iVXsv3nQ%3D%3D&trk=flagship3_search_srp_jobs,Data Analytics Engineer,"65 US$/h - 75 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 días,Estados Unidos,"Acerca del empleo
Job Description

Data Analytics and Visualization Engineer
Remote
6 Months Contract (Possible Extension)

Top 3 Skills Needed or Required 
Design and implement data models to organize and structure data for efficient analysis.
Power BI
Experience with programming languages supported by Azure Databricks and GCP Bigquery, including Python and SQL

Description:
We are seeking a skilled Data Analytics and Visualization Engineer to join our data team. In this role, you will be responsible for designing, developing, and maintaining data analytics solutions and creating impactful visualizations to support data-driven decision-making across the organization.
Key Responsibilities:
Develop and maintain data pipelines to extract, transform, and load data from various sources into our data warehouse.
Design and implement data models to organize and structure data for efficient analysis.
Write complex SQL queries and develop data transformation logic to prepare data for analysis.
Well versed with Azure databricks and GCP Bigquery platforms.
Experience with programming languages supported by Azure Databricks, including Python, and SQL
Create interactive dashboards and data visualizations using tools like Tableau, Power BI.
Collaborate with cross-functional teams to understand business requirements and translate them into technical specifications.
Optimize data processes and queries for improved performance and scalability.
Implement data quality checks and ensure data accuracy and consistency.
Develop and maintain documentation for data processes, models, and visualizations.
Stay up-to-date with the latest trends and technologies in data analytics and visualization.
Knowledge of clickstream tools like Qunatum Metrics, Googkla Analytics.
Knowledge of Javascript.

What are the day-to-day responsibilities? 
Understand assigned Merchandising product
Help product team with metric definition, modeling, measurement and reporting
Design and implement data models to organize and structure data for efficient analysis.
Build Visualization to generate insights for Product improvements.

 JobDiva # 24-17281

Best Regards

Ajeet Kumar - Team Lead – IT Recruitment
Direct: 469-342-3572 / Desk : (201) 524-9600 Ext.7790
ajeetk@ustechsolutionsinc.com
10 Exchange Place, Jersey City, NJ 07302, USA

About US Tech Solutions:
US Tech Solutions is a global staff augmentation firm providing a wide range of talent on-demand and total workforce solutions. To know more about US Tech Solutions, please visit www.ustechsolutions.com.

US Tech Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.","Analítica de datos, Google BigQuery, Google Cloud , Ingeniería de datos y SQL, Calidad de datos, Especificaciones técnicas, Microsoft Azure, Modelo de datos y Necesidades empresariales",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3978289157/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=bfIcBAtnAo%2FDVzF%2F2BRLfA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 semana,"San Antonio, TX","Acerca del empleo
Integrated DNA Technologies (IDT) is the leading manufacturer of custom oligonucleotides and proprietary technologies for genomics applications. Our work is complex and cutting-edge, and our team members are curious, creative thinkers who understand that good data drives smart decisions. At IDT, we realize that although science may be uniform, people are unique. We promote a culture where engaged people are motivated and have opportunities to achieve their full potential, as part of one global team.

IDT is one of 10 Life Sciences companies of Danaher. Together, we accelerate the discovery, development and delivery of solutions that safeguard and improve human health.

This position is part of the Data Engineering Team within Business Intelligence located in Coralville, IA and will be fully remote. At IDT, we are one global team. We celebrate our differences, engage in healthy debate, and are inclusive. Together, we accomplish great things.

In this role, you will have the opportunity to:

Analyze, troubleshoot, and integrate complex data systems to develop accurate, automated, and reusable solutions and data pipelines
Participate in data architecture design and analytical model development
Collaborate with business associates to define data analysis and reporting requirements, and partner with technical associates to ensure infrastructure meets expectations for speed and reliability
Work within and contribute to the continued improvement of IDT’s standards of development lifecycle, testing and validation, documentation, and security
Lead and/or contribute to the successful completion of intradepartmental and cross-functional projects

The essential requirements of the job include:

Bachelor’s degree in Computer Science, Business, Engineering or related subject or equivalent experience/certifications required
Minimum of five years technical experience with SQL/T-SQL required
Advanced understanding and knowledge of relational database design and BI architecture, core concepts, methods, and best practices required


It would be a plus if you also possess previous experience in:

Power BI and/or Fabric and Azure Synapse
Python, R or other scripting languages

At IDT we believe in designing a better, more sustainable workforce. We recognize the benefits of flexible, remote working arrangements for eligible roles and are committed to providing enriching careers, no matter the work arrangement. This position is eligible for a remote work arrangement in which you can work remotely from your home. Additional information about this remote work arrangement will be provided by your interview team. Explore the flexibility and challenge that working for IDT can provide.

The salary range for this role is $85,000-$95,000 annually. This is the range that we in good faith believe is the range of possible compensation for this role at the time of this posting. We may ultimately pay more or less than the posted range. This range may be modified in the future.

This job is also eligible for bonus/incentive pay.

We offer comprehensive package of benefits including paid time off, medical/dental/vision insurance and 401(k) to eligible employees.

Note: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, commission, benefits, or any other form of compensation and benefits that are allocable to a particular employee remains in the Company's sole discretion unless and until paid and may be modified at the Company’s sole discretion, consistent with the law.

At Danaher we bring together science, technology and operational capabilities to accelerate the real-life impact of tomorrow’s science and technology. We partner with customers across the globe to help them solve their most complex challenges, architecting solutions that bring the power of science to life. Our global teams are pioneering what’s next across Life Sciences, Diagnostics, Biotechnology and beyond. For more information, visit www.danaher.com.

Danaher Corporation and all Danaher Companies are committed to equal opportunity regardless of race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity, or other characteristics protected by law. We value diversity and the existence of similarities and differences, both visible and not, found in our workforce, workplace and throughout the markets we serve. Our associates, customers and shareholders contribute unique and different perspectives as a result of these diverse attributes.

The EEO posters are available here.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform crucial job functions, and to receive other benefits and privileges of employment. Please contact us at applyassistance@danaher.com to request accommodation.","Analítica de datos, Arquitectura de datos, Base de datos relacional, Ciencia de datos, Extraer, transformar y cargar (ETL) y SQL, Bases de datos, Ciencias de la computación, Diseño de bases de datos y Modelado de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3977360375/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=s8jIoLRoyvsPg26VXHq7jA%3D%3D&trk=flagship3_search_srp_jobs,Data visualization Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Role: CDM Data Visualization Engineer RI
Location: Remote
Duration: 1 Year contract

Note: Clearance eligibility (Must be US Citizen or Permanent resident GC with a minimum 5 years of residence in the USA to get this clearance)

Description & Requirements::
""Lead the effort to gather, analyse and model client data (customers, financials, operational, organizational, access channel), key performance indicators, and/or market data (competitors, products, suppliers), using a broad set of analytical tools and techniques to develop quantitative and qualitative business insights and improve decision-making.

Expertise in areas:
1. Microsoft Power BI – Expert
2. Data Visualization – Expert
3. Delivery Excellence – Expert
4. Big Data Architecture – Advanced
5. Data Analytics – Expert
Lead and manage architecture solution, support, and development of ad-hoc and long-term Power BI Reports.
Requirements gathering, Design, development, and implementation of Power BI Reports ad-hoc and long-term reports for a big data application.
Provides business and technical skills across all phases of the program lifecycle. Designs for continuity of functional or technical requirements through a set of designs, test plans, training materials, software or technical deliverables.
Assumes a significant role in the development of functional and technical information system designs, development of software designs, programming, system testing, business process redesign or business process and architectures; or the resolution of risks and issues related to these functional or technical requirements or activities.
Interfaces with other project teams and interacts with clients at the supervisory level.
Required education:
Minimum Bachelor's degree.

Additional Preferred Skills:
o Azure Databricks- Intermediate
o Spark- Intermediate.","Analítica de datos, Big data, Microsoft Power BI y Visualización de datos, Azure Databricks, Excelencia operativa, Mecanismo de desarrollo limpio (MDL), Microsoft Azure, Planificación de pruebas, Pruebas del sistema y Rediseño de procesos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984728605/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=9eOVQE7jYneH4yFKn53yyg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Ada, OK","Acerca del empleo
Job Summary:

The Data Engineer II is a mid-level technical contributor in the organization who works along similar lines of the software engineering discipline of developing and building the applications and systems that run the company, with focus on the data aspects of the systems and services, working and collaborating with teams in the backend systems, the frontend user experience, the products, or the tools to keep the business running.

Responsibilities:

Help us achieve our mission to provide affordable legal access for all, through the interface of leading-edge technology!

We are looking for a Data Engineer II to join the Data Engineering team and partner with our Analytics, Engineering, and Data Science teams to analyze data requirements, build and maintain data pipelines to consume data from various product and service application, and ensure all data are flowing through a modern ELT (extract/load/transform) process to extra. Data Engineering Team ensures that SLAs (service level agreements) for data movement are upheld, data is landed with integrity and quality, and supports both functional and non-functional business requirements.

This role will be part of the greater engineering department at PPLSI (aka LegalShield), which is the cornerstone of all technology and product focused developments for the company. We are a diverse and skillful team encompassing software engineering, data engineering, quality assurance (QA), and technical program management for the company. We partner closely with peers across product management, design, IT, and business stakeholders. Together we work to create, preserve, and improve well-functioning, well-designed, multi-platform systems and products for both internal and external customers.

If you thrive in a collaborative environment of talented, supportive, results-driven, and customer-focused teammates, this is the place for you!

Responsibilities

As part of the Data Engineering team at PPLSI, the Data Engineer II will

Develop and maintain data pipelines, ELT (extract/load/transform) processes, and data storage solutions that ensure data quality, consistency, and availability using a variety of tools including Apache Airflow, Apache NiFi, Matillion and Python 
Work closely with product/program management, engineering teams and business stakeholders to design and build data products 
Research, design and drive implementation of solutions to further modernize data pipeline platform 
Structure Data and Queries for Performance: work with senior members of the Data Engineering team to partner with data analysts, data scientists, and business intelligence developers to model and sanitize data for algorithm development, reporting, performance, and cost management 

Qualifications

2-5 years of industry experience in Data Engineering 
Bachelor's degree in Data Science, Computer Science, a related field, or equivalent industry experience 
Experience with Python or other modern scripting languages 
Experience with developing data pipelines with orchestration tools such as Airflow, Dagster, Apache Oozie 
Experience with leveraging APIs, databases, and streams 
Familiar with basic tools and concepts in the data warehouse space, and in writing and analyzing SQL statements 
Familiar with software development phases, including design, implementation, testing, and maintenance 
Willingness to learn new approaches and technologies 
Ability to work well with other engineers in a collaborative environment 
Attention to detail and quality with ability to troubleshoot and analyze
Open-minded approach to new ideas and approaches 
Willingness to challenge others and be challenged 
Share in the goal of creating the best design and architecture possible 

Physical Requirements/ Work Environment

Employee must be able to sit or stand for long periods of time, with the physical ability to work at the computer or other sedentary tasks for long periods of time. Employee can conduct duties discreetly and impartially. If working remotely, employee is able to work in a space that allows them to effectively complete their job tasks, including having reliable internet connectivity and the ability to participate effectively while on phone and video calls. Employee has regular and predictable attendance and punctuality.

Additional Information:

Location:

Remote Job Posting

Department:

9310 Engineering

Time Type:

Full time

Commitment to Equal Opportunity

PPLSI conforms to all the laws, statutes, and regulations concerning equal employment opportunities. We strongly encourage women, minorities, individuals with disabilities and veterans to apply to all of our job openings. We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, national origin, status as a protected veteran, and basis of disability or any other federal, state or local protected class. We prohibit retaliation against individuals who bring forth any concerns, orally or in writing, to the employer or the government, or against any individuals who assist or participate in the investigation of any concerns or otherwise oppose discrimination.

If you require a reasonable accommodation to complete the application process, please contact Human Resources at: humanresources@legalshieldcorp.com.","Airflow, Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Ciencias de la computación y Oozie",Solicitar
https://www.linkedin.com/jobs/view/3984727271/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=fVm9mocpaZfVYorRCyCaLw%3D%3D&trk=flagship3_search_srp_jobs,Data Analytics and Visualization Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Bentonville, AR","Acerca del empleo
Description

We are seeking a skilled Data Analytics and Visualization Engineer to join our data team.

In this role, you will be responsible for designing, developing, and maintaining data analytics solutions and creating impactful visualizations to support data-driven decision-making across the organization.

Key Responsibilities

Develop and maintain data pipelines to extract, transform, and load data from various sources into our data warehouse.

Design and implement data models to organize and structure data for efficient analysis.

Write complex SQL queries and develop data transformation logic to prepare data for analysis.

Well versed with Azure data bricks and GCP Big query platforms.

Experience with programming languages supported by Azure Databricks, including Python, and SQL

Create interactive dashboards and data visualizations using tools like Tableau, Power BI.

Collaborate with cross-functional teams to understand business requirements and translate them into technical specifications.

Optimize data processes and queries for improved performance and scalability.

Implement data quality checks and ensure data accuracy and consistency.

Develop and maintain documentation for data processes, models, and visualizations.

Stay up-to-date with the latest trends and technologies in data analytics and visualization.

Knowledge of clickstream tools like Quantum Metrics, Google Analytics.

Knowledge of JavaScript.

Education: Bachelors Degree","Analítica de datos, Lenguajes de programación, Python y SQL, Azure Databricks, Especificaciones técnicas, JavaScript, Microsoft Azure, Modelo de datos y Necesidades empresariales",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3985568897/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=AAT5kUZg5nKGeDr8ONo2Wg%3D%3D&trk=flagship3_search_srp_jobs,Engineer - Data Visualization Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,"Washington, DC","Acerca del empleo
Data Visualization EngineerWashington, D.C

MUST:ctive Secret clearance required5+ years of experience in IT and Data Visualization3+ years experience with Tableau, Power BI, or D3.js,3+ years of data analysis and data management conceptsbility to mentor junior engineers is a huge plusExcellent communication and collaboration skillsStrong portfolio demonstrating data visualization expertiseBachelors degree in Information Design, Data Science, Computer Science or related field

DUTIES:Working with stakeholders to understand data needs and requirementsDesigning and develop visually compelling and informative dashboards, reports, and infographicsDeveloping and maintain documentation, standards, and best practices for data visualizationPerforming data analysis to identify trends and insights, and use that knowledge to design effective visualizationsWorking closely with cross-functional teams to develop and implement data visualization solutionsContinuously improve and innovate data visualization practices

Progression Inc. is an equal opportunity and affirmative action employer. Progression Inc. is committed to administering all employment and personnel actions on the basis of merit and free of discrimination based on race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or status as an individual with a disability. Consistent with this commitment, we are dedicated to the employment and advancement of qualified minorities, women, individuals with disabilities, protected veterans, persons of all ethnic backgrounds and religions according to their abilities. #indpro","Almacenamiento de datos, Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos y Visualización de datos, Bases de datos y Ciencias de la computación",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3967583011/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=GgnWFfVKNChZEIYbfDSGfg%3D%3D&trk=flagship3_search_srp_jobs,Expression of Interest - Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"San Francisco, CA","Acerca del empleo
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.

The Marlee Talent Pool is a pilot project designed to:

Help job seekers get discovered by our partners based on their anticipated hiring needs
Provide optional support and resources for job seekers in their career endeavours
Help individuals understand, and bring out the best in themselves and each other

The Marlee Talent Pool process:

Once you express your interest, you will be asked to complete the Marlee work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.

About Marlee (Fingerprint For Success)

Backed by 20+ years of research, Marlee’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviours, and performance.

Ultimately, we help people find purpose and fulfillment at work, and help build and scale high-performing teams.

Keep in mind that joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your Marlee work style assessment to match the hiring needs of our partners.

Your feedback is a gift! Write to us via: hello@getmarlee.com to help co-create the future of recruitment, together.

Powered by JazzHR

nDQQX1ZDCa","Almacenamiento de datos, Analítica de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos y Visualización de datos, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984164779/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=Ryg3y%2FIJoL8%2Fb9TrJzaRFw%3D%3D&trk=flagship3_search_srp_jobs,100% Remote -  Full Time - Data Warehouse Engineer - Python Healthcare Epic SSIS,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,Estados Unidos,"Acerca del empleo
Amtex Systems Inc is an information technology and talent solutions company offering talent and BI consulting to the companies in US for over 20 years.

Our solutions are designed to fill resource gaps, by providing the right candidates who deliver value to the organization. Our propensity to nurture and build strong relationships with our clients helps us better understand their business demands and gives us the ability to provide services that are on time and rise above the rest.

Full Time - Permanent position - Data Warehouse Engineer Python Healthcare Epic SSIS Azure Data Factory

Need to have the certifications .

100% REMOTE

Needs to work PST

Experience

7+ years of experience as a Data Engineer
In-depth knowledge of SQL, data warehouses, and data transformation techniques
Proven experience with designing and building data pipelines
Expert knowledge of metadata management and related tools
Advanced knowledge of data ETL concepts, processes, and tools such as MS SSIS, ADF
Advanced knowledge of Python
Ability to read and understand various data structures
Ability to be organized and proficient at tracking tasks, defining next steps, and following project plans 
Advanced knowledge of database and data warehousing concepts, including data lakes, relational and dimensional database design concepts, and data modeling practices
Intermediate knowledge of Jupyter Notebooks
Familiarity with Agile project management methods such as SCRUM, Lean, and/or Kanban
Advanced knowledge of healthcare data structures, workflows, and concepts, from Electronic Health Record systems like Epic
Knowledge of Azure cloud platform, Fabric data platform, ADF and DevOps is highly preferred

Education

Bachelor’s degree in a technical, scientific, and/or healthcare discipline; or equivalent work experience.

Certifications

Epic Cogito, Clarity, and Caboodle certifications are required","Almacenamiento de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL), Jupyter y SQL Server Integration Services (SSIS), Bases de datos, Diseño de bases de datos, Gestión de metadatos, Modelado de datos y Planes de proyecto",Solicitud sencilla
https://www.linkedin.com/jobs/view/3947882193/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=2gMgKJvosFFPRvSygTplhQ%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Reporting Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Austin y alrededores, Texas","Acerca del empleo
Title: Junior Data Reporting Engineer
Location: Austin, TX (Onsite role)
Type: Fulltime with Intelliswift Software Inc 
 
Minimum Qualifications:
· 2+ years of solid hands-on experience with complex SQL scripting and Dashboard development. 
· Hands-on experience with design, development, and support of data analysis. 
· Experience with data platform and visualization technologies such as Google plx dashboards, Data Studio, Looker, GoogleSQL, and BigQuery. 
· Strong design and development skills with meticulous attention to detail. 
· Familiarity with Agile Software Development practices and working in an agile environment. 
· Strong analytical, troubleshooting, and organizational skills. Ability to analyze and troubleshoot complex issues, and proficiency in multitasking. 
· BS degree in Computer Science, Math, Statistics, or equivalent academic credentials.","Google Data Studio, Python, SQL y Tableau, Informes y Panel de control",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984654819/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=3nS5HXuatvWUX63IudYBAA%3D%3D&trk=flagship3_search_srp_jobs,"Azure/SQL Data Engineer/Analyst-Seattle, WA (Remote)","En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Seattle, WA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Azure/SQL Data Engineer/Analyst

Location: Seattle, WA (Remote)

Duration: 12+ Months

Rate: DOE

Technology

US Citizens and Green cards are Preferred.

SQL Server 2019 / Azure SQL, Good SQL & ETL skills.

Strong expertise on T-SQL and working knowledge on SSIS & ADF.

Strong knowledge on Excel – Pivots, PowerPivot, VBA .

Working knowledge of PowerBI.

Strong data analytical skills, understanding and experience in Data Quality.

Ability to make business conclusions from data, connect scenarios.

Background of working in an Agile / Sprint based work environment.

Knowledge on DAX scripting and BPC-Standard & Expense – recommendation.

Additional Skills

Understanding Data Management & BI concepts, Data Analysis, Analytics, Cubes, Reporting Technologies, ETL.

Working knowledge/ concepts in Azure PaaS technologies – Databricks, ADLS / ADW / Synapse is a big plus.

Prior experience in Microsoft landscape is a big plus.

Ability to deal with ambiguity and work in agile environment.

Excellent communications skills with business users / stakeholders.

Attention to detail.

Strong troubleshooting skills.

Hands on experience with SharePoint.

Responsibilities Include

Provides extensive troubleshooting and technical expertise in identifying data issues that impact data metrics.

Implements measures / solutions to proactively track data quality issues.

Gather and Confirms project requirements by studying user requirements, conferring with others on project team.

Participate in requirements / user acceptance tests/ business functionality tests.

Provides support to the users on any data related questions/issues reported.

Documenting processes, supporting partner teams, participating in user training.

Solving data discrepancies and providing business intelligence insights.

Monitor data extract, transform, and load (ETL) jobs and respond to any job failures.

Troubleshoot complex data related issues escalated from Tier 1.

Flexibility to support production issues during off hours.

Communication of ongoing issues / incidents to business community.","Extraer, transformar y cargar (ETL), SQL y SQL Server Integration Services (SSIS), Calidad de datos, Cubes, Expresiones de análisis de datos (DAX), Microsoft Azure, Planificación de negocios y sistemas de control, Requisitos del usuario y Resolución de incidencias",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3984930556/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=F8XCPuo9CDforHaR4SyT7g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Azure Databricks with Spanish),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,Estados Unidos,"Acerca del empleo
Strong Experience in Databricks Migration
Experience in GCP / Azure, Databricks, PMO
Fluent in Speaking Spanish
Comfortable travelling to Mexico",Azure Databricks y Microsoft Azure,Solicitud sencilla
https://www.linkedin.com/jobs/view/3982176542/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=X9NI7w%2FoX4JhpnKDWWMKzQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"90 US$K/año - 120 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 6 días,"Pittsburgh, PA","Acerca del empleo
The Company
We are partnering with a company on a mission is to accelerate the transition to a reliable, decarbonized grid. They deliver software solutions to electric grid operators, utilities, and renewable energy project developers to tackle the ""interconnection bottleneck,"" the process by which large-scale solar, wind, and battery projects connect to the power grid. If you are interested in solving the most pressing challenges of the world’s most critical infrastructure, we’d love to hear from you!

Your Impact (Responsibilities)
We are looking for a Data Engineer who is comfortable taking ownership of complex projects while collaborating with existing members of the team. You will work to make this core software product faster, easier, and cheaper for renewable energy projects to be deployed on the grid. You will play a leading role in collecting and analyzing user-facing data, including:

Architecting and deploying data pipelines for scraping and aggregation of public datasets
Processing and analyzing data to develop insights around renewable energy interconnection
Developing intuitive dashboards and visualizations in modern frontend web technologies

Your work will help project developers deploy more (and better) zero-carbon generation projects on the grid!

About You
Strong Python coding experience; alternatively, experience with Java/other modern programming languages for data pipelines and a demonstrated ability to learn and adopt new coding techniques
Experience with cloud-native ETL data pipelines
Expertise in cloud-based relational databases
Familiarity with Git or other version control system
Experience using AWS, Azure, or Google Cloud Platform
Strong analytical problem solving skills
A passion for learning, teaching, and building beautiful software
STEM degree with 1-2 years of applicable software engineering experience is a plus, but we will consider entry-level engineers with previous internship or project experience

This is an onsite position in Pittsburgh, PA. We are recruiting for an early-career data engineer and unfortunately at this time will not consider mid-senior career engineers willing to take a junior-level position.","Canalizaciones de datos, Computación en la nube y Python",Solicitar
https://www.linkedin.com/jobs/view/3964470480/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=HXWCciH2jLTFHBAX%2FCu%2B5Q%3D%3D&trk=flagship3_search_srp_jobs,Expression of Interest - Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Chicago, IL","Acerca del empleo
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The Marlee Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavours.
Help individuals understand, and bring out the best in themselves and each other.
The Marlee Talent Pool process:
Once you express your interest, you will be asked to complete the Marlee work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Marlee (Fingerprint For Success)
Backed by 20+ years of research, Marlee’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviours, and performance.
Ultimately, we help people find purpose and fulfillment at work, and help build and scale high-performing teams.
Keep in mind that joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your Marlee work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via: hello@getmarlee.com to help co-create the future of recruitment, together.","Almacenamiento de datos, Analítica de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3972524781/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=utsshdHTS1xDVLQc2mMOpw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (onsite),"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Juno Beach, FL","Acerca del empleo
Title:  Data Engineer (onsite)

Work authorization: any (No H1)

Location:  Juno Beach, FL

PLEASE SEND LOCAL CANDIDATES ONLY (currently located in , FL)

Experience: 5+years

Must-have skills: +3 y of IT exp; 1-2 y of data engineering exp; Python, SQL; Knowledge of Salesforce CRM; Salesforce CRM; data visualization tools (Tableau, PowerBI

Requirements

 Availability to work 100% at the Client’s site in Juno Beach, FL (required);
 Experience working as Data Engineer (1-2 years);
 Experience with Python, SQL, and other programming or scripting languages;
 Experience with Salesforce CRM, including customizing objects, profiles, roles, permissions, and workflows;
 Bachelor’s degree in computer science, Engineering, Statistics, or a related field","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , SQL y Visualización de datos, Ciencias de la computación, Permissions y Salesforce.com",Solicitud sencilla
https://www.linkedin.com/jobs/view/3960575674/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=GDYZ3sCgjjFWk16l0o1MUg%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Scientist/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Nueva York, NY","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3964470529/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=cJCGo2hp1s8jLIxZoI7QVQ%3D%3D&trk=flagship3_search_srp_jobs,Expression of Interest - Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"San José, CA","Acerca del empleo
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The Marlee Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavours.
Help individuals understand, and bring out the best in themselves and each other.
The Marlee Talent Pool process:
Once you express your interest, you will be asked to complete the Marlee work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Marlee (Fingerprint For Success)
Backed by 20+ years of research, Marlee’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviours, and performance.
Ultimately, we help people find purpose and fulfillment at work, and help build and scale high-performing teams.
Keep in mind that joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your Marlee work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via: hello@getmarlee.com to help co-create the future of recruitment, together.","Almacenamiento de datos, Analítica de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3919925063/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=FWLXj%2BZc2ExWA2XDGhslUA%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Junior/Entry,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"San Francisco, CA","Acerca del empleo
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this market also our candidates get multiple job offers and $100k + salaries.

 please check the below links to see success outcomes of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 https://www.youtube.com/watch?v=OFoqPTNORew

 https://www.youtube.com/watch?v=-HkNN1ag6Zk

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

 https://youtu.be/bJJl27D8bh0

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3964466830/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=lhh45hYRtcFrXTXMJlY54Q%3D%3D&trk=flagship3_search_srp_jobs,Expression of Interest - Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Houston, TX","Acerca del empleo
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The Marlee Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavours.
Help individuals understand, and bring out the best in themselves and each other.
The Marlee Talent Pool process:
Once you express your interest, you will be asked to complete the Marlee work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Marlee (Fingerprint For Success)
Backed by 20+ years of research, Marlee’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviours, and performance.
Ultimately, we help people find purpose and fulfillment at work, and help build and scale high-performing teams.
Keep in mind that joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your Marlee work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via: hello@getmarlee.com to help co-create the future of recruitment, together.","Almacenamiento de datos, Analítica de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3976196157/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=Aozw1SRfUt9l7HaDy%2BbgJw%3D%3D&trk=flagship3_search_srp_jobs,,,,,,,
https://www.linkedin.com/jobs/view/3973711160/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=HtSGtmex3TC5Opsm44MH4A%3D%3D&trackingId=l1vs5LENH9iMqbhPB0YtVw%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Junior/Entry,Mira las tendencias de contratación recientes de SynergisticIT. Probar Premium por 0 AR$,hace 2 semanas,"Tampa, FL","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers get employed in the tech job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

please check the below links to see the success outcomes and salaries of our candidates .

We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)(phone number removed)di

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements.

No phone calls, please. Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3981862413/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=oX4TGgk2uzXsDfpISkYyyQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 4 días,"Oceanside, CA","Acerca del empleo
Who are we:
Magnaflow is a leading manufacturer and supplier based in Oceanside, CA of premium products to the automotive aftermarket industry such as catalytic converters, performance exhaust and replacement exhaust. Through the Camburg division in Huntington Beach, CA. Magnaflow also supplies race-inspired performance suspension products and vehicle uplifting services, Magnaflow is focused on future growth through both new business development and new product introduction. Our websites are www.magnaflow.com and www.camburg.com
Salary Range: 
 $94,900.00 to $121,200.00 USD
What you will be doing:
We are seeking a skilled BI Analyst specializing in Databricks to join our dynamic BI team. The ideal candidate will have a strong background in data analysis, data engineering, and data visualization, with specific expertise in using Databricks to transform and analyze data. You will work closely with stakeholders across the organization to deliver actionable insights and support data-driven decision-making.
Responsibilities:
Design, develop, and maintain scalable data pipelines and ETL processes using Databricks.
Collaborate with data scientists, data engineers, and business stakeholders to understand data requirements and translate them into effective Databricks solutions.
Perform data analysis to identify trends, patterns, and insights that drive business decisions.
Create and maintain interactive dashboards and reports using Databricks, Power BI, or similar visualization tools.
Optimize and troubleshoot Databricks workflows to ensure efficient data processing.
Ensure data quality and integrity by implementing best practices in data governance and validation.
Stay up-to-date with the latest trends and advancements in Databricks and BI technologies.
Requirements:
Bachelor’s degree in Computer Science, Information Systems, Data Science, a related field, or equivalent experience
3+ years of experience in a BI or data analytics role, with a focus on Databricks.
Strong proficiency in Databricks, including Spark, Delta Lake, and MLflow.
Experience with SQL, Python, or Scala for data manipulation and analysis.
Familiarity with cloud platforms such as Azure, AWS, or GCP.
Knowledge of data visualization tools like Power BI, Tableau, or similar.
Excellent problem-solving skills and attention to detail.
Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams
What’s in it for you?
Competitive Salary
PTO, Sick Pay, Birthday Holiday, Paid Holidays
Medical, Dental, Vision
401k Matching (Up to 5%)
Childcare assistance
Education assistance
Company sponsored events
Growing department and team
 #MF","Analítica de datos, Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Calidad de datos, Ciencias de la computación, Manipulación de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3984238258/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=ycgdM3emkLLvHyK%2FFB6IJQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - National Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Minnetonka, MN","Acerca del empleo
At UnitedHealthcare, we’re simplifying the health care experience, creating healthier communities, and removing barriers to quality care. The work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable, and equitable. Ready to make a difference? Join us to start Caring. Connecting. Growing together.

As a Data Engineer, you will be responsible for the management and manipulation of mostly structured data, with a focus on building business intelligence tools, conducting analysis, performing normalization operations, and assuring data quality for the CEO (Client Experience and Operations) area. Depending on the specific role and business line, you will be responsible for creating specifications to bring data into a common structure, creating product specifications and models, developing data solutions to support analyses, performing analysis, interpreting results, developing actionable insights and presenting recommendations for use across the company. You will be responsible for partnering up with stakeholders to understand data requirements and develop tools and models such as segmentation, dashboards, data visualizations, decision aids and business case analysis to support the organization. Other roles involved could include producing and managing the delivery of activity and value analytics to external stakeholders and clients.

You’ll enjoy the flexibility to telecommute* from anywhere within the U.S. as you take on some tough challenges.

Primary Responsibilities

This internal customer facing position will work directly with multiple levels of the business stakeholders within our CEO teams to develop advanced UI reporting solutions. This position provides visibility, analytical, ad hoc and development capabilities leveraging data from different functional areas across the CEO landscape
Working in a matrix environment by partnering with other partners in the CEO program to deliver data and reporting needs to support the expansion of the platform through new business and migration
Works in an Agile framework within a matrix environment working in sprints and utilizing agile tools (e.g., Rally) 
Instills an Agile framework within the team and across the matrix environment to operate as applicable and fully utilizing Rally tools
Build, maintain and/or adhere to a structured data governance process to be used across all datasets with a focus on quality and accuracy
Identify and participate in the resolution of data integrity issues and organizational problems 

Functional Competencies

Demonstrate and apply understanding of UnitedHealth Group's business (e.g., specific business capabilities, functions, processes, and business cycles) and knowledge of operations, goals, and policies and procedures of internal business partners (e.g., information contacts) to provide effective support to internal and/or external customers
Manage and protect data, adhering to applicable legal/regulatory requirements (e.g., HIPAA, PHI, PII, DOI, state and federal regulations)
Propose and/or define long-term strategies for implementing process and/or data and reporting improvements
Review competitive intelligence (e.g., report formats; aggregation levels) to identify trends and opportunities for new reporting solutions and data strategy
Identify and/or provide opportunities for additional training and learning to support process and report improvements
Create and/or update presentation documents and materials to summarize results (e.g., written reports; PowerPoint deck; Tableau; graphs and/or charts)
Review and/or identify appropriate data infrastructure to use based on customers' needs in alignment with PADU
Develop business context diagrams (e.g., business data flows, process flows) to analyze/confirm the definition of project requirements
Demonstrate understanding of the difference between business requirements and technical solutions and define approach for storing and updating business requirements
Collaborate with business and technical stakeholders (e.g., business owners, process owners, domain experts) to identify specific business requirements. Perform reviews with all stakeholders to obtain approval/signoff of project requirements documents (e.g., walkthroughs)
Update progress to project schedule to track/measure one’s progress fulfilling aligned tasks. In addition to supporting ongoing monitoring by keeping project documentation or applications updated (e.g., Rally)

You'll be rewarded and recognized for your performance in an environment that will challenge you and give you a clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Required Qualifications

High School Diploma/GED (or higher)
4+ years of experience in a data engineer position
3+ years of SQL/TSQL Development experience 
3+ years of experience performing significant data analysis and report development
3+ years of experience relational databases, database structures and design, systems design, data management, data warehouse
3+ years of experience with Data Modeling, ETL construction with advanced job scheduling
Intermediate level of programming and troubleshooting knowledge 
Intermediate level of proficiency in Microsoft Excel, and Word 

Preferred Qualifications

1+ years of developing reports in Power BI, Tableau, SSRS or equivalent BI tools
Experience with Databricks, Snowflake 
Experience with Azure Data Factory 
Experience with Power Platform, Power Automate, Power BI
Prior experience/knowledge of the different domains within health insurance
Prior experience/knowledge of the different domains within Client Experience and Operations
All Telecommuters will be required to adhere to UnitedHealth Group’s Telecommuter Policy.

California, Colorado, Nevada, Connecticut, New York, New Jersey, Rhode Island, Hawaii or Washington Residents Only: The salary range for California, Colorado, Nevada, Connecticut, New York, New Jersey, Rhode Island or Washington residents is $70,200 to $137,800 per year. Pay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. UnitedHealth Group complies with all minimum wage laws as applicable. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Application Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants.

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone-of every race, gender, sexuality, age, location and income-deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission. 

Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity / Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law. 

UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.

#RPO","Azure Data Factory y Extraer, transformar y cargar (ETL), Datasets, Microsoft Power Automate, Modelado de datos, Necesidades empresariales, Programación del trabajo, SQL Server Reporting Services (SSRS), Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3980266934/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=Q20mD3ICcwwVx6x8cKGUfw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 semana,Estados Unidos,"Acerca del empleo
ABOUT US
We believe that in a world of digital overload, customers crave tangible connections and delightful experiences.

Our fun, smart, and driven team is passionate about delivering on this mission and helping our users succeed with our innovative direct mail platform, which enables thousands of brands to send individually personalized postcards to customers and prospects.

PostPilot is a fast-growing marketing automation platform for the top ecommerce brands in the industry to send powerful postcard marketing campaigns and handwritten cards (using our proprietary robots!) to customers as easily as email.

Our proprietary platform and technology creates individually personalized, automated direct mail campaigns as easily as email and for less than the cost of a click. We help online brands reach and retain more of their customers as email engagement declines, iOS makes it harder to target customers online, and competition is driving up CPCs. We cut through the digital clutter to drive remarkable incremental revenue and profits, and have the data, case studies, and growth to back it up (3X YoY, >100 headcount and growing fast). We are profitable and recently raised a Series A growth investment from Summit Partners and the founders of Klaviyo.

THE ROLE 
In this role, you will work closely with our data analytics, data science, and engineering teams. This will be an individual contributor role. We’re looking for people with significant experience building data pipelines and working with analytics in SaaS businesses, and in particular, MarTech/ecommerce/retail businesses.

This is a full- time remote position. We’re open to hiring outside of the US for the right candidate, but are looking for someone in the Americas, so that we can collaborate in a similar time zone.

You will be responsible for:
Implementing and maintaining high-scale, high-availability data pipelines across all 1st party and 3rd party systems in our stack, both internally to our application platform as well as externally to and from our customers and vendors, and related tooling.
Ensuring our data platform and pipelines supports the ongoing customer-facing BI tooling, needs, and dependent in-app functionality.
As required, leads external data implementation projects from a technical standpoint (data feeds, integrations, etc.), works directly with vendors and customers where appropriate to successfully complete data integration initiatives.
Managing our data platform, built on GCP, from both a technical and cost-control perspective.

ABOUT YOU
Strong technical chops: You’re comfortable with the tools of the trade, and have a strong understanding of GCP, BigQuery, ETLs, IAC (we use Terraform) among other tools.
Self-starter: As this is our first data engineering hire, you will need to be comfortable working independently and taking initiative. You will have the opportunity to shape our data engineering practices.
Experience in a fast-paced startup environment: You're comfortable with ambiguity and can thrive in a dynamic setting where priorities may shift.
Problem-solver: You have a knack for troubleshooting and enjoy finding creative solutions to complex data challenges.

OUR TOOLING STACK
Our data infrastructure is built using Google Cloud Platform and we rely heavily on BigQuery for data warehousing. We're still in the early stages of developing our ETL processes and data pipelines, but we plan to continue using a ""monolith"" approach. This means we'll keep our code and tools centralized to make development and maintenance easier. We'll heavily rely on IAC and a single repository for all our code using GitOps. This role will be instrumental in setting up the system architecture and defining the best practices we'll use in our data operations.","Análisis de datos, Canalizaciones de datos, Google BigQuery y Ingeniería de datos, Iniciativa, Resolución de problemas y Software como servicio (SaaS)",Solicitar
https://www.linkedin.com/jobs/view/3955060892/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=eRki8rrD3WcnljlEwL2%2FGg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,Estados Unidos,"Acerca del empleo
Grow with Us

This position is in our Maple Grove, MN 

ProAg has an exciting opportunity for a Data Engineer to join our data team. We are looking for individuals who want to embrace the advantages of data and dedicate importance to ensure our data structures are the easiest and most efficient in the industry. Bring your passion for data to help data solutions to help as we serve our farmers, agents, and re-insurers.

You will be primarily responsible for the analysis, design, development, testing, implementation and maintenance of new and existing data structures. Responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. Supports our software developers, data architects, data analysts and data scientists on data initiatives.

In This Exciting Opportunity You Will

Build the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources.
Maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that utilize the data to provide actionable insights into operational efficiency and other key business performance metrics
Work with the stakeholders and business teams to assist with data-related technical issues and support

What You’ll Bring

2-3 years of relevant and progressive professional experience in data analysis, design and development.
Bachelor’s degree in a related field or equivalent education and/or experience.
Ability to work in a dynamic problem-solving environment and synthesize strategy, plans, and solutions.
Demonstrated ability to deliver in a complex business environment.

What We Represent

Part of something bigger: We offer a career with purpose as you support the farmers and ranchers who create food, fuel and fiber for the world.
Personal connections: We are built on strong relationships and appreciation of your individuality.
A team who cares: We look out for each other personally and professionally because we care about each other.
Innovators by trade: We’re committed to a brighter tomorrow for our team members and for agriculture.
The best of both worlds: We combine personal connections with powerful resources, thanks to our culture and the backing of Tokio Marine HCC.

While our nation weathers economic storms, ProAg, a member of the Tokio Marine HCC group of companies, is positioned as a financially strong and well-capitalized insurer. We’re known for our quick response and fast, accurate claims settlement. We understand how important this is because many of us are farmers and ranchers ourselves. With more than 90 years of service to our agents & insureds, we stand committed to continuing the principles that ProAg was founded on: Integrity, Loyalty and Customer Service.

The Tokio Marine HCC Group of Companies offers a competitive salary and employee benefit package. We are a successful, dynamic organization experiencing rapid growth and are seeking energetic and confident individuals to join our team of professionals. The Tokio Marine HCC Group of Companies is an equal-opportunity employer. Please visit www.tokiomarinehcc.com for more information about our companies.","Almacenamiento de datos, Analítica de datos, Análisis de datos, Canalizaciones de datos, Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3917185512/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=Ylr9KLpTUPVZ2b5H4k59oQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,"Fairfax, VA","Acerca del empleo
Full-time, Remote - Location Requirement: Must be located in the Eastern Time Zone

Available for W-2 or 1099 Individual.

Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a trueMid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.

Responsibilities:

Analyze system requirements and design responsive algorithms and solutions
Use big data and cloud technologies to produce production quality code
Engage in performance tuning and scalability engineering
Work with team, peers and management to identify objectives and set priorities
Perform related SDLC engineering activities like sprint planning and estimation
Work effectively in small agile teams
Provide creative solutions to problems
Identify opportunities for improvement and execute

Requirements:

Minimum 4years of proven professionalexperience working in the IT industry within U.S
Bachelor'sin Computer Science or related domain
Experience with cloud based Big Data technologies
Experience withbig data technologies like Hadoop, Spark and Hive
AWS experience (S3 and EMR)
Proficiency in Hive / Spark SQL / SQL. Experience with Spark
Experience with one or more programming languages like Python,Java,Scala
Ability to push the frontier of technology and independently pursue better alternatives

Thanks for applying!

Powered by JazzHR

WwXkuCvB6I","Apache Spark, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos , Requisitos del sistema y Scala, Análisis de sistemas y Ciencias de la computación",Solicitud sencilla
https://www.linkedin.com/jobs/view/3901938333/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=PTeYAGaUVP1PknkeiUQ2Ow%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.",Publicado de nuevo hace 3 días,Estados Unidos,"Acerca del empleo
DataAnnotation is committed to creating quality AI. Join our team to help train AI chatbots while gaining the flexibility of remote work and choosing your own schedule.

We are looking for a proficient Data Engineer to join our team to train our AI chatbots to code. You will work with the chatbots that we are building in order to measure their progress, as well as write and evaluate code.

In this role you will need to be proficient in at least one programming language (Python, JavaScript, HTML, C++, C# and SQL) and able to solve coding problems (think LeetCode, HackerRank, etc). For each coding problem, you must be able to explain how your solution solves the problem.

Benefits:

This is a full-time or part-time REMOTE position
You’ll be able to choose which projects you want to work on
You can work on your own schedule
Projects are paid hourly, starting at $40+ USD per hour, with bonuses for high-quality and high-volume work

Responsibilities:

Come up with diverse problems and solutions for a coding chatbot
Write high-quality answers and code snippets
Evaluate code quality produced by AI models for correctness and performance

Qualifications:

Fluency in English (native or bilingual level)
Proficient in at least one programming language (Python, JavaScript, HTML, C++, C# and SQL)
Excellent writing and grammar skills
A bachelor's degree (completed or in progress)","Python y SQL, C#, C++, HTML y JavaScript",Solicitar
https://www.linkedin.com/jobs/view/3965754086/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=XWLV52jzEK%2FV6E7Hq1HxKw%3D%3D&trk=flagship3_search_srp_jobs,AWS Data Engineer (Python),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,"Seattle, WA","Acerca del empleo
Location: Fully remote anywhere within the US

Senior Data Engineer

Our client is seeking a Senior Data Engineer to accelerate data sharing between applications, engineers, and data scientists. The successful candidate will demonstrate mature programming and design ability. The Data Engineer role supports data delivery, serving, design, monitoring, and security management for data science, graph, and machine learning engineering teams.

Requirements:

7+ years of relevant experience
Demonstrable AWS knowledge - EC2, ECS, Lambda, S3, EMR
Strong programming ability in Python or Scala
Experience with SQL and relational data
3+ years experience in one or more big data technology : S3, HDFS, Spark, Presto, etc. SPARK preferred
Experience designing and implementing ETL pipelines to support Data Science and/or Machine Learning

Nice to have

Feature Store build experience and Dagster
Financial industry experience

Powered by JazzHR

7QBR2sE3Ix","AWS Lambda, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python, SQL y Scala, Amazon EC2, Amazon S3 y Presto",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983860015/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=09FuFWVigsIegrdnzBYoTg%3D%3D&trk=flagship3_search_srp_jobs,Analytics Engineer | Especialista,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,Estados Unidos,"Acerca del empleo
Job Description

O time de Data & Analytics na Blip é o time responsável por suprir a empresa com informações estratégicas, empoderá-la na geração de conhecimento, guiá-la em tomadas de decisão data-driven e impulsionar a inovação através dos dados. Tendo, desta forma, um papel muito importante na evolução da Blip.

Sobre Seu Papel No Time

Como Analytics Engineer o seu papel será o de desenvolver soluções de dados para as áreas de negócio da Blip. Criar, manter, documentar e otimizar pipeline de dados, realizar o processo de ETL e modelar os dados, garantindo qualidade, confiança e performance no consumo desses dados.

Você estará trabalhando diretamente com um ambiente de Big Data desafiador, com um acelerado crescimento do volume das informações. Terá também a oportunidade de fazer parte de um time que está impulsionando a cultura data-driven e aumentando a autonomia na geração de conhecimento da empresa.

Se você é uma Pessoa com Deficiência (PcD), saiba que todas as vagas da Blip também são inclusivas. Estamos esperando sua inscrição! 🚀

Responsibilities and assignments

Qual será seu papel no time?

Desenvolver, suportar, documentar e testar soluções de disponibilização de dados em ambiente cloud, em parceria com as demais especialidades dos times;
Documentar todo o processo de desenvolvimento dos produtos de dados;
Identificar, implementar e evoluir pontos de melhoria em soluções de dados já existentes;
Lidar com diversos contextos do negócio, entendendo as interfaces entre domínios de dados e como se relacionam, e comunicar de forma eficaz entre as áreas do negócio;
Prestar papel de consultoria sobre o ambiente de dados;
Apoio para o desenvolvimento dos demais Analytics Engineers.

Requirements And Qualifications

O que esperamos de você:

Conhecimento nas linguagens Python, PySpark, SQL;
Conhecimento em ferramentas de criação e orquestração de ETL (Azure Data Factory, Airflow);
Conhecimento de ferramentas de versionamento de código;
Conhecimento em modelagem multidimensional de dados;
Conhecimento em ambiente Cloud;
Goste de desafios, de resolver problemas e tenha facilidade de aprendizado.

E Será Um Diferencial Se Você

Conhecimento em Databricks, Azure e PowerBI;
Experiência com esteira de desenvolvimento CI/CD.

Additional information

Vantagens e benefícios

Nada Básico Que Amamos

Vale Alimentação ou Refeição no valor de R$968,00 fixo (inclusive nas férias);
Vale transporte para modelos de trabalhos híbrido ou presencial;
Day-off: dia de folga pelo seu aniversário;
Auxílio home office para modelos de trabalho remoto ou híbrido;
Modelos de trabalho 100% remoto, híbrido ou presencial;
Bonificação anual de acordo com resultados da empresa e políticas vigentes;
Vale Cultura para usar em livrarias, papelarias, comprar livros, ingressos de teatro, shows, cinema e mais; 
Reembolso do registro da troca do nome social para pessoas trans;
Empréstimo Consignado: parceria com alguns bancos que facilita na contratação de um empréstimo consignado;
No dress code.

Saúde e Bem-estar

Assistência médica de cobertura nacional para Blippers e dependentes, coparticipativo e com desconto de R$1,00 em folha;
Assistência odontológica para Blippers e dependentes;
Wellhub Gympass para Blippers e dependentes: plataforma que dá acesso a academias, estúdios e apps de bem-estar para você cuidar da sua saúde;
Psicologia viva: plataforma de atendimentos psicológicos online com coparticipação;
Seguro de vida sem custo para Blippers;
Horário flexível: nosso regime de 40h semanais pode ser feito em horário flexível, desde que alinhados com a sua liderança;

Família

Licença estendida para todas as pessoas gestantes (180 dias) e para todas as pessoas que assumem responsabilidade parental (30 dias), extensível para papais e mamães que adotarem ou obtiverem a guarda judicial da criança e para casais homoafetivos;
Auxílio creche para crianças até 71 meses;
Auxílio mensal para Baby Blippers com deficiência para tratamento e/ou escola especializada;
Auxílio recém-nascido de R$1.000,00, válido também nos casos de adoção.

Carreira e Formação

Apoio em capacitações, conforme as políticas de elegibilidade e alinhamentos das áreas;
Lifelong Learning: contamos com a Degreed, uma das maiores plataformas de aprendizagem do mundo com acesso a conteúdos ilimitados sobre diversos temas;
Blip Idiomas: cursos gratuitos com aulas ao vivo de idiomas pela plataforma GoFluent;
Galena: descontos em graduações e cursos de desenvolvimento em instituições educacionais.

Process stages

Step 1: Registration1Registration
Step 2: Mapeamento Comportamental 💡2Mapeamento Comportamental 💡
Step 3: Bate papo com Talent3Bate papo com Talent
Step 4: Bate Papo com Liderança4Bate Papo com Liderança
Step 5: Proposta5Proposta
Step 6: Hiring6Hiring

PRAZER, SOMOS A BLIP! 🚀

Aqui oferecemos uma experiência surpreendente, rápida e inteligente para os seus clientes, porque Blip é o futuro! 💬

Somos a Blip, uma plataforma de interações inteligentes, onde as empresas se encontram com clientes em vários canais de comunicação, como WhatsApp, Instagram, Facebook ou no chat do seu site.

Aqui nós temos um time de Blippers que vive inovação no dia dia, com um ponto de vista único para evoluir as jornadas de comunicação, sempre com confiança para aprender mais! E no nosso próprio ritmo, vamos muito mais longe! 🚀

A Blip é feita de pessoas para pessoas! Somos especialistas, inquietos e bem-humorados e é assim que nós entregamos conversas no ritmo das pessoas. Somos líderes de mercado na América Latina, com Blippers atuando em vários lugares do mundo, sempre com confiança para ir longe!

Valorizamos pessoas em primeiro lugar e por isso consideramos todos os grupos de diversidade nas nossas vagas. 😉

E se você é uma Pessoa com Deficiência (PcD) ou Neurodivergente, saiba que todas as vagas da Blip também são inclusivas! Estamos esperando sua inscrição! 🔑","Analítica de datos, Apache Spark, Ciencia de datos y Ingeniería de datos, Lenguaje de consulta (query)",Solicitar
https://www.linkedin.com/jobs/view/3921114097/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=L6LNfD7%2F72hnH6Itj2SoMQ%3D%3D&trk=flagship3_search_srp_jobs,Jr. Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"Boston, MA","Acerca del empleo
Jr. Data Engineer

Resonsibilities:
Responsible for designing, developing, modifying, and evaluating programs for internal functional areas including finance, human resources, and marketing
Analyzes existing programs or formulates logic for new systems, devises logic procedures, prepares flowcharting, performs coding, and tests/debugs programs
Develops conversion and system implementation plans
Prepares and obtains approval of system and programming documentation
Recommends changes in development, maintenance, and system standards
Trains users in conversion and implementation of system
pecific focus on mongo DB, python & spark
Qualifications:
Experience with ISPARK, MongoDB and Python--3 to 5 years
Degree/Certification: STEM Specific College Degree Required--then 3 to 5 years of experience","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , MongoDB, NoSQL y Python, Bases de datos, Ciencia, tecnología, ingeniería y matemáticas (CTIM) y Planes de implementación",Solicitar
https://www.linkedin.com/jobs/view/3984550103/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=Y6MaEXI%2BsQinTHGs70cvnA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,"Irving, TX","Acerca del empleo
Note: Only US Citizens and Green Card Holders are considered for this position (W2 Requirement, Full -Time)

Position title: Data engineer

Location: 800 Connecticut Avenue, Norwalk, CT

Onsite: 3 days a week

Perm: 130k-160k

Relo candidates: Yes that's fine

Status: anyone that does not require current or future sponsorship (meaning if someone requires sponsorship in 3-5 years, we cant use)

Interview process: apex screening, then two video interviews to hire

Must have: 5 years experience or more, databricks, building databricks and using Azure datawarehouse or azure synapse

Current State:

 Migrated three businesses into one
Consolidating subledgers and CRM systems
 Just migrated to Azure cloud à would be using azure synapse for analytics
 Currently using TCS but it's not going well, poor support model and struggling with time zone difference
Has a team of 7 resources, SQL Server/SSIS backgrounds and really struggling to keep pace or learn new technologies
 Current reporting is a lot of manual excel spreadsheets
 Future State End Goal:
 Enterprise Datawarehouse with self-service reporting
Hasn't started this process at all, would need to road map how to consolidate the current 3 DWs to one first
Add in Azure Snapse to automate the reporting
Needs to implement MDM, Data Quality & Data Governance, doesn't exist right now","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Azure Databricks, Bases de datos y Microsoft Azure",Solicitar
https://www.linkedin.com/jobs/view/3984549201/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=U4duGF%2FNXmjrJoBN6A4eaQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,Estados Unidos,"Acerca del empleo
Required Skills

Job Description

As a contract Data Engineer at Bezos Academy, you will be a critical member of our data engineering team. You will help build and maintain the data infrastructure that is essential to Bezos Academy having a positive impact across the country and will help us make decisions that will shape our organization for years to come.

You are an experienced data engineer who operates autonomously, loves to dig into data and learn new domains, knows how to keep things simple, and finds creative, cost-effective and scalable ways to solve problems. You will help us establish best-in-class data security and work closely with stakeholders to understand use cases, build data pipelines and warehouses, and ensure data quality. You will monitor data for bias and work to eliminate it. You may also have the opportunity to lend your skills to other exciting data and analytic needs as we continue to grow in size and complexity. Above all, you share our passion for expanding access to high quality, Montessori-inspired preschool within underserved communities as we scale to hundreds of schools nationwide.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Calidad de datos",Solicitar
https://www.linkedin.com/jobs/view/3958502474/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=VUqKplsnu0wjze6a2xFYIQ%3D%3D&trk=flagship3_search_srp_jobs,Expression of Interest - Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,"Boston, MA","Acerca del empleo
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The Marlee Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavours.
Help individuals understand, and bring out the best in themselves and each other.
The Marlee Talent Pool process:
Once you express your interest, you will be asked to complete the Marlee work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Marlee (Fingerprint For Success)
Backed by 20+ years of research, Marlee’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviours, and performance.
Ultimately, we help people find purpose and fulfillment at work, and help build and scale high-performing teams.
Keep in mind that joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your Marlee work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via: hello@getmarlee.com to help co-create the future of recruitment, together.","Almacenamiento de datos, Analítica de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3964469572/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=juBRLvR7GmP4Sf9Fk2GJqA%3D%3D&trk=flagship3_search_srp_jobs,Expression of Interest - Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Phoenix, AZ","Acerca del empleo
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The Marlee Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavours.
Help individuals understand, and bring out the best in themselves and each other.
The Marlee Talent Pool process:
Once you express your interest, you will be asked to complete the Marlee work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Marlee (Fingerprint For Success)
Backed by 20+ years of research, Marlee’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviours, and performance.
Ultimately, we help people find purpose and fulfillment at work, and help build and scale high-performing teams.
Keep in mind that joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your Marlee work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via: hello@getmarlee.com to help co-create the future of recruitment, together.","Almacenamiento de datos, Analítica de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3818359690/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=CK72BlHjUu4XUKIYZwQtMQ%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Scientist/Analyst/Engineer - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Boston, MA","Acerca del empleo
At SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.

 Why Us ? 

SynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.

 REQUIRED SKILLS For Java /Software Programmers 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java , javascript , C++ or software programming 
 Spring boot, Microservices, Docker, Jenkins and REST API's experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis 

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 

Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://www.youtube.com/watch?v=EmO7NrWHkLM 

 https://www.youtube.com/watch?v=NVBU9RYZ6UI 

 https://www.youtube.com/watch?v=Yy74yvjatVg 

SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube

For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ 

 We are looking for the right matching candidates for our clients 

 Please apply via the job posting 

 REQUIRED SKILLS For Java /Full Stack/Software Programmer 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java , javascript , C++ or software programming 
 Spring boot, Microservices, Docker, Jenkins and REST API's experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning Positions 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow 

 If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

 No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Análisis predictivo, Ciencia de datos, Programación, Reconocimiento de patrones y Visualización de datos, Ciencias de la computación, Comunicación y Java",Solicitar
https://www.linkedin.com/jobs/view/3873770335/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=ViJMiSbT9eJMDGHKbQITEg%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Analyst/Engineer/Scientist - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 meses,"San Francisco, CA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 If you applied for a job and got emails from our Job Placement Program team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

 please check the below links to see success outcomes of our candidates  and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/ 

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 https://www.youtube.com/watch?v=OFoqPTNORew 

 https://www.youtube.com/watch?v=-HkNN1ag6Zk 

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://youtu.be/bJJl27D8bh0 

For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/

 We are looking for the right matching candidates for our clients 

 Please apply via the job posting 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Programmer 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java , javascript , C++ or software programming 
 Spring boot, Microservices, Docker, Jenkins and REST API's experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning Positions 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow 

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements. 

 No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3982299753/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=WPuaZ1rEGVNXMWUA1Y5OFA%3D%3D&trk=flagship3_search_srp_jobs,Cloud Data Engineer- Alteryx - 100% Remote - 1921,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 días,"Houston, TX","Acerca del empleo
 Location: 100% Remote

Term: 12 to 36 months

Conract Type: Must be W2 only (no c2c, no 1099)

Salary: Upto $128/hr DOE

Project Details

Data management and analytics visualization are growing areas of emphasis across the client. As a data software engineer (visual analytics support facilitator) within Enterprise Data Services, you will be an integral part of a dynamic team responsible for the effective utilization and daily support of Alteryx Designer and Alteryx Server.
This position is responsible for contributing to the success of the System's Alteryx Shared Service by leading process improvement initiatives related to content management, platform utilization, customer onboarding, and end user technical support. This role requires versatility and a drive to learn and pursue continuous improvement with the ability to work well in a diverse and cross-functional team.
Projects will range in duration and complexity, and they will require direct consultation with business partners, technical subject matter experts, and IT service providers.

Key Activities Include But Are Not Limited To

Work with the team to manage and support an Alteryx ecosystem including Alteryx Server and Alteryx Designer.
Alteryx Designer directly involved supporting users with designing workflows, schedules and troubleshooting simple and complex technical problems.
Recognize and fill gaps as necessary to resolve tickets and address inquiries when service requests spike.
Create and maintain detailed internal process and self-service knowledge base documentation.
Selected candidate will support the Alteryx user community by building relationships and communicating frequently.

 

Qualifications & Job Requirements

Minimum of 2+ years experience with Alteryx Server & Alteryx Designer.
Alteryx certifications preferred.
Moderate to high skills in data visualization and data management required.
Moderate skill with scripting and query languages such as SQL, Python, and PowerShell required.
Experience with AWS or other public cloud providers strongly preferred.
Strong customer service and communication skills, including the ability to patiently explain technical concepts to end-users of varying expertise via formal presentations and informal coaching/consultation.
Strong customer-focus, using their expectations and user experience as a basis to define what success looks like for the ecosystem
Strong organization and time management skills required.
Associate degree from a two-year college or technical school, or equivalent combination of education and/or work-related experience.
Bachelor's degree from a four-year college or university preferred.
May require some weekend or evening hours.
Ability to work independently and with general supervision and direction. May consult with more senior staff in decision making.

Note

No 3rd party vendors or candidates 
US Citizenship Required - Federal requirement","Analítica, Analítica de datos, Ciencia de datos, Minería de datos, Query Languages, Visualización y Visualización de datos, Alteryx, Expertos en la materia y Lenguaje de consulta (query)",Solicitar
https://www.linkedin.com/jobs/view/3984946352/?eBP=BUDGET_EXHAUSTED_JOB&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=1TtvOzgy9UjbBZQPYg0%2BJw%3D%3D&trk=flagship3_search_srp_jobs,DataStage Developer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 días,Estados Unidos,"Acerca del empleo
DataStage Developer 
Required Skills:
DataStage
SQL
Data Warehousing (Must have)
Desired Skills:
AWS
Python (Good to have)
Responsibilities:
Review and discuss briefs with key personnel assigned to projects.
Design and build scalable DataStage solutions.
Configure clustered and distributed scalable parallel environments.
Update data within repositories, data marts, and data warehouses.
Assist project leaders in determining project timelines and objectives.
Monitor jobs and identify bottlenecks in the data processing pipeline.
Test and troubleshoot problems in ETL system designs and processes.
Improve existing ETL approaches and solutions used by the company.
Provide support to customers regarding issues related to data storage, handling, and access.
Qualifications:
Bachelor's degree in computer science, information systems, or a similar field.
Demonstrable experience as a DataStage developer.
IBM DataStage certification or similar qualification","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Herramientas ETL, SQL, SQL Server Integration Services (SSIS) y Transact-SQL (T-SQL), Almacenamiento, Bases de datos y DataStage",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3980086716/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=y%2FsP87O%2BNoM54Ly9G9q6bg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Tampa, FL","Acerca del empleo
Tampa Florida - Full Time – Active Top Secret Clearance required

Altamira delivers a variety of analytic and engineering capabilities to the US National Security community, but the tech culture and the caliber of the individuals that bring these capabilities to fruition are what really set us apart. We’re a curious, responsive, dedicated bunch spread across many corporate cultures. Dayton, OH is highly focused on the Space-based mission set with a heavy emphasis on sensor exploitation and analysis; Tampa, FL focuses on ‘art-of-the-possible’ analytics of all kinds with an emphasis on graph technologies, NLP, and wrangling complex data sets; and all forces converge at our headquarters in the Northern Virginia/Washington DC area where we host our tech events and support engineering and analytic missions across several IC and DOD agencies.

While our work occurs in different states and different mission domains, we’ve got analytics at the heart of every operation and genuine curiosity for new methods, techniques, and solutions. Our specialties are data science and analytics, data engineering, software engineering, and end-to-end analytic solutions architecture. We’ve also got some awesome benefits like the Altamira Healthy Living program, with ongoing competitions and a flexible spending stipend for health and wellness-related items.

Description

As a Data Engineer you will:

Use SQL, Python, and ETL tools. Manage data flow and platforms
Develop and maintain data integration pipelines to consolidate disparate data
Identify data needs and proper methods for data gathering, extraction, integration, preparation, quality, and governance from internal and external sources in context of meeting customer expectations. 
Collaborate with other professionals to continuously build a broader and deeper data science and analytic capability relevant to the customer’s mission areas. 

Qualifications

Minimum of a Bachelor’s Degree
At least five years of experience in data engineering
Demonstrated problem solving skills 
Demonstrated presentation skills communicating complex analytics including, but not limited to, presenting at professional or academic conferences. 

Security Clearance

Due to the nature of this work, an active DoD Top Secret clearance and eligibility for SCI is required","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, Aptitudes para hacer presentaciones, Bases de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3833864750/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=TnX37d1O2ergrGBoQBVVMA%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Brooklyn, NY","Acerca del empleo
SYNERGISTICIT is aware that the Job Market is Challenging due to almost 300,000 Tech Layoffs within the past year due to which The Job market is flooded with thousands of laid off Techies who are competing with existing Jobseekers. For entry level Job seekers to get client interviews and jobs they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.

Since 2010 we have helped Jobseekers differentiate themselves by providing the clients with candidates who have the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this economy no client wants or has the resources to take an entry level person and spend resources on upgrading their skills and on top of that pay the jobseeker. That's the specific reason there are so many techies both experience and freshers who are unemployed.

Clients have now the option to hire remote workers from anywhere so for a Jobseeker its important to introspect and see how they can become better and have the skills and technologies to meet client requirements.

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped thousands of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates looking to make their careers in IT Industry

We welcome candidates with all visas and citizens to apply.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then you should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

Candidates who Lack Experience or are freshers with No actual on Job experience with projects with clients

Have had a break in careers

Lack Technical Competency Or Skills Being Demanded By Clients

Different visa candidates (Like OPT/H4EAD/L2EAD )who want to get employed and settle down in the USA

please check the below links to see success outcomes of our candidates

https://www.synergisticit.com/candidate-outcomes/

We are also silver sponsors at Oracle Cloudworld , Las vegas from sept 18-21st— please visit us

https://www.oracle.com/cloudworld/sponsor-listing/#synergistic-it

Watch the below videos of us participating at Industry events with the Top companies in Technology at Oracle Cloud world /Oracle Java one (Las vegas) and at Gartner Data Analytics Summit (Florida)

Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube

https://www.youtube.com/watch?v=OAFOhcGy9Z8

https://www.youtube.com/watch?v=EmO7NrWHkLM

https://www.youtube.com/watch?v=NVBU9RYZ6UI

https://www.youtube.com/watch?v=Yy74yvjatVg

SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube

If you have relevant skills and industry experience, please apply

For preparing for interviews please visit

https://www.synergisticit.com/interview-questions/

REQUIRED SKILLS For Java /Software Programmers

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Project work on the skills
Knowledge of Core Java , javascript , C++ or software programming
Spring boot, Microservices, Docker, Jenkins and REST API's experience
Excellent written and verbal communication skills

For data Science/Machine learning

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Project work on the technologies needed
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis

Please understand skills and relevant experience on real world projects are required by clients for selection even if its Junior or entry level position the additional skills and Project work with hands on experience building projects at client site are the only way a candidate can be picked by clients. If not having the skills or hands on project work at client site then candidates can optionally opt for skill enhancement to gain the required skills and project work.

No third party candidates or c2c candidates

please only apply to the posting

No phone calls please. Shortlisted candidates would be reached out.","Analítica, Analítica de datos, Ciencia de datos y Lenguajes de programación, Ciencias de la computación, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3964103369/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=QnNv3%2BkIv%2FxzS%2F2UvewNwQ%3D%3D&trk=flagship3_search_srp_jobs,Junior/Entry Level Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 semanas,"Los Ángeles, CA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this market also our candidates get multiple job offers and $100k + salaries.

please check the below links to see success outcomes of our candidates .

(url removed)

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación y Programación, Comunicación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3984344126/?eBP=BUDGET_EXHAUSTED_JOB&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=oRh5i%2BhQE6AA3j0ek62YVg%3D%3D&trk=flagship3_search_srp_jobs,MicroStrategy/ GCP Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 días,"Richardson, TX","Acerca del empleo
hybrid - (Richardson, TX ; Scottsdale, AZ ; Buffalo Grove, IL ; Wellesley, MA ; Smithfield, RI) - Local Candidates of either location - Local candidates under 60 Min commute of either location

Must have valid LinkedIn

Need DL and Visa Copy

MicroStrategy/ GCP Data Engineer 

Overview

Someone to work on converting several reporting projects from Teradata to Google Cloud Platform
Needs Cloud migration experience 
We are looking for strong MicroStrategy and SQL skills 
Good interpersonal and communication skills are required as well as attention to detail.

Must Haves

Must have strong MicroStrategy Experience
Cloud Migration experience 
Strong Google Cloud Platform skills
Strong SQL skills 
Data Conversion work: Ideally from Teradata to GCP experience 
Strong communication and collaboration skills, ability to communicate technical concepts and implications to business partners
Ability to handle multiple projects and activities in a timely, organized manner
High levels of self-motivation and attention to detail.","Extraer, transformar y cargar (ETL), Google Cloud , Ingeniería de datos y SQL, Atención al detalle, Comunicación, Comunicación interpersonal, Conversión de datos, MicroStrategy y Teradata",Solicitud sencilla
https://www.linkedin.com/jobs/view/3981619517/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=PeVXxx%2FDafOdHaJfEdfFfA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Remote | WFH,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 días,"Coppell, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Are you passionate about transforming raw data into actionable insights? We're seeking a Data Engineer to join our team and make a real impact. If you're a self-starter with a knack for data profiling, cleansing, and modeling, and you thrive in Agile environments, we'd love to hear from you!

Key Responsibilities

 Utilize your SQL skills to manage and analyze data efficiently.
 Profile, cleanse, and model data to drive insightful outcomes.
 Collaborate effectively in Agile environments and lead Scrum meetings.

Preferred Qualifications

 Experience with SAP and Oracle JDE.
 Familiarity with Jira and data visualization tools such as Tableau or Qlik.
 Strong communication skills and a proactive approach to problem-solving.

What We Offer

 A dynamic remote work environment.
 Opportunities to work on impactful projects with a supportive team.

If you're ready to leverage your data expertise and take your career to the next level, apply today!

Employment Type: Full-Time","Analítica de datos, Análisis de datos, Ciencia de datos, Ingeniería de datos , SQL y Visualización de datos, Comunicación, Contabilidad exigida por ley, JD Edwards y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3960108171/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=URPbkQEpisR09cco0%2Bvinw%3D%3D&trk=flagship3_search_srp_jobs,"Operations Data Engineer (Remote - Work From Anywhere, USA)","En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Media jornada
Coincide con tus preferencias de empleo. El tipo de empleo es Media jornada.
Sin experiencia",hace 1 mes,"Georgia, Estados Unidos","Acerca del empleo
Operations Data Engineer

CINC Systems is a fully integrated cloud-based homeowners’ association accounting and property management software system. We are the largest SaaS player in its space, growing rapidly to fulfill industry needs. Our organization thrives on our fun and fast-paced atmosphere, creating the perfect place to grow your career and we’re interested in expanding our team with the right individual, someone with hunger and motivation. We are looking for perseverance, grit, and a “do whatever it takes” attitude to contribute their skills for the operations team and for our customer’s growth.

Our team is building the future, and the Data Engineer will be a critical part of implementing client data, reliability and with a high degree of accuracy. This unique role will support our CINC implementation Project Managers and client onboarding. The Operations Data Engineer reports to the Director of Implementation Operations and supports all operational stages of implementations of new clients and new modules. Our team combines raw information from different sources to create clean, consistent data for our onboarding clients. These areas include data staging, loading, analyzing, management, data manipulation for our accounting application, and image creation for our CINC custom mobile apps. Our Data Operations Team works together on the evolution of our data tools, and we are looking to add the right person to our growing team. Our over-arching team objective is to free up implementation resources by building efficient, scalable deliveries for implementations and operational activities. Read on if this sounds like something that you can be passionate about with us.

What You’ll Do:

Dive into existing customer systems, pull integrated data sets, review data, reformat, provide findings and insights to the client, and import data into the client’s CINC database.
Work with our reliable processes to extract, transform, and load data, document and QA the results, learn our existing documented processes and help us build our future ETL processes.
Support ad-hoc reporting, data scrubbing, and customer-facing data analysis and clean-up, ensuring data is up to date and clear of outdated information.
Write, execute, and maintain Python scripts, VBA scripts, Excel macros and additional Excel tools. 
Create digital graphic designs utilizing customer logos for external facing mobile apps.
Work closely with the Project Management team to maximize the efficiency of the implementation process, including managing the inventory of client deliverables such as data sources, reports, task requirements and prioritization.


What You’ll Bring: 

Critical & creative thinking skills used to predict and avoid issues, and solution old and new issues resourcefully.
Strong team player with a flexible skillset and excels as a high-performing individual contributor with the ability to work effectively under pressure and meet established goals and objectives.
Very comfortable with technology, including Excel, Access, Canva, Adobe Illustrator, VBA, SQL, databases, Python, SQL and VBA. Some familiarity with writing code is required, does not need to be specifically Python or VBA.
Excellent verbal and written communications skills.
Great team player that works well with others, both inside and outside their team and organization.
Planning, organizing and effective time management skills, with the ability to complete work with a high degree of accuracy.
Experience in the following areas preferred but not required:
Data Analyst or Business Analyst
Database development or administration
Programming or previous Data Engineer work
Tech support
Working with offshore teams
Bachelor’s degree or equivalent work experience.
Programming experience a plus.

CINC is an Equal Opportunity Employer of women, minorities, protected veterans and individuals with disabilities.

Benefits

Work with an amazing team and leadership that really cares!
Opportunities for growth
Medical
Dental
Vision
Life insurance
Flexible Spending Plan (Healthcare & Childcare)
401 K via OneAmerica (up to 2% matching)
136 hours of PTO per year (5.23 hours accrued every pay period)
11 paid holidays per year
Annual bonus for all positions
Free snacks when in office","Analítica de datos, Desarrollo de base de datos y Visual Basic for Applications (VBA), Adobe Illustrator, Bases de datos, Canva, Diseño gráfico, Informes ad hoc, Macros en Microsoft Excel y Manipulación de datos",Solicitar
https://www.linkedin.com/jobs/view/3983812860/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=bhjIqAca6ChPK51ryVnLbQ%3D%3D&trk=flagship3_search_srp_jobs,BI Data Engineer - W2 Only,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Role – BI Data Engineer

Location – Remote PST

Remote

Need USC/ GC ONLY

Required Qualifications For BI Profile

BE/B.Tech in Computer Science or related field
5+ years of enterprise software design and development experience
Proficiency in SQL, with experience in writing and optimizing complex queries
Professional, practical programming using Spark, SQL, Scala/Python
Strong technical expertise in Azure, Big Data, Apache Nifi, Hadoop, HDInsights, ADF, ADW etc.
Experience working with reporting and visualization tools like Power BI, Tableau, or similar.
Hands-on experience with scripting languages like Powershell/Bash for automation and data manipulation tasks.
Extensive knowledge and experience in data warehousing, Streaming data processing (ETL), e-metrics/measurement, business intelligence, information retrieval, parallel and distributed computation
Experience in implementation of Cloud Computing concepts and platforms
Experience in analyzing very large real world datasets and hands-on approach in data analytics
Experience with Test Driven Development, Continuous Integration, Continuous Deployment, Telemetry etc.
Great design and problem-solving skills, with a strong bias for quality and engineering excellence
Excellent verbal and written communications skills
Excellent problem-solving and debugging skills with a solid understanding of testing practices
Proven sense of high accountability and self-drive to take on and see through big challenges
Experience working in a global delivery model
Experience with SCRUM, Devops or similar Agile development/implementation methodologies
Familiarity with automation tools – Visual Studio etc

Sagar Kumar 

Sibitalent Corp. 

Desk : +1 972 853 8270 Ext : 270

E-Mail: sagar.kumar@sibitalent.com

Website:www.sibitalent.com /www.exarcainc.com

Office – 101, E, Park Blvd.-Suite 600, Plano, TX 75074","Analítica de datos y Extraer, transformar y cargar (ETL), Buenas prácticas de pruebas, Comunicación, Datasets, Depuración de programas, Desarrollo de software, Diseño de software, Manipulación de datos y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3981060529/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Ic4lern2U5ztUA%2BME6UEeQ%3D%3D&trackingId=gkxR15IiPaipwhjkigfAkQ%3D%3D&trk=flagship3_search_srp_jobs,Expression of Interest - Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 días,"San Antonio, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.

The Marlee Talent Pool is a pilot project designed to: Help job seekers get discovered by our partners based on their anticipated hiring needs. Provide optional support and resources for job seekers in their career endeavours. Help individuals understand, and bring out the best in themselves and each other. The Marlee Talent Pool process: Once you express your interest, you will be asked to complete the Marlee work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise. About Marlee (Fingerprint For Success) Backed by 20+ years of research, Marlee’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviours, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high-performing teams.

Keep in mind that joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your Marlee work style assessment to match the hiring needs of our partners. Your feedback is a gift! Write to us via: hello@getmarlee.com to help co-create the future of recruitment, together. Powered by JazzHR","Almacenamiento de datos, Analítica de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3984112405/?eBP=BUDGET_EXHAUSTED_JOB&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=xt7WnT47DQVE5zgLDUTbcg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 4 días,"Wellesley, MA","Acerca del empleo
Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.

Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.

Position Summary

Understands the Enterprise data systems and acquires knowledge on the relevant processes need for project delivery.
Participate in project estimation process and provide inputs to Tech Lead.
Participate in Agile scrum activities/project status meetings on regular basis.
Participate in User story grooming/Design discussion with technical lead.
Analyzes complex Data structure from disparate data sources and design large scale data engineering pipeline.
Uses strong programming skills to build robust data pipelines for ETL (Extract / Transform / Load) processes, designs database systems and develops tools for data processing.
Perform all Data Engineering job activities EDW/ETL project development/testing and deployment activities.
Work closely with the developers on the ETL Jobs/Pipelines development.
Create the Project process/automation by integrating the involved components.
Documents data engineering processes, workflows, and systems for reference and knowledge-sharing purposes.
Implements data quality checks and validation processes to ensure the accuracy, completeness, and consistency of the data.
Be a team player and work with team members for Business solution and implementation.


Required Qualifications

1+ years of Experience with Python
1+ years of Experience with SQL
1+ years of hands-on Experience with a major cloud platform (GCP, AWS, Azure)


Preferred Qualifications

Experience with ETL (Extract, Transform, Load).
GCP Experience – BigQuery, Cloud SQL, Python, Cloud composer/Airflow , Cloud Storage, Dataflow/Data Fusion, DataProc.
Hands-on experience building and deploying data transformation and processing solutions using Teradata utilities (BTEQ, TPT, FastLoad & SQL Queries).
GCP – Data Engineer certification strongly preferred.
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.
Strong problem-solving skills and critical thinking ability
Strong collaboration and communication skills within and across teams
Knowledge in BI Tools Looker Studio, Tableau, PowerBI, etc
Must understand software development methodologies including waterfall and agile.
Health Care/PBM domain experience
Excellent communication and presentation skills.


Education

Bachelors Degree in computer science or engineering or equivalent work experience

Pay Range

The typical pay range for this role is:

$86,520.00 - $173,040.00

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.

For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits

We anticipate the application window for this opening will close on: 07/31/2024

Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud y Ingeniería de datos, Aptitudes para hacer presentaciones, Calidad de datos, Ciencias de la computación, Comunicación y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3954154392/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=Hk8OQgd22e92AJ2BHZjTIQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"California, Estados Unidos","Acerca del empleo
We are looking for a Data Engineer with an affinity for Product Management to help redesign and optimize our data schema to enhance our platform’s compliance reporting and anomaly detection capabilities. The ideal candidate will have technical aptitude and data engineering/analytics experience. They will be able to see the big picture and understand interdependencies while being able to dive into the bits and bytes when needed.

What You’ll Do

Optimize and redesign Kiteworks’s audit log schema to serve the growing needs for reporting and insight delivery to customers’ security and compliance teams.
Work with engineering to design and implement our data vision and ensure consistency across features.
Collaborate with other product managers to define audit log and data related requirements of features.
Communicate regularly with senior management, sales, marketing, business development, engineering and QA to strategize, plan, and build products.
Train sales, sales engineering, support engineering on newly introduced features and reports.

What You’ll Bring

4+ years in data engineering and/or product management.
Must be located in Pacific time zone, local to the SF Bay Area a plus.
Experience with global teams and willingness to meet 2-3 days a week in evening hours.
Experience with internet-based applications and platforms.
Experience with agile development processes.
Comfortable communicating and presenting to global teams and leadership.
Experience collaborating with design & research methodologies and using insights to make data-driven decisions.
Up to four trips to San Mateo headquarters (if fully remote), and up to four international trips per year.
Cybersecurity and compliance experience; expertise with databases, networks, hardware, firewalls, and encryption a plus, but not required.

Company Overview

Kiteworks is a highly profitable and growing software company making a meaningful impact in the security and compliance space. Our mission is to empower organizations to effectively manage risk in every send, share, receive, and save of sensitive content. To this end, we created a platform that delivers content governance, compliance, and protection to customers. The platform unifies, tracks, controls, and secures sensitive content moving within, into, and out of their organization, significantly improving risk management and ensuring regulatory compliance on all sensitive content communications.

Perks And Benefits

Base pay depends on many factors, such as location, education, experience, and skills. Base pay is only one part of Kiteworks competitive Total Rewards package that can include benefits, perks, equity, and bonuses. The base pay range is subject to change and may be modified in the future. 
MBO eligible 
100% paid Medical, Dental, Vision and Basic Life Insurance. Benefits begin on your first day! 
Option of Health Savings Account (HSA) or Flexible Savings Account (FSA) 
Generous paid time off (PTO) plus paid sick time, holidays, parental leave, and volunteer days off 
401k match program 
Eligible donation match program 
Referral Bonuses 
Stock equity -- every employee is granted stock options when they walk in the door 

Hiring Process

Kiteworks is an equal opportunity employer. We eagerly seek applicants of diverse background and hire without regard to race, color, gender identity, religion, national origin, ancestry, citizenship, physical abilities (or disability), age, sexual orientation, veteran status, or any other characteristic protected by law.

Interested in the details of our privacy policy? Read more here.

In compliance with applicable law, all persons hired will be required to verify identity and eligibility to work and to complete employment eligibility verification. Note: Our stewardship of the data of thousands of customers means that a criminal background check is required to join Kiteworks.

Please note that visa sponsorship is not available for this position.

Headhunters and recruitment agencies may not submit resumes/CVs through this website or directly to managers. Kiteworks does not accept unsolicited headhunter and agency resumes and will not pay fees to any third-party agency or company that does not have an active, signed agreement with Kiteworks.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Comunicación",Solicitar
https://www.linkedin.com/jobs/view/3979907409/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=3hrZn12gF7GV7ZEmpappQA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"126 US$K/año - 131 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Denver, CO","Acerca del empleo
Pinnacol Assurance does just one thing, and does it better than anyone: provide caring workers' compensation protection to Colorado employers and employees. And while we may be a little biased, we believe that our work shapes communities and changes lives.

We have big hearts and love big ideas. We've been around for more than 100 years, but don't let that fool you. Pinnacol is committed to taking care of Colorado employers and workers in the most innovative ways. We celebrate continuous improvement, new ideas, compassion, teamwork, integrity and excellence.

What you'll do:

As a Data Engineer, you play a key part in realizing our mission to provide customers with tools they love to use. As a member of the Data Transformation Team you will be responsible for data management tasks including design, development, and data security in a Google Cloud Platform environment. You will leverage your experience and skills to design solutions that will deliver Pinnacol data products while operating as a key contributor on a team of dedicated professionals working toward a common product vision.

What you can expect:

Build new data products and systems from various internal and third party data sources that use custom code in Python, SQL/NoSQL, Google Cloud-Composer(Airflow), Apache Beam and other technologies
Create reliable, maintainable and scalable data solutions
Champion software and data engineering best practices by developing, refining, iterating, integrating, testing, staging, and deploying maintainable technical solutions.
Develop secure solutions which adhere to data privacy laws and regulations using Data Loss Prevention toolsets and secure storage practices.
Enhance our platform capabilities and make recommendations to support a rapidly growing organization.
Identify system or program problems efficiently and effectively and propose pragmatic solutions.
Participate in code pull/merge request and design reviews
Create design documents that satisfy business requirements, follow adopted methodologies and lead to efficient, easy to maintain, and reliable systems
Document and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code
Remain current on the external environment for industry leading tools, technology, analytics and best practices to continuously improve Pinnacol's customer value proposition.
Train other employees as needed
Present business and technical subjects to team members or other internal groups
Perform other duties as assigned

What you need to be successful:

Knowledge of data engineering best practices and experience in implementing them through designing and implementing data solutions in cloud environments in one or more of the following cloud environments (GCP, AWS or similar)
Experience with ETL/ELT tooling and practices using batch (Airflow or similar) or near-real time (Spark, Apache, Flink, Kafka, or similar) methodologies
Experience with container technologies especially Docker 
Knowledge of Cloud-based Data storage and security best practices

Competencies:

Adaptation: Ability to adapt in order to work effectively in ambiguous or changing situations, and with diverse individuals and groups
Communication: Ability to communicate highly technical ideas to a wide variety of audiences, including technical and non-technical audiences
Innovation: Ability to generate novel solutions and creative ideas in order to identify system or program problems, and propose pragmatic solutions
Teamwork: Ability to work effectively with people and cooperate with others in developing and releasing software with a team

We can't do our work without people like you.

Our employees are extraordinary and committed to making a difference. Here's some of the ways we show our appreciation.

Our benefits go beyond the basics. You'll get to choose from diverse benefit offerings for medical, dental and vision.
We care about each other. We enjoy a positive, collaborative work environment. We are hard workers and high performers.
We love who you are. Pinnacol is on a journey to embody diversity, equity and inclusion. We're committed to creating a culture that deeply values differences, where everyone feels like they belong. 
Take a day (or 20!) off. Enjoy 20 paid days off your first full year plus 9 paid holidays.
Take care of yourself. Sign up for unique wellness programs, including on-site, company-paid fitness facilities and classes
Get your learning on. We promote a learning culture to help you master your current job and cultivate the skills of the future through a variety of on-site, online, and off-site professional development opportunities.
Give back and get paid. Through our employee volunteer program, Pinnacol in Action, employees receive paid time off to volunteer with Colorado nonprofits.
Share in our success. You'll have the opportunity to earn a quarterly incentive, up to 12 percent of your annual base salary, when your team exceeds their goals and objectives.

When we find the right person, we try to put our best foot forward with an offer that excites you. We consider what you'd like to be paid, the skills and experience you bring, what similar jobs pay in the Denver area and make sure there's equal pay for equal work among those you'll be working with. The compensation amount for this role is targeted at $126,000 - $131,000. Final offer amounts are determined by multiple factors including your experience and expertise and may vary from the amounts listed above.

Want to love your work? Apply today!

Pinnacol is committed to working with and providing reasonable accommodations to applicants with disabilities. To request assistance with the application process, please email recruiting_team@pinnacol.com.

This posting will close 14 days after being published; August 1, 2024

Salary Range

$126,000—$131,000 USD","Airflow, Analítica de datos, Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud y Ingeniería de datos, Comunicación, Documentos de diseño y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3963153541/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=JUWts8PjHCMy8bXDGKjkIg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,"Nueva York, NY","Acerca del empleo
Overview:

Data Engineers serve a unique and important role in daily operations at Wider Circle. Customer data is the bedrock of our business, and Data Engineering is responsible for laying the foundation for our success. Data Engineers work with internal and external stakeholders to gather, validate, clean and move data inside and outside the organization using technology and automation. Our data engineering team is also responsible for quality curation of data to ensure our products

You will be joining a talented, fully remote Data Science, Engineering and Analytics team that handles a wide range of requests including customer data processing, weekly report automation, new product development and complex data integration.

Company Overview

At Wider Circle, we connect neighbors for better health. Wider Circle's groundbreaking Connect for Life® program brings neighbors together in-person and online for health, wellness, and social activities that improve mental and physical health. We create webs of community circles by employing local and culturally competent engagement specialists, whose hand-on-hand approach to forming trusted circles is informed by a sophisticated analytics platform. We are on a mission to make the world a better place for older adults and disadvantaged communities.

Responsibilities

Conceptualize data architecture (visually) and implement practically into logical structures
Manage internal SLAs for data quality and frequency
As a partner to data science and analytics provide modeled data for analysis and investigation
Setting up data ingestion schemes of raw data into S3 and Redshift
Executing automation to deploy data pipelines
Provide expert support for solving complex problems of data integration across multiple data sets
Performing testing of data after ingesting and database loading
Updating and evolving our data ecosystem to streamline processes for maximum efficiency


Requirements

Technical Requirements

Experience with AWS or similar (S3, Redshift, RDS, EMR) 3+ Years
Strong abilities with SQL & Python 3+ Years
Experience using API's for data extraction and updating
Experience with Git and version control


Preferred:

Experience with Healthcare Data (Claims, CDAs/HRAs, Eligibility)
Experience using Salesforce (Salesforce API)
Matillion, Mulesoft or related tooling
Airflow, cron or other automation tools
Experience working with Data Packages written in R or Python
Experience partnering with Data Scientists to optimize or productionalize models


Benefits

As a venture-backed company, Wider Circle offers competitive compensation, including:

Performance-based incentive bonuses
Opportunity to grow with the company
Comprehensive health coverage, including medical, dental, and vision
401(k) Plan
Paid Time Off
Employee Assistance Program
Health Care FSA
Dependent Care FSA
Health Savings Account
Voluntary Disability Benefits
Basic Life and AD&D Insurance
Adoption Assistance Program
Training and Development
Salary $120,000-$140,000

And most importantly, an opportunity to make the world a better place!

Wider Circle is proud to be an equal-opportunity employer that does not tolerate discrimination or harassment of any kind. Our commitment to Diversity & Inclusion supports our ability to build diverse teams and develop inclusive work environments. We believe in empowering people and valuing their differences. We are committed to equal employment opportunity without consideration of race, color, religion, ethnicity, citizenship, political activity or affiliation, marital status, age, national origin, ancestry, disability, veteran status, sexual orientation, gender identity, gender expression, sex or gender, or any other basis protected by law","Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Bases de datos, Calidad de datos, Control de versiones, Modelado de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982246357/?eBP=BUDGET_EXHAUSTED_JOB&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=kXUL86SKZAatbQkdHp6O7w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"73 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 días,"California, Estados Unidos","Acerca del empleo
***W2 ONLY, C2C CANDIDATES WILL NOT BE CONSIDERED***

Are you a data architect passionate about building scalable data pipelines?

We're seeking a seasoned Senior Data Engineer to design, develop, and maintain our data infrastructure. You'll play a pivotal role in transforming raw data into actionable insights that drive our business forward.
What You'll Do:
Build Scalable Data Pipelines: Architect, develop, and optimize ETL processes to extract, transform, and load large datasets into our data warehouse.
Create a Semantic Layer: Develop ETLs and virtualized views to create a unified and accessible data layer for business users.
Collaborate and Innovate: Work closely with engineering teams to identify and integrate new data sources, driving data-driven decision making.
Master Data Orchestration: Leverage Apache Airflow or similar tools to efficiently manage and automate data pipelines.
Troubleshoot and Optimize: Identify and resolve data, system, and performance issues to ensure data quality and reliability.
Communicate Effectively: Collaborate with business and technical stakeholders to gather requirements, articulate data deliverables, and provide technical guidance.
What You'll Need:
Proven experience in designing and building ETL pipelines
Proficiency in Python for data processing and analysis
Strong SQL skills for data manipulation and query optimization
Experience with cloud-based data platforms, preferably AWS
Familiarity with data orchestration tools like Apache Airflow or AWS MWAA
Ability to thrive in a fast-paced, dynamic environment
Experience with data visualization and preparation tools such as Alteryx and Tableau","Amazon Web Services (AWS), Apache, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Tableau, Alteryx",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983485539/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=Bsh9boVU%2BXInr375lqioPg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Nueva York, NY","Acerca del empleo
About Pair Team

Pair Team is on a mission to improve the wellbeing of underserved communities by connecting them to high-quality care.

Pair Team cares for the highest-need Medicaid recipients through a community-led model. We build local partnerships with shelters, food pantries, and other community-based organizations to turn them into a site of care. As a support system for the community, we provide wraparound clinical services, up-skill CBO staff to become Community Health Workers, and utilize our proprietary data-driven technology platform, Arc, for care coordination. Through Medicaid MCOs, we provide healthcare for hard-to-reach, high-need individuals, while sharing healthcare dollars with community groups to expand their social support programs.

Our Values

Lead with integrity: We keep our commitments and take responsibility for our actions. We are dependable and choose authenticity over perfection.
Embrace challenges: We leave our egos at the door and step forward into discomfort instead of back into safety. We help each other to learn and provide feedback using candor and kindness.
Break through walls: We go the extra mile for our patients, partners and one another, and we run toward hard things. We are resilient in our push for consistent improvement and challenge the status quo.
Act beyond yourself: We build each other up and respect boundaries. We seek first to understand and assume positive intent.
Care comes first: We hold ourselves to the highest standards for our patients. We are relentless in the pursuit of our mission, and ensure that we are taking care of ourselves in order to care for others.

In the News

Forbes: For Pair Team, Accessibility Is About Delivering Healthcare To Those Who Need It The Most
TechCrunch: Building for Medicaid's regulatory moment with Neil Batlivala from Pair Team
Axios: Pair Team collects $9M for Medicaid-based care

About the Opportunity

We are seeking an experienced Data Engineer to own data engineering outcomes for our Tech team. We are a force multiplier, owning the data, analysis, and knowledge infrastructure that enables ourselves and our teammates to move faster and smarter. In this role you will have the ability to focus on the critical data problems at our company today, and play an instrumental part in impacting our future growth.

Your mission will be to empower decision making with data, maintain data integrity and security, enable scalability and agility. Your work includes ingesting data, building ETL pipelines and creating services and tools for others to use data more efficiently. You will work on developing and enhancing our data infrastructure, defining processes for data monitoring and alerting as well as maintaining data integrity in our data ecosystem. You will work with cross functional teams and internal stakeholders to define requirements and build solutions to meet the requirements. You will work with other engineers to ensure that our data platform and infrastructure are scalable and reliable.

This is a fully remote position.

What You'll Do

Collaborate with product managers, data scientist, engineers, clinical leaders and operational leaders to define requirements and data specifications and translate them into technical solutions
Develop, deploy and maintain data processing pipelines, datasets, and other solutions to support operational and product outcomes
Define and manage overall schedule and availability for a variety of data sets
Work closely with other engineers to enhance infrastructure, improve reliability and efficiency
Make smart engineering and product decisions based on data analysis and collaboration
Act as in-house data expert and make recommendations regarding standards for code quality and timeliness
Architect cloud-based data infrastructure solutions to meet stakeholder needs
Develop support systems to scale the team and achievable outcomes

What You'll Need 

Strong desire to work in an early stage startup environment that is fast paced, complex, and has minimal barriers to make decisions (no ""red tape"")
You have 4+ years of professional experience working with data pipelines, products, and tools and have built and maintained complex data integrations or pipelines
Experience working in a fast-paced startup environment OR experience working in a data-centric healthcare role
An undergraduate degree in Engineering, Computer Science, another equivalent field or experience
Familiarity with statistical modeling and some/all of the following: regression analysis, clustering, and decision trees
A well-developed opinion on modern data infrastructure, tools, and patterns.
Excellent technical vision; you know the right architecture and patterns to build functional, durable, and maintainable product and able to make the right trade-offs when considering project scope
Experience with products that automate or streamline complex workflows
Ownership mindset – own driving results for the mission, business, and customer experience through technical solutions
Strong collaboration skills with thought partners from engineering, design, product, operations, clinical and executive leadership
Clear and concise communication
Passion for helping individuals experiencing complex chronic needs such as homelessness, severe mental illness, and substance use disorder

Tech Stack

You should have experience with some of the following:

Experience with managing large Looker deployments or similar
Experience with cloud-based data warehouse: Snowflake
Experience with relational SQL and NoSQL databases
Experience with object-oriented/object function scripting languages: Golang, Python, Java, C++, Scala, etc.
Experience with data pipeline and workflow management tools like Airflow

Because We Value You

Competitive salary: $115,000 - $180,000 (depending on experience) 
Equity compensation package
Flexible vacation policy – take the time you need to recharge
Comprehensive health, vision & dental insurance
$50 employer contribution to active HSA accounts
401k through Guideline
Life insurance and AD&D
Work entirely from the comfort of your own home
Monthly $100 work from home expense stipend 
We provide the equipment needed for the role
Opportunity for rapid career progression with plenty of room for personal growth!

Pair Team is an Equal Opportunity Employer. At Pair Team, we value diversity and strive to provide an inclusive environment for all applicants and employees. All applicants will be considered without regard to race, color, religion, sex (including pregnancy, gender identity, and sexual orientation), national origin, marital status, age, disability, political affiliation, military service, genetic information, or any other characteristic covered by federal, state, or local law. 

Pair Team participates in E-Verify to verify employment eligibility for new hires.

Any offer of employment at Pair Team is conditioned upon passing a pre-employment background check. Following a conditional job offer, candidates will undergo comprehensive employment background checks, including; criminal history, reference checks, and driving records if a role requires vehicle use.

The talent team will only reach out via email from @pairteam.com email addresses. We do not conduct any TA business outside of our @pairteam.com emails. If you're ever concerned about spam or fraudulent activity, please reach out to recruiting@pairteam.com.

Note: Please be aware that while we sincerely appreciate your interest, due to the high volume of requests, we're unable to respond to general position inquiries sent to recruiting@pairteam.com. To apply for a position with us, please submit your application for the role you are interested in. Our team regularly reviews applications and will reach out to candidates whose qualifications align with our current openings listed below.

Thank you!","Airflow, Almacenamiento de datos, Analítica de datos, Canalizaciones de datos, Ciencia de datos y Ingeniería de datos, Bases de datos, Ciencias de la computación, Gestión de flujos de trabajo y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3890650480/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=zLvaqgH32EQqo9VilGojgg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (United States),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 4 meses,Estados Unidos,"Acerca del empleo
Our Solution

Demyst unlocks innovation with the power of data. Our platform helps enterprises solve strategic use cases, including lending, risk, digital origination, and automation, by harnessing the power and agility of the external data universe. We are known for harnessing rich, relevant, integrated, linked data to deliver real value in production. We operate as a distributed team across the globe and serve over 50 clients as a strategic external data partner. Frictionless external data adoption within digitally advancing enterprises is unlocking market growth and allowing solutions to finally get out of the lab. If you like actually to get things done and deployed, Demyst is your new home.

The Opportunity

As a Data Engineer at Demyst, you will be powering the latest technology at leading financial institutions around the world. You may be solving a fintech's fraud problems or crafting a Fortune 500 insurer's marketing campaigns. Using innovative data sets and Demyst's software architecture, you will use your expertise and creativity to build best-in-class solutions. You will see projects through from start to finish, assisting in every stage from testing to integration.

To meet these challenges, you will access data using Demyst's proprietary Python library via our JupyterHub servers, and utilize our cloud infrastructure built on AWS, including Athena, Lambda, EMR, EC2, S3, and other products. For analysis, you will leverage AutoML tools, and for enterprise data delivery, you'll work with our clients' data warehouse solutions like Snowflake, DataBricks, and more.

Demyst is a remote-first company. The candidate must be based in the United States.

Responsibilities

Collaborate with internal project managers, sales directors, account managers, and clients' stakeholders to identify requirements and build external data-driven solutions
Perform data appends, extracts, and analyses to deliver curated datasets and insights to clients to help achieve their business objectives
Understand and keep current with external data landscapes such as consumer, business, and property data.
Engage in projects involving entity detection, record linking, and data modelling projects
Design scalable code blocks using Demyst's APIs/SDKs that can be leveraged across production projects
Govern releases, change management and maintenance of production solutions in close coordination with clients' IT teams


Requirements

Bachelor's in Computer Science, Data Science, Engineering or similar technical discipline (or commensurate work experience); Master's degree preferred
1-3 years of Python programming (with Pandas experience)
Experience with CSV, JSON, parquet, and other common formats
Data cleaning and structuring (ETL experience)
Knowledge of API (REST and SOAP), HTTP protocols, API Security and best practices
Experience with SQL, Git, and Airflow
Strong written and oral communication skills
Excellent attention to detail
Ability to learn and adapt quickly


Benefits

Distributed working team and culture
Generous benefits and competitive compensation
Collaborative, inclusive work culture: all-company offsites and local get togethers in Bangalore
Annual learning allowance
Office setup allowance
Generous paid parental leave
Be a part of the exploding external data ecosystem
Join an established fast growth data technology business
Work with the largest consumer and business external data market in an emerging industry that is fueling AI globally
Outsized impact in a small but rapidly growing team offering real autonomy and responsibility for client outcomes
Stretch yourself to help define and support something entirely new that will impact billions
Work within a strong, tight-knit team of subject matter experts
Small enough where you matter, big enough to have the support to deliver what you promise


Demyst is committed to creating a diverse, rewarding career environment and is proud to be an equal opportunity employer. We strongly encourage individuals from all walks of life to apply.","Airflow, Extraer, transformar y cargar (ETL), Ingeniería de datos , JSON, Pandas (Software) y Validación del sistemas informáticos (CSV), Code::Blocks, Comunicación, Limpieza de datos y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3963956999/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=EoJHv79Mum5rvzxQT3Biyw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer CA,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
Data Engineer CA - Walnut Creek (Client Recruiter - Aisha simpson)

Job Description:

Identify all possible use cases and business outcomes for allocated areas of ownership
Prepare logging specifications and collaborate with engineering teams to instrument logging
Envision and build a self-serve data platform which teams can use to answer their own questions with data
Identify, collect and transform real-world user interaction, API's and server events data into scalable/extensible schema models
Create foundational data capabilities by leveraging your existing data leadership in addition to constantly learning about new technologies
Design and implement scalable data repositories to integrate qualitative and quantitative research data
Manage the delivery of high impact dashboards and data visualizations
Inform, influence, support, and execute our product decisions and product launch
Design, build and launch new data extraction, transformation and loading processes in Production
Must be able to lead a team and manage client's expectation

Basic Qualifications

Minimum 2 years' experience in Python
Minimum 2 years' experience in SQL
Minimum 2 years' experience in Data Visualization
High School Diploma/GED","Analítica, Analítica de datos, Ciencia de datos, Ingeniería de datos , Minería de datos, Python, SQL, Visualización y Visualización de datos, Liderazgo de equipos",Solicitar
https://www.linkedin.com/jobs/view/3982657082/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=Rjg8ITRlwbD%2BhIkj%2FzDrug%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, People Analytics","96 US$K/año - 180 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 6 días,"Seattle, WA","Acerca del empleo
Employee Applicant Privacy Notice

Who we are:

Shape a brighter financial future with us.

Together with our members, we’re changing the way people think about and interact with personal finance.

We’re a next-generation financial services company and national bank using innovative, mobile-first technology to help our millions of members reach their goals. The industry is going through an unprecedented transformation, and we’re at the forefront. We’re proud to come to work every day knowing that what we do has a direct impact on people’s lives, with our core values guiding us every step of the way. Join us to invest in yourself, your career, and the financial world.

The role:

We are looking for a talented and detail-oriented Data Engineer to tackle big data challenges. You will design, build, and maintain critical data pipelines and datasets, supporting areas like recruiting, compensation, talent management, and learning and development. Your work will enhance data accessibility and empower the People Team and business leaders to make informed decisions with high-quality, reliable data.

Key Responsibilities:


Develop and maintain robust data pipelines and datasets.
Build foundational data products for key business areas.
Enhance self-service data capabilities for the People Team.
Ensure high standards in ETL operations and big data pipeline management.


Join us to drive impactful change and support SoFi's mission of fostering a thriving workplace through data excellence.

What you’ll do:


Design and develop robust data architectures and data pipelines to support data ingestion, processing, storage, and retrieval. Evaluate and select appropriate technologies, frameworks, and tools to build scalable and reliable data infrastructure.
Optimize data engineering systems and processes to handle large-scale data sets efficiently. Design solutions that can scale horizontally and vertically.
Collaborate with cross-functional teams, such as other data engineers, people analysts, data scientists,, and business stakeholders, to understand data requirements and deliver solutions that meet business needs. Effectively communicate complex technical concepts to non-technical stakeholders.
Optimize data engineering systems and processes to handle large-scale data sets efficiently. Design solutions that can scale horizontally and vertically,
Enforce data governance policies and practices to maintain data integrity, security, and compliance with relevant regulations. Collaborate with data governance and security teams to implement robust data protection mechanisms and access controls.


What you’ll need:


A bachelor's degree in Computer Science, Data Science, Engineering, or a related field;
2+ years of experience in data engineering and analytics technical strategy. 
Strong Knowledge in data engineering tools and frameworks; Python / SQL / Orchestration Tools / Containers / etc..
Proficiency in relational database platforms and cloud database platforms such as Snowflake, Redshift, or GCP
Thorough knowledge of data modeling, database design, data architecture principles, data operations, OOP, and CI/CD.
Strong analytical and problem-solving abilities, with the capability to simplify complex issues into actionable plans.
Experience in the HR / People function is advantageous.


Compensation And Benefits

The base pay range for this role is listed below. Final base pay offer will be determined based on individual factors such as the candidate’s experience, skills, and location.

To view all of our comprehensive and competitive benefits, visit our Benefits at SoFi page!

SoFi provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion (including religious dress and grooming practices), sex (including pregnancy, childbirth and related medical conditions, breastfeeding, and conditions related to breastfeeding), gender, gender identity, gender expression, national origin, ancestry, age (40 or over), physical or medical disability, medical condition, marital status, registered domestic partner status, sexual orientation, genetic information, military and/or veteran status, or any other basis prohibited by applicable state or federal law.

The Company hires the best qualified candidate for the job, without regard to protected characteristics.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

New York applicants: Notice of Employee Rights

SoFi is committed to embracing diversity. As part of this commitment, SoFi offers reasonable accommodations to candidates with physical or mental disabilities. If you need accommodations to participate in the job application or interview process, please let your recruiter know or email accommodations@sofi.com.

Due to insurance coverage issues, we are unable to accommodate remote work from Hawaii or Alaska at this time.

Internal Employees

If you are a current employee, do not apply here - please navigate to our Internal Job Board in Greenhouse to apply to our open roles.","Arquitectura de datos, Extraer, transformar y cargar (ETL), Google Cloud y Ingeniería de datos, Amazon Redshift, Ciencias de la computación, Modelado de datos, Productos de datos, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3940378315/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=dcFfGKd88S8Moqx2EW3wcg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"76,4 US$K/año - 165,4 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 1 semana,"Redmond, WA","Acerca del empleo
The Azure Core Organization is responsible for creating the foundation of Microsoft’s Cloud Platform for utility computing. This platform is one of the lowest levels of the services software/hardware stack and includes an efficient, virtualized computational substrate, a fully automated service management system, and a comprehensive set of highly scalable storage services.

The Azure Compute Capacity and Efficiency (AC2E) team is the team in Azure Core tasked with managing all aspects of Compute capacity and efficiency management across the fleet. Capacity Management needs to ensure that on the one hand, there is sufficient capacity across all regions, allocation domains, and hardware infrastructure to meet all customer demand, while on the other hand ensuring that capacity is provisioned efficiently thereby avoiding overspending and COGS/CAPEX impact. At the scale of Azure’s business, managing this trade-off across the entire Azure Compute fleet is an enormously complex task, where improvements can make the difference between customer allocation failures on the one hand, and gargantuan savings on the other.

As a Data Engineer in the team, you will work closely with our software engineers, program managers, and data scientists across different teams within Azure Core. You will also collaborate with a variety of internal partner teams across Azure and Microsoft. You will build reliable, secured, highly scalable, performant data pipelines to enhance the Azure Compute allocation, deliver capacity management and efficiency improvements. The value of your work will be reflected in improvements to the Azure platform, Azure service capacity fulfillment rate, customer satisfaction, and various efficiency metrics, including COGS reduction. You will have opportunities for mentorship, accelerate your career growth, and work on truly high-business impact areas.

Microsoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond.

In alignment with our Microsoft values, we are committed to cultivating an inclusive work environment for all employees to positively impact our culture every day.

Responsibilities

Data Engineering, insights generation and analytics with deep domain knowledge understanding.
With detailed instructions, you’ll implement code to extract raw data, validate its quality, and ensure the correct data is ingested within your area of work after cooking, data transformation.
You will follow data modeling and handling procedures to maintain compliance with all applicable laws and policies across your assigned workstreams.
Integrating analytics intelligence into the Azure platform
Advocate for best practices in data engineering.
Other
Embody our Culture and Values
Qualifications

Required Qualifications: 

Bachelor's Degree in Computer Science, or related field OR equivalent experience.

Other Requirements

Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.

Additional / Preferred Qualifications

Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering , or related field AND 1+ year(s) experience in business analytics, data science, data modeling or data engineering work

OR Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field

OR equivalent experience.

Experience in business analytics, data science, software development, data modeling OR data engineering work.

Data Engineering IC2 - The typical base pay range for this role across the U.S. is USD $76,400 - $151,800 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $100,300 - $165,400 per year.

Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here:

https://careers.microsoft.com/us/en/us-corporate-pay

Microsoft will accept applications and processes offers for these roles on an ongoing basis.

#Azurecorejobs

Microsoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations.","Almacenamiento de datos, Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Ingeniería informática, Matemáticas y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3984655828/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=Mbi8pf%2BXr5NJe0iuiAcq4w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Jersey City, NJ","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Data Engineer (with Strong Python Coding)

Jersey City NJ (Hybrid role)-3 days onsite

USC,GC,GC EAD only

Must Have

Python, Pyspark (3-4 years)

AWS

Data Transformation exp.

Nice To Have

Certifications - AWS Solutions architect

Financial Domain

Summary

 Executes creative software solutions, design, development, and technical troubleshooting with ability to think beyond routine or conventional approaches to build solutions or break down technical problems
 Develops secure and high-quality production code, and reviews and debugs code written by others
 Identifies opportunities to eliminate or automate remediation of recurring issues to improve overall operational stability of software applications and systems","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , PySpark y Python, Bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3984547515/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=xwEKMhNAX21R2M6S4n1lGw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,"Dallas, TX","Acerca del empleo
Role: Data Engineer

Job Location: Hybrid(Candidate must be local to Dallas, TX, St. Louis, MO, Charlotte, NC or Denver, CO)

Duration: 12+ Months

Interview: Telephonic/Skype

Prefer USC/GC (Unfortunately client is not sponsering for this position)

Job Description:

Data Engineer who has solid experience in DevOps and CI/CD automation

 Experience building, configuring the automation of data pipelines using (ideally using Scala, Python or Java but open to other OOP languages)
 Experience automating the migration of data from on-premise to the cloud (AWS)
 Experience building EMR clusters in in Amazon (Amazon EMR)
 Experience configuring/ building containerization in Docker and/ or Kubernetes & experience building clusters in Docker/ Kubernetes","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3982281585/?eBP=BUDGET_EXHAUSTED_JOB&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=5S3MJQddI1CrEIyTsuN1Qg%3D%3D&trk=flagship3_search_srp_jobs,Temporary Data Engineer/Data Analyst Intern,Presencial Prácticas,hace 3 días,"Chicago, IL","Acerca del empleo
Department

Booth Marketing and Communications: Institutional Research and Strategic Insights: Data Analysts 2

About The Department

The University of Chicago Booth School of Business is the second-oldest business school in the U.S. and second to none when it comes to influencing business education and business practices. Since 1898, the school has produced ideas and leaders that shape the world of business. Their rigorous, discipline-based approach to business education transforms students into confident, effective, respected business leaders prepared to face the toughest challenges.

Chicago Booth has the finest set of facilities of any business school in the world. Each of the four campuses (two in Chicago, one in London, and one in Hong Kong) reflects the architectural traditions of its environs while offering a state-of-the-art learning environment.

Chicago Booth Is Proud To Claim

an unmatched faculty.
degree and open enrollment programs offered on three continents.
a global body of nearly 56,000 accomplished alumni.
strong and growing corporate relationships that provide a wealth of lifelong career opportunities.

As part of the world-renowned University of Chicago, Chicago Booth shares the University's core values that shape the distinctive intellectual culture. At Booth, they constantly question and test ideas, and seek proof. This extraordinarily effective approach to business leads to new ideas and innovative solutions. Seven of the Booth faculty members have won Nobel Prizes for these ideas - the first business school to achieve this accomplishment.

For more information about the University of Chicago Booth School of Business, please visit: http://www.chicagobooth.edu/.

Job Information

Job Summary

The Data analytics and Institutional Research (IRSI) teams in Chicago Booth School of Business are seeking a part-time temp/intern to support analysis, research and data engineering functions.

Responsibilities

Assist the Senior Data Scientist with data engineering tasks related to digital analytics and marketing research.
Assist in digital analytics functions including the creation of analytics dashboards in Adobe Analytics and analyzing data to help deliver insightful and valuable reporting, analysis and recommendations.
Aid in the documentation of analytics dashboards, processes, and best practices.
Collaborate with the analytics team to derive relevant metrics, targets, and data collection strategies, including but not limited to SEO, SEM, display advertising, email marketing, social media marketing in order to understand Key Performance Indicators (KPIs) of campaigns and marketing activities.
Collaborate with the IRSI team to collect, organize, and analyze data from various internal data systems as well as from external sources.
Help execute various research and analytics studies.
Work with the digital team to ensure the integrity of our data through ongoing maintenance and governance.

Competencies

Strong proficiency with data organization, manipulation, and analysis.
Strong attention to detail and prioritize and execute tasks on time.
Proven ability to successfully complete projects with stringent deadlines.
Work independently and demonstrated ability to collaborate.
Work discreetly with sensitive and confidential data.
Work independently with little supervision; possess a self-motivated disposition; troubleshoot/identify opportunities for improvement and recommend effective changes.

Additional Requirements

Education, Experience, or Certifications: 

Education

High school diploma/GED required.
Bachelor’s degree preferred. 

Experience

Relevant experience required.

Technical Knowledge Or Skills

Strong proficiency in survey building and coding through Qualtrics.
Proficiency in SQL and data processing libraries such as pandas. 

Working Conditions And Physical Requirements

This is a hybrid position.
This is a temporary, part-time position of approximately 15 hours per week.

Required Documents

Resume/CV

When applying, the document(s) MUST be uploaded via the My Experience page, in the section titled Application Documents of the application.

Benefit Eligibility

No

Requires Compliance with University Covid-19 Vaccination Requirement

No

Pay Frequency

Hourly

Pay Range

Depends On Qualifications

Scheduled Weekly Hours

15

Union

Non-Union

Job is Exempt

No

Drug Test Required

No

Motor Vehicle Record Inquiry Required

No

Health Screen Required

No

Posting Date

2024-07-25

Remove from Posting On or Before

2025-01-25

Posting Statement

The University of Chicago is an Affirmative Action/Equal Opportunity/Disabled/Veterans and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender, gender identity, national or ethnic origin, age, status as an individual with a disability, military or veteran status, genetic information, or other protected classes under the law. For additional information please see the University's Notice of Nondiscrimination.

Staff Job seekers in need of a reasonable accommodation to complete the application process should call 773-702-5800 or submit a request via Applicant Inquiry Form.

We seek a diverse pool of applicants who wish to join an academic community that places the highest value on rigorous inquiry and encourages a diversity of perspectives, experiences, groups of individuals, and ideas to inform and stimulate intellectual challenge, engagement, and exchange.

All offers of employment are contingent upon a background check that includes a review of conviction history. A conviction does not automatically preclude University employment. Rather, the University considers conviction information on a case-by-case basis and assesses the nature of the offense, the circumstances surrounding it, the proximity in time of the conviction, and its relevance to the position.

The University of Chicago's Annual Security & Fire Safety Report (Report) provides information about University offices and programs that provide safety support, crime and fire statistics, emergency response and communications plans, and other policies and information. The Report can be accessed online at: http://securityreport.uchicago.edu. Paper copies of the Report are available, upon request, from the University of Chicago Police Department, 850 E. 61st Street, Chicago, IL 60637.","Analítica de datos, Ciencia de datos, Ingeniería de datos , Pandas (Software) y SQL, Comunicación, Desarrollo educativo general (GED) de EE. UU., Microscopía electrónica de barrido, Procesamiento de datos y Qualtrics",Solicitar
https://www.linkedin.com/jobs/view/3984707293/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=VOvjLnATaGMiBYaxfLl%2BoQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Salt Lake City, UT","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Job Summary: Jerry Seiner is looking for an exceptional data engineer with experience in PostgreSQL, Big Query, and Python. As the sole member of the Business Intelligence team, the Data Engineer works closely with multiple departments and vendors throughout the organization to transform data into actionable information. The ideal candidate will have experience crafting visual data representations by using Excel, Vena, Tableau and Looker Studio. This position is in Salt Lake City, Utah and relocation would be required. A hybrid schedule is available after training. Key Responsibilities: Gather and document requirements for the data warehouse, cloud database solutions, and technologies. Help maintain and support ETL jobs and coding, pulling data from various source systems and loading data into the data warehouse. Testing new data pipelines or workflow processes. Develop and maintain databases, data systems, dashboards, and reports to facilitate data analysis and visualization. Perform data cleaning, validation, and preprocessing to ensure data accuracy and integrity. Conduct advanced statistical analysis and modeling to support business forecasting and decision-making processes. Participate in developing and implementing data-related projects, including data governance and quality initiatives. Design and implement intuitive and actionable user reporting using data visualization tools such as Tableau, Vena, and Looker Studio. Collaborate with stakeholders to understand and translate business requirements into data analysis solutions Interact with employees as a technical resource to troubleshoot problems with the delivered BI solutions. Ability to explain advanced data concepts to non-technical users and provide customer assistance and ad hoc training. Fulfill ad hoc data requests and analysis on time. Strong workload management skills and abilities, with an emphasis on the ability to prioritize and attention. Other Data Engineering tasks as determined. Required Qualifications Bachelor's degree in Computer Science, Statistics, Mathematics, Economics, or a related field. Two years of data warehouse experience including one year of experience working on projects with multiple deliverables Strong proficiency in data manipulation, analysis, and visualization using Big Query, SQL, Python, R, Excel, or Tableau tools. Experience with data querying, data modeling, and database design. Capable of developing ETL code. Solid understanding of statistical concepts and methods (e.g., hypothesis testing, regression, clustering). Excellent troubleshooting and problem-solving skills, attention to detail, and passion for working with complex datasets. Ability to effectively communicate technical information to both technical and non-technical stakeholders. Ability to quickly learn new tools and technology. Nice to Have: One year of experience in data analysis and report design/development using BI software such as Tableau, Looker Studio etc. Applicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time. Family-owned since 1980, The Jerry Seiner Dealerships believes in helping all employees find the career opportunity that is right for them. We encourage the individual growth and development of our employees through our many employee-focused benefits. For more information about our organization, please visit : Experience "" The Seiner Difference! "" At Jerry Seiner, we are a family-owned business proudly serving the Utah community since 1980 and have recently expanded into Arizona and Nevada. The Seiner culture believes in helping our team members achieve their highest level of success through training, mentoring, and career advancement. We encourage individual growth and development of our employees through our employee-focused benefits. Explore the many career opportunities that come with being a part of the Seiner team. Benefits may include : Paid Time Off (PTO) accumulates from Day 1 Grow with us! Educational reimbursement Health, Dental & Vision Insurance Employer Health Saving Account contribution each month 401k Retirement plan Year-end bonus programs Great discounts on vehicle purchases Discounts on parts and service Referral bonuses for vehicle purchases Closed Sundays! Pre-employment screenings, including but not limited to your background screening, drug test, and motor vehicle record, are required. Job Posted by ApplicantPro","Analítica de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Limpieza de datos, Manipulación de datos, Modelado de datos y Resolución de problemas",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3973767581/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=rvtVg8h32E%2FMhm1TYKVJVg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Analyst,"75,8 US$K/año - 120 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 3 días,"Carlsbad, CA","Acerca del empleo
Work Schedule

Standard (Mon-Fri)

Environmental Conditions

Office

When you’re part of a team at Thermo Fisher Scientific, you provide important work with real-world impact; and be enthusiastically supported in achieving career goals. Employees are valued and recognized for their performance, all while collaborating with engaged and dynamic teammates. This role supports our Biosciences division, which represents over $4B in sales annually.

We are looking for an experienced data engineer/analyst to support end-to-end data analytics projects for the divisional marketing team. Are you astute at simplifying sophisticated data with outstanding communication skills?

Location:

Hybrid role - 3 days a week in a local Thermo Fisher Scientific office

Primary responsibilities:

Leverage data integrations using systems such as AWS Athena, Redshift, Cognos, SharePoint and Power BI Services among other technologies

Propose data development optimizations and implement to deliver solutions and enhanced reporting capabilities

Provide timely and consistent updates and recommendations on dataflow and data pipeline operational issues and improvements to team and partners

Become a domain authority of data sources used by the Analytics team (Sales Force, Adobe Analytics, Redshift, internal databases, etc.) along with reports built

Consult data source owners to understand and detail data collection and reporting processes

Champion data quality and integrity through proactive sleuthing and oversight

Establish alerts for data connection issues both technical and beyond encouraged thresholds (anomalies)

Influence and maintain strong and balanced team culture through active involvement, inclusion, and leadership!

Proactive self-education and contribution to advance our analytics practices, methods, and strategy

Experience requirements:

 Experience in taking requirements and understanding the needs of the business to tackle problems
 Experience in the design and implementation of the different data processes like profiling, exploration, collection, processing, cleaning, and preparation of data, ETL, ELT, ETLT data processes and data pipelines with Python, SQL, Non-SQ, MS Power BI Services and Power Query
 Experience in semantic data models and schemas like flat, star and snowflake schemas
 Experience in crafting business intelligence reports/dashboards from scratch
 Strong graphics and visualization aesthetics to convey messages simply and impact-fully

Minimum Qualifications and Experience:

 Bachelor’s degree in the sciences or engineering

5+ years experience in a data analytics or engineering role with a solid understanding of technical, business, and operational process

Proven ability with Power BI Desktop and Service (or similar business intelligence and data visualization tool), M, Python and SQL desirable

Advanced MS Excel

Experience with GitHub and agile scrum

At Thermo Fisher Scientific, each one of our 90,000 extraordinary minds have a unique story to tell. Join us and contribute to our singular mission—enabling our customers to make the world healthier, cleaner and safer.

Thermo Fisher Scientific is an EEO/Affirmative Action Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status. We will make reasonable accommodation as required.

Compensation And Benefits

The salary range estimated for this position based in New York is $75,800.00–$120,000.00.

This position may also be eligible to receive a variable annual bonus based on company, team, and/or individual performance results in accordance with company policy. We offer a comprehensive Total Rewards package that our U.S. colleagues and their families can count on, which includes:

A choice of national medical and dental plans, and a national vision plan, including health incentive programs
Employee assistance and family support programs, including commuter benefits and tuition reimbursement
At least 120 hours paid time off (PTO), 10 paid holidays annually, paid parental leave (3 weeks for bonding and 8 weeks for caregiver leave), accident and life insurance, and short- and long-term disability in accordance with company policy
Retirement and savings programs, such as our competitive 401(k) U.S. retirement savings plan
Employees’ Stock Purchase Plan (ESPP) offers eligible colleagues the opportunity to purchase company stock at a discount

For more information on our benefits, please visit: https://jobs.thermofisher.com/global/en/total-rewards","Analítica, Analítica de datos y Extraer, transformar y cargar (ETL), Astute, Calidad de datos, Lenguaje de consulta (query), Modelo de datos, Panel de control, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3982754760/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=7wtDdwkZP4QsGA2CfkF%2FLA%3D%3D&trk=flagship3_search_srp_jobs,Principal Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,"Boston, MA","Acerca del empleo
The Role

Digital Therapeutics is creating innovative user journeys spanning patients, caregivers, and health care providers, creating unique opportunities for data-fueled products and features that will transform the definition and delivery of medicine. Akili is seeking a Principal Data Engineer to steer our operational and analytical data infrastructure into that digital future.

Responsibilities

Anchoring the Data Engineering team in solid engineering discipline, while delivering exceptional code.
Driving the strategic selection, design, and implementation of technologies to enable comprehensive availability and usability of data.
Managing computing/storage resources and third-party tooling, to balance performance and cost (both person-time and money).

Qualifications

10+ years as a software engineer, 5+ years specifically in data engineering
Expert-level SQL, high proficiency with Python (especially the Pandas/NumPy data stack)
Deep knowledge of storage, transport, and encoding of diverse formats.
Fluency with the AWS data ecosystem
Fundamental understanding of the principles for managing privacy and security of user data
Bonus: DevOps chops, productionisation of ML models

At Akili, We Are Committed To

Be Bold - We are pioneers. We take risks, embrace discomfort, aim high, and act fearlessly. We break down barriers for our patients.
Be Creative - We are inventors. Our success is dependent upon thinking beyond the status quo. We are extremely open-minded, take nothing for granted, and create unprecedented solutions for our patients.
Be Inclusive - We hear ALL voices. We encourage all input, embracing our strengths and leveraging our differences, and treat each other (and our patients) with utmost respect.
Be Accountable - We own it. We take responsibility for our own work and the success of our colleagues. We make, and keep our commitments. Our patients are counting on us to hold each other to the highest standard.
Be Well -We improve lives. We are committed and obsessed with the health of our patients, employees, and ourselves. All in service of improving the well being of the world.

About Akili Interactive

At Akili Interactive, our mission is to ignite new hope for those living with cognitive impairment. We’re bringing together world-class neuroscience with the latest technology and video game entertainment to create a new class of digital medicines, delivered through captivating video game experiences. We are out to prove that effective medicine for serious illnesses can also be fun and engaging.

Our flagship product, EndeavorRx*, is the first ever FDA-cleared prescription treatment delivered through a video game to treat children living with ADHD. This is only the beginning, as we continue to boldly challenge the status quo of today’s medicine. *

Diversity, Equity and Inclusion

Akili is, by our very nature, founded on diversity. We are composed of employees from incredibly diverse industries such as gaming, software, clinical research, biotechnology, and the pharmaceutical industry, all coming together with a common mission; to challenge the status quo of medicine. Inclusivity and wellness are 2 of our foundational cultural commitments.

Diversity and inclusion are incredibly important to all of us. We are committed to hearing all voices. Akili Interactive provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity or gender expression.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , NumPy, Pandas (Software), Python y SQL",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3981843385/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=7ii%2F7mUKhOkM4OL3LvUWWA%3D%3D&trk=flagship3_search_srp_jobs,Data Warehouse Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Dahlgren, VA","Acerca del empleo
Altamira delivers a variety of analytic and engineering capabilities to the US National Security community, but the tech culture and the caliber of the individuals that bring these capabilities to fruition are what really set us apart. We’re a curious, responsive, dedicated bunch and are looking for a Data Warehouse Engineer with scripting, database design/development using NoSQL, SQL, and other data warehouse experience to join our Joint Warfare Analysis Center (JWAC) team.

The Role: The Data Warehouse Engineer will contribute to creating a sustainable data supply chain in support of the JWAC’s data science program. This individual will work on a small team to develop methodologies for processing structured and unstructured datasets, and to inform computational environments.

Clearance Requirement

TS/SCI

Required Knowledge, Skills And Abilities

Bachelor’s Degree (BS) or higher in computer science, computer engineering, data analytics, mathematics, statistics and/or operations research discipline
Some experience in some or all the following: code writing, debugging, documenting, and testing, for software creation, data warehouse design and creation.
Minimum one (1) year experience applying Hadoop Ecosystem technologies. Preferred within the last (5) years.
Minimum one (1) year experience utilizing Linux configuration management
Work with little supervision to resolve problems to customer's satisfaction

Preferred Knowledge, Skills And Abilities

Minimum three (3) years' full-time experience in a computer science or computer engineering discipline.
Minimum one (1) year experience with scripting (e.g., PERL, PowerShell, Scala, Ruby)
Minimum one (1) year experience utilizing ETL technologies (to include at least one of these streaming capabilities: Storm, Kafka, NiFi, Spark).
Minimum one (1) year experience utilizing cloud services (Amazon Web Services preferred)
Minimum one (1) year experience with NoSQL search engine technologies (e.g., Solr, Lucene, HBASE, ElasticSearch).
Experience using an enterprise source control service. Git/GitHub is preferred.
Experience with Agile DevOps, including operations and development engineers participating together in the entire service lifecycle, from design, Test Driven Development (TDD), build and test automation, release configuration.

Altamira is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status.

Altamira requires all employees to be fully vaccinated against COVID-19. In accordance with applicable law, Altamira will provide an exemption to this requirement as a reasonable accommodation due to a disability or a sincerely held religious belief or practice that prevents receipt of the vaccine.","Almacenamiento de datos, Analítica de datos, Apache Kafka y Extraer, transformar y cargar (ETL), Apache NiFi, Ciencias de la computación, Depuración de programas, Documentación, Gestión de configuración y Ingeniería informática",Solicitar
https://www.linkedin.com/jobs/view/3956580558/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=g0hwyx0dnU%2FglOZCauUXlw%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Analyst/Engineer (Remote),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Atlanta, GA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3974877502/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=PgXCx33ekPgcB51z8cyXmA%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Analyst/Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Nashville, TN","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this market also we have been able to help our candidates get multiple job offers and $100k + salaries.

please check the below links to see the success outcomes of our candidates our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

(url removed)

We regularly interact with the Top Tech companies to give our candidates a competitive advantage see us exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like Apple, Google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry?

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

(url removed)

We are looking for the right matching candidates for our clients

REQUIRED SKILLS For Java /Full Stack/Software Programmer

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, and REST API experience

Excellent written and verbal communication skills

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, and data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Comunicación oral, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3972664077/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=z9Lmk%2FYpK5nQsy%2BUs04ZxQ%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Junior/Entry,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Baltimore, MD","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this Job market also, our candidates can achieve multiple job offers and $100k + salaries.

please check the below links to see the success outcomes and salaries of our candidates .

We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)(phone number removed)di

All Positions are open for all visas and US citizens

We at Synergistic understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and are looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelor's degree or master degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernetes, and REST API experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who match client requirements.

No phone calls, please. Shortlisted candidates would be reached out. No third-party or agency candidates or C2C candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3919922379/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=J8LE2omMKYZZBNKwwNZ8Vg%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"Flagstaff, AZ","Acerca del empleo
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this market also our candidates get multiple job offers and $100k + salaries.

 please check the below links to see success outcomes of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 https://www.youtube.com/watch?v=OFoqPTNORew

 https://www.youtube.com/watch?v=-HkNN1ag6Zk

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

 https://youtu.be/bJJl27D8bh0

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3984727065/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=Ocgfxwzgr3hhiAKfXKdi0Q%3D%3D&trk=flagship3_search_srp_jobs,Big Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Atlanta, GA","Acerca del empleo
We are looking for a seasoned big data developer to join our team.

Responsibilities

Creation and automation of data pipelines in Azure platform using SQL, PySpark and Python

Feature Engineering/Selection/Data Transformation for Forecasting modelling

Requirements

Role focuses on Feature Engineering/Selection/Data Transformation for Forecasting modelling
Minimum 8+ years of experience
Skills - PySpark , Data Bricks, Azure platform( Data lake, Blob, Azure Data Factory), Python (good to have).
Must be able to design a solution in ETL based on the requirements
Good in Data Analysis and sourcing.
Must have strong SQL/Pyspark programming.
Must be good at data analysis and profiling
Knowledge of Data Science & APIs will be an added advantage
Candidate should be able to work independently on the tasks assigned
Knowledge of Agile ceremonies.
PowerBI knowledge is a plus

Nice to have

Ready to help team with DS/ML tasks like model training, tuning and deploying to production

We offer

Opportunity to work on cutting-edge projects
Work with a highly motivated and dedicated team
Competitive salary
Flexible schedule
Benefits package - medical insurance, vision, dental, etc.
Corporate social events
Professional development opportunities
Well-equipped office

About Us

Grid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.","Analítica de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , PySpark, SQL y Sqoop, Apache Impala",Solicitar
https://www.linkedin.com/jobs/view/3983056247/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=mC3L8523tVqYspaWhKJ9sw%3D%3D&trk=flagship3_search_srp_jobs,Python Developer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 días,"Louisville, KY","Acerca del empleo
Title: Python Developer
Visa: USC, GC, H1b & EAD
Only on W2, No Sub-Vending
Must be able to start in 2 weeks
Location: 100% remote
 Roles:
30 total python developers---- still figuring out the technologies outside python (do not send me data engineers)

What I need to sub:
Need Valid Linkedin
Need one verified via LI manager (if not verified then I can’t use)",Python,Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982795427/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=BP2XHMC%2Bdk9unJwSCXjOPg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Robert Half, is seeking the following. Apply via Dice today!

Description

We are offering a long-term contract employment opportunity for a Data Engineer in Philadelphia, Pennsylvania. The role primarily involves working in the veterinary services industry, with a focus on data integration for partner hospitals. The workplace environment is collaborative, with team members working together on multiple platforms, including mobile applications and internal systems.

Responsibilities:

Work on data integration projects for partner hospitals, ensuring smooth access to internal platforms.

Utilize technologies such as Apache Kafka, Apache Spark, and Apache Airflow for optimal data processing.

Employ cloud technologies such as Google Cloud Platform for data stream operations and project completion.

Develop and maintain databases for efficient data management and retrieval.

Use algorithm implementation to create effective solutions for data processing challenges.

Work with Analytics, API Development, and AWS technologies to enhance data processing capabilities.

Leverage skills in EO/IR systems for optimal data handling and processing.

Be responsible for the completion of the data stream to ensure all data is processed accurately.

Work collaboratively with internal team members to ensure knowledge transfer and project success.

Actively engage in the monitoring and maintenance of customer accounts, taking appropriate action when needed.

Requirements

Proficiency in Apache Kafka and Apache Spark is essential

Demonstrated experience with Cloud Technologies such as AWS Technologies and Google Cloud Platform

Strong understanding of Database management and Data Flow

Familiarity with EO/IR systems

Proven ability in Algorithm Implementation

Previous experience in Analytics

Proficiency in Apache Hadoop

Experience in API Development

Knowledge of Apache Airflow is preferable

Technology Doesn't Change the World, People Do.

Robert Half is the world's first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go. Download the Robert Half app and get 1-tap apply, notifications of AI-matched jobs, and much more.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit roberthalf.gobenefits.net for more information.

2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking ""Apply Now,"" you're agreeing to Robert Half's Terms of Use.","Almacenamiento de datos, Apache Kafka, Apache Spark, Ciencia de datos, Desarrollo de API, Extraer, transformar y cargar (ETL), Google Cloud , Hadoop y Ingeniería de datos, Bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982967329/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=JOUvHEtLot6%2FA4PfiZRxJg%3D%3D&trackingId=d9oJVy6OFb%2BIA6LuoXSkrg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Irving, TX","Acerca del empleo
Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.

Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.

Position Summary

If you are passionate about making a difference in the work you do, being recognized for your talent and expertise in solving complex and challenging analytical/data problems while contributing to the success of key strategic initiatives, consider growing your career with a Fortune 5 healthcare leader!

As part of PSS Data Engineering and Analytics team within Analytics and Behavior Change department you will,

Analyze complex Data structure from disparate data sources and design, develop and deliver an analytic solution.
Enable data-driven strategic decision-making and identify opportunities for existing product improvement and new product development.
Implement data quality checks and validation processes to ensure the accuracy, completeness, and consistency of the data.
Build highly scalable and extensible data marts on prem / on cloud to support internal customers.
Experiment with available tools and advice on new tools to determine optimal solution given the use case requirements.
Translate business needs into analytical solution.
Use strong programming skills in SQL, Python, R, or any of the major languages to build data pipelines.
Use BI tools such as Tableau to generate and present key insights to stakeholders.
Document process, workflow, and methodology for reference and knowledge sharing purpose.


Required Qualifications

2+ years of business experience leading analyses and initiatives with track record of business impact
1+ years of experience developing analytic and reporting methodology
1+ years of experience building tableau dashboards or using any other data visualization tools to reflect business analytic needs
2+ years of experience using SQL or SAS or Python to access, extract, manipulate and summarize data
1+ year of experience working directly with business partners to understand the business needs and translating those needs into analytic solutions
2+ years of experience communicating complex technical subjects to non-technical audiences via both written and oral communication.


A master’s degree in the right discipline (engineering, MIS (management information systems), business analytics, business intelligence etc.) in lieu of experience will be considered.

Preferred Qualifications

Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources
Ability to understand complex systems and solve challenging analytical problems
Healthcare sector experience preferred
Demonstrates strong ability to communicate technical concepts and implications to business partners
Basic understanding of mathematical analysis methods, machine learning models, statistical analysis, and predictive modeling


Education

Minimum bachelor’s degree in engineering or analytics discipline is required, Advanced degree in an engineering or analytics field is preferred.

Pay Range

The typical pay range for this role is:

$72,100.00 - $144,200.00

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.

For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits

We anticipate the application window for this opening will close on: 07/24/2024

Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.","Analítica de datos, Ciencia de datos, Data Marts, Ingeniería de datos y Modelos predictivos, Análisis estadístico, Análisis matemático, Calidad de datos, Comunicación y Matemáticas",Solicitar
https://www.linkedin.com/jobs/view/3984571485/?eBP=BUDGET_EXHAUSTED_JOB&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=j1kSyc%2Fnw3ckaMbtxIRZ8w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Remote),"114,5 US$K/año - 168,7 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 3 días,Estados Unidos,"Acerca del empleo
What You’ll Do

As a Data Architect, you will develop multi-functional relationships and work closely with data engineering and data science teams to drive near- and longer-term initiatives to improve our product, and customer experience. Common Services Platform Engineering provides the basic building blocks for the Cisco Security Cloud with a great emphasis on Data Platform.

This would be a perfect fit for an experienced software specialist with system-level understanding and expert-level knowledge of end-to-end cloud solutions

Who You’ll Work With

A Data Architect in the in Cisco Secure Common Services Engineering, you will work with a team of cybersecurity experts and innovative engineers who support the products and developers across Cisco Security. We put our people first, we take bold steps together, and we value transparency each step of the way.

We’re adding more talented members to our growing team who will help us take Platform and Security to the next level because we believe that there is always room for growth.

Who You Are

To be successful in this role, you’d be a role model who exemplifies our culture and embraces our product principles. These include:

 Make it secure by design, private by default 
 Simplify. To quote Colin Chapman, founder of Lotus Cars “Simplify, then add lightness” 
 Move fast 
 Constantly improve 
 Design products people [really] love 
 Obsessing over the customer 

Minimum Qualifications

 BS or MS in Computer Science or equivalent combination of graduate degree and work experience 
 3 + years of work experience building Data and ML Infra, machine learning & NLP problems involving some or all the following parts of the pipeline: data extraction, developing and training machine learning models, working the new model to scale production deployments 
 3 + years of experience programming skills in Python, Scala, PySpark or C++ 

Preferred Qualifications

 3 + years of work experience architecting software infrastructure, developing, and delivering software products/services with one or more core cloud provider services, preferably AWS, GCP 
 Strong background of building and leading cloud platform with a focus on security, reliability, scalability and performance 
 Confirmed understanding of SaaS and PaaS 
 Proficiency in Data Structures, Micro services, CI/CD, automation, containerization technologies, Kafka Streaming, Spark, GitHub, Jenkins, etc 
 Work experience with DevOps tools and technologies 
 Strong written and verbal communication skills and excellent attention to detail and accuracy 
 Have a passion for complex data/engineering challenges 
 Have the ability to communicate ideas and solutions to both technical and non-technical audiences 

Why Cisco

Secure 

We're global, we're adaptable, we're diverse, and our security portfolio is as extensive as it is groundbreaking. Have you heard of Threat, Detection & Response, Zero Trust by Duo, Common Services Engineering, or Cloud & Network Security? Those are only a few of our product teams! The only thing we're missing is YOU.

Join an enterprise security leader with a start-up culture, committed to driving innovation and giving you the opportunity to make an impact. We #InnovateToWin and we know we're better together, that's why we're dedicated to inclusivity, collaboration, and diversity in everything we do.

We're proud to be the Best Small and Mid-Size Enterprises Security Solution Cisco Secure continues to grow and evolve year after year with 100% of Fortune 100 Companies using our products, and we're excited to see the new heights we'll reach with your passion for security, your customer focus, and your desire to change things up!

There are so many amazing reasons to join Cisco. Learn more here !","Ciencia de datos, DevOps, Ingeniería de datos , PySpark, Python y Scala, Atención al detalle, Ciencias de la computación, Comunicación y Comunicación oral",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3952360342/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=Wzphn6FRa%2B%2FS%2FW3mEtkC7w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (REMOTE),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,Estados Unidos,"Acerca del empleo
DivIHN (pronounced “divine”) is a CMMI ML3-certified Technology and Talent solutions firm. Driven by a unique Purpose, Culture, and Value Delivery Model, we enable meaningful connections between talented professionals and forward-thinking organizations. Since our formation in 2002, organizations across commercial and public sectors have been trusting us to help build their teams with exceptional temporary and permanent talent.

Visit us at https://divihn.com/find-a-job/ to learn more and view our open positions.

Please apply or call one of us to learn more

For further inquiries regarding the following opportunity, please contact our Talent Specialist

Lavanya at 224 369 0873 

Title: Data Engineer (REMOTE)

Location: Remote

Job Description

Client is seeking a dynamic, motived, energetic and driven Data Engineer!

Purpose And Scope

Design, develop and implement enterprise data movement, transformation, and storage to facilitate application data migration / transfer and reporting structures.

Utilizes technologies to facilitate enterprise data pipelines, spark notebooks, dataflows, data lakes, SQL Serverless, and data warehouse processes and artifacts from internal and external application data sources.

Essential Responsibilities

 Focuses on data as an entity and does analysis on data in terms of content, integrity, and security.
 Works on design and implementation of data flows/pipelines – focuses on control and optimization of data movement and transformation.
 Works on data integration between different systems or source/sink requirements.

Specific Job Duties

 Design, develop, test, and manage the overall framework to facilitate analysis and processing of enterprise data working closely with the Data Architect to direct and optimize the flow of data within the framework and ensure consistency of data delivery and utilization across multiple projects.
 Design how data will be stored, accessed, used, integrated, and managed by different data regimes and digital systems, working with data users to determine, create, and populate optimal data architectures, structures, and systems.
 Recommend and implement ways to improve data reliability, efficiency, and quality; shall evaluate, compare, and improve the different approaches including the design patterns innovation, data lifecycle design, data ontology alignment, annotated datasets, and elastic search approaches.
 Process, clean, and verify the integrity, accuracy, completeness, and uniformity of enterprise data sets, integrating external or new datasets into existing datasets, as required.
 Design, implement, and operate data management systems for business intelligence needs.
 Plan, design, and optimize data throughput and query performance.
 Build data and analytics proofs that will offer deeper insight into datasets, allowing for critical discoveries surrounding key performance indicators and user activity.
 Perform research and analysis of large data sets to include operational data and perform data validation and visualization and other statistical analyses.
 Support change management activities for enterprise data analysis. Document all processes, models, and activities.
 Perform all other position related duties as assigned or requested.

Work Environment, Physical Demands, And Mental Demands

Typical office environment with no unusual hazards, occasional lifting to 20 pounds, constant sitting while using the computer terminal, constant use of sight abilities while reviewing documents, constant use of speech/hearing abilities for communication, constant mental alertness, must possess planning/organizing skills, and must be able to work under deadlines.

Quality - Quality is the foundation for the management of our business and the keystone to our goal of customer satisfaction. It is our policy to consistently provide services that meet customer expectations. Accordingly, each employee must conform to the Amentum Quality Policy and carry out job activities in compliance with applicable Amentum Quality System documents and customer contracts. Each employee must read and understand his/her Quality Management and Customer Satisfaction responsibilities.

Procedure Compliance - Each employee must read, understand, and implement the general and specific operational, safety, quality and environmental requirements of all plans, procedures and policies pertaining to his/her job.

Minimum Position Knowledge, Skills, And Abilities Required

 Bachelor's degree in Computer Science or related field and 2-4 years of experience.
 Excellent communications and analytical skills; demonstrated working knowledge and experience of several of the following technologies/tools is desired:
 Microsoft SQL (T-SQL)
 Oracle SQL (PL-SQL)
 Azure Synapse
 Azure ADLS
 Azure Pipelines
 Azure Spark Notebooks (SCALA, Python, Spark SQL)
 REST API
 JSON Files
 Parquet Files
 Delta Lake Files
 Data Vault 2.0
 SQL Serverless
 Synapse Data Warehouse (MPP - Dedicated)
 Microsoft DevOps and GitHub
 Agile Development
 Data Encryption / Decryption

About Us

DivIHN, the 'IT Asset Performance Services' organization, provides Professional Consulting, Custom Projects, and Professional Resource Augmentation services to clients in the Mid-West and beyond. The strategic characteristics of the organization are Standardization, Specialization, and Collaboration.

DivIHN is an equal opportunity employer. DivIHN does not and shall not discriminate against any employee or qualified applicant on the basis of race, color, religion (creed), gender, gender expression, age, national origin (ancestry), disability, marital status, sexual orientation, or military status.","Analítica de datos, Integración de datos y PL/SQL, Análisis estadístico, Bóveda de datos, Ciencias de la computación, Datasets, Datos empresariales, Informática sin servidor y Validación de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3977455107/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=gBHMwx0nu2lnHaJo6m9PIA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
ABOUT US: 
CareQuest Institute for Oral Health® is a national nonprofit championing a more equitable future where every person can reach their full potential through excellent health. We do this through our work in philanthropy, analytics and data insights, health transformation, policy and advocacy, and education as well as our leadership in dental benefits and innovation advancements. We collaborate with thought leaders, health care providers, patients, and local, state, and federal stakeholders, to accelerate oral health care transformation and create a system designed for everyone. To learn more, visit carequest.org.

JOB SUMMARY: 
The Data Engineer will lead activities in expanding and optimizing our data pipeline architecture as well as optimizing the data flow. Design and build out data models used for analytical reporting and data science needs. Support data analysts and data scientists on projects and will ensure data are structured and delivered in a consistent manner throughout ongoing work. Manage the operational aspects of data platform solutions, including administering accessing controls, performance tuning, and automation of tasks. Develop and implement standard operating procedures associated with data engineering in line with organizational goals. Perform routine and ad hoc data extractions, create and deliver reports in a timely manner, and other analysis summaries as requested. Additional responsibilities include delivering routine information/data and assisting in the preparation & configuration of Azure Cloud environment.

PRIMARY JOB RESPONSIBILITIES:
Setup and configure data warehouse of dental and medical claims, practice management systems, electronic health records data, and other structured and unstructured health data.
Strong knowledge of Business Intelligence and Data Warehouse development methodologies
Experience with the following tools and platforms: Azure Data Factory, Azure Storage Account, Azure DevOps, Azure Storage Explorer, Databricks, and Snowflake.
Experience with data ingestion from various third-party data sources
Develop and implement standard operating procedures to ensure compliance with licensing, legal, and ethical requirements
Recommend the best practices for management, monitoring & optimization of data
Ensure that data pipelines and data stores are high-performing, efficient, organized, and reliable, given a specific set of business requirements and constraints.
Process and analyze extracts of data from practice management system such as Dentrix, EagleSoft, Curve Dental, EPIC, Ace Dental
Prepare routine and ad hoc data extractions, reports, and analysis summaries
Research and design Electronic Data Interchange processes
Serve as technical resource for all department staff
Lead technical implementation projects
Develop and maintain database and dashboard applications for monitoring measures by CareQuest Institute staff.
Handle technical application development projects as assigned
Perform new and existing set-up and maintenance processes
Other duties as needed or required. 

JOB QUALIFICATIONS:
Required:
Bachelor’s degree in computer science or a related information technology field
1-3 years prior related technical and business experience required
1-3 years of experience with SQL programming
1-2 years of experience with cloud database technologies, such as Azure, AWS, Google Cloud, Databricks, or Snowflake
Proficiency in the following technical applications/programs necessary
Relational Databases (SQL Server preferred)
Knowledge of one or more data science languages/programs (Python or R preferred) 

Preferred:
Experience with medical or dental claims data, electronic health records, and/or dental practice management software preferred
Experience using data analytics and/or report development tools like PowerBI and Tableau
Hands-on expertise with SQL, Python, Azure data services, and Snowflake experience building out a complex ETL/ELT pipeline
Experience implementing security and compliance requirements and working with different data modeling techniques.
Experience designing and implementing data warehouse methodologies  

PHYSICAL DEMANDS:
Incumbent must be able to communicate effectively.
Manual dexterity and sitting is required in carrying out position own position responsibilities (i.e., use of personal computer).
Ability to travel or move about within and outside serviced facilities required.
Incumbent works primarily in either a private or shared office environment.

The specific statements shown in each section of this description are not intended to be all-inclusive. They represent typical elements and criteria necessary to successfully perform this position.

** In accordance with CareQuest Institute for Oral Health’s Compliance Plan, all employees must conduct CareQuest Institute for Oral Health business and activities in accordance with applicable laws, regulations, professional standards and ethical standards and report potential compliance or ethical issues to CareQuest Institute for Oral Health’s designated Compliance Officer. **

CareQuest Institute for Oral Health’s Affirmative Action Program affirms our commitment to make reasonable accommodation for known physical or mental limitation of otherwise-qualified individuals with disabilities or special disabled veterans, unless the accommodation would impose an undue hardship on the operation of our business and activities. Please see Human Resources for additional information regarding this program.","Almacenamiento de datos, Canalizaciones de datos, Capacidad de análisis, Ingeniería de datos y SQL, Análisis sanitario, Definición de requisitos, Elaboración de informes de datos, Ingesta de datos, Modelado de datos y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984145635/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=xT1ln1XcgNClbh59J4FuxQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 4 días,Estados Unidos,"Acerca del empleo
*Must have or ability to obtain a US Government Security Clearance (Public Trust Level 5)*

About Us:
Metas Solutions is a professional services firm headquartered in Atlanta, GA. We are an 8(a)-certified, Woman Owned Small Business (WOSB/EDWOSB) headquartered in Atlanta GA with offices at StudioPlex in Atlanta's Old Fourth Ward neighborhood. Metas is a professional services firm that offers technical assistance and consultation to the federal government in the areas of public health capacity building and information technology ideation and implementation. See www.metassolutions.com for further details about us and careers with Metas.

Job Description:
Metas Solutions has an “immediate” opening for an experienced Data Engineer to work Remotely. The client is a one stop shop for supporting the acquisition, evaluation, management, and analytics of large healthcare data assets for their agency.

Responsibilities:
Provides operational, data management, technical/engineering, and analytic support along with tooling that enables analytics, visualization, and machine learning on large volumes of complex healthcare data for multiple public health use cases.
A dedicated engineering team is needed to develop, enhance, and scale technical capabilities to make effective use of the large healthcare data investments managed by client Data Hub in an Enterprise Data Analytics and Visualization (EDAV) platform.
These activities are critical to supporting client programmatic and response activities.

Qualifications:
Bachelor’s degree and 3+ years of experience with data engineering, developing, and implementing data pipelines.
3+ years of experience developing with Python and SQL in a professional environment.
Familiarity or experience with software delivery lifecycle practices, including continuous integration, configuration management, unit testing, and Agile development.
Experience working with and optimizing large-scale data systems.

Preferred Skills:
3+ years of experience working with big data technologies and distributed computing processing.
3+ years of experience working in Azure or AWS cloud in development capacity.
Experience with data ingest and analytics tools, including Data Factory, Databricks, Spark, Airflow, Power BI, Tableau.
Ability to implement ITIL methodologies.
Ability to quickly learn technical concepts.
Ability to communicate with multiple functional groups and leadership.
Ability to display a positive, can-do attitude to solve challenges and collaborate effectively.

Security, Salary, and Benefits:
Must be US Citizen or with the ability to obtain a US Government security clearance (Public Trust 5) within a reasonable period.
Market competitive salary, commensurate with experience and education.
Comprehensive benefits package available, Medical, Dental, Vision and Life Insurance, Paid Time Off (PTO), 401K with company match, growth, and promotion opportunities.

 We are an Equal Opportunity Employer/Veterans/Disabled","Azure Data Factory, Canalizaciones de datos, Microsoft Power BI, Microsoft SQL Server y Python, Azure Databricks, Gestión de configuración, Implantación de software, Implementación de ITIL y Microsoft Azure",Solicitar
https://www.linkedin.com/jobs/view/3984277014/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=EcYmuV7ZWB3xhdmmVF0v9Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 4 días,"Downers Grove, IL","Acerca del empleo
Overview

Good enough isn’t for us. Duly Health and Care’s team members show up every day driven to exceed expectations. We see and support the remarkable in every person within and beyond the walls of our work.

Duly Health and Care works to understand what matters most to you. We recruit and retain team members who share a relentless passion and pride for helping others live happier and healthier lives. We invest in helping our team members develop their talents in a way that is rich in personal meaning. We invite you to join us, fulfill your purpose and make your mark!

Holistic benefits designed to help our team members flourish in all aspects of their lives, including: 

Comprehensive medical and prescription drug benefits that include medical coverage at 100% (after deductible) when utilizing a Duly provider.
$5,250 Tuition Reimbursement per year. 
40 hours paid volunteer time off. 
A culture committed to Diversity, Equity, and Inclusion (DEI) and Social Impac
t12 Weeks parental leave at 100% pay and a financial benefit for adoption and surrogacy for non-physician team members. 
401(k) Match 
Profit-sharing program 

Responsibilities

Are you ready to challenge the expected to deliver the extraordinary?

The Data Engineer is responsible for expanding and optimizing data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer supports developers, architects, data analysts and data scientists on initiatives and will ensure optimal data delivery consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products.

Major Responsibilities

Create and maintain data pipeline.
Assemble data sets that meet functional / non-functional business requirements.
Implement internal process improvements: automating manual processes, optimizing data delivery.
Build extraction, transformation, and loading of data from a wide variety of data sources.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Keep data separated and secure across national boundaries.
Work with data and analytics experts to strive for greater functionality in our data systems.

Qualifications

The Experiences You Bring

Working knowledge of SQL and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience supporting and working with cross-functional teams in a dynamic environment.","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Datasets, Establecer prioridades del trabajo, Lenguaje de consulta (query) y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3979921780/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=K6XHamm2mDe7oelK40VPnw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,Estados Unidos,"Acerca del empleo
BDR Solutions, LLC, (BDR) supports the U.S. Federal Government in successfully achieving their mission and goals. Our service and solution delivery starts with understanding each client's end-state, and then seamlessly integrating within each Agency's organization to improve and enhance business and technical operations and deployments.

BDR is seeking a Data Engineer to join our growing team! The duties of this position can be performed 100% virtually from the candidate's home office, however the candidate must be able to support EST business hours. This position requires US Citizenship with a Public Trust or the ability to obtain one.

Data Engineer - (Military Veterans are highly encouraged to apply)

Responsibilities

Assisting both internal teams and external stakeholders with questions pertaining to health data received on claims and reference data sets.
Use SQL to run queries on the database.
Using data to solve technical issues, including (but not limited to) the use of this data to address VA concerns, mission objectives, and legislative priorities.

Required Minimum Qualifications

Bachelor's degree in information technology, Information Systems, or other related discipline ( 8 years of additional relevant experience may be substituted for education)
Requires Public Trust clearance
5 of years of experience in the capacity of Data Scientist, preferably in support of a Federal Agency
Experience working with the following software products: Adobe Acrobat, MicrosoftA® Office Suite, Microsoft SharePoint, Rally, Jira, GitHub
Excellent Communication Skills - it is incredibly important to describe findings to a technical and non-technical audience.

To do this, the candidate would need to understand structure, format, and the intended use of the following:

Health Data Exchange Standards

HL7
ASC X12
HIPAA (Transactions, Code Sets)

Health Reference Data

Diagnosis Codes (ICD-10)
Procedure Codes (CPT, HCPCS)
National Drug Codes (NDC)
Healthcare Provider Identifiers and Taxonomy (NPI)

Healthcare EDI Transaction Types (Professional, Institutional, Dental)

Healthcare Claim Transaction (837)
Healthcare Claim Payment/Advice (835)
HIPAA 5010 Coordination of Benefit (COB)
Retail Pharmacy Transaction (NCPDP)

Desired Skills and Qualifications

Familiarity and understanding of the following Healthcare EDI transaction types would be preferred but not required:

Benefits Enrollment and Maintenance (834)
Healthcare Eligibility/Benefit Inquiry (270)
Healthcare Eligibility/Benefit Response (271)
Healthcare Electronic Attachments (275)
Healthcare Claim Status Request (276)
Healthcare Claim Status Notification (277)
Healthcare Service Review Information (278)
Healthcare Explanation of Benefits (EOB)

In addition, U.S Citizenship is required. Select applicants will be subject to a government security investigation and must meet eligibility requirements for access to classified information and be able to obtain a government-granted security clearance. Individuals may also be subject to a background investigation including, but not limited to criminal history, employment and education verification, drug testing, and creditworthiness.

BDR is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, marital status, disability, veteran status, sexual orientation, or genetic information.
Aptitudes y experiencia deseables
DATA SCIENCE, DATA SCIENTIST, HL7","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Confianza ciudadana, Intercambio electrónico de datos, Sistema HCPCS y X12",Solicitar
https://www.linkedin.com/jobs/view/3978469478/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=XQnAb%2FXv0WccQX71Wg96Og%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"55,41 US$/h - 64,16 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,"Odessa, TX","Acerca del empleo
Description

We are offering a contract to hire employment opportunity for a Senior Data Engineer in Houston, Texas. This role is crucial to our corporate data technology unit, where you will be responsible for setting up and maintaining our data infrastructure. Your primary focus will be on improving and hardening our data lakehouse in AWS and leading technical projects through Agile methodologies. You'll work closely with Data Analysts and cross-functional teams to provide solutions that meet their requirements.

Responsibilities

 Design and implement scalable and reliable data solutions in AWS, optimizing data storage, processing, and retrieval.
 Lead technical projects focusing on Agile methodologies, ensuring efficient delivery of data engineering solutions.
 Collaborate with Data Analysts and stakeholders to optimize data integration according to their needs.
 Develop and maintain data pipelines, ensuring data accuracy and accessibility.
 Identify, design, and implement internal process improvements, including designing infrastructure for increased scalability and optimized data delivery.
 Work with data modeling, data warehousing, and building ETL pipelines.
 Implement data security and compliance measures.
 Monitor and optimize the performance of the data infrastructure.
 Assist stakeholders with data-related technical issues.
 Stay updated with the latest industry trends and technologies in data engineering.

Skills

 Proficiency in Apache Spark, Database, EO/IR systems, Erwin Data, HDFS, AB Testing, Analytics, AWS Technologies, Business Intelligence (BI), Business Requirement Document, Microsoft SQL, ETL - Extract Transform Load.

Requirements

 Proficient in Apache Spark
 Extensive knowledge of Database management
 Familiarity with EO/IR systems
 Experience with Erwin Data
 Understanding of HDFS
 Proficient in AB Testing
 Strong analytical skills
 Experience with AWS Technologies
 Knowledge of Business Intelligence (BI)
 Ability to prepare Business Requirement Document
 Proficiency in Microsoft SQL
 Experience in ETL - Extract Transform Load tasks
 Bachelor's degree in computer science, data science, information systems, or related field
 Excellent problem-solving and analytical skills
 Strong communication skills, both written and verbal
 Ability to work collaboratively in a team environment
 Strong attention to detail and accuracy.

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Comunicación, Erwin, Modelado de datos, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982658046/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=OCkb%2Bik1IB858dvC%2FCBdgQ%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, People Analytics","96 US$K/año - 180 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 6 días,"Nueva York, NY","Acerca del empleo
Employee Applicant Privacy Notice

Who we are:

Shape a brighter financial future with us.

Together with our members, we’re changing the way people think about and interact with personal finance.

We’re a next-generation financial services company and national bank using innovative, mobile-first technology to help our millions of members reach their goals. The industry is going through an unprecedented transformation, and we’re at the forefront. We’re proud to come to work every day knowing that what we do has a direct impact on people’s lives, with our core values guiding us every step of the way. Join us to invest in yourself, your career, and the financial world.

The role:

We are looking for a talented and detail-oriented Data Engineer to tackle big data challenges. You will design, build, and maintain critical data pipelines and datasets, supporting areas like recruiting, compensation, talent management, and learning and development. Your work will enhance data accessibility and empower the People Team and business leaders to make informed decisions with high-quality, reliable data.

Key Responsibilities:


Develop and maintain robust data pipelines and datasets.
Build foundational data products for key business areas.
Enhance self-service data capabilities for the People Team.
Ensure high standards in ETL operations and big data pipeline management.


Join us to drive impactful change and support SoFi's mission of fostering a thriving workplace through data excellence.

What you’ll do:


Design and develop robust data architectures and data pipelines to support data ingestion, processing, storage, and retrieval. Evaluate and select appropriate technologies, frameworks, and tools to build scalable and reliable data infrastructure.
Optimize data engineering systems and processes to handle large-scale data sets efficiently. Design solutions that can scale horizontally and vertically.
Collaborate with cross-functional teams, such as other data engineers, people analysts, data scientists,, and business stakeholders, to understand data requirements and deliver solutions that meet business needs. Effectively communicate complex technical concepts to non-technical stakeholders.
Optimize data engineering systems and processes to handle large-scale data sets efficiently. Design solutions that can scale horizontally and vertically,
Enforce data governance policies and practices to maintain data integrity, security, and compliance with relevant regulations. Collaborate with data governance and security teams to implement robust data protection mechanisms and access controls.


What you’ll need:


A bachelor's degree in Computer Science, Data Science, Engineering, or a related field;
2+ years of experience in data engineering and analytics technical strategy. 
Strong Knowledge in data engineering tools and frameworks; Python / SQL / Orchestration Tools / Containers / etc..
Proficiency in relational database platforms and cloud database platforms such as Snowflake, Redshift, or GCP
Thorough knowledge of data modeling, database design, data architecture principles, data operations, OOP, and CI/CD.
Strong analytical and problem-solving abilities, with the capability to simplify complex issues into actionable plans.
Experience in the HR / People function is advantageous.


Compensation And Benefits

The base pay range for this role is listed below. Final base pay offer will be determined based on individual factors such as the candidate’s experience, skills, and location.

To view all of our comprehensive and competitive benefits, visit our Benefits at SoFi page!

SoFi provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion (including religious dress and grooming practices), sex (including pregnancy, childbirth and related medical conditions, breastfeeding, and conditions related to breastfeeding), gender, gender identity, gender expression, national origin, ancestry, age (40 or over), physical or medical disability, medical condition, marital status, registered domestic partner status, sexual orientation, genetic information, military and/or veteran status, or any other basis prohibited by applicable state or federal law.

The Company hires the best qualified candidate for the job, without regard to protected characteristics.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

New York applicants: Notice of Employee Rights

SoFi is committed to embracing diversity. As part of this commitment, SoFi offers reasonable accommodations to candidates with physical or mental disabilities. If you need accommodations to participate in the job application or interview process, please let your recruiter know or email accommodations@sofi.com.

Due to insurance coverage issues, we are unable to accommodate remote work from Hawaii or Alaska at this time.

Internal Employees

If you are a current employee, do not apply here - please navigate to our Internal Job Board in Greenhouse to apply to our open roles.","Arquitectura de datos, Extraer, transformar y cargar (ETL), Google Cloud y Ingeniería de datos, Amazon Redshift, Ciencias de la computación, Modelado de datos, Productos de datos, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982653748/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=1%2FJlyPtYVRsI37I%2BBZvB4A%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, People Analytics","96 US$K/año - 180 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 6 días,"San Francisco, CA","Acerca del empleo
Employee Applicant Privacy Notice

Who we are:

Shape a brighter financial future with us.

Together with our members, we’re changing the way people think about and interact with personal finance.

We’re a next-generation financial services company and national bank using innovative, mobile-first technology to help our millions of members reach their goals. The industry is going through an unprecedented transformation, and we’re at the forefront. We’re proud to come to work every day knowing that what we do has a direct impact on people’s lives, with our core values guiding us every step of the way. Join us to invest in yourself, your career, and the financial world.

The role:

We are looking for a talented and detail-oriented Data Engineer to tackle big data challenges. You will design, build, and maintain critical data pipelines and datasets, supporting areas like recruiting, compensation, talent management, and learning and development. Your work will enhance data accessibility and empower the People Team and business leaders to make informed decisions with high-quality, reliable data.

Key Responsibilities:


Develop and maintain robust data pipelines and datasets.
Build foundational data products for key business areas.
Enhance self-service data capabilities for the People Team.
Ensure high standards in ETL operations and big data pipeline management.


Join us to drive impactful change and support SoFi's mission of fostering a thriving workplace through data excellence.

What you’ll do:


Design and develop robust data architectures and data pipelines to support data ingestion, processing, storage, and retrieval. Evaluate and select appropriate technologies, frameworks, and tools to build scalable and reliable data infrastructure.
Optimize data engineering systems and processes to handle large-scale data sets efficiently. Design solutions that can scale horizontally and vertically.
Collaborate with cross-functional teams, such as other data engineers, people analysts, data scientists,, and business stakeholders, to understand data requirements and deliver solutions that meet business needs. Effectively communicate complex technical concepts to non-technical stakeholders.
Optimize data engineering systems and processes to handle large-scale data sets efficiently. Design solutions that can scale horizontally and vertically,
Enforce data governance policies and practices to maintain data integrity, security, and compliance with relevant regulations. Collaborate with data governance and security teams to implement robust data protection mechanisms and access controls.


What you’ll need:


A bachelor's degree in Computer Science, Data Science, Engineering, or a related field;
2+ years of experience in data engineering and analytics technical strategy. 
Strong Knowledge in data engineering tools and frameworks; Python / SQL / Orchestration Tools / Containers / etc..
Proficiency in relational database platforms and cloud database platforms such as Snowflake, Redshift, or GCP
Thorough knowledge of data modeling, database design, data architecture principles, data operations, OOP, and CI/CD.
Strong analytical and problem-solving abilities, with the capability to simplify complex issues into actionable plans.
Experience in the HR / People function is advantageous.


Compensation And Benefits

The base pay range for this role is listed below. Final base pay offer will be determined based on individual factors such as the candidate’s experience, skills, and location.

To view all of our comprehensive and competitive benefits, visit our Benefits at SoFi page!

SoFi provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion (including religious dress and grooming practices), sex (including pregnancy, childbirth and related medical conditions, breastfeeding, and conditions related to breastfeeding), gender, gender identity, gender expression, national origin, ancestry, age (40 or over), physical or medical disability, medical condition, marital status, registered domestic partner status, sexual orientation, genetic information, military and/or veteran status, or any other basis prohibited by applicable state or federal law.

The Company hires the best qualified candidate for the job, without regard to protected characteristics.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

New York applicants: Notice of Employee Rights

SoFi is committed to embracing diversity. As part of this commitment, SoFi offers reasonable accommodations to candidates with physical or mental disabilities. If you need accommodations to participate in the job application or interview process, please let your recruiter know or email accommodations@sofi.com.

Due to insurance coverage issues, we are unable to accommodate remote work from Hawaii or Alaska at this time.

Internal Employees

If you are a current employee, do not apply here - please navigate to our Internal Job Board in Greenhouse to apply to our open roles.","Arquitectura de datos, Extraer, transformar y cargar (ETL), Google Cloud y Ingeniería de datos, Amazon Redshift, Ciencias de la computación, Modelado de datos, Productos de datos, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3841321915/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=q6jepxi9tobg3czGPN0%2FHg%3D%3D&trk=flagship3_search_srp_jobs,Data Visualization Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 meses,Estados Unidos,"Acerca del empleo
Job Title – Data Visualization Engineer

Job Location – 100% Remote -EST Hours

Duration - 6 Months+

Visa -USC,GC Only

Mode Of Interview - Phone/Skype

Note - Need LinkedIn

Job Description –

The reason they're needing to backfill this role is because it's clear the people they've had in the role are doing multiple projects and aren't engaged with this position as a full-time role.

Key to this role is experience with PowerBI in the Azure Cloud from the ground up. They don't care about Tableau or legacy PowerBI - they have all that. They need someone who can design and deploy dashboards in Power BI Services for Azure cloud, and we need to show this on resume and in write-up if possible.

Work samples are helpful but not required if the experience is demonstrated.

Candidates listing tons of different technologies (typical C2C resumes) that don't show the Power BI / Azure cloud for most recent project(s) will get rejected.

This person needs to interact with engineers and other end-users / stakeholders, and they've definitely rejected candidates who were heads-down report writer types.

This is part of the Data Governance team that includes the legacy Power BI folks so the ability to lead/train on the cloud side is important.

Interview process is via MS Teams one and done (1 hour).

Engagement says 6 months but they anticipate it will go much longer with extensions (I’ve had a few people who have contracted for 18-24 months).

SAIF folks need to have their own equipment (they don’t provide laptops).

Data Visualization Engineer

Our client is looking for a Data Visualization Engineer to join their team. This is a remote position.

Data Visualization Engineer Responsibilities

Integrate, analyze complex data and develop innovative insights from that data.

Work with business users and analysts to help identify data trends, and patterns within large, complex data sets.

Translate the story behind the data in an accessible way to non-technical audiences.

Create visually appealing and easily understandable dashboards and visualizations that convey data insights in a concise and meaningful way.

Prototype and develop advanced data-driven visualization components and interfaces that can be easily consumed by internal users as well as external customers and injured workers.

Research modern visualization and data access tools, evaluate fit for needs of the organization, facilitate, promote adoption among self service business users.

Data Visualization Engineer Qualifications

Strong PowerBI with Azure Cloud experience required.

10 or more years of experience in data analytics and data visualization.

Strong analytical and problem-solving skills, with a track record of delivering valuable data-driven insights.

Understanding of data modeling and database design principles.

Proficient in SQL and programming languages such as Python or similar.

Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams.

Ability to work independently and prioritize tasks in a fast-paced environment.

Strong attention to detail and commitment to data accuracy and completeness.","Analítica, Analítica de datos, Arquitectura de datos, Ciencia de datos, Minería de datos, Visualización y Visualización de datos, Bases de datos, Microsoft Azure y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3960676382/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=qprr9wE8nN4KiJPaTFna2g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
Job Description

🔎 #Conheça um pouco sobre a área: 

Estamos à procura de um Data Engineer Pleno para integrar nosso time de dados. O profissional será responsável por desenvolver e manter pipelines de dados robustos e escaláveis, garantir a qualidade e a integridade dos dados, além de colaborar com diferentes equipes para fornecer soluções de dados eficientes. Se você tem paixão por dados, habilidades técnicas avançadas e experiência com grandes volumes de dados, queremos conhecer você!

Responsibilities and assignments

🎯#Desafios que você vai encontrar:

Desenvolver, manter e otimizar pipelines de dados utilizando Apache Spark.
Implementar e gerenciar soluções de dados na plataforma Databricks.
Projetar e implementar arquitetura de dados na AWS, utilizando serviços como S3, Redshift, Glue, Lambda, entre outros.
Garantir a qualidade e integridade dos dados através de práticas de validação e monitoramento.
Colaborar com equipes de análise de dados, ciência de dados e outros stakeholders para entender necessidades de dados e fornecer soluções eficazes.
Participar na definição e implementação de boas práticas de engenharia de dados.
Documentar processos, pipelines e arquitetura de dados de forma clara e detalhada.

Requirements And Qualifications

✔️ #Conhecimentos importantes para a posição:

Formação em Ciência da Computação, Engenharia de Dados, Sistemas de Informação ou áreas relacionadas.
Experiência comprovada como Data Engineer ou em funções similares.
Conhecimento avançado em Apache Spark.
Experiência com Databricks e suas funcionalidades.
Experiência em arquitetura de dados na AWS, incluindo serviços como S3, Redshift, Glue e Lambda.
Sólidos conhecimentos em SQL e na modelagem de dados.
Experiência com ferramentas de ETL/ELT.
Capacidade de trabalhar de forma colaborativa em um ambiente ágil.
Boas habilidades de comunicação e capacidade de traduzir requisitos de negócio em soluções técnicas.

⭐ #Diferencial para a posição:

Experiência com outras plataformas de nuvem como Azure ou Google Cloud.
Conhecimento em linguagens de programação como Python ou Scala.
Experiência com ferramentas de orquestração de workflows, como Apache Airflow.
Certificações em AWS, Databricks ou outras relacionadas.

Additional information

💜 #PraVocê Nossos Beneficios:

PraVocê No Dia a Dia 🌞

Vale Refeição ou Alimentação; 
Gympass; 
Auxílio home-office; 
Flexibilidade de horários;

PraVocê & Família👩‍👩‍👧‍👦

Plano de Saúde e Plano Odontológico; 
Auxílio creche (até os 6 anos completos da criança):
Licença-Maternidade, Paternidade e Adotante Estendidas (#todasasfamíliasimportam); 
Seguro de Vida;
Day Off de Aniversário (Um dia de folga para tirar no dia ou durante o mês do seu aniversário.);
Dia da Família (Um dia de folga para mamães e papais tirarem entre os meses de maio e agosto e aproveitar como quiserem.);
Pausa Mental (Uma semana corrida de folga em JANEIRO para que descansem e recarreguem as baterias.).

PraVocê AINDA MAIS🔝

Senso de propósito ao fazer parte de um time que está construindo algo que será perene e trará frutos para a sociedade, seja a Cortex em si enquanto ""startup"" em rápido crescimento que gerará cada vez mais empregos, seja o produto Plataforma Cortex, que será usado por cada vez mais usuários no Brasil e exterior.
Ambiente de trabalho descontraído, jovem, empreendedor e meritocrático, sem espaço para política.. ;-)
Oportunidade de desenvolvimento de carreira e crescimento numa empresa que não para de crescer.

Valorizamos, cultivamos e respeitamos as diferenças, por isso proporcionamos um ambiente aberto e inclusivo. 

Todas as nossas posições são elegíveis para pessoas com deficiência.

Process stages

Step 1: Registration1Registration
Step 2: Screening People 📞2Screening People 📞
Step 3: Talk People 💬3Talk People 💬
Step 4: Avaliação Técnica 📝4Avaliação Técnica 📝
Step 5: Match Cultural 💜5Match Cultural 💜
Step 6: Proposta 💰6Proposta 💰
Step 7: Hiring7Hiring

Tecnológica, inquieta e ousada, bem-vindo à Cortex!🚀

Somos a Cortex, a plataforma brasileira líder em inteligência de Go-To-Market e o maior investimento em Inteligência Artificial aplicada a negócios na América Latina. Nosso propósito é transformar com inteligência o caminho de pessoas e negócios.

Usamos IA e Ciência de Dados para promover uma gestão completa e contínua do Go-to-Market de marcas líderes no país. Nossa plataforma é a única que coleta e normaliza dados externos, sejam eles firmográficos, de mídia ou de localização, e os unifica com dados internos de clientes para criar painéis de analytics flexíveis, análises geográficas e fluxos de trabalho baseados em dados.

Grandes investidores acreditam no nosso negócio. Temos como parceiros alguns dos maiores e principais fundos de investimento em tecnologia, como Lightrock, Riverwood Capital e SoftBank.

VEM FAZER PARTE DO TIME!

Aqui na Cortex você encontrará um ambiente receptivo às diferenças e às necessidades das pessoas. Crescemos rápido porque crescemos juntos.

Tudo isso faz com que ser cortexiane seja uma experiência única de descobertas, transformação e crescimento a partir de um ambiente colaborativo de criação e realização. Vamos juntos?

#BeCortex","Airflow, Almacenamiento de datos, Analítica de datos, Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , SQL y Scala, Amazon Redshift",Solicitar
https://www.linkedin.com/jobs/view/3982521052/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=j77%2BepPnNbFdNVpidCcabw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 6 días,"Tallahassee, FL","Acerca del empleo
Job ID: 57620

Position Number: 00081638

Job Title: Data Engineer

Department: Information Technology Services

Location: Tallahassee, FL

Responsibilities

Provides technical support and maintenance of our ETL platforms and other data integration tools and processes. Assists in identifying and implementing improvements and enhancements to our data integration systems, methods and processes. Also analyzes and resolves a variety of issues and program configurations involving and supporting our data integration and analytical systems and processes.

Qualifications

Bachelor’s degree in computer science, MIS or related field and 3 years of experience in computer programming or a closely related field. Alternatively, the employer would also accept a Master’s degree in computer science, MIS, or other related field and 1 year of experience in computer programming or a closely related field.

Considerations

This is an A&P position.

This position requires successful completion of a criminal history background check. THE BACKGROUND CHECK WILL BE CONDUCTED AS AUTHORIZED AND IN ACCORDANCE WITH UNIVERSITY POLICY 4-OP-C-7-B11.

How To Apply

If qualified and interested in a specific vacancy as advertised, apply to Florida State University at https://jobs.fsu.edu.

Applicants are required to complete the online application with all applicable information. Applications must include work history and all education details (if applicable).

Equal Employment Opportunity

An Equal Opportunity/Access/Affirmative Action/Pro Disabled & Veteran Employer. FSU’s Equal Opportunity Statement can be viewed at: http://www.hr.fsu.edu/PDF/Publications/diversity/EEO_Statement.pdf

recblid s9w7g1yp002nx26urpk6wy5i7be630","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3946685302/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=5O5rStiwBJxha4ffLe7now%3D%3D&trk=flagship3_search_srp_jobs,Data Warehouse Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 1 mes,"Commack, NY","Acerca del empleo
We are IntelliShift, a rapidly growing B2B SaaS company with more than 20 years of expertise in fleet management technology. IntelliShift is the fleet intelligence platform for safety and operations teams, and we empower construction, utilities, field services, and last mile delivery businesses to make the intelligent shift from siloed data using point solutions, to one simple, powerful platform. We provide these customers with a level of insight they've never had before to improve safety, establish next generation operational efficiency, and make intelligent decisions.

We are looking for an experienced Data Warehouse Engineer to join our software development team. We are focused on enabling our customers to make data-driven decisions that deliver value to their shareholders. This includes enabling analytical insights into their business operations, fleet management, and driver behavior. Solving these challenges involves working with technologies on the leading edge of data gathering, processing, and warehousing.

What you will do:

Help design, build, and maintain a multi-terabyte data warehouse used across several functions and geographies
Participate in project planning, estimation and status activities
Research and resolve support tickets and escalations
Monitoring and management of data quality procedures and practices
Design, development, monitoring, and management of ELT/ETL pipelines and procedures
Design and development of reports, stored procedures and functions
Stay on top of new technology developments in data warehousing, big data processing, and other relevant fields. 


What you bring:

2+ years' experience of Snowflake Data Warehouse or similar technology
Bachelor's degree in computer science or related field
4+ years and Intermediate to Advanced knowledge of SQL
4+ years and Intermediate to Advanced stored procedure/function development
4+ years and Intermediate to advanced stored procedure/function development
Solid understanding of data warehousing concepts and architecture
Experience developing data warehouse reports and analytic queries. 
2+ years' experience in SSRS or similar reporting tool
Attention to detail
Ability to research data issues and resolve them in a timely manner


Preferable skills:

Data replication concepts
Experience developing ETL/ETL using stored procedures
Experience with JavaScript development
Machine Learning experience


The values you'll live by as part of the team:

Always Put the Customer First - with the customer experience in mind, build trust and loyalty
Embrace and Drive Change - have an innovative mindset and embrace the change
Think Bigger - commit to growing the organization and grow as an individual. 
Be a Good Human - treat everybody with respect and always do what is best
Execute with Passion and Urgency - we need to be the very best at what we do. 
Drive Trust and Transparency - open and honest communication, trust each other and take risks. 


Benefits

We offer competitive compensation, commensurate with experience $115,000 - $125,000. We also offer outstanding benefits to simplify the lives of our employees and show them how much we appreciate their contributions. IntelliShift provides company-subsidized medical insurance for all employees (and largely subsidized coverage for families), dental, vision, and 401K with a 4% company contribution.","Almacenamiento de datos, Extraer, transformar y cargar (ETL) y SQL, Ciencias de la computación, Comunicación, Herramientas de informes, Procedimientos de almacenado, SQL Server Reporting Services (SSRS), Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3969248814/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=a%2Bn5w%2B2qOq2rG%2FRUuLIbsg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Miami, FL",,", ",Solicitud sencilla
https://www.linkedin.com/jobs/view/3977512993/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=i%2B61srAjFo8SJ%2BkC%2B%2Fioag%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"New Rochelle, NY","Acerca del empleo
This is a remote position.

 DISCLAIMER:  This job posting is intended for active pooling of candidates who will become part of our talent pool. Your qualifications will be assessed against both current and future job openings. Should your application align with a role that corresponds to your skills and experience, and an opportunity arises, our recruitment team will reach out to you immediately. Please note that this does not guarantee immediate placement or contact. Additionally, we exclusively consider applications from individuals who are currently reside in the US/Canada during their application process.

 Salary: $65,000 - $75,000 per annum

 Experience Required:  Minimum 1 year of project experience

Summary:

The National Analytics Team at Phoenix is responsible for managing client projects that focus on employers’ health benefits strategies, including financial projections, carrier selection, plan design optimization, employee contribution setting, IBNR valuations, benchmarking, well-being initiatives, and renewal negotiations.

As a part of the R&D team, the Data Scientist helps to develop new models and tools that our local consultants will use to service their individual clients. This position provides the opportunity to work with many key thought leaders across each region as we further innovate and digitize our overall strategy when it comes to consulting.

Essential Duties and Responsibilities (include but are not limited to):

Assist in developing models/tools that will be used across the company 
Assist in audits to ensure integrity of data 
Debug, test, and maintain new and existing models 
Provide R&D support and manage model enhancement requests 
Train users/answer questions on new models/developments 
Actively seek out ways to improve processes, reduce risk, increase productivity and lower costs 

Skills:

2+ years of experience with advanced Excel /VBA best practices to create Macros with code that is easy to understand, transferable and easy to maintain 
Experience working with visualizing tools such as Tableau, Power BI, Domo, Pivots/Advanced Excel 
Proficiency in data visualization and scripting/programming languages (SQL, R, Python) desired 
Comfortable working with complex/big data sets 
Self-motivated, Well-organized high achiever with the ability to handle several projects/tasks simultaneously 
Fast learner with strong problem solving and quantitative skillset 
Critical thinker with ability to see the big picture 
Dedicated work ethic with a commitment to client service excellence 

Education, Training and Experience:

Bachelor's/Master's degree in fields such as: data science, data engineering, data mining, computer science, computer engineering, statistics, mathematics, or equivalent quantitative discipline 
2-4 years of relevant work experience 

Relevant health actuarial consulting and/or health plan/insurance company experience (preferred)","Analítica, Analítica de datos, Ciencia de datos, Reconocimiento de patrones, Visual Basic for Applications (VBA) y Visualización de datos, Ciencias de la computación, Espíritu de superación, Macro y Resolución de problemas",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982760001/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=TnV7nziQJgZ%2BN%2BVna0xogQ%3D%3D&trk=flagship3_search_srp_jobs,Senior Analytics Data Engineer,"170 US$K/año - 210 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,"San Francisco, CA","Acerca del empleo
At Trunk, our mission is to help teams create high-quality software quickly. Merge conflicts, poor code quality or consistency, flaky tests, and dozens of other distractions quickly drain the productivity and morale of those teams. Engineering teams that can stay focused on designing, implementing, and delivering software will build magical, high-quality projects - and they will be happier doing it. We're building the tools that empower teams to land code faster and develop happier.

We are building the foundation for a modern software engineering team. Our founders started this journey in 2021 and have designed, delivered, and scaled software at some of the world's largest and fastest-growing tech companies - Uber, Google, YouTube, and Microsoft. We're building a game-changing company, and we hope you are excited to be a part of that audacious goal.

Software has eaten the world; almost every company produces software in some form or fashion, so our addressable market is virtually every company on earth. We're going after every engineering team on the planet - we're starting with smaller teams, but there are literally hundreds of thousands of companies out there for us to empower and maybe only a handful (Google, Facebook, Amazon), that are outside our scope. We are building the DevEx platform to empower the world.

In 2022, we raised a $25M Series A led by Initialized Capital (Garry Tan) and a16z (Peter Levine), with investments from Haystack Ventures, Garage VC, Tom Preston Warner (Founder/CEO of GitHub), Geoff Schmidt (Founder/CEO Apollo GraphQL), Nicolas Dessaigne (Founder/CEO Algolia), and Oleg Rognysky (Founder/CEO Peopl.ai).

What you'll do 🧑‍💻

Build data pipelines, text analysis algorithms, query engines, and decision making engines
Apply robust and fault-tolerant approaches to create scalable ingestion and data-processing systems
Debug, profile and optimize distributed data-intensive applications, improve their latency, accuracy, resource consumption, and throughput
Work with existing applications built with Spark, S3, Timescale, Python and Rust
Directly implement services and features that leverage the results of your data pipeline
Implement and improve machine learning and data pipelines

We're looking for 🔎

5+ years of experience as an engineer with a strong understanding of key concepts in distributed systems
3+ years of extensive experience in building and deploying data applications
Fluency in at least one, and ideally more than one, of these languages: Java/Scala/Kolin, Python, Go, Rust, or C++
Good understanding of following concepts: partitioning, replication, map-reduce, indexing, and CAP
Experience with distributed storage systems (S3, HDFS, Hive, ClickHouse, Elastic, etc), distributed processing engines (Spark, etc), and message queues (Kafka, SQS, etc)
Passion for building large-scale ML applications and improving software engineers' productivity
Some understanding of key concepts in natural language processing, machine learning, or statistical analysis
Some experience with machine learning stack (pandas, PyTorch, numpy, sci-kit, transformers, etc)

What we offer 🎁

Unlimited PTO
Competitive salary and equity
Work-life balance
Flexibility to be fully or partly remote
Up to $200/month stipend for coworking space for remote folks
Few meetings, so you can ship fast and focus on building
One Medical membership on us!
Top-notch medical, dental, vision, short-term disability, long-term disability, and life insurance
All insurance is 100% company-paid ($0 premiums) for employees and highly subsidized for dependents
FSA, HSA with company contributions, and pre-tax commuter benefits
401(k) plan
Paid parental leave ( up to 12 weeks)

The salary and equity range for this role are: $170K - $210K and .15% - .35%.

Please note that the compensation range provided is a general guideline only and is subject to change based on location, qualifications, and experience.

Don’t meet every single requirement? At Trunk, we are dedicated to building a diverse and inclusive workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.

If you need assistance or an accommodation due to a disability, we're happy to help accommodate. Please contact us at recruiting@trunk.io.","Aprendizaje automático, Ciencia de datos, Extraer, transformar y cargar (ETL), Python, Reconocimiento de patrones y Scala, Análisis estadístico, Java, Rust (lenguaje de programación) y Sistemas distribuidos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3977350851/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=uGsApyaXIg%2FeuYMc33btWw%3D%3D&trk=flagship3_search_srp_jobs,Engenheiro de Dados [Time de Dados],"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,Estados Unidos,"Acerca del empleo
Job Description

 Nosso Engenheiro de Dados [Time de Dados] será responsável por estudar, propor e implementar melhorias e soluções inovadoras e concretas de engenharia de dados, realizando a sustentação do ecossistema de engenharia de dados e das pipelines de dados de larga escala.

Responsibilities and assignments

Como Engenheiro de Dados com atuação no Time de Dados, esperamos que você possa:

Estudar, propor e implementar melhorias e novas soluções modernas e concretas de engenharia de dados;
Atuar na sustentação do ecossistema de engenharia de dados;
Realizar a criação e sustentação de pipelines de dados de larga escala;
Ajudar a construir um pipeline robusto de dados, de forma escalável;
Atuar em conjunto com outros DEs para avançar o conhecimento do time em conhecimentos gerais de dados.

Requirements And Qualifications

Graduação completa em áreas como Computação, Sistema de Informação; Engenharias, Estatística, Tecnologias e correlatas;
Conhecimento em soluções de dados baseada em nuvem (buckets, bootstrap de clusters, virtualização, etc);
Programação: Python, Spark, Bash, Git e Terraform;
Conhecimento em ferramentas de Lakehouse: Delta, Iceberg, Hudi;
Airflow;
CI/CD;
Docker;
Conhecimento em uma ferramenta de testes unitários em dados como GX e Deequ;
Entender conceitos gerais de dados, como: gerenciamento e monitoramento de clusters de dados; ERM, Star Schema, Snowflake Schema; Data Lake; Data Warehouse; Data Lakehouse; Camadas de Lakehouse; Data Mesh; Data Mart; Modern Data Stack; Governança de dados (parte de segurança, distribuição de dados para consumo); Lineage de dados; Documentação de dados.
Será um diferencial: configuração de cluster Hadoop.

Additional information

Ambiente informal, de muita troca e amizade;
Pessoas apaixonadas pelo que fazem;
Um aprendizado novo por dia;
Local que está buscando seu melhor todos os dias;
Crescimento e mil oportunidades ao seu redor;
Salário competitivo;
Vale Refeição (Caju);
Auxílio Academia;
Ajuda de Custo Home Office;
Plano de Saúde;
Auxílio Creche - Mulheres em Retorno de Licença Maternidade
Vale-Educação Anual;
PLR;
Possibilidades de se tornar sócio/sócia da empresa :)

Process stages

Step 1: Registration1Registration
Step 2: Triagem2Triagem
Step 3: Entrevista RH3Entrevista RH
Step 4: Entrevista Técnica4Entrevista Técnica
Step 5: Entrevista Cultura5Entrevista Cultura
Step 6: Proposta6Proposta
Step 7: Hiring7Hiring

Somos a Big Data!

Somos líderes de mercado no segmento de Inteligência Artificial no Brasil, desde 2012!🚀

Transformamos dados em inteligência de negócio e impulsionamos resultados dos nossos clientes através de recomendações assertivas e otimização de preço de venda. Aqui, nossos desafios não somente são cumpridos, mas abraçados com entusiasmo para construir produtos que geram valor e um impacto real.

Valorizamos pessoas com mentalidade empreendedora, daquelas que não se contentam com o funcionalismo e buscam constantemente maneiras de fazer a diferença! Como dizemos por aqui, “saia dos máximos locais e alcance os melhores resultados possíveis!”

Nosso ambiente é para quem busca protagonismo, força e proatividade. Queremos embarcar em desafios juntos, crescendo e evoluindo como time, sempre com foco no que realmente importa: sermos donos do nosso destino e protagonistas da nossa história!

Junte-se a nós e faça parte de uma equipe que valoriza a iniciativa e a vontade de fazer a diferença.

Estamos empolgados para receber você e juntos alcançarmos novos horizontes! 💙

E olha só alguns dos clientes que já cresceram com a gente: Dexco, Haleon, Kimberly Clark, Eagle Rock, Liberty Coca-Cola, Nivea, Jonhson & Jonhson, Sanofi & Medley, Germed, Natura, Kraft Heinz, P&G, Nestlé, WP Lab, Colgate Palmolive, Banco Santander, Red Bull e muitos outros.","Ciencia de datos, Gobierno de datos, Hadoop, Ingeniería de datos , Minería de datos, Plataforma como servicio (PaaS) y Tecnología de almacenamiento de datos, Esquema en estrella, Procesamiento de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3969117565/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=nw2ZiyxMQiOBxccaJFDWoQ%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Entry/Junior,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Palo Alto, CA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this Job market also, our candidates can achieve multiple job offers and $100k + salaries.

please check the below links to see the success outcomes and salaries of our candidates .

We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)(phone number removed)di

All Positions are open for all visas and US citizens

We at Synergistic understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and are looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernetes, and REST API experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who match client requirements.

No phone calls, please. Shortlisted candidates would be reached out. No third-party or agency candidates or C2C candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3833864741/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=ziJeOLtIo6iCLvK14aYbsg%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist/Analyst/Engineer - Junior Level,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Dallas, TX","Acerca del empleo
The Job Market is Challenging due to almost 300,000 Tech Layoffs since October 2022 due to which thousands of laid off Techies are competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications forget about getting client interviews.  As the Saying goes ""when the Going gets tough the Tough get going”  Candidates who want to make a tech career they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.

Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs.  Clients now post covid can also hire remote workers which increases even more competition for jobseekers.

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT Industry

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.

please check the below links to see success outcomes of our candidates

https://www.synergisticit.com/candidate-outcomes/

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

Oracle CloudWorld Event (OCW) Las Vegas 2023/ 2022 | SynergisticIT - YouTube

https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrn https://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Q https://www.youtube.com/watch?v=OAFOhcGy9Z8 https://www.youtube.com/watch?v=EmO7NrWHkLM https://www.youtube.com/watch?v=NVBU9RYZ6UI https://www.youtube.com/watch?v=Yy74yvjatVg

For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/

We are looking for the right matching candidates for our clients

Please apply via the job posting

REQUIRED SKILLS For Java /Full Stack/Software Programmer

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Project work on the skills
 Knowledge of Core Java , javascript , C++ or software programming
 Spring boot, Microservices, Docker, Jenkins and REST API's experience
 Excellent written and verbal communication skills

For data Science/Machine learning Positions

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Project work on the technologies needed
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
 Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación, Reconocimiento de patrones y Visualización de datos, Ciencias de la computación, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3938216831/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=0Q38K1ydi6Q3KssqC831QQ%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Scientist/Analyst/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Phoenix, AZ","Acerca del empleo
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this market also our candidates get multiple job offers and $100k + salaries.

 please check the below links to see success outcomes of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 https://www.youtube.com/watch?v=OFoqPTNORew

 https://www.youtube.com/watch?v=-HkNN1ag6Zk

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

 https://youtu.be/bJJl27D8bh0

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3984930057/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=c8IcfRaV9SuXOxYd9mdNdw%3D%3D&trk=flagship3_search_srp_jobs,Remote Work - Need Lead Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,Estados Unidos,"Acerca del empleo
100% remote, client is in PA

Need active and Valid LinkedIn

Must be W2 or 1099

Ingestion pipelines

Note: This is a step below the manager. There will be no hand holding.

In addition to the experience below, the manager would need someone with LEAD experience. They won’t be managing people directly but they will be doing code reviews and more strategic work than hands on coding themselves.

Must Haves

AWS

Creation of Data Lakes

Python

Redshift

Glue

Snowflake – big plus

Seasoned data engineer with experience in data infrastructure.

Well-versed in using Python, SQL, Redshift, and AWS.
Competent in creating data lakes and integrating data in AWS.
Experience in building, mentoring, and growing a global engineering team.
Exceptional at communicating technical concepts.
Experience building and operating large scale production data pipelines.
A passion for data solutions and willingness to pick up new programming languages, technologies, and frameworks.
CI/CD processes and source control tools such as GitHub and related DevOps processes
Desire to learn new technologies and ability to analyze the applicability of a technology in business context.
Adept at defining and articulating a product vision, future roadmap in collaboration with leadership and key partners.
Proven track record of managing projects and delivering projects on time and on budget.
Strong analytical skills with an emphasis on data-driven decision making.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Bases de datos, Lagos de datos, Toma de decisiones basadas en datos y Visión de producto",Solicitud sencilla
https://www.linkedin.com/jobs/view/3967945138/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=JWW8ixbLQAvtbpINoIQd6Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 días,"Houston, TX","Acerca del empleo
The CGM Data Analytics team within COG Technology is building a leading data and analytics platform. We partner and build batch and real-time analytics solutions for traders, quants, desk analysts and central data teams across the Commodities and Global Markets business.

At Macquarie, our advantage is bringing together diverse people and empowering them to shape all kinds of possibilities. We are a global financial services group operating in 34 markets and with 55 years of unbroken profitability. You’ll be part of a friendly and supportive team where everyone - no matter what role - contributes ideas and drives outcomes.

 What role will you play? 

You will work directly with our business, developing data analytics capabilities and solutions, such as the data catalog, data science workbench, visualisation tools, and real-time analytics. You’ll be working with our public-cloud based data lake, data pipelines for internal operational and external fundamental and alternative market data. You will join us on our journey to continually improve the robustness of our solutions and the speed with which we can respond to our business’s needs.

What You Offer

Interest in Data and Automation 
Ability to code and debug in Python 
Knowledge of GIT and CI/CD 
Awareness of Cloud eg. AWS

We love hearing from anyone inspired to build a better future with us, if you're excited about the role or working at Macquarie we encourage you to apply.

About Technology 

Technology enables every aspect of our business, for our people, our customers and our communities. Bring your unique perspective and join a global team who is passionate about accelerating the digital enterprise, connecting people and data, building platforms and applications and designing tomorrow’s technology solutions.

Benefits

Macquarie employees can access a wide range of benefits which, depending on eligibility criteria, include:

Hybrid and flexible working arrangements 
One wellbeing leave day per year and minimum 25 days of annual leave
Primary caregivers are eligible for 20 weeks paid leave along with 12 days of transition leave upon return to work and 6 weeks paid leave for secondary caregivers
Paid volunteer leave and donation matching
Range of benefits to support your physical, psychological and financial wellbeing 
Employee Assistance Program, a robust behavioural health network with counselling and coaching services
Recognition and service awards

 Our commitment to diversity, equity and inclusion 

We are committed to providing a working environment that embraces diversity, equity and inclusion. As an inclusive employer, Macquarie does not discriminate on the grounds of age, disability, sex, sexual orientation, gender identity or expression, marriage, civil partnership, pregnancy, maternity, race (including color and ethnic or national origins), religion or belief.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3979924544/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=JO5eEEkRLQh1cO2o4wW1dA%3D%3D&trk=flagship3_search_srp_jobs,Azure Data Engineer - Remote(Fulltime),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 1 semana,"Jersey City, NJ","Acerca del empleo
Must have exp with Azure ADF, Databricks, Synapse
Good knowledge and experience in Azure data integration services such as Azure ADF, Databricks, Synapse.
Design, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.
Work together with data scientists and analysts to understand the needs for data and create effective data workflows.
Create and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.
Utilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.

Aptitudes y experiencia deseables
DATA ENGINEER, ADF, DATABRICKS, SYNAPSE, DATA LAKE, DATA FACTORY, DATABASE, ETL, STORAGE, SQL, AZURE","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Integración de datos y SQL Server Integration Services (SSIS), Azure Databricks, Microsoft Azure y Oracle Application Development Framework",Solicitar
https://www.linkedin.com/jobs/view/3982896376/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=Re4pNdcpPNYUR%2BF%2BzZonYw%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Entry/Junior,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Atlanta, GA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Greetings,

My name is Manav, and I'm a Talent Development Recruiter at SynergisticIT. I came across your profile in our resume database and noticed that you are actively searching for job opportunities. I wanted to reach out to you with an exciting opportunity that aligns with your skills and aspirations as your profile.

I'm excited to introduce you to our comprehensive Full-Time Job Placement Program, meticulously designed to empower individuals like you to excel in the ever-evolving IT landscape.

Our Program Brings a Host Of Unparalleled Benefits

Real-world, Hands-on Experience : Immerse yourself in real-time projects using cutting-edge technologies to enhance your practical skills.

Enhanced Professional Profile : Acquire valuable IT certifications that will set you apart in the competitive job market.

Simulated Real-world Projects : Gain hands-on experience through simulated projects that mirror actual industry challenges.

Comprehensive Interview Preparation : Master various interview formats and techniques, ensuring you're well-prepared for any assessment.

Fortune 500 Client Placements : Secure the chance to work with industry giants, opening doors to lucrative salaries ranging from $75,000 to $160,000.

Financial Support for Education : Explore our Partial ISA programs, designed to provide financial assistance for your education journey.

Our program caters to a range of IT domains, including Java, Full-stack Development, Data Science, Machine Learning, Python, and AI roles. Whether you're a recent graduate, transitioning careers, or looking to amplify your IT skills, our program has something exceptional to offer.

To be eligible for this program, we prefer the following(Not Mandatory):

A degree (BS/MS/AS) in Computer Science, IT, Computer Engineering, Information Systems, Mathematics, Statistics, or a related field.

Proficiency in coding, programming, or data analysis.

Strong Teamwork Skills And Effective Communication Abilities.

Specific skill requirements for different roles include:

Java Programmers : Proficiency in C, C++, Core Java, Spring Boot, and Hibernate.

Data Scientists or Machine Learning Experts : Expertise in Python, Django, Deep Learning, and NLP.

But that's not all!

Here's a glimpse of the benefits that await you :

Transformative IT Journey : Elevate your IT skills and unlock unparalleled career prospects through our tailored program.

Diverse Audience : Designed for recent graduates, ambitious tech enthusiasts, job seekers, and those reentering the job market.

Visa Holder Opportunities : Explore stable and competitive full-time roles, with options including OPT, CPT, H4EAD, L2EAD, GC, and PR holders.

Competitive Edge : Stand out in a market marked by over 250,000 tech layoffs since December 2022.

Industry Giants Connection : Our success stories include placements at tech titans like Google, Apple, PayPal, and Amazon.

Real-World Experience : Benefit from a curriculum enriched with hands-on projects and certifications from Microsoft, IBM, Oracle, and AWS.

Application Success : Receive guidance to overcome rejections and ensure your RESUME grabs attention and opens doors.

Lucrative Income Potential : Our graduates enjoy an impressive average annual income ranging from $75,000 to $150,000.

Swift Employment : Minimize potential income loss with our focus on swift career placement.

Fulfilling Career : Shape a prosperous future in the dynamic IT realm with expert guidance.

To get a deeper insight into our organization and program, I encourage you to explore our success stories at . You can get detailed info regarding benefits, outcome, cost under ""THE ADVANTAGES OF SYNERGISTICIT PROGRAM "". For any questions, our comprehensive FAQ section may have the answers you seek: .

To provide you with a quick overview, I invite you to watch this brief video:

If you find this opportunity appealing, I invite you to reach out to me. Let's discuss the program in further detail and address any inquiries you may have. Feel free to respond to this email, and if you have an updated resume, please attach it for our reference. You can also reach me directly at .

If you're not currently seeking opportunities or prefer communication at a later date, please let me know your availability. Your success is our utmost priority, and we are dedicated to supporting you throughout your job search journey.

Thank you for investing your time in reading this email. I eagerly anticipate the opportunity to assist you in achieving your career aspirations.

Best regards,

Manav

Sr. IT Recruiter

Phone: (phone number removed)

Email

Website:

Address: 39141 Civic Centre Dr, Fremont, CA 94539, United States","Analítica de datos, Análisis de datos y Capacidad de análisis, Ciencias de la computación, Comunicación, Hibernate, Java, Plataforma Java, Preparación de entrevistas y Spring Boot",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3978541629/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=TVk%2F4xkjVw2KKdH%2B0HX6Gw%3D%3D&trackingId=mRtEP%2F21ZFbIY1Q3J6GPzg%3D%3D&trk=flagship3_search_srp_jobs,Production Support ETL Data Developer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,Estados Unidos,"Acerca del empleo
Jd

ETL Developer with tickets & production support
P&C and/or reinsurance and guidewire","Almacenamiento de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Guidewire, Reaseguros y Seguros de bienes y responsabilidad civil",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983581973/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=xIU4PxgT1sWGHnTNSsNHLQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,Estados Unidos,"Acerca del empleo
About FinThrive

FinThrive is advancing the healthcare economy. 

For the most recent information on FinThrive’s vision for healthcare revenue management visit finthrive.com/why-finthrive.

Award-winning Culture of Customer-centricity and Reliability

At FinThrive we’re proud of our agile and committed culture, which makes FinThrive an exceptional place to work. Explore our latest workplace recognitions at https://finthrive.com/careers#culture.

Our Perks And Benefits

FinThrive is committed to continually enhancing the colleague experience by actively seeking new perks and benefits. For the most up-to-date offerings visit finthrive.com/careers-benefits.

Impact you will make

Join a group of highly talented and motivated engineers with significant healthcare experience to help a thriving Healthcare function. Build a highly functional and efficient Big Data platform that brings together data from disparate sources and allow FinThrive to design and run complex algorithms to provide insights into the Healthcare business operations. Partner with internal business, product, and technical teams to analyze requirements to deliver solutions to complex issues. Participate in development and oversee maintenance of code to ensure consistency, quality, best practices, and performance of applications.

What you will do

Build a highly functional and efficient Big Data platform that brings together data from disparate sources and allow FinThrive to design and run complex algorithms providing insights to healthcare business operations
Build ETL Data Pipelines in Azure Cloud using Azure ADF and Databricks
Build Spark Streaming and Near Real Time applications
Partner with internal business, product, and technical teams to analyze complex requirements and deliver solutions
Participate in development, automation, and maintenance of application code to ensure consistency, quality, reliability, scalability, and system performance
Deliver data and software solutions working on Agile delivery teams 

What you will bring

Bachelor’s degree in computer science or a related discipline 
1+ years of data engineering in an enterprise environment 
1+ years of experience writing production code in Python, PySpark or Scala 
Strong coding experience in at least one programming language. Python preferred.
Experience with Big Data technologies in the Cloud such as Spark, Databricks, Hive, Sqoop, or any other equivalent components. Azure preferred.
Experience with any Streaming applications such as Kafka, Spark Streaming, Azure Event Hub
Experience in having built and deployed to Production reasonably complex ETL Data Pipelines. 
Experience working with git and CI/CD tools 
Proven background in Distributed Computing, ETL development, and large-scale data processing 

What we would like to see

Experience within healthcare field in a similar role
Proficiency in SQL and query optimization 
Experience in Azure PowerShell
Experience with Azure ADF, Azure Databricks.
Experience with SQL Server. Preferred but not required.
Experience with CI/CD, DevOps.
Knowledge and passion for software development – including software architecture, functional and non-functional aspects 

FinThrive’s Core Values and Expectations

Demonstrate integrity and ethics in day-to-day tasks and decision making, adhere to FinThrive’s core values of being Customer-Centric, Agile, Reliable and Engaged, operate effectively in the FinThrive environment and the environment of the work group, maintain a focus on self-development and seek out continuous feedback and learning opportunities
Support FinThrive’s Compliance Program by adhering to policies and procedures pertaining to HIPAA, FCRA, GLBA and other laws applicable to FinThrive’s business practices; this includes becoming familiar with FinThrive’s Code of Ethics, attending training as required, notifying management or FinThrive’s Helpline when there is a compliance concern or incident, HIPAA-compliant handling of patient information, and demonstrable awareness of confidentiality obligations

Physical Demands

The physical demands and work environment characteristics described here are representative of those that a colleague must meet to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

Statement of EEO 

FinThrive values diversity and belonging and is proud to be an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. We're committed to providing reasonable accommodation for qualified applicants with disabilities in our job application and recruitment process.

FinThrive Privacy Notice for California Resident Job Candidates



Know Your Rights

Pay Transparency Notice

FinThrive is an Equal Opportunity Employer and ensures its employment decisions comply with principles embodied in Title VII, the Age Discrimination in Employment Act, the Rehabilitation Act of 1973, the Vietnam Veterans Readjustment Assistance Act of 1974, Executive Order 11246, Revised Order Number 4, and applicable state regulations.

© 2024 FinThrive. All rights reserved. The FinThrive name, products, associated trademarks and logos are owned by FinThrive or related entities. RV092724TJO

finthrive.com | FinThrive Careers | FinThrive Benefits & Perks | Physical Demands","Apache Spark, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Ciencias de la computación, Lenguaje de consulta (query), Optimización y Optimización de consultas",Solicitar
https://www.linkedin.com/jobs/view/3948553989/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=Vsl%2BY397yVWUAnKBJ6Csfw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,Estados Unidos,"Acerca del empleo
Title: Data Engineer

Duration: 6+ months

Location: remote

Job Description

The Main function of a Data Engineer is to collect data requirements, costs, access, usage, and availability for business scenarios. They design a data model, which translates into design specifications to model the flow of data across various pipelines. Leads conversations with data scientists, specialists, and makes improvements to data models and schemas to connect, ingest, and enable analytical requirements.

This person will be working with the Hololens Hardware – Design Validation Engineering team, which develops test solutions (hardware, software, and data infrastructure) to improve the design of the product. They will be working with multiple disparate datasets, coming from different teams and locations, and will be standing up secure services to make a common data view available to the program.

Specific skills: Microsoft Synapse, SQL, Python, DAX, PowerBI.

Experience with Microsoft’s portfolio of services, data infrastructure utilizing cloud services.

Experience with data interpretation layers: Power BI content, R (programming language for statistical analysis), JMP, Kusto, manipulating data, data deployments

Microsoft synapse or Microsoft SQL Azure based tools

Candidates submitted to this role must be US Citizens because one of the projects they’ll be supporting utilizes trade restricted data for the US Army.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Análisis estadístico, Azure Kusto, Data Interpretation, Expresiones de análisis de datos (DAX) y JMP",Solicitud sencilla
https://www.linkedin.com/jobs/view/3973514834/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=265Zpp24aG6Flg9OmA9JDw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,"Coral Springs, FL","Acerca del empleo
This is a remote position.

Job Role: Data Engineer

Synergy in Solutions and Success. SYMVOS, transforming businesses for a better tomorrow.

About us information: As a leading provider of business transformation services, SYMVOS is dedicated to driving innovation now to create a better tomorrow. Using cutting-edge digital technologies, we offer business solutions that assist our clients in solving problems and creating possibilities.

Overview: As a Data Engineer, you will play a crucial role in designing, building, and maintaining scalable data pipelines and infrastructure for our organization. You will work closely with data scientists, analysts, and other stakeholders to ensure optimal data flow and integration for analytics, machine learning, and business intelligence purposes. This role requires a deep understanding of data architecture, ETL processes, data modeling, and proficiency in programming and scripting languages.

Key Responsibilities:

Data Pipeline Development: 
Design, implement, and maintain scalable and efficient data pipelines to ingest, transform, and store large volumes of data from various sources (e.g., databases, APIs, logs). 
Optimize data pipelines for performance, reliability, and scalability. 
Data Integration and ETL Processes: 
Develop and implement ETL processes to cleanse, transform, and integrate data into data warehouses or data lakes. 
Ensure data quality and consistency across different data sources. 
Data Modeling: 
Design and implement data models and schemas to support analytics and reporting requirements. 
Collaborate with data analysts and scientists to understand data requirements and translate them into technical solutions. 
Database Management: 
Manage and optimize databases (SQL and NoSQL) for performance and scalability. 
Implement database schema changes, indexes, and optimizations as needed. 
Data Infrastructure: 
Design and deploy infrastructure for data storage and processing, considering factors such as availability, reliability, and cost-effectiveness (e.g., cloud services like AWS, Azure, GCP). 
Collaboration and Communication: 
Work closely with cross-functional teams (e.g., data scientists, analysts, software engineers) to support their data infrastructure needs. 
Communicate technical concepts and solutions effectively to non-technical stakeholders. 
Monitoring and Maintenance: 
Monitor data pipelines and infrastructure to ensure data availability, integrity, and performance. 
Perform troubleshooting and resolve issues related to data processing and storage. 
Skills and Qualifications:

5+ years of proven experience as a Data Engineer or in a similar role. 
Strong programming skills in languages such as Python, Java, Scala, or similar for data manipulation and scripting. 
Proficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL). 
Experience with big data technologies and frameworks (e.g., Hadoop, Spark, Kafka). 
Familiarity with cloud platforms and services (e.g., AWS, Azure, GCP) and containerization technologies (e.g., Docker, Kubernetes). 
Understanding of data warehousing concepts and architectures. 
Knowledge of data modeling, ETL tools, and data integration techniques. 
Bachelor’s degree in Computer Science, Engineering, or a related field (Master’s degree preferred). 
Relevant certifications (e.g., AWS Certified Big Data - Specialty) are a plus. 

EEO Employer:

SYMVOS Corporation is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. SYMVOS Corporation will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require accommodation in using our website for a search or application, please contact info@symvos.com.

SYMVOS Corporation is a tech consulting firm that partners with Software Development, Artificial Intelligence, Data Analytics, and Cybersecurity clients. When you join SYMVOS, you become part of a team that values Excellence Through Quality.

Please follow our LinkedIn page to stay updated with our opportunities: https://www.linkedin.com/company/symvos/","Arquitectura de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Herramientas ETL, Calidad de datos, Comunicación, Manipulación de datos, Modelado de datos, Modelo de datos y Requisitos de información",Solicitar
https://www.linkedin.com/jobs/view/3976873686/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=5xTubN1%2BYZqTBmk4xl%2F7vA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Job Details:
Position: Data Engineer
Location: Remote Opportunity - Johnston, IA
Duration: 06+ Months Contract Opportunity

Project Scope and Brief Description:

The position is for work in the bioinformatics space, principally writing new and/or maintaining existing bioinformatics workflows and pipelines such as a Eukaryote Genome Annotation Pipeline. As such the role requires knowledge of Cloud technologies (AWS, Kubernetes, Container orchestration) as well as experience with industry-level scientific workflow management.

Responsibilities:

Design, develop, optimize, and maintain scalable bioinformatics workflows for processing and analyzing large-scale genomics datasets in the cloud and in-house
Include a flexible modular architecture into the workflows to enable the exchange of analysis components and different algorithms
Implement the bioinformatics data processing pipelines using workflow management tools and programming languages such as Python
Work with team members to perform quality control and validation of pipelines to ensure accuracy and reproducibility of results
Document the development processes, including code, workflows, data flow diagrams, and standard operating procedures, following software development and DataOps best practices

Required Qualifications:

Previous experience developing industrial scale scientific data workflows.
Strong programming skills in Python including libraries for Data Science such as NumPy, Pandas, NetworkX, matplotlib, etc.
Working knowledge of container technologies (such as Docker, ContainerD, or Podman) and container orchestration.
Experience with data pipeline tools (like Argo, Ray, AirFlow, Redun or NextFlow).
Familiarity with the AWS platform (IAM, EC2, S3, CloudWatch, Spot instances) and Kubernetes, EKS, ECS, AWS Batch or other Cloud compute architectures.
Ability to work both independently and collaboratively with good communication skills. Interest in learning new technologies
 Preferred Qualifications:

Specific experience analyzing large genomics datasets
Familiarity with common bioinformatics tools and datatypes for the analysis of NextGen sequencing data
Familiarity with statistical analysis methods and tools commonly used in bioinformatics analysis such as Gene Expression or ChIPSeq
Knowledge of any additional programming languages such as C, Rust, Perl, R, Unix Shell or others","Amazon Web Services (AWS), Kubernetes y Python, Argo, Container Orchestration, Data Operations y Gestión de flujos de trabajo",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3977114806/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=au4YbguEPlf2e6mJtyEnvw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Job Location: Hybrid (Closest Capgemini office)

Job Description:
As a Data Engineer on the Insurance Tech Platform team, you’ll be a key contributor in crafting and building numerous internal tools and services to support insurance products, which help our communities handle risk with one-of-a-kind protection programs that are unmatched in the travel industry.

Key Responsibilities
Collaborate with multi-functional partners such as Product, Operations, and Engineers to craft and deliver high-quality products.
Build efficient and reusable backend components and architect reliable, performant, and scalable solutions!
Identify areas of improvement and drive standard processes.

Required Skills: 
Experience with Big Data technologies (Hadoop, Hive Spark, Presto, Flink, Kafka).
Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and advance effective product solutions.
Experience in crafting and deploying dedication systems with reliable monitoring and logging practices.
Working knowledge of relational databases and query authoring (SQL).
Experience or desire to work collaboratively in multi-functional teams with design, product, data science, and research partners.
Ability to optimally connect with non-technical partners.
Expertise in architecting and developing solutions to ambiguous problems, and integrations across multiple teams with significant impact.
Bachelor’s degree or equivalent experience.

Life at Capgemini
Capgemini supports all aspects of your well-being throughout the changing stages of your life and career. For eligible employees, we offer: 
Flexible work 
Healthcare including dental, vision, mental health, and well-being programs.
Financial well-being programs such as 401(k) and Employee Share Ownership Plan
Paid time off and paid holidays 
Paid parental leave.
Family building benefits like adoption assistance, surrogacy, and cryopreservation
Social well-being benefits like subsidized back-up child/elder care and tutoring
Mentoring, coaching, and learning programs.
Employee Resource Groups 
Disaster Relief 

About Capgemini Engineering
World leader in engineering and R&D services, Capgemini Engineering combines its broad industry knowledge and cutting-edge technologies in digital and software to support the convergence of the physical and digital worlds. Coupled with the capabilities of the rest of the Group, it helps clients to accelerate their journey towards Intelligent Industry. Capgemini Engineering has more than 55,000 engineer and scientist team members in over 30 countries across sectors including Aeronautics, Space, Defense, Naval, Automotive, Rail, Infrastructure & Transportation, Energy, Utilities & Chemicals, Life Sciences, Communications, Semiconductor & Electronics, Industrial & Consumer, Software & Internet. 
Capgemini Engineering is an integral part of the Capgemini Group, a global leader in partnering with companies to transform and manage their business by harnessing the power of technology. The Group is guided every day by its purpose of unleashing human energy through technology for an inclusive and sustainable future. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of €22.5 billion.
Get the Future You Want | www.capgemini.com

Disclaimer
Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship. 
Capgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.
Click the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law
Please be aware that Capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process!
Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by Capgemini.",Bases de datos,Solicitud sencilla
https://www.linkedin.com/jobs/view/3970456459/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=VpaaU9QHZSZN8NQ99iOLng%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
Data Engineer
 Exciting opportunity to join the data management organization that creates the nation’s most comprehensive source of person-based incarceration, justice, and risk intelligence data.
 Candidates will need to be highly motivated, creative, self-directed, and thrive in small project teams. 
 RESPONSIBILITIES: 
Participate in a critical migration project, importing data from SQL Server into BigQuery and consolidating it into a single data warehouse.
Ensure seamless data migration, consolidation, and reconciliation with a focus on accuracy and reliability.
Collaborate with cross-functional teams to maintain and enhance data quality throughout the project lifecycle.
This is a 5 month contract opportunity with a leading global organization based in Atlanta, GA. There is an potential to extend at the end of the contract! Successful candidates have a high level of initiative and thrive in a fast paced, enterprise environment. 

Visionaire Partners offers all full-time W2 contractors a comprehensive benefits package for the contractor, their spouses/domestic partners, and dependents. Options include 401k with up to 4% match, medical, dental, vision, life insurance, short and long term disability, critical illness, hospital indemnity, accident coverage, and both Medical and Dependent Care Flexible Spending Accounts.

REQUIRED SKILLS:
Experience converting MS SQL Server PROCs to Google BigQuery SELECTs
Advanced MS SQL Server and Google BigQuery skills 
Strong problem-solving skills and attention to detail.
Proven experience in cloud-based data migration projects.

PREFERRED SKILLS:
Tableau experience
 Must be authorized to work in the US. Sponsorships are not available.
Pay Range: $60-70.92/hour W2",Google BigQuery y Microsoft SQL Server,Solicitud sencilla
https://www.linkedin.com/jobs/view/3979934059/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=n%2FKWKYA6vdw9OqwJmXDzbg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Houston, TX","Acerca del empleo
Job Description

Join a high-performing, dynamic development team at Kroger dedicated to building innovative solutions to supply chain challenges. The ideal candidate for this Senior Data Engineer role will have expertise in Databricks, preferably in building robust ETL solutions, and in Python (PySpark) and Delta tables. This is an exciting opportunity to contribute to our data infrastructure and play a key role in driving our organization's data initiatives forward.

Qualifications

Bachelor's degree in Computer Science, Engineering, or a related field; or equivalent work experience.

Proven experience (X years) as a Data Engineer or similar role, with a strong background in building and maintaining ETL pipelines.

Expertise in Databricks, Python, PySpark, and Delta tables for data processing and manipulation.

Hands-on experience with developing and deploying Azure-based solutions by using Azure Data Lake Storage, Azure Databricks, Azure Synapse Analytics, or other Azure services.

Solid understanding of data modeling principles, relational databases, and data querying concepts.

Excellent problem-solving skills and the ability to analyze complex technical issues independently.

Strong communication skills with the ability to effectively collaborate with team members and communicate technical concepts to non-technical stakeholders.

Experience working in an Agile or iterative development environment is a plus.","Base de datos relacional, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y PySpark, Bases de datos, Ciencias de la computación, Comunicación, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3956125245/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=lr%2BhFJOanMyfLnuVw0tDeQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"126,1 US$K/año - 186,8 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,"San Francisco, CA","Acerca del empleo
Overview

 Working at Atlassian 

Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

Your future team

You will be part of a world-class Data Engineering team, where you will:

influence product teams
inform Data Science and Analytics Platform teams
partner closely with data consumers and producers to ensure quality and usefulness of data assets


You will be involved in strategizing measurement, collecting data, and generating insights.

This work will help understand and improve product experience and engagement, optimize efficiency and costs, and drive strategy.

Responsibilities

Data Engineering is a multi-faceted role, where you can have focused or broad set of responsibilities, including:

Defining metrics
Instrumenting logging
Acquiring/ingesting data
Architecting & modeling data
Transforming data
Ensuring data quality, governance, and enablement
Alerting, visualization, and reporting
Developing at scale and improving efficiency
Working autonomously and adding perspective to the team


Qualifications

Minimum Qualifications 

Bachelor’s / Masters degree (or equivalent work experience) in a STEM field
2+ Years of Experience in Data Engineering or related fields
Expertise in Python or other modern programming language development experience
Knowledge of advanced level capabilities in SQL and relational databases experience
Solid working knowledge of ETL (ELT), Data modeling, DataQuality, and Data Visualization/Dashboarding
Experience working with product analytics


Additional/Desirable Qualifications

4+ Years of Experience in Data Engineering or related fields OR 2+ Years of experience in Data Engineering along with a post-graduate degree or equivalent in a related field
An understanding of the SaaS business model and data structures
Experience in Enterprise Software product development


Compensation

At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:

Zone A: $140,100 - $186,800

Zone B: $126,100 - $168,100

Zone C: $116,300 - $155,000

This role may also be eligible for benefits, bonuses, commissions, and equity.

Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.

Our Perks & Benefits

Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit  go.atlassian.com/perksandbenefits  to learn more.

About Atlassian

At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.

For San Francisco Only: Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

To learn more about our culture and hiring process, visit  go.atlassian.com/crh","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Arquitectura técnica, Bases de datos, Calidad de datos y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3985017928/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=lY9rcGB6mogJtzPd9rfFqw%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 3 días,Estados Unidos,"Acerca del empleo
Our client is looking for a Data Engineer 

The Sr. Azure Data Engineer will lead the creation of high-value data-driven solutions leveraging our client’s proven implementation methodology and solutions for enterprise projects. You will also contribute to technical pre-sales activities as required. Responsibilities include designing solution architecture, defining requirements, and leading the project delivery team. You will work collaboratively across all sales, service delivery, and project management organizations to serve clients.
The ideal candidate will have extensive experience with Microsoft/Azure data services and Databricks technology. Proficiency with the Databricks platform and implementing enterprise Data Lakehouse is required. Candidates will be expected to contribute to all stages of the data lifecycle including data ingestion, data modeling, data profiling, data quality, data transformation, data movement, and data curation.

Key Responsibilities:
Lead and manage the development of large-scale data warehouse solutions.
Translate business requirements into functional and technical requirements.
Design, develop, and maintain data pipelines using Databricks and Microsoft Azure services.
Solution Azure Cloud-based enterprise data architectures.
Automate data pipelines and processes.
Engage directly with client stakeholders, managers, and end-users.
Collaborate with the Cyclotron team including Project Managers, Software Engineers, and Business Analysts.
PLACEMENT CRITERIA & REQUIREMENTS:
Focus on Azure data engineering solutions.
8+ years of data engineering delivery experience.
3+ years of Databricks engineering development experience.
2+ years of technical team leadership or technical management experience.
Current Azure and/or Databricks certifications.
TECHNICAL SKILLS REQUIRED:
General Architecture:
Design large-scale data warehouse solutions.
Interpret business requirements into functional and technical requirements.
Develop and maintain data pipelines using Databricks and Microsoft Azure services.
Solution Azure Cloud-based enterprise data architectures.
Automate data pipelines.
Information Architecture:
Data Modeling principles for relational and dimensional data structures.
Data Warehouse design principles.
Data Lake design principles, Data Virtualization.
Hybrid enterprise data architecture.
Strong knowledge of data warehouse concepts and T-SQL relational/non-relational databases for data access and Advanced Analytics.
Experience with Python, SQL, DAX, M.
Reporting and data visualization tools, specifically PowerBI.
Practical knowledge of Microsoft SQL Server.
Experience in multidimensional and/or tabular models (SSAS).","Almacenamiento de datos, Apache Spark, Aprendizaje automático, Big data, Extraer, transformar y cargar (ETL), Hadoop, Hive, Ingeniería de datos y Transact-SQL (T-SQL), Bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982177522/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=ehfco0pLl1syZEL%2BfB78WA%3D%3D&trk=flagship3_search_srp_jobs,100% Remote - Healthcare Data Engineer - Contract to Hire - Immediate Interview !!!,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 6 días,Estados Unidos,"Acerca del empleo
Role: Senior Healthcare Data Engineer
Location: 100% Remote
Duration: 2-3 Months Contract To Hire

Job Description 
Overall 12+ in IT Experience.
Minimum of 8+ years of experience in Healthcare Domain
Minimum of 10+ years of experience in Data Engineering.
Required technical skills: Python, PySpark, SQL, AWS

Required Domain:
Healthcare/Insurance Candidate is expected to have experience in Shell scripting
Extensive data analysis skills and Good communication skills","Amazon Web Services (AWS), PySpark y Python",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984552042/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=2hulLWog5lohntbCINobHg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (with NIKE exp.),"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,"Dearborn, MI","Acerca del empleo
Required skill set: 

 Linux 
 Hadoop 
 Hive 
 HQL (Hive Query Language) 
 Python 
 Spark 
 Pyspark 
 Big data processing 
 Software resource technical skill, such as; Pyspark,Python, Scala, Java, Hadoop hive, big data processing experience. 

Awareness and good to have on hands-on of Cloudera Data Platform, ETL, (ok if not all )

Understanding of databases like Teradata, SQL, Hive, HBase, NoSQL, etc..

Hands-on Pyspark/Python programming exposure and Good knowledge of Spark SQL.

Strong in Software Programming/Engineering with a good understanding of DevOps, GitHub etc.

Must know the Hadoop concept and open to learning new technology/toolset

Learning attitude and flexible with project and timings

Good communication and presentation skills.

 GCP will be an added advantage","Apache Spark, Hadoop, Hive, PySpark y SQL, Aptitudes para hacer presentaciones, Comunicación, HiveQL, Lenguaje de consulta (query) y Presentaciones",Solicitar
https://www.linkedin.com/jobs/view/3983408723/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=cuF%2FD4FoO4P34da30besdQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 días,Estados Unidos,"Acerca del empleo
Job Description

 Data Engineer - Remote 

 Job Purpose 

Resource will be part of Data Analytics Team (Operations Analytics group) and involved in Database development, Big Data development and ETL activities contributing to Analytics Projects and Paid POVs for Infosys BPO analytics.

 Duties And Responsibilities 

 Understanding requirements provided by client 
 Analyzing the requirements and document the requirement analysis 
 Come up with Development plan and individual task estimation 
 Design and develop Data flow pipelines using RDBMS or Big Data platform 
 Development and Implement Database design, ETL and prepare database for Visualization and analytics. 

 Qualifications 

 Basic 

 Bachelor’s degree in computer science, engineering, or equivalent work experience 
 2 years of professional experience in related field 

 Skills 

 Hands on experience with Data warehouse and database development (5-8 years) using any of the leading database systems. 
 Hands on experience with Database development including ETL (min 5 years), Sql and Analytical Sql Queries with any of the leading databases e.g. Postgres, Oracle, Sql Server etc. 
 Hands on with Cloud ecosystem and tools 
 Advanced working knowledge of Python 
 Fair working knowledge of Python, R programming for Data analysis 
 Familiar with Reporting tools like Tableau, PowerBI etc. 
 Team Player, Good Communication and Analytics skills 

 About Us 

Infosys BPM Limited, a wholly owned subsidiary of Infosys Limited (NYSE: INFY), provides end-to-end transformative business process management (BPM) services for its clients across the globe. The company’s integrated IT and approach enables it to unlock business value across industries and service lines, and address business challenges for its clients. Utilizing innovative business excellence frameworks, ongoing productivity improvements, , , and cutting-edge technology platforms, Infosys BPM enables its clients to achieve their cost reduction objectives, improve process efficiencies, enhance effectiveness, and deliver superior customer experience.

Infosys BPM has 42 delivery centers in 16 countries spread across 5 continents, with 57,908 employees from 124 nationalities, as of June 2023.

The company has been consistently ranked among the leading BPM companies globally and has received over 60 awards and recognitions in the last 5 years, from key industry bodies and associations like the Outsourcing Center, SSON, and GSA, among others. Infosys BPM also has very robust people practices, as substantiated by the various HR-specific awards it has won over the years. The company has consistently been ranked among the top employers of choice, based on its industry leading HR best practices. The company’s senior leaders contribute widely to industry forums as BPM strategists.

 EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity/Nationality 

 About Us 

Infosys BPM Limited, a wholly owned subsidiary of Infosys Limited (NYSE: INFY), provides end-to-end transformative business process management (BPM) services for its clients across the globe. The company’s integrated IT and approach enables it to unlock business value across industries and service lines, and address business challenges for its clients. Utilizing innovative business excellence frameworks, ongoing productivity improvements, , , and cutting-edge technology platforms, Infosys BPM enables its clients to achieve their cost reduction objectives, improve process efficiencies, enhance effectiveness, and deliver superior customer experience.

Infosys BPM has 42 delivery centers in 16 countries spread across 5 continents, with 57,908 employees from 124 nationalities, as of June 2023.

The company has been consistently ranked among the leading BPM companies globally and has received over 60 awards and recognitions in the last 5 years, from key industry bodies and associations like the Outsourcing Center, SSON, and GSA, among others. Infosys BPM also has very robust people practices, as substantiated by the various HR-specific awards it has won over the years. The company has consistently been ranked among the top employers of choice, based on its industry leading HR best practices. The company’s senior leaders contribute widely to industry forums as BPM strategists.

 EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity/Nationality","Almacenamiento de datos, Analítica de datos, Arquitectura de datos, Desarrollo de base de datos y Extraer, transformar y cargar (ETL), Bases de datos, Ciencias de la computación, Comunicación, Modelado de datos y Sistemas de bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3967198486/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=sHaUxbdaN%2F8HPB7tVmW2jg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Arlington, MA","Acerca del empleo
Leader Bank is looking for exceptionally dedicated team members to join one of the region’s fastest growing community banks and mortgage lenders. At the time of its founding in 2002, Leader Bank had one branch office, $6.5 million in assets and 7 team members. Since then, the Bank has become one of the most successful banks in Massachusetts with $4 billion in assets, more than 400 team members, 7 branch offices, and annual mortgage originations of over $5 billion.

Exemplary products and an innovative spirit have driven Leader Bank’s rapid growth over the years, and our team members embrace these values. Our mission is to obsess over our clients, make them feel valued, and maintain long-term relationships with them by constantly enhancing our products and processes to always be improving our client experience. For our team members, Leader Bank prioritizes competitive compensation and benefits, a healthy work-life balance, and an environment that fosters diversity and inclusion.

Responsibilities

Write and maintain efficient data pipelines and ETL processes
Assemble, prepare, and analyze large data sets
Implement methods to improve data reliability, quality, and security
Identify opportunities for data enrichment
Develop analytic tools, programs, and reports
Build and enhance data models that align with business outcomes
Maintain our data stack

Qualifications

Bachelor of Science in Data Engineering or related area of study
Experience in designing databases, warehouses, and analytical system
Experience with Snowflake, Matillion, Retool is a plus
Experience deploying machine learning models is a plus
Advanced SQL skills, proficient in Python
Passion for problem solving

Leader Bank offers an excellent compensation and benefits package including 401k plan with corporate match, medical and dental insurance, and the opportunity to work for a fast growing, local organization.

Leader Bank, N.A. is an Equal Opportunity and Affirmative Action employer and does not discriminate on the basis of race, color, religion, age, gender, marital status, sexual orientation, national origin, disability, military status, veteran status, or any other protected class.

Founded in 2002, Leader Bank is a Massachusetts-based entrepreneurial financial institution that approaches banking differently. The core tenets of Leader Bank include client services, exemplary products, and innovation to meet the needs of its clients. Leader Bank’s best-in-class staff has been at the forefront of supporting the bank’s rapid growth and client-oriented solutions, as the bank has continued to expand its commercial and retail products and solutions over the last two decades. Leader Bank is a committed corporate citizen and prides itself on partnering with and supporting philanthropic organizations.","Almacenamiento de datos, Canalizaciones de datos, Capacidad de análisis, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Modelo de datos, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3971890363/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=bT8ZygwKi9xd5i3m%2Fe89ag%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",hace 2 semanas,"Mountain View, CA","Acerca del empleo
Qualifications

 Minimum qualifications: 

 Bachelor's degree in Data or Engineering, or related technical disciplines or equivalent practical experience. 
 Experience with BI Analytics and Data ETL (Extraction, Transformation, and Loading) with large amounts of data 
 Proficiency with Standard SQL and Dashboarding with DataStudio 
 Experience with standard data metrics such as shape, distribution, standard deviation, and the corresponding visualizations such as histograms, trends, bars and charts 
 Understanding of standard data quality metrics 

Preferred Qualifications

 Preferred Qualifications: 

 Proficiency with PLX, F1, Dremel 
 Familiarity with GCP BigQuery 
 Familiarity with with Jupyter Notebooks & Python 

 Other Skills: 

 Analytical and quantitative skills with the ability to use data (including SQL) and metrics to back up assumptions, evaluate outcomes 
 Self-starter with a firm sense of accountability and ownership on various deliverables 
 Ability to dive into a complex data set , find the hidden truth, and turn it into a plan of action.

Aptitudes y experiencia deseables
SQL, PYTHON, PIPELINES, PIPELINE","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery y Ingeniería de datos, Bases de datos, Calidad de datos y Panel de control",Solicitar
https://www.linkedin.com/jobs/view/3967444646/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=jOrFoQWLqhp4btnPXckraQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Ontario, CA","Acerca del empleo
Hi

Please share your updated profile for the below role

Data Engineer

Mississauga, ON

Fulltime

CAD 130K/annum

Must have Spark

Data engineering and ingestion effort and other ETL expertise

Database knowledge - Oracle / SQL / Data Analytics
Regulatory reporting knowledge preferred
Good communication skills
Open / Ability to learn new technologies
Very good Analytical Skills
Experience in ETL is preferable

Thanks and regards,

Martin Raj

Recruitment Director

Email: martin@atikatechnologies.com; Voice: +1 985 616 1829
Aptitudes y experiencia deseables
DATA ENGINEER, ETL, SPARK, ORACLE, SQL","Almacenamiento de datos, Analítica de datos, Apache Spark, Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos y Comunicación",Solicitar
https://www.linkedin.com/jobs/view/3984231549/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=4WjnVkhZDeSciHLmXLnTiA%3D%3D&trk=flagship3_search_srp_jobs,,,,,,,
https://www.linkedin.com/jobs/view/3983290713/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=KCJP7rNEZeIScF0aOjh5yQ%3D%3D&trk=flagship3_search_srp_jobs,Data/ Analytics Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Filadelfia, PA","Acerca del empleo
The Data Engineer will focus on modernizing, building out and maintaining our data technology infrastructure and processes. As part of the Product, Data & Engineering team, you will operationalize our data strategy, ensure quality and accuracy of data and data models, reporting & downstream data products, and create and manage AI/ML operations such as model development and training, model validation and deployment.

The Data Engineer will use software engineering best practices, follow an iterative approach with regular user feedback, and work with the agile practitioner to prioritize work and manage multiple deployments per day. The role is a unique opportunity to create and shape the technology, methods and data related processes

About Us

The Philadelphia Inquirer is a public benefit corporation owned by the nonprofit Lenfest Institute for Journalism. Together, we’re at the center of a critical mission to create a lasting future for ambitious, engaging, and useful local journalism. We’re doing this, in part, by deepening our connection with the communities we serve. Our integrated digital and print platforms are the Philadelphia region’s largest media network. We’re passionate about building a sustainable model for indispensable local journalism, and we take pride in finding diverse, dynamic, and talented individuals to help push our team forward.

What You’ll Do

Continuously iterate and improve upon our data platform, enabling business decisions, activation and AI/ML processes through high-quality, trustworthy data. 
Collaborate directly with a group of talented and mission-driven professionals, including data engineers, data analysts, agile practitioners, and product managers. 
Own the development, testing, documentation and evangelism of our core data models. 
Develop and maintain efficient and scalable data pipeline architecture for collecting data across a variety of sources, enabling different functions to leverage transformed data for analytics and operations. 
Evaluate, engineer, and operationalize tooling and processes to enhance development workflows. 
Work with modern data and cloud technologies, including dbt Cloud, Airflow, Terraform, GitHub Actions, and BigQuery. 
Contribute to the open source data community by maintaining several public code repositories. 
Help ensure the success and longevity of local journalism in the Philadelphia region. 

Who We Are Looking For

3+ years hands-on experience in analytics and/or data engineering 
Intellectually curious, intrinsically motivated with a bias toward action 
An entrepreneurial mindset that is focused on stakeholder results 
Ability to work independently and asynchronously in a hybrid work environment. Excellent written communication skills. 
Professional experience with dbt, Airflow, Python, GitHub, and Terraform 
Experience developing code in a collaborative environment. Able to comfortably use git and give thoughtful PR reviews 
Proficient in interacting with RESTful APIs, including capturing, cleansing, and storing web responses 
Working knowledge of CI/CD practices 
Experience with cloud data warehouses (e.g. BigQuery, Snowflake), including cost optimization strategies such as partitioning and clustering 
Advanced SQL skills including use of analytic window functions and modular query design 
Knowledge of strategies and tooling to monitor and enforce data quality standards 
Experience with Google Cloud Platform 
Contextual knowledge of services such as Google Analytics 360/4, Auth0, Salesforce CRM, and Salesforce Marketing Cloud 
Experience writing scripts to automate the provisioning and maintenance of systems in a distributed, virtualized infrastructure; familiarity with containerized application development 
Hands-on experience with one or more of the following: AI/ML processes to including data labeling, model development and training, model validation, and model deployment 
Familiarity with managed cloud-based options for building machine learning models 
Working knowledge of statistics is a plus 

The Philadelphia Inquirer is committed to attracting a diverse group of interested people to join our team, so please err on the side of applying even if you’re unsure if you meet all the qualifications. We encourage anyone to apply who shares our passion for indispensable journalism and our drive to create a sustainable business model to support it.

As an equal opportunity employer, The Inquirer is building an inclusive and equitable workplace where everyone feels a sense of belonging. We especially encourage people from marginalized and underrepresented backgrounds to apply, including people of color, women, people from the LGBTQIA+ community, and people with disabilities.","Airflow, Analítica, Google Cloud y Ingeniería de datos, Calidad de datos, Comunicación, Comunicación escrita, Lenguaje de consulta (query), Modelo de datos y Validación de modelos",Solicitar
https://www.linkedin.com/jobs/view/3885181821/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=SE62EVhbL7bQmFZ3FudP1A%3D%3D&trk=flagship3_search_srp_jobs,,,,,,,
https://www.linkedin.com/jobs/view/3974901901/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=kTeQq%2BmnLETXtmDmCBR%2BzQ%3D%3D&trk=flagship3_search_srp_jobs,Matillion/Snowlake/AWS Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
Matillion/Snoiwflake/AWS Data Engineer","Amazon Web Services (AWS) y Integración continua y entrega continua (CI/CD), Matillion ETL y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3885186087/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=mCM0VO5swO7pRAoix%2Fn%2BNw%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Analyst/Engineer/Scientist,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 meses,"Charlotte, NC","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this market also we have been able to help our candidates get multiple job offers and $100k + salaries.

 please check the below links to see success outcomes of our candidates  and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 https://www.youtube.com/watch?v=OFoqPTNORew 

 https://www.youtube.com/watch?v=-HkNN1ag6Zk 

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://youtu.be/bJJl27D8bh0 

 We are looking for the right matching candidates for our clients 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Programmer 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java , javascript , C++ or software programming 
 Spring boot, Microservices, Docker, Jenkins and REST API's experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning Positions 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow 

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements. 

 No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3974877414/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=U1kloOmjPebg0rrJP2d3iw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Analyst - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Manhattan, NY","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

please check the below links to see success outcomes, salaries of our candidates .

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)(phone number removed)di

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos y Lenguajes de programación, Ciencias de la computación, Comunicación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3982894604/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=URtOsKEX%2FMlc3JJlW8EQiA%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Scientist/Engineer - Junior,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Baltimore, MD","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

If you applied for a job and got emails from our Job Placement Program team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

please check the below links to see success outcomes of our candidates and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

(url removed)

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)

(url removed)

For preparing for interviews please visit (url removed)

We are looking for the right matching candidates for our clients

Please apply via the job posting

REQUIRED SKILLS For Java /Full Stack/Software Programmer

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3978695707/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=0R2RybOSTsbePGIJ3It7dg%3D%3D&trk=flagship3_search_srp_jobs,"SQL Data Engineer, Wellmed - ONSITE in San Antonio or Austin, TX","Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"San Antonio, TX","Acerca del empleo
For those who want to invent the future of health care, here's your opportunity. We're going beyond basic care to health programs integrated across the entire continuum of care. Join us to start Caring. Connecting. Growing together.

This position is focused on management, analysis, and reconciliation of healthcare data. The ideal candidate will have experience in healthcare related finance and have a mindset towards process optimization. You will be part of a small team that supports a large team. Much of your work will involve problem solving, and gathering requirements from internal teams.

If you are located in San Antonio, TX, you will have the flexibility to work remotely*, as well as work in the office as you take on some tough challenges. Hybrid role - on site 3 days per week, remote 2 days per week

Primary Responsibilities

ETL Development
Reconciliation of eligibility and financials
Report development
SQL development 
Process improvement 

You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.

Required Qualifications

ETL experience
Experience as an analyst - preferably financial analysis
Experience in building reports (Tableau or Power BI)
Expert knowledge of Excel
Expert knowledge of SQL
Ability to work from office at least 3 days per week

Preferred Qualifications

Experience with healthcare claims adjudication
Experience with healthcare revenue cycle
Proven solid people skills
Proven to be skilled at presentation
All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy

California, Colorado, Connecticut, Hawaii, Nevada, New Jersey, New York, Rhode Island, Washington, Washington, D.C. Residents Only: The salary range for this role is $70,200 to $137,800 annually. Pay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. UnitedHealth Group complies with all minimum wage laws as applicable. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.

Application Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants.

At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone-of every race, gender, sexuality, age, location and income-deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.

Diversity creates a healthier atmosphere: OptumCare is an Equal Employment Opportunity/Affirmative Action employers and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.

OptumCare is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.","Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , SQL y Tableau, Creación de informes, Gestión del ciclo de ingresos, Habilidades sociales, Oracle Financials y Presentaciones",Solicitar
https://www.linkedin.com/jobs/view/3983530655/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=wfezvI8MgtrPZYxZp5FXMg%3D%3D&trk=flagship3_search_srp_jobs,Hybrid Work - Need MicroStrategy/ GCP Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Richardson, TX","Acerca del empleo
MicroStrategy/ GCP Data Engineer

hybrid - (Richardson, TX ; Scottsdale, AZ ; Buffalo Grove, IL ; Wellesley, MA ; Smithfield, RI) - Local Candidates of either location - Local candidates under 60 Min commute of either location

Must have valid LinkedIn

Need DL and Visa Copy

MicroStrategy/ GCP Data Engineer 

Overview

Someone to work on converting several reporting projects from Teradata to Google Cloud Platform
Needs Cloud migration experience 
We are looking for strong MicroStrategy and SQL skills 
Good interpersonal and communication skills are required as well as attention to detail.

Must Haves

Must have strong MicroStrategy Experience
Cloud Migration experience 
Strong Google Cloud Platform skills
Strong SQL skills 
Data Conversion work: Ideally from Teradata to GCP experience 
Strong communication and collaboration skills, ability to communicate technical concepts and implications to business partners
Ability to handle multiple projects and activities in a timely, organized manner
High levels of self-motivation and attention to detail.","Google Cloud , Ingeniería de datos y SQL, Atención al detalle, Comunicación, Comunicación interpersonal, Conversión de datos, Handle Multiple Projects, MicroStrategy y Teradata",Solicitud sencilla
https://www.linkedin.com/jobs/view/3978461004/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=W%2BrbEws9fS8b6bO3YmDF2Q%3D%3D&trackingId=%2F0HhpcfsBuYEPbCqH2pTnQ%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Skagway, AK","Acerca del empleo
This is a remote position.

 DISCLAIMER:  This job posting is intended for active pooling of candidates who will become part of our talent pool. Your qualifications will be assessed against both current and future job openings. Should your application align with a role that corresponds to your skills and experience, and an opportunity arises, our recruitment team will reach out to you immediately. Please note that this does not guarantee immediate placement or contact. Additionally, we exclusively consider applications from individuals who are currently reside in the US/Canada during their application process.

 Salary: $60,000 - $70,000 per annum

 Experience Required:  Minimum 1 year of project experience

Skills and Abilities:

Strong knowledge of R or Python for data analysis and modeling. 
Proficiency in statistical programs such as R, SAS, MATLAB, or Python. 
Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). 
Basic understanding of SQL, Javascript, XML, JSON, and HTML. 
Ability to learn new methods quickly and work under deadlines. 
Excellent teamwork and communication skills. 
Strong analytical and problem-solving abilities. 
Basic understanding of SQL, Javascript, XML, JSON, and HTML. 

Preferred:

Knowledge of actuarial concepts and life, health, and/or annuity products. 
Experience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. 
Familiarity with Microsoft DeployR. 
Exposure to insurance risk analysis. 
Basic experience in computational finance, econometrics, statistics, and math. 
Knowledge of SQL and VBA. 
Familiarity with R or Python for predictive modeling","Analítica de datos, JSON, Modelos predictivos y SQL, Análisis cuantitativo, Análisis financiero cuantitativo, Computational Finance, Econometría, HTML y JavaScript",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3966762156/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=zLlXVV1i59K8d2q7Tsv%2FdA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer I,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 4 días,Estados Unidos,"Acerca del empleo
Job Summary
The Data Engineer I is responsible for creating and maintaining pipelines that provide essential data for reporting and analytics across the company.

Job Expectations

Designs and develops pipelines that support data ingestion, curation, and provisioning of complex enterprise data to support analytics and reporting in our current technology stack.
Provides successful deployment and provisioning of data solutions to required environments.
Designs and builds data architecture and applications that successfully enable speed, quality, and efficient pipelines.
Under the guidance of a senior lead member, full participation in the data pipeline continuous integration and continuous delivery (CI/CD) processes is required.
Work under guidance of a senior lead member to develop data pipeline jobs throughout their lifecycle.
Assist in the design and build efficient data models for robust business intelligence, analytics, and engineering needs that remain.
Demonstrate initiative by seeking potential business issues and proactively solve them.
Analyze and translate business needs into data models to support long-term, scalable, and reliable solutions.
Interacts with cross-functional customers and development team to gather and define requirements.
Reviews discrepancies in requirements and resolves with stakeholders in a timely manner.
Build strong cross-functional partnerships with Data Scientists, Analysts, Product Managers and Software Engineers to understand data needs and deliver on those needs.
The duties and responsibilities described above may provide only a partial description of this position. This is not an exhaustive list of all aspects of the job. Other duties and responsibilities not outlined in this document may be added as necessary or desirable, with or without notice.


Knowledge, Skills And Abilities
1+ years of programming skills with Python.
Experienced in Agile methodologies & DevOps approach to maintaining pipelines and databases.
Excellent knowledge of software engineering fundamentals.
Experience in developing pipelines with CICD principles
Proficiency with Databricks (DLT, Medallion Architecture, Lakehouse Concepts, etc) preferred. 
Experience with SQL.
Had exposure to data modeling principles and patterns (star and snowflake DM, ER).
Relational and non-relational data structures, theories, principles, and best practices.
Knowledge of data privacy regulations (GDPR, CCPA, CRPA) and the impact these regulations have on data engineering framework.
Strong problem-solving and analytical skills.
Passion for data engineering and for enabling others by making their data easier to access.
Be proactive, requiring minimal supervision with strong time management or organization skills.
Must be an inquisitive learner and have a thirst for improvement.
Excellent verbal and written communication skills.
Equipment Knowledge

Experience with Microsoft Office Suite (Word, Excel, PowerPoint)
Experience with Google Business Suite (Gmail, Drive, Docs, Sheets, Forms) preferred.
Databricks Engineer Associate Certification preferred.
AWS Certifications, preferred
■AWS Certified Solutions Architect – Associate/Professional

■AWS Certified Developer– Associate/Professional

■AWS Certified DevOps Engineer – Associate/Professional

■AWS Certified Data Analytics

■AWS Certified Security - Specialty

■AWS Certified Cloud Practitioner


Experience Requirements
1-3 years as a Data Engineer


Education Requirements
Bachelor or Master's degree in Computer Science, Engineering, Information Systems, or related (STEM) fields preferred, or a combination of education and equivalent work experience

Judgment/Reasoning Ability Able to identify, troubleshoot and resolve problems quickly using sound judgment, poise and diplomacy. Ability to use judgment and reasoning skills, and determine when to escalate issues, as required, in a timely manner.

Physical Demands The physical demands described here are representative of those that must be met by a Team Member to successfully perform the essential functions of this job. While performing the duties of this job, the Team Member is regularly required to talk and hear. The Team Member is frequently required to sit, walk, climb stairs, use hands and fingers, bend, stoop and reach with hands and arms. Reaching above shoulder heights, below the waist or lifting as required to file documents or store materials throughout the work day. The Team Member may occasionally lift or move office products and supplies up to 25 pounds. Proper lifting techniques required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

Work Environment The noise in the work environment is usually moderate. Other factors are

Hectic, fast-paced with multi-level distractions
Professional, yet casual work environment
Office / Warehouse environment
Ability to work extended hours as required
The anticipated pay scale for this position can be found below, however the pay range applicable to you may vary by geographic location based on where the job is located or where you work. The final pay offered to a successful candidate will be dependent on several factors that may include but are not limited to the type and years of experience within the job, the type of years and experience within the industry, education, etc. iHerb, LLC is a multi-state employer and this pay scale may not reflect positions that work in other states or locations.

Employees (and their families) that meet eligibility criteria as outlined in applicable plan documents are eligible to participate in our medical, dental, vision, and basic life insurance programs and may enroll in our company’s 401(k) plan. Employees will also be eligible for Time Off and Paid Sick Leave pursuant to the company’s policies. Employees will enjoy paid holidays throughout the calendar year. Eligibility requirements for these benefits will be controlled by applicable plan documents.

Hired applicant may be awarded Restrict Stock Units and receive annual bonuses pursuant to eligibility and performance criteria defined in the respective plan documents and policies.

For more information on iHerb benefits, visit us at iHerbBenefits.com.

Anticipated Pay Scale

$69,510—$126,382 USD


Staffing Agency Submission Notice
iHerb does not accept unsolicited 3rd party (""Agency"") candidates. If you are an Agency, please send any requests to be considered as a supplier in our Vendor Management System to staffingvendors@iherb.com. Do not contact iHerb employees directly. If requested to work on a role, any Agency candidates would be presented through the internal recruiting organization.


About IHerb
iHerb is on a mission to make health and wellness accessible to all. We offer Earth’s best-curated selection of health and wellness products, at the best possible value, delivered with the most convenient experience.

We’re the world’s largest eCommerce platform dedicated to vitamins, minerals, and supplements, and other health and wellness products. For more than 25 years, we’ve been making it simple for people all over the world to purchase the highest quality products. From supplements to skincare to grocery items, we ship over 50,000 products, from over 1,800 brands direct to our customers in 180+ countries.

Our vision is to become the #1 destination for health and wellness across the world.

With a passion for wellness and a mind for innovative solutions, iHerb team members share a vision for a healthier world that drives them each day. Our 5 Shared Values unite our global team

Focus on the Customer

 Empower Our People
 Be Entrepreneurial & Pivot Quickly
 Embrace Diversity & Inclusion
 Strive for Simplicity

IHerb Benefits
At iHerb, we are dedicated to offering programs designed to help our employees and their families stay healthy, live well, and plan for their financial future. Built on a strong foundation, our programs provide options and upgrades with flexibility, protection, and security in mind. For the comprehensive benefits list, visit www.iHerbBenefits.com. For our international team members, you may be eligible for benefits depending on the country where you are employed. The Talent Acquisition Partner/local HR representative will go over the benefits you are eligible for.

iHerb is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status. iHerb provides equal employment opportunities to all applicants for employment and prohibits discrimination and harassment.","Analítica de datos, Canalizaciones de datos y Ingeniería de datos, Capacidad de razonamiento, Comunicación, Diplomacia, Modelado de datos, Modelo de datos, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3977961487/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=nsU8%2B8nBlTjLf1L3gI4Q2g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,"California, Estados Unidos","Acerca del empleo
Position:
Data Engineer
Remote
Only W2

Position Responsibilities:
Build high-performance algorithms, prototypes, and enable predictive models
Understands business requirements and applies them to complex programming and analysis.
Communicates (oral and written) recommendations and mentors/provides guidance to less experienced colleagues.
Partners with various departments to understand and incorporate standards information and requirements into work procedures.
Identifies, analyzes and provides feedback to departmental standards, norms, and new goals/objectives.
Analyzes existing applications and systems and formulates logic for new systems, devises logic procedures, logical database design, performs coding and tests/debugs programs with an operational mindset.
Works on complex data & analytics-centric problems having broad impact that require in depth analysis and judgment to obtain results or solutions.
Designs and deploys new complex Enterprise systems and enhancements to existing systems ensuring compatibility and inter-operability.
Resolves application programming analysis problems of broad scope within procedural guidelines. May seek assistance from the supervisor or more skilled programmers/analysts on unusual or especially complex problems that cross multiple functional/technology areas.
Delivers best-in-class software as part of a software delivery team.
Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed
Plans work to meet assigned general objectives; progress is reviewed upon completion and solutions may provide an opportunity for creative/non-standard approaches.

Qualifications:
Experience in data acquisition, data set process, improving data reliability, quality and efficiency
Experience developing in Python using Spark and other data tools
Familiar with AWS technologies like Glue, S3 and RedShift
Experience with large data sets to address business issues
Experience with modern software delivery practices, including source control, testing, continuous delivery
Experience delivering product with Agile methodologies
Humble – is open to being coached, has high EQ and is self-aware
Hungry – desires to get things done while honoring people, and seeks better ways to do the job
Collaborative – has strong interpersonal skills; cares about and works well with teammates
Willingness to impact beyond defined role
Bachelors Degree in Computer Science or job-related discipline or equivalent experience","Amazon Web Services (AWS), Ingeniería de datos , PySpark, Python y Responsabilidad como profesional, AWS Glue y Amazon Redshift",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979916138/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=glYA%2BHR7YNk%2FCPekGI4Kxg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,"San Diego, CA","Acerca del empleo
At FuturHealth, we're on a mission to create a product where every individual feels inspired and empowered to confidently take charge of their wellbeing. We believe in and are dedicated to offering personalized and holistic approaches to combat health concerns.

Operating at the intersection of health and technology, we provide a comprehensive range of services, including telemedicine, personalized nutrition plans, medication delivery, and insurance solutions. We believe that wellbeing is deeply personal, and the path to becoming the best version of yourself is paved with progress, not perfection. Our central focus is on utilizing scientifically-proven prescriptions and telehealth initiatives to tackle obesity head-on.

Our Data Team is the backbone of informed decision-making and strategic planning. We collaborate with all stakeholders across the organization, including Finance, Customer Experience, Marketing, Product, and more. Our team is the primary source of truth, ensuring that data is accurate, reliable, and accessible to drive business success.

What You Will Do:

Design, develop, and maintain ETL processes to ensure reliable data pipelines.
Work closely with data analysts to understand data requirements and deliver high-quality datasets.
Optimize data pipelines for performance and scalability.
Collaborate with cross-functional teams to gather requirements and deliver data solutions.
Monitor and troubleshoot data pipelines and infrastructure to ensure consistent data flow.
Perform data analysis to support business decision-making.
Develop and maintain dashboards and reports to communicate data insights.
Conduct data validation and quality checks to ensure data accuracy and consistency.
Provide ad-hoc data support and analysis for various business needs.

It's a Perfect Match If You Have:

At least 3 years of experience in Data Engineering and 5 years of experience in Data field, 
Strong knowledge of SQL and Python.
Ability to understand processes from both a business and engineering perspective.
Experience with Google BigQuery or other MPP systems.
Understanding of data modeling.
Proficiency in data analysis and visualization tools (e.g., Tableau, Power BI).
Experience in building and maintaining reports and dashboards.

Preferred Qualifications:

Experience with our stack: Python, GCS, BigQuery, DBT, Airflow, Metabase
Experience implementing Data Quality solutions.
Familiarity with monitoring and alerting tools (Grafana, Prometheus)

Additional Company Info

Founded by a team with a proven track record of delivering exceptional experiences to over 25 million consumers, they began by revolutionizing personalized nutrition through an intuitive mobile app that adapts meal plans to individual metabolisms and preferences. With seamless integrations with food services and a thriving community of like-minded individuals, the formation of FuturHealth began to further empower people on their health journey.

Securing Series Seed Funding in December 2023, FuturHealth is backed by Corazon Capital, Oversubscribed Ventures, and InterAlpen Partners and already achieved unprecedented revenue growth of 600% from Q4 2023 to Q1 2024; we are also proud to be maintaining this impressive momentum. We are currently seeking visionary people to join us in our journey to revolutionize healthcare and empower individuals to lead healthier lives. If you are passionate about making a meaningful impact and driving innovation in the health and wellness and technology space, we invite you to explore opportunities and be a part of the future of healthcare.

What we can offer:

A comprehensive and generous total rewards package
Health and Dental Insurance coverage
Fully remote work policy
Unlimited Paid time offAuthentic startup exposure
Enabling direct impact on our expanding venture
Embrace a culture valuing autonomy and accountability, where success is result-driven

Note: Benefits and compensation listed may vary based on the country of your employment and the nature of your employment with Futurhealth.","Airflow, Analítica de datos, Extraer, transformar y cargar (ETL), Google BigQuery y Ingeniería de datos, Calidad de datos, Datasets, Modelado de datos, Panel de control y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3910529473/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=2uHtikl5n9gqKEnlDTutHA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"90 US$K/año - 130 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 3 semanas,Estados Unidos,"Acerca del empleo
Datavant protects, connects, and delivers the world’s health data to power better decisions and advance human health. We are a data logistics company for healthcare whose products and solutions enable organizations to move and connect data securely. Datavant has a network of networks consisting of thousands of organizations, more than 70,000 hospitals and clinics, 70% of the 100 largest health systems, and an ecosystem of 500+ real-world data partners.

By joining Datavant today, you’re stepping onto a highly collaborative, remote-first team that is passionate about creating transformative change in healthcare. We hire for three traits: we want people who are smart, nice, and get things done. We invest in our people and believe in hiring for high-potential and humble individuals who can rapidly grow their responsibilities as the company scales. Datavant is a distributed, remote-first team, and we empower Datavanters to shape their working environment in a way that suits their needs.

The Healthjump delivery data engineers are masters of data ETL. We create and maintain integrations for over 60+ EHR’s powering every patient use case you can think of. Our team is full of go getters that work in a matrixed 100% remote environment on agile teams that support customers from go live to support for the live of the integration.

You will:

Create and maintain ELT/ETL processes for existing and new systems.
Collaborate with development and business teams to understand requirements and define source system data flows
Develop and maintain ETL/ETL specifications for data integration development
Define and deliver consistent data modeling and data architecture standards, methodologies, guidelines and techniques
Document, implement and maintain the data pipeline architecture and related business processes
Serve as a source of knowledge of industry practices and processes.
Participate in the development of enterprise standards and guidelines for data model quality and accuracy
Audit project level data model quality deliverables to ensure that practices and standards are met
Analyze information and data requirements and understand effects of data inconsistencies
Identify inefficiencies in current architecture and processes and communicate solutions in a manner that gets support from the teams involved
Perform cost and sizing estimates for projects
Collaborate with the project coordinator and the rest of the agile team to identify epics, stories and estimate effort
Create and maintain data dictionary documents, table and data lineage models and produce artifacts to support project development and communicate project information to customers


Requirements:

Bachelors in Computer Science or other engineering degree equivalent


What you will bring to the table:

Master of SQL and Python Languages
Experience with using Airflow


Bonus points if Requirements:

Healthcare data experience
Knowledge of clinical systems (e.g. Cerner, Epic, Meditech, etc.) and standard Acute/Ambulatory workflows.
Preferred AWS Cloud Practitioner or above certification


We are committed to building a diverse team of Datavanters who are all responsible for stewarding a high-performance culture in which all Datavanters belong and thrive. We are proud to be an Equal Employment Opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, or other legally protected status.

Our compensation philosophy is to be externally competitive, internally fair, and not win or lose on compensation. Salary ranges for this position are developed with the support of benchmarks and industry best practices.

We’re building a high-growth, high-autonomy culture. We rely less on job titles and more on cultivating an environment where anyone can contribute, the best ideas win, and personal growth is driven by expanding impact. The range posted is for a given job title, which can include multiple levels. Individual rates for the same job title may differ based on their level, responsibilities, skills, and experience for a specific job. The estimated salary range for this role is $90,000 - $130,000.

At the end of this application, you will find a set of voluntary demographic questions. If you choose to respond, your responses will be anonymous and used to help us identify areas of improvement in our recruitment process. (We can only see aggregate responses, not individual responses. In fact, we aren’t even able to see if you’ve responded or not.) Responding is your choice and it will not be used in any way in our hiring process.","Airflow, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Analyze Information, Ciencias de la computación, Comunicación, Diccionario de datos y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3820944169/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=dgtN28ToFIIKtXvkQ4OEBg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 3 semanas,Estados Unidos,"Acerca del empleo
StackAdapt is a self-serve advertising platform that specializes in multi-channel solutions including native, display, video, connected TV, audio, in-game, and digital out-of-home ads. We empower hundreds of digitally-focused companies to deliver outcomes and exceptional campaign performance everyday. StackAdapt was founded with a vision to be more than an advertising platform, it’s a hub of innovation, imagination and creativity.

We're looking to add Data Engineers to our data science team! This team works on solving complex problems for StackAdapt's digital advertising platform. You'll be working directly with our data scientists, data engineers, Engineering team, and CTO on building pipelines and ad optimization models. With databases that process millions of requests per second, there's no shortage of data and problems to tackle.

Want to learn more about our Data Science Team: https://alldus.com/ie/blog/podcasts/aiinaction-ned-dimitrov-stackadapt/

Learn more about our team culture here: https://www.stackadapt.com/careers/data-science

Watch our talk at Amazon Tech Talks: https://www.youtube.com/watch?v=lRqu-a4gPuU

StackAdapt is a Remote First company, and we are open to candidates located anywhere in the US for this position.

What you'll be doing: 

Design modular and scalable real time data pipelines to handle huge datasets
Understand and implement custom ML algorithms in a low latency environment
Work on microservice architectures that run training, inference, and monitoring on thousands of ML models concurrently

What you'll bring to the table:

Have the ability to take an ambiguously defined task, and break it down into actionable steps
Have deep understanding of algorithm and software design, concurrency, and data structures
Experience in implementing probabilistic or machine learning algorithms
Interest in designing scalable distributed systems
A high GPA from a well-respected Computer Science program
Enjoy working in a friendly, collaborative environment with others

StackAdapters enjoy:

Competitive salary + equity
401K matching
3 weeks vacation + 3 personal care days + 1 Culture & Belief day + birthdays off
Access to a comprehensive mental health care platform
Health benefits from day one of employment
Work from home reimbursements
Optional global WeWork membership for those who want a change from their home office
Robust training and onboarding program
Coverage and support of personal development initiatives (conferences, courses, etc)
Access to StackAdapt programmatic courses and certifications to support continuous learning
Mentorship opportunities with industry leaders
An awesome parental leave policy
A friendly, welcoming, and supportive culture
Our social and team events!

If this role speaks to you then please apply - we'd love to speak with you. Due to a high volume of interest, only those shortlisted for interview will be contacted.

StackAdapt is a diverse and inclusive team of collaborative, hardworking individuals trying to make a dent in the universe. No matter who you are, where you are from, who you love, follow in faith, disability (or superpower) status, ethnicity, or the gender you identify with (if you’re comfortable, let us know your pronouns), you are welcome at StackAdapt. If you have any requests or requirements to support you throughout any part of the interview process, please let our Talent team know.

About StackAdapt

We've been recognized for our diverse and supportive workplace, high performing campaigns, award-winning customer service, and innovation. We've been awarded:

Ad Age Best Places to Work 2024

G2 Top Software and Top Marketing and Advertising Product for 2024

Campaign’s Best Places to Work 2023 for the UK

2024 Best Workplaces for Women and in Canada by Great Place to Work®

#1 DSP on G2 and leader in a number of categories including Cross-Channel Advertising","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación y Datasets",Solicitar
https://www.linkedin.com/jobs/view/3983286295/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=MsJPT4jQ237yB8ZWCHOZVw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"60 US$/h - 70 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 días,"Chicago, IL","Acerca del empleo
Description

Job Description:

We are seeking a highly skilled and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a crucial role in building, optimizing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to ensure data quality, governance, and efficient data flow throughout the organization.

Core Skills

Strong Software Engineering background, in Java, RESTful APIs, any experience with Springboot is valued
Strong Python skills along with popular and widely used Python libraries, performing data cleansing, wrangling, augmentation,
Exploratory data analysis, building cloud data pipelines, data lineage, governance(bronze, silver, gold), Data Quality analysis, Featurization techniques, sampling.
SQL, NoSQL Databases, Elasticsearch
Awareness of Vector databases, LM/LLM/GEN-AI based tools, libraries and frameworks(e.g; LangChain, Agentic, Semantic Kernel).
Awareness of ML models like Random forest, KNN, Time series forecasting
Awareness of Neural Network Architectures, Transformers, ANN, BERT, GPT models, Open Source foundational models(e.g; LLama2/3).
Awareness/usage of mlFlow
Cloud(Azure is valued over AWS/GCP) based managed services(e.g; Azure OpenAI, Azure ML, Azure Data fcatory, ADLS GEN2).
Awareness of Performance evaluation of ML/DL/LLM systems, drift handling
Experience with PySpark/Databricks is valued, not mandatory.

Requirements

Education:

Master's degree (preferred) or Bachelor's degree in Computer Science, Mathematics, Statistics, Engineering, Information Technology, or related fields

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Analítica de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y PySpark, Bases de datos, Calidad de datos, Limpieza de datos y Transferencia de Estado Representacional (REST)",Solicitar
https://www.linkedin.com/jobs/view/3970059125/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=jAQNMrKS48YYhqfZ741ygA%3D%3D&trk=flagship3_search_srp_jobs,Data/Analytics Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
Here at Wilson Language Training, we are committed to working together for our mission to achieve literacy for all. We believe literacy is a fundamental right and should be attainable for all people. We strive to reflect this belief in our work.

The success of our team members is no less paramount. We’re dedicated to ensuring that every Wilson employee experiences truly satisfying professional development while feeling inspired to bring their authentic selves to work. Are you ready to be a changemaker?

Wilson Language training is growing and is looking to hire a Data/Analytics Engineer.

As an Analytics Engineer you will bridge the gap between data engineering and data analysis. You will be responsible for building and maintaining robust data pipelines, transforming raw data into actionable insights, and collaborating with various teams to support data-driven decision-making.

Key Responsibilities

Data Pipeline Development: Design, develop, and maintain scalable data pipelines to ingest, process, and store large volumes of data from various sources.
Data Modeling: Create and manage data models to support analytics and reporting needs, ensuring data accuracy and consistency.
ETL Processes: Implement ETL (Extract, Transform, Load) processes to ensure the smooth flow of data from source systems to data warehouses.
Collaboration: Work closely with data analysts and business stakeholders to understand data requirements and deliver data solutions that meet their needs.
Data Quality: Implement data quality checks and monitoring to ensure the reliability and accuracy of data.
Optimization: Optimize data processing and storage for performance and cost efficiency.
Documentation: Maintain comprehensive documentation of data pipelines, models, and processes.
AdHoc Querying & Data Integrations: Assist Business Systems team with adhoc querying and reporting from ERP/CRM systems. Help with data integrations between these systems and our custom built applications.
Innovation: Stay up-to-date with industry trends and best practices in data engineering and analytics, and apply them to improve existing processes.
Understand and display WLT’s values.
Other duties as assigned

Qualifications

Education: Bachelor’s degree in Computer Science, Data Science, Information Technology, or a related field. A master’s degree is a plus.
Experience: 3+ years of experience in data engineering, analytics, or a related field.
Technical Skills: Proficiency in Microsoft SQL, dBT, Python, and/or R. Experience with data pipeline tools (e.g., Apache Airflow, Luigi) and data warehousing solutions (Azure SQL, Azure Fabric, Azure Synapse).
Tools: Familiarity Microsoft Power BI.
Cloud Platforms: Experience with cloud platforms like Azure Fabirc.
Problem-Solving: Strong analytical and problem-solving skills with a keen attention to detail.
Communication: Excellent communication skills with the ability to explain complex technical concepts to non-technical stakeholders.

Preferred Qualifications

Knowledge of machine learning and predictive analytics.
Experience with version control systems (e.g., Git).
Understanding of data governance and security best practices.

Wilson has identified the anticipated pay range for this role based on the many factors that we consider in defining compensation levels for our roles, including market data, and internal equity considerations. Actual pay, and allocation between base and any target discretionary bonus, will vary based on geographic location, education, work experience, skills, market data, and internal equity considerations. Wilson offers competitive benefits, including:

Medical, dental, vision, and Life & Disability Insurance
401k plan with partial employer match
Paid Time Off
Paid holidays
Tuition reimbursement
“O’Connor days,” which refers to a company-wide office closure between Christmas and New Year’s Eve, as well as other perks.

Anticipated Salary range: $117,000 - $152,000.

Wilson Language Training is an Equal Opportunity, Drug-Free Employer Committed to Diversity in the Workplace. M/W/D/V","Git, Azure Service Fabric y SQL de Azure",Solicitud sencilla
https://www.linkedin.com/jobs/view/3973741662/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=IjJXaDzB3sHADX1gidndRA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"San José, CA","Acerca del empleo
Role Description

Data Engineer

Associate II - Data Engineer

Who We Are

Born digital, UST transforms lives through the power of technology. We walk alongside our clients and partners, embedding innovation and agility into everything they do. We help them create transformative experiences and human-centered solutions for a better world.

UST is a mission-driven group of over 39,000+ practical problem solvers and creative thinkers in over 30+ countries. Our entrepreneurial teams are empowered to innovate, act nimbly, and create a lasting and sustainable impact for our clients, their customers, and the communities in which we live.

With us, you’ll create a boundless impact that transforms your career—and the lives of people across the world.

Visit us at UST.com.

You Are

UST is searching for a Data Engineer who will interpret data and analyses results using statistical techniques under supervision.

The Opportunity

 Complies and assist in mining and acquiring data from primary and secondary sources; reorganizing the data in a format that can be easily read by either a machine or a person.
 Assist in identifying analyzing and interpreting trends or patterns in data sets' generating insights and helping clients make better decisions.
 Conducts research on specific data sets to enable the senior analysts in their work.
 Assist in managing master data including creation updates and deletions.
 Help developing reports and analysis that effectively communicate trends patterns and predictions using relevant data.
 Provide support with technical writing and editing as required.
 Develop analytics to identify trend lines across several data sources within the organization.
 Assists senior analysts in examining and evaluate existing business processes and systems and offer suggestion for changes.

This position description identifies the responsibilities and tasks typically associated with the performance of the position. Other relevant essential functions may be required.

What You Need

 UDP Knowledge
 Data governance and management
 Data processing
 Storage
 Transformation
 Data Modeling
 ⁠Data Visualization
 Master Data management
 ⁠ETL
 Technologies: AWS, Azure, Queries, My SQL, Oracle, Snowflake
 Advanced English

What We Believe

We’re proud to embrace the same values that have shaped UST since the beginning. Since day one, we’ve been building enduring relationships and a culture of integrity. And today, it's those same values that are inspiring us to encourage innovation from everyone to champion diversity and inclusion, and to place people at the center of everything we do.

Humility

We will listen, learn, be empathetic and help selflessly in our interactions with everyone.

Humanity

Through business, we will better the lives of those less fortunate than ourselves.

Integrity

We honor our commitments and act with responsibility in all our relationships.

Equal Employment Opportunity Statement

UST is an Equal Opportunity Employer.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

UST reserves the right to periodically redefine your roles and responsibilities based on the requirements of the organization and/or your performance.

#UST

Skills

Microsoft Technologies,Data Management,Query Optimization


Aptitudes y experiencia deseables
Microsoft Technologies,Data Management,Query Optimization","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Gobierno de datos, Ingeniería de datos y Visualización de datos, Comunicación, Modelado de datos y Redacción técnica",Solicitar
https://www.linkedin.com/jobs/view/3958102359/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=l0IGGFx1PmSsx51b16zyJg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Brentwood, TN","Acerca del empleo
Remote | Product and Development | Full-Time 

Conexess Group is aiding a large financial client in their search for a senior Data Engineer. This is an immediate, full time job opening and we are only able to accept candidates that are eligible to work on W2. This is an individual contributor role, working closely with other data team members to complete the building out of ETL pipelines and Operational Data Store (ODS). Your day-to-day tasks will include extracting data from company applications, transforming it according to business logic, and efficiently loading it into our Operational Data Store (ODS). You will work with members from the cross-functional team to help them with the consumption of the data. You will be expected to use your expertise to optimize the process for performance and accuracy. Your commitment to continuous improvement will drive enhancements in our data architecture and ensure that our data system can scale.

Responsibilities

 Daily Responsibilities 

 Contribute to building out ETL pipeline to include data from all company products 
 Write performant and maintainable code 
 Ensure the quality of ETL process 
 Communicate well with other members of the data team and across the functional area 
 Advocate for best practices and continuous improvement 
 Participate in architecting and building a scalable data system 
 Mentor team members on their area of expertise 

 Required Skills: 

 3&plus; Years of experience as Data Engineer designing and implementing data warehouse and data lake solutions on Microsoft Azure. 
 Experience with Azure Data Factory is required 
 Azure Data Lake is a plus 
 Exposure to Microsoft Fabric is a plus 
 Very strong background in MS SQL Server is required 
 Expertise in data architecture using different database types and data formats 
 Expertise in building data pipelines to clean, enrich, and transform data 
 Experience with Python is a plus and highly desirable 
 Expertise in database design and tuning techniques 
 Strong understanding of ETL process and tooling 
 Experience with version control systems (Azure DevOps, Git) 
 Familiar with CI/CD concept 

 Preferred Skills: 

 Experience working in horizontally scaling systems 
 Exposure to Azure SQL/warehouse/data lake products such as Azure SQL, Synapse, Databricks, ADLS 
 Exposure to Microsoft Fabric is a plus 
 Experience with other cloud data stacks (Google, AWS) is a plus 
 Familiarity with message/event driven architecture patterns and distributed systems architecture 
 Familiarity with systems integration 
 An automation mindset 

 Expectations: 

 Intentional mentorship: We are dedicated to teaching and growing talent and expects everyone to help those less experienced. 
 Honesty: Whether reviewing someone's code, participating in retrospectives, or working with your team on what direction to take a project we expect openness and honesty. Honesty creates trust, and we believe that all great teams are built on trust. 
 Low Ego: Have confidence in your skills and experience but be willing to alter your opinions and ideas when another, better one comes along. Have strong opinions, but loosely held. 
 Deep Curiosity: You will be expected to research new and exciting technologies, perfect the use of existing technologies, and Client new libraries and tools that can affect change across the organization. 
 Motivation: You are a natural self-starter, and you enjoy solving problems. You can solve the problem with minimal instruction and figuring out what should be done. 

 Benefits Information: 

 A fun, fast-paced work environment 
 Responsible PTO Plan that meets or exceeds state and local medical and family leave laws 
 11 paid holidays 
 Community and social events to keep you connected and engaged 
 Mental Health Benefits 
 Medical, Dental and Vision insurance 
 Company-paid Group Life Insurance, Short- and Long-Term Disability 
 Flexible Spending Account & Health Savings Account 
 Aflac Benefits – Critical Illness, Cancer Protection, & Hospital Choice 
 Pet Insurance 
 401 (k) with company match with eligibility on Day 1 of employment 
 2 Paid Volunteer Time Off Days 
 And much more!","Almacenamiento de datos, Arquitectura de datos, Azure Data Factory, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Arquitectura técnica, Comunicación, Control de versiones y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3973041895/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=2X2WZYHG%2FaM155i9PDmSFg%3D%3D&trk=flagship3_search_srp_jobs,Data Reporting Engineer,"110 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,Estados Unidos,"Acerca del empleo
VideoAmp is on a mission to create the best employee and workplace experience where people can bring their whole self to work everyday. We believe that accomplishing something great requires a special group of people who work hard, drive results and have a blast while doing it - people who challenge the status quo and embody our values. People who say “I’ll find a way” instead of saying “it can’t be done.”

At VideoAmp, we believe in challenging advertising paradigms to maximize value for clients. We do this by enabling companies to execute on business outcomes across their media investment instead of more traditional media metrics. VideoAmp is the software and data solutions company powering the convergence of linear TV and digital video advertising. Our solutions connect linear TV viewership with addressable data assets to benefit the marketing and media industries. This enables marketers and content owners to holistically plan, transact, and measure deduplicated audiences across digital video, OTT, connected and linear TV advertising.

Data Reporting Engineer, (Location: United States, Remote)

The VideoAmp Engineering team is looking for a Data Reporting Engineer who will be passionate about creating quality, trusted solutions as part of a collaborative and dynamic organization. The successful candidate will be a detailed-oriented self-starter with a passion for growth and a high standard for their work. Technologies We Use: Sigma, Tableau, AWS, Google Cloud, Kubernetes, Databricks, Airflow, Spark, Postgres, Snowflake, Go, Python, gRPC, Bazel

Qualifications

Experience with BI tools such as Sigma, Tableau, and Looker
Experience telling a story with data to an audience of non-technical Adtech clients
Experience in structuring big data and turning it into easily digestible visualizations
Experience in AdTech ecosystem, linear, streaming and digital advertising
Experienced in SaaS solutions that automate traditional media planning, measurement and activations
Fluency in working with SQL, Python, or HTMX

Additional Preferred Experience

Experience with Javascript
Experience building ETL/ELT stream/batch pipelines on big data platforms such as Snowflake, Spark or others
Collaborate with peers across the entire range of development activities that includes distilling engineering designs from product requirements and data science, development of work plans, implementation, testing, productization, monitoring, and maintenance
Strong problem-solving skills in optimizing solutions for improved performance, scalability and reduced infrastructure costs
Understanding of ad-tech terms and methodologies a plus
Experience with data privacy and secure architectures. Experience with data cleanrooms a plus
B.S. or equivalent in Computer Science, Math, or similarly technical field preferred. Advanced degree is a plus

What We Offer

This position has a minimum salary of $110,000.00 + Equity + Benefits. The actual compensation offer will be determined by a number of factors, including, but not limited to, applicant's qualifications, skills, and experience.

Discretionary and flexible paid time off
In addition to standard US holidays off, VideoAmp employees receive a full week off for Summer Break and another full week off for Winter Break
Comprehensive medical, dental, and vision benefits for you and your dependents - including multiple options fully covered by VideoAmp
Professional and personal support with licensed therapists and career coaches
Unlimited financial wellness sessions with Origin financial advisors
401k Plan
Pre-Tax Savings Plans, HSA & FSA
$10,000 towards services and treatments that support family planning
Cell Phone Reimbursement
Paid Maternity and Parental Leave for All Family Additions

We are on a collective mission to bring equity and transparency into media measurement, and that begins with an honest, inclusive culture. At VideoAmp, we empower our team to thrive through collaboration, mentorship, and innovation. If you're passionate about all things data and tech, and have extensive knowledge and curiosity around the digital media space, we’d love to hear from you.

Above all, if you're eager to influence and support the revolutionary goals of a dynamic, cross-functional team - join us and make an impact!","Analítica, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Minería de datos y Visualización, Elaboración de informes de datos, Resolución de problemas y Tecnología en publicidad",Solicitar
https://www.linkedin.com/jobs/view/3846989674/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=bVTB1Qb1kmG5V0cnYnx3zg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 4 meses,Estados Unidos,"Acerca del empleo
Location: Remote

Duration: 6 Months+

Data Analytics Engineer

Data Analytics Engineer is responsible for understanding, preparing, processing and analyzing data to make data valuable and useful for operations decision support.

Accountabilities in this role

Partners with Business Analysis and Analytics teams.

Demonstrates problem-solving ability that allows for effective and timely resolution of system issues, including but not limited to production outages.

Develops and supports Standard processes to harvest data from various sources and perform data blending to develop advanced data sets and analytical cubes and data exploration.

Queries, data exploration and transformation, basic statistical methods.

Writes Python scripts to databases.

Focuses on database work with a blend of strong technical and communication skills.

Demonstrates ability to learn and navigate in large complex environments.

Required Skills

5+ years of Data Engineering experience
Database development experience
3+ years of Python experience
BI experience with Looker
Experience with HealthCare EHR data, and FHIR standards, 
Experience with Data modeling, 

Knowledge of API proxies","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Desarrollo de base de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, Bases de datos, Comunicación y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3903879731/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=FVm1OSzTy4eoOuo0AdAg0A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"San Francisco, CA","Acerca del empleo
About This Role

Strava is the leading digital community for active people with more than 125 million athletes, in more than 190 countries. The platform offers a holistic view of your active lifestyle, no matter where you live, which sport you love and/or what device you use. Everyone belongs on Strava when they are pursuing an active life.

We are seeking data engineers to join our Data Platform team. Our vision is that key decisions and product at Strava can be greatly enriched with data to benefit athletes and the business. Our mission is to build and support a platform that enables access to data through self-service and extensible tools.

This means we listen to all corners of the company for opportunities where data can make a difference. We distill this down and use modern technologies to develop both generalized and special-purpose data solutions.

For more information on compensation and benefits, please click here.

This is a hybrid role based in our San Francisco office.

You’re excited about this opportunity because you will:

Collaborate with people across teams and functions that hold deep curiosity for data.
Work with hefty data systems at the global scale of Strava.
Deliver value more through software, leaning into tooling and automation rather than repetitive toil.
Grow your expertise in the steadily evolving technologies and ecosystem of data.


You will be successful here by:

Holding empathy for the users of our platform to truly understand the challenges we tackle for them.
Fostering an inclusive and motivating team culture to help everyone achieve their best.
Caring about high quality and reliable code, but also the end user experience.
Solving problems efficiently, creatively, and completely despite constraints in time or resources.
Understanding how critical it is we maintain a high bar of data security and privacy.


We’re excited about you because you:

Have the ability to adapt and apply evolving data technologies to business needs (which means the list of bullets below will change over time!).
Have developed software using programming languages like Python, Scala, Java, Go, Ruby, etc.
Have sufficient familiarity to understand SQL queries in the context of data pipelines (i.e. dbt).
Have experience with distributed data tools (i.e. Spark, Flink, Kafka) on large datasets.
Have worked with cloud-data warehouses (i.e. Snowflake, BigQuery, Redshift) or other warehousing solutions.
Have an understanding of underlying infrastructure needed to serve production services (i.e. Kubernetes, AWS, GCP, Azure).


About Strava

Strava is Swedish for “strive,” which epitomizes who we are and what we do. We’re a passionate and committed team, unified by our mission to connect athletes to what motivates them and help them find their personal best. And with billions of activity uploads from all over the world, we have a humbling and audacious vision: to be the record of the world’s athletic activities and the technology that makes every effort count.

Strava builds software that makes the best part of our athletes’ days even better. And just as we’re deeply committed to unlocking their potential, we’re dedicated to providing a world-class, inclusive workplace where our employees can grow and thrive, too. We’re backed by Sequoia Capital, Madrone Partners and Jackson Square Ventures, and we’re expanding in order to exceed the needs of our growing community of global athletes. Our culture reflects our community – we are continuously striving to hire and engage diverse teammates from all backgrounds, experiences and perspectives because we know we are a stronger team together.

Despite challenges in the world around us, we are continuing to grow camaraderie and positivity within our culture and we are unified in our commitment to becoming an antiracist company. We are differentiated by our truly people-first approach, our compassionate leadership, and our belief that we can bring joy and inspiration to athletes’ lives — now more than ever. All to say, it’s a great time to join Strava!

Strava is an equal opportunity employer. In keeping with the values of Strava, we make all employment decisions including hiring, evaluation, termination, promotional and training opportunities, without regard to race, religion, color, sex, age, national origin, ancestry, sexual orientation, physical handicap, mental disability, medical condition, disability, gender or identity or expression, pregnancy or pregnancy-related condition, marital status, height and/or weight.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.

California Consumer Protection Act Applicant Notice","Almacenamiento de datos, Apache Kafka, Apache Spark, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery y Ingeniería de datos, Amazon Redshift y Datasets",Solicitar
https://www.linkedin.com/jobs/view/3972706730/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=YS0CO42Hi88C3gMxTvt0rQ%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer I, PLEX-SIA","Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 2 semanas,"Austin, TX","Acerca del empleo
Description

Amazon is looking for a Data Engineer (DE) to join the North America Supply Chain (NASC)'s Science, Intelligence, & Analytics (NASC-SIA) team.

The successful candidate will be a self-starter, comfortable with ambiguity, and able to create and maintain automated processes efficiently. They will set the vision, define the roadmap, and work alongside stakeholders across organizations to deliver results. They know and love working with various AWS Cloud and Data Technologies, understand the principles and techniques of distributed computing, and are enthusiastic about working with data in diverse forms, types, volumes, velocities, etc. This role will have an immediate and lasting impact on both internal and external customers, and provides a unique opportunity to impact Amazon's Supply Chain business at scale for those willing to roll up their sleeves and dive deep to achieve results.

Key job responsibilities

 Implement & maintain an AWS Cloud Infrastructure to securely store, process, and deliver NASC planning data.
 Build and support a modern data architecture that integrates a wide range of available AWS data technologies.
 Ensure the accessibility, trustworthiness, usability, and security of NASC planning data.

About The Team

As part of North America Supply Chain Science Intelligence Analytics team (NASC-SIA), the NASC-SIA Engineering team is responsible for building and maintaining the critical data infrastructure, data management platform, and various planning tools that support NASC's planning and operational needs.

Basic Qualifications

 1+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines
 Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala)
 Experience with one or more scripting language (e.g., Python, KornShell)
 Experience as a data engineer or related specialty (e.g., software engineer, business intelligence engineer, data scientist) with a track record of manipulating, processing, and extracting value from large datasets

Preferred Qualifications

 Experience with big data technologies such as: Hadoop, Hive, Spark, EMR
 Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc.
 Exposure to Supply Chain business operations, particularly the labor planning processes.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com Services LLC

Job ID: A2695372","Apache Spark, Arquitectura de datos, Big data, Extraer, transformar y cargar (ETL), Hadoop, Hive, Ingeniería de datos y PL/SQL, Lenguaje de definición de datos (DDL) y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3972705795/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=2J9Gn5lSft78G3RDmzH2ow%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer I, PLEX-SIA","Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 2 semanas,"Bellevue, WA","Acerca del empleo
Description

Amazon is looking for a Data Engineer (DE) to join the North America Supply Chain (NASC)'s Science, Intelligence, & Analytics (NASC-SIA) team.

The successful candidate will be a self-starter, comfortable with ambiguity, and able to create and maintain automated processes efficiently. They will set the vision, define the roadmap, and work alongside stakeholders across organizations to deliver results. They know and love working with various AWS Cloud and Data Technologies, understand the principles and techniques of distributed computing, and are enthusiastic about working with data in diverse forms, types, volumes, velocities, etc. This role will have an immediate and lasting impact on both internal and external customers, and provides a unique opportunity to impact Amazon's Supply Chain business at scale for those willing to roll up their sleeves and dive deep to achieve results.

Key job responsibilities

 Implement & maintain an AWS Cloud Infrastructure to securely store, process, and deliver NASC planning data.
 Build and support a modern data architecture that integrates a wide range of available AWS data technologies.
 Ensure the accessibility, trustworthiness, usability, and security of NASC planning data.

About The Team

As part of North America Supply Chain Science Intelligence Analytics team (NASC-SIA), the NASC-SIA Engineering team is responsible for building and maintaining the critical data infrastructure, data management platform, and various planning tools that support NASC's planning and operational needs.

Basic Qualifications

 1+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines
 Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala)
 Experience with one or more scripting language (e.g., Python, KornShell)
 Experience as a data engineer or related specialty (e.g., software engineer, business intelligence engineer, data scientist) with a track record of manipulating, processing, and extracting value from large datasets

Preferred Qualifications

 Experience with big data technologies such as: Hadoop, Hive, Spark, EMR
 Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc.
 Exposure to Supply Chain business operations, particularly the labor planning processes.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com Services LLC

Job ID: A2695372","Apache Spark, Arquitectura de datos, Big data, Extraer, transformar y cargar (ETL), Hadoop, Hive, Ingeniería de datos y PL/SQL, Lenguaje de definición de datos (DDL) y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3984545752/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=Dybno28DLGBcBDjxurOHxg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,"Irving, TX","Acerca del empleo
Role : Data Engineer

Location: Hybrid onsite in Chicago, IL

 

Skill sets:
MUST HAVE
AWS (s3, redshift, EMR, EC2, lambda, SNS),
unix shell scripting,
python
spark
scala 
PREFER TO HAVE
Any fullstack
API
Microservices


 

Project/ day to day:
Day to day- modernization of the data engineering platform
Run on Ab Initio and moving to sunset by end of this year
Converting those jobs into cloud- AWS jobs and written in python/ scala and run on spark and emr
AR flow for production support- apache airflow system (new learning for data engineers)
FUTURE- goal is to take this team to the next level… Will start working on together things like data analytics and machine learning o Expanding horizon and involve in machine learning and data analytics
Training- new engineers will have 8-10 weeks of overlap with the existing contractors to transfer the knowledge","Ab Initio, Almacenamiento de datos, Apache Spark, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift y Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3980520273/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=ezXo8%2F3U%2FIjNZNkzn1UD8Q%3D%3D&trk=flagship3_search_srp_jobs,Researcher (Data Science/Engineer),"74 US$K/año - 98,6 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Chicago, IL","Acerca del empleo
Chapin Hall is seeking an experienced Researcher who will work collaboratively with Chapin Hall staff. This position will report directly to a Research Fellow. This position allows for remote work and periodic in-person attendance at Chicago, IL and project-specific meetings. The successful applicant is expected to work normal business hours and must have flexibility in their work schedule to accommodate business demands.

Chapin Hall is seeking an individual to work on a team engaged in a national cross-site evaluation and other projects focused on development of cross-system and community pathways research methodologies. The Researcher will join an interdisciplinary technical assistance and an evaluation team comprised of national partners working to design a viable alternative to child welfare initiative. Chapin Hall is serving as the cross-site evaluator on the project and our partners range from foundation and university partners, government agencies to local service providers, and families with lived experience.

The Researcher’s main contribution to research projects is to manage, maintain, and upkeep Chapin Hall’s administrative data sources from multiple state human services agencies. In this capacity, the individual will oversee data management, linkage, documentation, sharing of administrative data with researchers and policy staff, and conduct statistical analysis, and presentation of data analysis when appropriate.

We are committed to creating an inclusive workplace and strongly encourage individuals of all backgrounds and experiences to apply.

Responsibilities

Uses R, Python, SQL/PostgreSQL, STATA, SAS, and/or other appropriate analytic software programs to independently convert, merge, and assemble large-scale administrative data that often needs to be cleaned for analysis for multiple projects.
Develops and maintains programming codes that transform administrative data into analyzable formats and/or that can be utilized for data analysis, with a strong emphasis on generalizability and transparency.
Automates existing and emerging processes for importing, documenting, integrating, and updating data as well as adhering to best practices around coding, documentation, and version control.
Provides details regarding data usage to researchers.
Performs modeling and complex statistical analyses under supervision.
Participates in discussions regarding study design and methods. Assists in developing study objectives, and designing sampling, randomization, and data collection procedures to achieve study objectives.
Writes sections of research results for funders and/or publications, and assists with report production activities.
Participates in meetings with external stakeholders and may present findings at meetings and/or conferences.
Performs additional duties as assigned.","Analítica, Analítica de datos, Análisis de datos, Análisis de datos estadísticos, Capacidad de análisis y Ciencia de datos, Análisis estadístico, Documentación, Proyectos de investigación y Recogida de datos",Solicitar
https://www.linkedin.com/jobs/view/3984580588/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=eJACxzScD9WgdGntXqrCPg%3D%3D&trk=flagship3_search_srp_jobs,Principal Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Director",hace 3 días,"Chicago, IL","Acerca del empleo
Be essential at Cars Commerce

At Cars Commerce, we're fanatical about simplifying everything about car buying and selling. We do right by our customers and consumers to better connect the industry with simplified and tierless technology to enhance, measure and drive local automotive retail. Whether through our No.1 most recognized marketplace, Cars.com, our industry-leading digital experience, Dealer Inspire, our trade and appraisal technology, AccuTrade, or our new Cars Commerce Media Network, Cars Commerce is essential for success in the automotive industry.

No one ever travels alone here: at its core, Cars Commerce is collaboration. In fact, it's built into the very fabric of our shared values. We like to say we Rise Together – putting people at the center of what we do, from consumer to customer to community. Life at Cars Commerce makes it easy when we share the ethos to be Open to All, encouraging open-minded communication because we know diverse thinking yields better outcomes. But critical to our success is Caring to Challenge and Taking Ownership, fueling a competitive spirit in a respectful environment where we think about tomorrow but act today. At our foundation, we have integrity, Doing the Right Thing, even when it's hard. It's our shared commitment to these values that makes Cars Commerce a place where growth becomes not only possible, but downright unavoidable.

But don't take our word for it. As a U.S. News & World Report Best Company to Work For in 2024, we're obsessive about the employee experience. We are among the top 20% being declared ""Best"" of our industry based on six critical factors that are important to employee wellbeing, like quality of pay, benefits, work life balance and more.

About the Role

Cars Commerce builds solutions for automotive dealerships, manufacturers, and consumers. Our driving force is to deliver one platform to simplify everything about buying and selling cars. Our platform enables dealerships and OEMs with innovative technical solutions and data-driven intelligence to better reach and influence ready-to-buy shoppers, increase inventory turn and gain market share. To support the growth and tighter integration of our existing brands like Cars.com, Dealer Inspire, and Accu-Trade we are seeking a Principal Data Engineering leader for our expanding Commerce Platform.

We're looking for a highly-motivated and enthusiastic Principal Data Engineer who will lead by example and be a force multiplier to other engineers building and supporting our critical data systems and services. This role is pivotal to leading a key part of the broader Platform strategy. Success in this role requires exceptional development, communication, and project management skills. You should have hands-on deep data engineering experience, and a strong belief in transparency and accountability. You have a large portfolio of previous experience delivering scalable data solutions in AWS. Ownership is a key characteristic you inspire your teams to ingrain in their culture.

Responsibilities

Understand the business purpose of Data and become an expert capable of making recommendations and decisions that represent both the business intent and the technical need
Develop technical plans to match the strategic vision of the organization and larger company
Build and deliver Data solutions - from POCs to customer facing services, ensure teams are working on the right priorities and share development responsibilities where needed
Lead architecture reviews, design recommendations, data architecture standards, proof of concepts, or domain specific solutions
Analyze and optimize - Identify gaps and lead teams through optimization processes
Author and share excellent documentation across the technology organization
Lead independent efforts and with minimal oversight, prioritizing on the fly, and persisting the expected communication and transparency needed when priorities change
Work closely with Management and Stakeholders bringing transparency to your work and outcomes
R&D into modern Data techniques and tools evaluating for relevance and value
Solicit requirements from collaborators and foster excellent communication with stakeholders
Mentor engineers to help upskill and build their portfolio of work
Lead by example to establish a culture of technical expertise and operational accountability
Lead/sponsor engineering culture around best practice
Establish and enforce training standards and performance expectations for individual contributors
First to the fire - leverage your deep troubleshooting knowledge and skills to improve quality continuously
On-call escalation participation as needed
Leading incident postmortem exercises to identify root causes and help teams prioritize preventative measures
Be an ambassador of Cars.com's Core Values:
Challenge & Collaborate; Start with the Consumer; Be Bold; Build Relationships, Deliver Results; Focus on the Outcome; Stay Open
Minimum Qualifications

Master's Degree in computer science, data science, or related degree or equivalent practical experience
12+ years of technical experience
Extensive experience and skills with data modeling, warehousing and building ETL pipelines
Experience designing & developing complex, real-time applications at enterprise scale; specifically Python and/or Scala
Extensive experience with one or more query language (e.g., SQL, PL/SQL, HiveQL, SparkSQL)
Demonstrated previous professional experience with tools and platforms like Spark Streaming, EMR, Kafka
Demonstrated previous professional experience developing modern data applications in the cloud, specifically AWS (RedShift, API-Gateway, S3, MSK)
Experience working with and developing REST API services 
Extensive experience in at least one programming language commonly used within Data Engineering, such as Python, Scala, or Java.
Solid understanding of Spark and ability to write, debug, and optimize Spark code in Python
Previous experience with any of ETL schedulers such as Airflow or similar frameworks.
Extensive experience with source code management systems Github
Experience developing CI/CD pipelines with tools such as Jenkins, CircleCI, or CodeDeploy
Excellent communication and collaboration skills
Experience documenting standards and best practices for teams of fellow engineers
Excellent troubleshooting and debugging skills
Experienced implementing security best practices
Extensive experience using modern monitoring tools (Cloudwatch, DataDog, etc) and establishing metrics, monitoring, alarming, and dashboards 
Extensive experience managing change management practices, policies, and procedures
Experience working in a software delivery process such as Agile 
Ability and willingness to manage ambiguity i.e., help shape solutions and prioritize deliverables to align with the Company's vision

Preferred Qualifications

Experience developing Spark streaming jobs 
Experience working with and managing Kafka
Experience working with Google Analytics
Strong track record of delivering results and are able to effectively prioritize and manage multiple competing priorities with various business stakeholders
Experience designing and setting operational metrics and reporting on said metrics



In the spirit of pay transparency, we are excited to share the base salary range for this position which is not inclusive of bonuses, benefits or other forms of compensation that the position may be eligible for. If you are hired at Cars Commerce, your final base salary compensation will be determined based on factors such as skills and/or experience. If the salary range is close to what you're seeking, then we encourage you to apply and learn more about the total compensation package for this position.

Salary Range

$149,800—$187,300 USD

Our Comprehensive Benefits Package includes:

Medical, Dental & Vision Healthcare Plans
401(k) with Company Match + Immediate Vesting
New Hire Stipend for Home Office Set-Up
Employee Stock Purchase Program
Generous PTO
Refuel - a service based recognition program where employees receive additional paid time away to learn grow and reset
Paid Holidays, Floating Holiday, Volunteer Day, Recharge Day
Learn more about our Benefits, Perks, & Culture on our LinkedIn Life Pages!

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. California Applicants: Click here to review our California Privacy Policy for Applicants. For current employees, please click here to review our California Privacy Policy for Employees.","Airflow, Apache Kafka, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Ciencias de la computación, Comunicación y Modelado de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3985088884/?eBP=BUDGET_EXHAUSTED_JOB&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=Z%2FXXt5eQhTasbC0gY4URRw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 3 días,"Texas, Estados Unidos","Acerca del empleo
Concerning Sponsorship: Unfortunately, we currently are not able to sponsor or transfer a sponsorship to Frost. Also, we are not able to sponsor candidates with an OPT student visas. 

This role will have a Hybrid Work from Home schedule. The candidate would need to live in Texas near one of our locations and will be required to travel to the Frost San Antonio Texas (One Frost Corporate Campus) office when needed. We will assist with relocation cost reimbursement.

Job Description
It’s about putting our best to the test.
Are you described as someone with an inquisitive mind and an innovative personality? Are you never satisfied with good enough? Does solving complex problems and ensuring top-quality systems excite you? If so, being a Data Engineer II with Frost could be for you.
At Frost, it’s about more than a job. It’s about having a flourishing career where you can thrive, both in and out of work. At Frost, we’re committed to fostering an environment that reflects our values and encourages team members to be the best they can be. In joining our adaptable, integrity-driven team, you’ll become part of Frost’s over 150-year legacy of providing unparalleled banking services.

Who You Are
As a Data Engineer II, you will lead the design, development and testing of complex reports and dashboards using various business intelligence tools at Frost. You’ll use your strong problem-solving skills to ensure that the systems are performing optimally and meet our high standards. You believe in effective communication and will have the opportunity to address potential problems and solutions to complex issues.

What You’ll Do
Develop and maintain data pipelines to automate data ingestion and processing
Develop reports and dashboards to support the lines of business and key initiatives
Perform Ad Hoc data requests for business users and Executive management as needed
Collaborate with stakeholders to identify data requirements
Develop and enforce data governance policies and standards
Always take action using integrity, caring, and excellence to achieve all-win outcomes

What You’ll Need
Bachelor's degree in Computer Science, Information Technology, or related field
2+ Years of experience as a data engineer or in a related role
Strong understanding of database technologies such as SQL and NoSQL
Strong knowledge of Cognos and Business Objects reporting tools
Strong problem-solving and analytical skills
Excellent written and verbal communication skills
Ability to work independently and as part of a team

Additional Preferred Skills
Familiarity with Big Data technologies, such as Hadoop, Spark, Hive, and Cloud native data engineering technologies

Our Benefits
At Frost, we care about your health, your family, and your future and strive to have our benefits reflect that. This includes:
Medical, dental, vision, long-term disability, and life insurance
401(k) matching
Generous holiday and paid time off schedule
Tuition reimbursement
Extensive health and wellness programs, including our Employee Assistance Program
Referral bonus program + more!
Since 1868, Frost has dedicated their expertise to provide exceptional banking, investment, and insurance services to businesses and individuals throughout Texas. Frost is one of the 50 largest U.S. banks by asset size and is a leader in banking customer satisfaction. At Frost, it’s about being part of something bigger. If this sounds like you, we encourage you to apply and see what’s possible at Frost.

Concerning Sponsorship: Unfortunately, we currently are not able to sponsor or transfer a sponsorship to Frost. Also, we are not able to sponsor candidates with an OPT student visas. 

This role will have a Hybrid Work from Home schedule. The candidate would need to live in Texas near one of our locations and will be required to travel to the Frost San Antonio Texas (One Frost Corporate Campus) office when needed. We will assist with relocation cost reimbursement.","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Ciencias de la computación, Comunicación, Comunicación oral, Panel de control y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3940332781/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=ll68ku3vST3AnyjioyySZw%3D%3D&trk=flagship3_search_srp_jobs,Junior/Entry Level Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Olympia, WA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos y Lenguajes de programación, Ciclo de vida de desarrollo de software (SDLC), Comunicación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3934097254/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=dW5s9FeBoxCblh2y%2B9I8lg%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Scientist/Analyst/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"Indianápolis, IN","Acerca del empleo
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this market also our candidates get multiple job offers and $100k + salaries.

 please check the below links to see success outcomes of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 https://www.youtube.com/watch?v=OFoqPTNORew

 https://www.youtube.com/watch?v=-HkNN1ag6Zk

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

 https://youtu.be/bJJl27D8bh0

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3971886645/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=Js47%2BWlrurCkfRMVGnTDcg%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist/Analyst/Engineer - Entry/Junior Level,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 semanas,"Dallas, TX","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates can achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergistic understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and are looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Positions 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernetes, and REST API experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who match client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or C2C candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3976192413/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=Ar1IwgZWEoPqepkH5STroA%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Analyst/Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Los Ángeles, CA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

If you applied for a job and got emails from our Job Placement Program team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

please check the below links to see success outcomes of our candidates and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

(url removed)

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)

(url removed)

For preparing for interviews please visit (url removed)

We are looking for the right matching candidates for our clients

Please apply via the job posting

REQUIRED SKILLS For Java /Full Stack/Software Programmer

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3970288048/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=APlkJPKGjokUV4lMOaMH2A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II: 24-01871,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Bellevue, WA","Acerca del empleo
Primary Skills: Python, AWS, SQL

Contract Type: W2

Duration: 11 Months

Location: Bellevue, WA Onsite

Pay Range: $60.00-$67 per hour

#IND1

Job Responsibilities

Transform complex data in AWS, developing and improving accessibility for customer data.
Design and implement metrics, tackle metadata modeling, reports, and dashboards development.
Collaborate with business customers to grasp business requirements and implement solutions to support analytical and reporting needs.
Ensure solutions are scalable and compliant with security and privacy standards.
Handle the complete data model development cycle in large-scale systems.

JOB REQUIREMENTS:

Proficiency in one modern scripting or programming language (e.g., Python, Java, Scala, NodeJS).
Extensive experience with AWS services (e.g., Lambda, Gateway, SNS, Firehose).
Strong foundation in SQL and familiarity with reporting tools (e.g., Quicksight, Tableau).
Prior e-commerce experience

ABOUT AKRAYA

Akraya is an award-winning IT staffing firm consistently recognized for our commitment to excellence and a positive work environment. Voted the #1 Best Place to Work in Silicon Valley (2023) and a Glassdoor Best Places to Work (2023 & 2022), Akraya prioritizes a culture of inclusivity and fosters a sense of belonging for all team members. We are staffing solutions providers for Fortune 100 companies, and our industry recognitions solidify our leadership position in the IT staffing space. Let us lead you to your dream career, join Akraya today!","Arquitectura de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Información de clientes, Metadata Modeling, Metadatos, Modelado de datos, Necesidades empresariales y Panel de control",Solicitar
https://www.linkedin.com/jobs/view/3980244276/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=2fhA3yMs3cyUlQk%2BzmOW9A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Meta Platforms, Inc. (f/k/a Facebook, Inc.), is seeking the following. Apply via Dice today!

Meta Platforms, Inc. (f/k/a Facebook, Inc.) has the following position in Menlo Park, CA:

Data Engineer: Responsible for the timely delivery of accurate data and the automation of product analytics. Telecommuting is permitted from anywhere in the U.S. (ref. code REQ-2406-138506: $186642/year - $196900/year).

Individual pay is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base salary only, and do not include bonus or equity or sales incentives, if applicable. In addition to base salary, Meta offers benefits. Learn more about benefits at Meta at this link: https://www.metacareers.com/facebook-life/benefits. For full information & to apply online, visit us at the following website http://www.metacareers.com/jobs & search using the ref code(s) above.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3828045174/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=ZiovBhnkXNAgyoUKFnuX1g%3D%3D&trackingId=PKQI1VebtsrfwWWx4GGT%2BQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 meses,"Newark, NJ","Acerca del empleo
Hybrid

Data Engineer

NJ

ETL, analyzing Alteryx data
Hands on Python
Fixed Income Bonds
SQL
BI tools","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979912619/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=hI8UpvaVFCr9EM6tiEecwA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,"San Diego, CA","Acerca del empleo
At FuturHealth, we're on a mission to create a product where every individual feels inspired and empowered to confidently take charge of their wellbeing. We believe in and are dedicated to offering personalized and holistic approaches to combat health concerns.

Operating at the intersection of health and technology, we provide a comprehensive range of services, including telemedicine, personalized nutrition plans, medication delivery, and insurance solutions. We believe that wellbeing is deeply personal, and the path to becoming the best version of yourself is paved with progress, not perfection. Our central focus is on utilizing scientifically-proven prescriptions and telehealth initiatives to tackle obesity head-on.

Our Data Team is the backbone of informed decision-making and strategic planning. We collaborate with all stakeholders across the organization, including Finance, Customer Experience, Marketing, Product, and more. Our team is the primary source of truth, ensuring that data is accurate, reliable, and accessible to drive business success.

What You Will Do:

Design, develop, and maintain ETL processes to ensure reliable data pipelines.
Work closely with data analysts to understand data requirements and deliver high-quality datasets.
Optimize data pipelines for performance and scalability.
Collaborate with cross-functional teams to gather requirements and deliver data solutions.
Monitor and troubleshoot data pipelines and infrastructure to ensure consistent data flow.
Perform data analysis to support business decision-making.
Develop and maintain dashboards and reports to communicate data insights.
Conduct data validation and quality checks to ensure data accuracy and consistency.
Provide ad-hoc data support and analysis for various business needs.

It's a Perfect Match If You Have:

At least 3 years of experience in Data Engineering and 5 years of experience in Data field, 
Strong knowledge of SQL and Python.
Ability to understand processes from both a business and engineering perspective.
Experience with Google BigQuery or other MPP systems.
Understanding of data modeling.
Proficiency in data analysis and visualization tools (e.g., Tableau, Power BI).
Experience in building and maintaining reports and dashboards.

Preferred Qualifications:

Experience with our stack: Python, GCS, BigQuery, DBT, Airflow, Metabase
Experience implementing Data Quality solutions.
Familiarity with monitoring and alerting tools (Grafana, Prometheus)

Additional Company Info

Founded by a team with a proven track record of delivering exceptional experiences to over 25 million consumers, they began by revolutionizing personalized nutrition through an intuitive mobile app that adapts meal plans to individual metabolisms and preferences. With seamless integrations with food services and a thriving community of like-minded individuals, the formation of FuturHealth began to further empower people on their health journey.

Securing Series Seed Funding in December 2023, FuturHealth is backed by Corazon Capital, Oversubscribed Ventures, and InterAlpen Partners and already achieved unprecedented revenue growth of 600% from Q4 2023 to Q1 2024; we are also proud to be maintaining this impressive momentum. We are currently seeking visionary people to join us in our journey to revolutionize healthcare and empower individuals to lead healthier lives. If you are passionate about making a meaningful impact and driving innovation in the health and wellness and technology space, we invite you to explore opportunities and be a part of the future of healthcare.

What we can offer:

A comprehensive and generous total rewards package
Health and Dental Insurance coverage
Fully remote work policy
Unlimited Paid time offAuthentic startup exposure
Enabling direct impact on our expanding venture
Embrace a culture valuing autonomy and accountability, where success is result-driven

Note: Benefits and compensation listed may vary based on the country of your employment and the nature of your employment with Futurhealth.","Airflow, Analítica de datos, Extraer, transformar y cargar (ETL), Google BigQuery y Ingeniería de datos, Calidad de datos, Datasets, Modelado de datos, Panel de control y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3982218445/?eBP=BUDGET_EXHAUSTED_JOB&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=Aa6hKumnU%2FMJ%2FcxWU0l%2F1A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"75 US$K/año - 115 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,Washington DC-Baltimore y alrededores,"Acerca del empleo
JPI has an upcoming opening for a mid to senior level Data Visualization Developer with at least 4 years professional experience and consulting experience with the federal government. The successful candidate will have experience applying management analysis processes, statistical methods, and advanced technical and analytical research techniques to determine solutions based on client requirements with an IT services/solutions-based scope. 

At JPI, we strive to empower our people and excel for our clients. We hold ourselves to high standards and prioritize our values of being one team with unwavering integrity. We are motivated by our mission and driven to deliver solutions that exceed expectations. Will you join us? 

Responsibilities:
Serve as a Power BI expert for all agency data product development teams, including:
Consulting with stakeholders using human centered design approaches to understand the need for data products and to identify a potential solution.
Translating customer requirements for data products (datasets, reports, dashboards) into a prioritized backlog of features and user stories.
Identifying potential data sources and conducting assessments to determine data quality.
Maintaining backlogs of work items in Agile development processes.
Designing data visualization strategies that make large or complex datasets more understandable, accessible, and usable.
Creating actionable interactive reports and dashboards using statistical and business intelligence tools (e.g. Power BI, Cognos, RStudio)
Executing testing protocols and procedures throughout the entire data product development process.

Requirements:
Bachelor’s Degree in related field
US Citizenship is required for this position
Must have the ability to obtain a security clearance
Federal experience is preferred for this position
Past experience using Power BI to structure and create data, design, develop and maintain interactive visualizations and dashboards.
Prior experience selecting data visualization chart types to effectively communicate information to a large group of technical and non-technical audiences.
Experience working in an Agile environment – past work as a team member to iteratively develop and deliver data products.
Background in using M and DAX languages to build Power BI models. Ability to communicate technical information to non-technical audiences (orally and in written form). Have the skillset to effectively collaborate across organizational barriers to accomplish high priority and high visibility goals.
Experience using human-centered design approaches to understand user needs and behavior.
Experience in data quality control.

JPI is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.

 JPI is hiring for several positions and will consider your application across our current and future openings. Expected compensation for this role is between $75,000 - $115,000, including a generous benefits package with comprehensive healthcare coverage. Please note that final compensation is dependent on a variety of factors and is reviewed regularly for both internal and external equity considerations.","Analítica, Analítica de datos, Ciencia de datos, Minería de datos, Visualización y Visualización de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984127563/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=WFcSwYEXNswZ4zePHiRucQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Atlanta, GA","Acerca del empleo
About Fanduel

FanDuel Group (“FanDuel"") is an innovative sports-tech entertainment company that is changing the way consumers engage with their favorite sports, teams, and leagues. The premier gaming destination in the United States, FanDuel consists of a portfolio of leading brands across gaming, sports betting, daily fantasy sports, advance-deposit wagering, and TV/media.

FanDuel has a presence across all 50 states with approximately 17 million customers and 28 retail locations. FanDuel is based in New York with offices in New Jersey , Georgia, California, Oregon, Canada and Scotland.

Its networks FanDuel TV and FanDuel+ are broadly distributed on linear cable television and through its relationships with leading direct-to-consumer over-the-top platforms.

FanDuel Group is a subsidiary of Flutter Entertainment plc, the world's largest sports betting and gaming operator with a portfolio of globally recognized brands and traded on the New York Stock Exchange (NYSE: FLUT).

THE ROSTER

At FanDuel, we give fans a new and innovative way to interact with their favorite games, sports and teams. We’re dedicated to building a winning team and we pride ourselves on being able to make every moment mean more, especially when it comes to your career. So, what does “winning” look like at FanDuel? It’s recognition for your hard-earned results, a culture that brings out your best work—and a roster full of talented coworkers. Make no mistake, we are here to win, but we believe in winning right. That means we’ll never compromise when it comes to looking out for our teammates. From creatives professionals to cutting edge technology innovators, FanDuel offers a wide range of career opportunities, best in class benefits, and the tools to explore and grow into your best selves. At FanDuel, our principle of “We Are One Team” runs through all our offices across the globe, and you can expect to be a part of an exciting company with many opportunities to grow and be successful.

THE POSITION

Our roster has an opening with your name on it

Our competitive edge comes from making decisions based on accurate and timely data. As a Data Engineer in Insights Engineering, you will work along side other engineers to ensure the proper upkeep of our analytic platforms within FanDuel organization by establishing platform governance and feature improvements to enable the business.

Collaboration is at the core of your role. You will have established analytic engineer capabilities as well as enablement skills to allow for data process excellence in the organization. In this role, you will be working closely with the data users across the organization to ensure the adoption of platform best practices. You’ll be also expected to influence the team to invent, evolve, and/or deprecate data solutions.

You should have a passion for imparting knowledge by working with analysts to improve data processes and to minimize areas of risk across our platforms. You will show a pragmatic approach to problem-solving; your experience should be built on a solid foundation of engineering development and analytic best practices.

THE GAME PLAN

Everyone on our team has a part to play

Conduct regular design process reviews and ensure development standards within the team.
Monitor and optimize query performance, identify bottlenecks, and implement solutions to enhance database performance.
Contribute to the design and architecture of complex data warehouse solutions using Redshift and Databricks, considering scalability, performance, and best practices.
Create and maintain documentation related to database design, configuration, and processes.
Build data pipelines for uses in production level infrastructure monitoring.
Share platform improvement findings in easy to consume formats, whether that is through dashboards or data modeling.

THE STATS

What we're looking for in our next teammate

Strong development experience with minimum +2 years developing code in one or more core programming languages (Python, Java, etc.)
Minimum +1 years of experience working with Redshift and/or Databricks, demonstrating a deep understanding of its capabilities and limitations.
Experience working in a cloud environments such as AWS, GCP, Azure and its product offerings.

Player Benefits

We treat our team right

From our many opportunities for professional development to our generous insurance and paid leave policies, we’re committed to making sure our employees get as much out of FanDuel as we ask them to give. Competitive compensation is just the beginning. As part of our team, you can expect:

An exciting and fun environment committed to driving real growth
Opportunities to build really cool products that fans love
Mentorship and professional development resources to help you refine your game
Be well, save well and live well - with FanDuel Total Rewards your benefits are one highlight reel after another 

FanDuel is an equal opportunities employer and we believe, as one of our principal states, “We Are One Team!” We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, Veteran status, or another other characteristic protected by state, local or federal law. We believe FanDuel is strongest and best able to compete if all employees feel valued, respected, and included. We want our team to include diverse individuals because diversity of thought, diversity of perspectives, and diversity of experiences leads to better performance. Having a diverse and inclusive workforce is a core value that we believe makes FanDuel stronger and more competitive as One Team!

The applicable salary range for this position is $108,000-$142,000, which is dependent on a variety of factors including relevant experience, location, business needs and market demand. This role may offer the following benefits: medical, vision, and dental insurance; life insurance; disability insurance; a 401(k) matching program; among other employee benefits. This role may also be eligible for short-term or long-term incentive compensation, including, but not limited to, cash bonuses and stock program participation. This role includes paid personal time off and 14 paid company holidays. FanDuel offers paid sick time in accordance with all applicable state and federal laws.","Almacenamiento de datos, Analítica de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Bases de datos, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3963469137/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=H%2BCv8Guo%2FrC7BGhcCN28zg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"93 US$K/año - 124,5 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 6 días,Estados Unidos,"Acerca del empleo
Overview:

Crisis Text Line provides free, 24/7, high-quality text-based mental health support and crisis intervention by empowering a community of trained volunteers to support people in their moments of need.

Our mission is at the intersection of empathy and innovation — we promote mental well-being for people wherever they are.

Our vision is an empathetic world where nobody feels alone.

Our core values are at the heart of all we do: connect with empathy, center equity, get it done together, and reflect and evolve.

Why you should join our team: 

Our work is transforming the way people in pain access support at their fingertips
Our work is innovative in the crisis response space
Our dynamic, fun, and diverse culture
Our meaningful cause, led by empathy and innovation
Our strong values at the center of all we do
Our commitment to diversity, equity and inclusion
Our commitment to engagement and belonging
Our commitment to our employees and their holistic wellbeing
Our value of work/life balance
Our growth mindset and prioritize professional development
Our leaders who truly care

What you'll be doing:

We are seeking a highly skilled and delivery-focused Data Engineer to join our team. The ideal candidate will have extensive experience in building, managing and optimizing data pipelines, leveraging Infrastructure as Code (IaC) methodologies. You should possess strong technical skills in SQL, Python, Spark, and PySpark and have hands-on experience with AWS and Databricks platforms.

In this role, you will actively collaborate with cross-functional teams to understand data requirements, design scalable and efficient solutions, and contribute to the continuous improvement of our data infrastructure. You will be responsible for implementing and maintaining data solutions on AWS and Databricks platforms. The ideal candidate must possess a proven track record of delivering high-quality results in a fast-paced environment, with a focus on innovation and continuous improvement.

Responsibilities: 

Collaborate with other engineering teams, data scientists, and business units to understand data requirements and improve data ingestion and integration processes.
Collaborate with cross-functional teams to understand data needs and deliver solutions that support business objectives.
Design, build, and maintain scalable and reliable data pipelines using Infrastructure as Code (IaC) principles and Databricks along with Datadog.
Design and implement scalable and secure integration solutions between the data sources and our data pipeline infrastructure.
Develop and optimize big data processing using Spark and PySpark within the Databricks environment.
Write complex SQL queries for data extraction, transformation, and loading (ETL) processes.
Implement automation and monitoring tools to ensure data accuracy and pipeline efficiency.
Maintain comprehensive documentation for all data engineering processes, including pipeline architectures, codebase, and infrastructure configurations.
Ensure that best practices and coding standards are followed and documented for future reference.
Stay updated with the latest technologies and methodologies in data engineering and propose improvements to current processes.

Qualifications:

Bachelor’s or Master’s in Computer Science/related field.
2+ years in data engineering or similar.
Expertise in complex data pipeline management with IaC (e.g., Terraform).
Strong experience in any of the Orchestration tools like Databricks Workflows, Airflows etc
Strong experience in any of the Streaming technologies like Kafka, Kinesis, Spark Streaming, Structured Streaming etc
Strong understanding of distributed systems, and machine learning basics.
Analytical, problem-solving, communication, and collaboration skills.
Experience in fast-paced, agile environments; delivery-focused.
Self-motivated with a passion for continuous improvement in data engineering.
Relevant certifications (e.g., AWS Certified Data Engineer - Associate).
Databricks Certified for Apache Spark (recommended but not required).

Reliable High-Speed Internet Required: Must have a stable high-speed internet connection to support seamless remote collaboration, virtual meetings, online job tasks, etc.

The full salary range for this position, across all United States geographies, is $93,000 - $124,500 per year. The upper portion of the salary range is typically reserved for existing employees who demonstrate strong performance over time. Starting salary will vary by location, qualifications, and prior experience; during the interview process, candidates will learn the starting salary range applicable for their location. We pay competitively in the tech-forward nonprofit space and offer a robust benefits package.

Only candidates in the following states will be eligible for employment: CA, CO, CT, FL, GA, HI, IL, IN, IA, MD, MA, MI, MO, NJ, NM, NY, NC, OH, PA, TN, TX, UT, VA, WA.

Benefits:

Crisis Text Line employee benefits are thoughtfully designed using an equity lens, acknowledging that we are all unique human beings with individual life circumstances that require flexibility and support.

Benefits include:

20 paid holidays including:
Federal holidays like Juneteenth and Labor Day
Election day
Holiday break from Dec 24 through January 1
2 renewal days 
2 floating holidays 
Flexible paid time off, including: 
15 vacation days
3 personal days
7 sick days 
Medical, dental, and vision benefits for the staff member and family at no cost to the employee
403B retirement plan (the nonprofit equivalent of a 401K): 3% contribution by Crisis Text Line to support building financial wellness, regardless of personal contribution
12 weeks paid parental leave (after 6 months of employment)
 Student loan repayment (after 2 years of continuous full time service)
Family support through a virtual childcare platform
Stipends/Allowances
Mental health (Monthly) 
Internet Service (Monthly) 
Professional Development (Annual)
Wellness (Annual)
Home office setup (One time/First year)
(Benefits are only for US-based employees. International benefits may differ).

Crisis Text Line is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. We provide reasonable accommodation to individuals who have a disability and meet the skill, experience, education, and other job-related requirements of the role to allow the individual to perform the essential functions of the job.","Analítica de datos, Apache Spark, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y PySpark, Ciencias de la computación, Comunicación y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982960945/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=TT8Do%2Fte7PMlKJ6bZBPSuQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"60 US$/h - 70 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.",hace 5 días,Estados Unidos,"Acerca del empleo
Job Description:
We are seeking a highly skilled and motivated Data Engineer to join our dynamic team. As a Data Engineer, you will play a crucial role in building, optimizing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to ensure data quality, governance, and efficient data flow throughout the organization.

Core Skills:
Strong Software Engineering background, in Java, RESTful APIs, any experience with Springboot is valued
Strong Python skills along with popular and widely used Python libraries, performing data cleansing, wrangling, augmentation,
Exploratory data analysis, building cloud data pipelines, data lineage, governance(bronze, silver, gold), Data Quality analysis, Featurization techniques, sampling.
SQL, NoSQL Databases, Elasticsearch
Awareness of Vector databases, LM/LLM/GEN-AI based tools, libraries and frameworks(e.g; LangChain, Agentic, Semantic Kernel).
Awareness of ML models like Random forest, KNN, Time series forecasting
Awareness of Neural Network Architectures, Transformers, ANN, BERT, GPT models, Open Source foundational models(e.g; LLama2/3).
Awareness/usage of mlFlow
Cloud(Azure is valued over AWS/GCP) based managed services(e.g; Azure OpenAI, Azure ML, Azure Data fcatory, ADLS GEN2).
Awareness of Performance evaluation of ML/DL/LLM systems, drift handling
Experience with PySpark/Databricks is valued, not mandatory.

Education:
Master's degree (preferred) or Bachelor's degree in Computer Science, Mathematics, Statistics, Engineering, Information Technology, or related fields","Almacenamiento de datos, Analítica de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y PySpark, Bases de datos, Calidad de datos y Limpieza de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3977511869/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=4%2BzqU2K%2Futk0wtFQEkfBEA%3D%3D&trk=flagship3_search_srp_jobs,AWS Data Engineer,"65 US$/h - 68 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"El Segundo, CA","Acerca del empleo
Job Title: AWS Data Engineer

Work Location: Remote

Required Skill

Cassandra, AWS Key Space, AWS EC2

Technical Skills

 Programming Languages - Python (Basics)
 Databases & Tools - Cassandra (Primark Skill), PostgreSQL, Oracle GG,
 Cloud Technologies - AWS Key Space, AWS EC2, S3, IAM, RDS, EBS, Cloud Watch
 Tools - JIRA, Confluence, Service Now, GitHub

Detail JD -

 Strong Hands -on/ experience on technologies such as Cassandra, Postgress, Oracle GG
 Expertise in data migration in cloud platform like Azure and AWS
 Experience in migrating Cassandra from EC2 to AWS Keyspace
 Working Experience primarily on Data store & streaming services related application
 Expertise in large data migration in cloud platform like Azure and AWS.
 Should have experience in AWS platform (EC2, S3, RDS, EBS)
 Real-time experience in Architecture, Design, Coding, Testing, Performance Analysis
 Experience in Data Structure, Algorithms, and Analytics based solutions.
 Should have experience in Linux/Unix
 Good to have basic knowledge of Confluent Kafka, Big data
 Should have experience in sustain / prod support activities

Years of Experience: 14.00 Years of Experience","Amazon Web Services (AWS), Apache Kafka, Lenguajes de programación y Python, Amazon EC2, Amazon RDS, Amazon S3, Cassandra, Linux y Unix",Solicitar
https://www.linkedin.com/jobs/view/3962983260/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=wKsF5OfymZBgs5GdGENwow%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 3 semanas,"Allen, TX","Acerca del empleo
At Jack Henry, we deliver technology solutions that are digitally transforming and empowering community banks and credit unions to provide enhanced and streamlined user experiences to their customers and members. Our best-in-class products are just the start as we lay the groundwork for the future of digital banking and payments. We hope you'll join us. We can't do it without you.

Join our Credit Union Solution team as a seasoned Data Engineer. As a key player in our modernization mission, you will be at the forefront, providing top-notch engineering support and development on building real-time data pipelines. You will work closely with our product team, researching and diagnosing data strategy to implement a performant data migration path for our customers.

At our core, we value more than just technical prowess. We prize strong internal and team relationships, fostering camaraderie, and injecting a heavy dose of fun into our day-to-day work environment. We're looking for naturally curious individuals, and self-starters, who possess the communication, organizational, and interpersonal skills to navigate complex challenges.
As you grow within the team, your talents and skills won't go unnoticed—we believe in recognizing and capitalizing on what you do best.

This position can be worked remotely located in the Allen, TX or San Diego, CA areas.

What you'll be responsible for: 
Data Pipeline Development: Design, develop, and maintain scalable and efficient data pipelines on Google Cloud Platform using tools such as Dataflow, Big Query, and Cloud Storage.
Database Management: Manage and optimize SQL Server databases, ensuring performance, reliability, and data integrity.
ETL Processes: Implement Extract, Transform, Load (ETL) processes to transform raw data into a usable format for analytics and reporting.
Streaming Data Processing: Leverage GCP streaming services like Pub/Sub to handle real-time data processing and integration.
Data Modeling: Design and implement effective data models to support business requirements and analytical needs.
Collaboration: Collaborate with cross-functional teams including data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions.
Performance Tuning: Conduct performance tuning, optimization, and troubleshooting for data pipelines and SQL Server databases.
Data Governance: Implement and enforce data governance policies, ensuring data quality, security, and compliance.
Documentation: Create and maintain documentation for data processes, data models, and system configurations.
May perform other job duties as assigned.

What you'll need to have: 
Bachelor’s degree in computer science, Engineering, or related field.
Minimum of 11 years of experience in Data Engineering
Proven experience as a Data Engineer with a focus on GCP and SQL Server technologies.
Strong proficiency in SQL and experience with query optimization.
Extensive experience with Data Extraction, Transformation, and Loading (ETL) from different data sources.
Providing support for existing data pipelines.
Monitoring and Reporting Power BI, Google Looker Studio.
Hands-on experience with GCP services such as Big Query, Dataflow, Pub/Sub, and Cloud Storage.
Experience managing and optimizing SQL Server databases.
Proficiency in data modeling and designing effective data structures.
Solid understanding of ETL processes and data integration strategies.
Familiarity with streaming data processing and real-time analytics.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.

What would be nice for you to have: 
GCP Professional Data Engineer Certification.
Experience with additional cloud platforms (AWS, Azure).
Knowledge of version control systems (Git).
Familiarity with orchestration tools (Apache Airflow, Google Composer).
Understanding of containerization and orchestration (Docker, Kubernetes).

If you got this far, we hope you're feeling excited about this opportunity. Even if you don't feel you meet every single requirement on this positing, we still encourage you to apply. We're eager to meet motivated people who align with Jack Henry's mission and can contribute to our company in a variety of ways.

Why Jack Henry?
At Jack Henry, we pride ourselves through our motto of, ""Do the right thing, do whatever it takes, and have fun."" We recognize the value of our associates and believe much of our company's strength and success depends on their well-being.
We demonstrate our commitment by offering outstanding benefit programs to ensure the physical, mental & financial well-being of our people is always met.

Culture of Commitment
Ask our associates why they love Jack Henry, and many will tell you it is because our culture is exceptional. We do great things together. Rising to meet challenges and seeking opportunities is part of who we are as an organization. Our culture has helped us stay strong through challenging times and we credit our dedicated associates for our success. Visit our Corporate Responsibility site to learn more about our culture and commitment to our people, customers, community, environment, and shareholders.

Equal Employment Opportunity
At Jack Henry, we know we are better together. We value, respect, and protect the uniqueness each of us brings. Innovation flourishes by including all voices and makes our business - and our society - stronger. Jack Henry is an equal opportunity employer and we are committed to providing equal opportunity in all of our employment practices, including selection, hiring, performance management, promotion, transfer, compensation, benefits, education, training, social, and recreational activities to all persons regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, genetic information, pregnancy, marital status, sex, gender, gender identity, gender expression, age, sexual orientation, and military and veteran status, or any other protected status protected by local, state or federal law.

No one will be subject to, and Jack Henry prohibits, any form of discipline, reprisal, intimidation, or retaliation for good faith reports or complaints of discrimination of any kind, pursuing any discrimination claim, or cooperating in related investigations.","Extraer, transformar y cargar (ETL), Google BigQuery y Ingeniería de datos, Ciencias de la computación, Lenguaje de consulta (query), Modelado de datos, Modelo de datos, Necesidades empresariales, Optimización y Optimización de consultas",Solicitar
https://www.linkedin.com/jobs/view/3947423140/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=RJlOx6xENScfsdNnpdiYhw%3D%3D&trk=flagship3_search_srp_jobs,Internship - Data Engineer,"Prácticas
Coincide con tus preferencias de empleo. El tipo de empleo es Prácticas.
Prácticas",Publicado de nuevo hace 5 días,"Topeka, KS","Acerca del empleo
Everly exists to empower generations to live life confidently. We do this by democratizing access to innovative solutions that simplify life planning. We create accessible and affordable life insurance solutions, planning tools and insights that help people navigate life’s journey.

We’re passionate about what we do every day , working together with integrity to build trust with each other and our consumers , to make an impact on empowering people's lives.

This is a remote position based in the US with occasional travel.

 About the role: 

We are seeking a motivated and technically skilled Data Engineer Intern to join the Data Science and Management team at Everly. In this role, you will focus on the curation of analytics-ready datasets, leveraging modern data warehousing and transformation tools such as Snowflake and dbt. You will also gain hands-on experience with data visualization tools to help in presenting insights derived from data. You will gain experience in converting business requirements to data requirements and also help in designing new data products focused on driving operational efficiencies. This internship is an excellent opportunity to develop your skills in a practical, fast-paced environment while contributing to meaningful projects.

 What You’ll Do  : 

 Collaborate with different Everly workstreams like underwriting, finance & actuarial to understand data needs and implement solutions. 
 Assist in the design, build, and maintenance of scalable and efficient data pipelines. 
 Work with Snowflake for data storage and retrieval, ensuring optimal performance and cost-effectiveness. 
 Utilize dbt for transforming and modeling data to create analytics-ready datasets. 
 Develop proficiency in SQL for querying and manipulating data. 
 Engage in data quality checks and validation to ensure accuracy and reliability. 
 Learn and apply data visualization tools and techniques to create insightful reports and dashboards. 
 Participate in code reviews and adhere to best practices in data engineering. 
 Document technical processes and data schemas for future reference and team use. 
 Stay updated with the latest trends and technologies in data engineering and visualization. 

 Skills and Experience: 

 Currently pursuing a degree in Computer Science, Data Science, Information Technology, or a related field. 
 Basic understanding of data warehousing concepts and ETL processes. 
 Familiarity with SQL and interest in learning more advanced techniques. 
 Exposure to Snowflake, dbt or similar data engineering tools is a plus. 
 Interest in data visualization and experience with tools like AWS Quicksight, Sigma or similar is desirable. 
 Strong analytical and problem-solving skills. 
 Excellent communication and teamwork abilities. 
 Eagerness to learn and adapt in a fast-paced environment. 

At Everly, we celebrate our diverse backgrounds and support our differences, and we are dedicated to equal employment opportunities regardless of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status.

We’re also committed to adding new perspectives to our team and invite applications from people of all walks of life. We understand that experience comes in many forms, so if you believe you’re close to what we’re looking for, please consider applying.

Apply Now","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Datasets, Revisión de código y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3984722214/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=0j%2FpTHB44rTpiAuFBA6HFg%3D%3D&trk=flagship3_search_srp_jobs,Data Visualization Developer II (Remote),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,Estados Unidos,"Acerca del empleo
American Specialty Health Incorporated is seeking a Data Visualization Developer II to join our Information Management team. This position will design and implement dashboards or workbooks with assigned data visualization tool with drill down capability that is easily ingestible by the target audience. This position plays a key role in standardizing reports and streamlining reporting processes and adheres to high standards of data integrity and dashboard delivery timeliness.

Salary Range 

American Specialty Health complies with state and federal wage and hour laws and compensation depends upon candidate’s qualifications, education, skill set, years of experience, and internal equity. $89.,300 to $128,000 Full-Time Annual Salary Range.

Remote Worker Considerations

Candidates who are selected for this position will be trained remotely and must be able to work from home (WFH) in a designated work area with company-provided technology equipment. This remote/WFH position requires you have a stable connection to your Internet Service Provider with the ability to participate by video in online meetings over a reliable and consistent network (minimum internet download of 50 Mbps and 10 Mbps upload speed).

Responsibilities

Design, develop, and implement reporting solutions with Tableau. 
Designs, develops, and maintains various user-friendly dashboards in Tableau using various reproducible data sources, stored procedures, views, calculations, parameters, and overall performance. 
Translates and refines customer reporting requests into actionable objectives, including identifying criteria, fields, grouping, summary options, time frames and other components required to build a successful report. 
Analyzes and translates non-technical user requirements into written report specifications and existing data sources. 
Responsible for consistent communication and gathering feedback from business users and stakeholders. 
Performs analysis of data sources to ensure data integrity, completeness, timeliness, and accuracy. Makes recommendations to improve efficiency of current processes. 
Provides user training and presentations as needed. 
Maintains assigned dashboard requests, determines resource needs for requests and works with Management to prioritize. 
Identifies and evaluates recurring database issues and researches trends within the Tableau environments. 
Responsible for timely delivery of dashboard deliverables to internal and external customers. 
Maintains detailed documentation, including tracking customer conversations, decisions made, reports delivered and one’s progress through multiple simultaneous projects. 
Responds to calls, correspondence, and email within department expectations. 
Supports updates to Tableau licensure and adherence to license requirements. 
Administers user, user groups, and scheduled instances for reports in Tableau Server. 
Proof-reads all documents and maintains accuracy of information and preparing work instructions and policy documents for department use. 
Maintains working knowledge of data visualization tools and follows best practices. 
Maintains working knowledge in areas of assigned responsibility, including but not limited to operational processes, data structure and content of proprietary systems. 
Serves as an operational data visualization expert for the team and the organization, which may include developing dashboard requests and interpreting data. 
Provides routine reporting to Manager with progress updates, barriers, and issues needing escalation. 
Generates and conducts quality review as needed. 
Analyzes efficiency of current processes and makes recommendations. 
Maintains confidentiality of all member and/or provider, proprietary and sensitive business information at all times. 

Qualifications

Bachelor’s degree in computer science, information systems, or data analytics-related education preferred or equivalent experience. If equivalent experience, high school diploma required. 
Minimum of 3 years Tableau Server and Desktop experience with proficiency in design and development of Executive and Operational Dashboards or additional experience with other data visualization tools such as Microsoft Power BI. 
Working knowledge of Microsoft SQL Server Management Studio (SSMS). 
Minimum of 1 year of progressive experience with relational databases performing database management. 
Highly Proficient in Microsoft Office, with advanced skills in Microsoft Excel. 
Experience with ad-hoc and custom reporting requests. 
Experience in a managed health care setting a plus. 
Experience with agile development/SCRUM a plus. 
Foundational understanding of how relational databases can be joined and queried, how data can be formatted for presentation, and the ability to learn and apply topics such as reading ERD’s and data catalogues. 
Serves as a subject matter expert (SME) consultant regarding the best ways to extract, exploit, and exemplify data visualization needs pursuant to business and departmental priorities. 
Able to independently consult with internal stakeholders on their unique business requests and to develop dashboards fulfilling those requests with unique insights. 
Experience gathering and refining business requirements, interviewing business users to understand and document data requirements with strong attention to detail. 
Good interpersonal and communication skills to explain solutions clearly to both technical and non-technical audiences. 
Functions independently yet with project priority planning from the Manager, Data Visualization. 
Knowledge of business operations, database structures, and reporting solutions to meet business needs. 
Critical thinking skills with the ability to solve complex challenges in a fast-paced work environment. 
Demonstrated ability to analyze information, problems, issues, situations, and procedures to develop effective solutions. 

Core Competencies 

Demonstrated ability to interact in a positive, respectful manner and establish and maintain cooperative working relationships. 
Ability to display excellent customer service to meet the needs and expectations of both internal and external customers. 
Excellent listening and interpersonal communication skills to identify critical core competencies based on success factors and organizational environment. 
Ability to effectively organize, prioritize, multi-task and manage time. 
Demonstrated accuracy and productivity in a changing environment with constant interruptions. 
Demonstrated ability to analyze information, problems, issues, situations, and procedures to develop effective solutions. 
Ability to exercise strict confidentiality in all matters. 

Mobility

Primarily sedentary, able to sit for long periods of time with ability to travel within and outside the facility.

Physical Requirements

Ability to speak, see and hear other personnel and/or objects. Ability to communicate both in verbal and written form. Ability to travel within the facility. Capable of using a telephone and computer keyboard. Ability to lift up to 10 lbs.

Environmental Conditions

Work-from-home (WFH) environment.

American Specialty Health is an Equal Opportunity/Affirmative Action Employer.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex including sexual orientation and gender identity, national origin, disability, protected Veteran Status, or any other characteristic protected by applicable federal, state, or local law.

Please view Equal Employment Opportunity Posters provided by OFCCP here.

If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access our career center as a result of your disability. To request an accommodation, contact our Human Resources Department at (800) 848-3555 x6702.

ASH will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company’s legal duty to furnish information.

#Tableau #SQLServer #Data #Server #Desktop #Datavisualization #BusinessIntelligence #BI","Analítica de datos, Base de datos relacional, Ciencia de datos, Microsoft Power BI, Minería de datos y Visualización de datos, Administración de bases de datos, Bases de datos, Necesidades empresariales y Panel de control",Solicitar
https://www.linkedin.com/jobs/view/3984544996/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=3zqmsCuESvdBFZdZfD0g8A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,Estados Unidos,"Acerca del empleo
Title: Data Engineer

Location: Remote

Contract Role

Someone with strong SQL skills is critical, including ETL and SQL views. They should have at least 3+ years ETL experience. 
Specific experience with AWS and Redshift would reduce the learning curve and be very helpful but is not critical. 
Good communication skills and ability to work independently yet with guidance are important. 
Experience with Salesforce, particularly Tableau CRM or Einstein Analytics (Recipies or Data Sync) are not needed but are a big plus. 
Python experience is preferred. 
Kafka experience is a nice-to-have, Informatica experience is a nice-to-have. 
The person would need to work US Central Time-zone hours; remote is acceptable.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Amazon Redshift, Bases de datos y Comunicación",Solicitar
https://www.linkedin.com/jobs/view/3982359734/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=h9EsAa%2BXqtbxObOVr38Iiw%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/BI Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Boston, MA","Acerca del empleo
Position Summary: 

The  BEST Program Data Analyst/BI Engineer (DA/BI) will work with a Team comprised of BEST Program staff, agency business and technical SMEs and the new solution system integrator (SI) and product vendor to design and implement the data information management and reporting strategies, tools, and outputs to provide a robust reporting and data analytics capacity for users of the new Financials solution.  The implementation of the new Financials solution will provide users with the ability to run queries and reports on the live data in this application directly rather than waiting for data to be available through warehouse reports. Data will be integrated across modules permitting more robust, real-time reporting and analytics capabilities

 Financials:

 Acquire to Retire (Capital Asset Management)
 Cash Management
 Cost Allocation
 Debt Management
 Grants Financial Management
 Operating Budget Management
 Order to Cash (Revenue/Accounts Receivable)
 Procure to Pay (Accounts Payable)
 Program Management
 Record to Report (Financial Reporting)
 Reporting/Analytics
 Sub-fund Management
 Vendor Management and Self Service

 Operations: 

 Maintenance and Warranty Management
 Incident Management
 Capacity Management
 Availability Management
 Performance Management
 Security Management
 Nightly Batch Jobs Management
 Software Change Management and Testing
 Disaster Recovery Testing and Management

 Specific Duties 

 Contribute to the development of data reporting and data access through database queries, reports, and dashboards. 
 Gather information from agency business users and technical staff regarding the types of data, frequency of reports or dashboards, distribution of reports or dashboards, and other information as needed to understand the current reporting needs of agencies. 
 Participate in developing an approach to identifying reporting and data analytics needs across the new solution user community. 
 Document configuration decisions for each standard report and dashboard and provide guidance to report developers during the configuration stage. 
 Participate in testing of configured reports and dashboards and assist in coordinating user testing of these materials. 
 Support and manage enterprise architecture’s data operations (optimize data processing, query performance and data quality) and data pipelines. 
 Integrate data from diverse sources using ETL processes and transform raw data into datasets used for analysis. 
 Implement robust security measures, monitor data access and develop access policies. 
 Provide technological expertise to capture, store and employ data catalogue and lineage to support governance initiatives. 
 Understand business requirements in the BI context and design data models to convert raw data to meaningful insights. 
 Perform SQL, DAX queries and functions in Power BI. 
 Receive training and hands on guidance from the system integrator and product vendor in how to build queries, reports, and dashboards in order to assist reports developers in their work. 
 Assist in the creation of job aids and other training materials to support report writing by agency users. 

 Required Skills:

 Experience in building complex queries, reports and dashboards using a range of web-based tools. 
 Knowledge of structured data, such as entities, classes, hierarchies, relationships, and metadata. 
 Proven ability to collaborate with business owners, information architects, content architects and other stakeholders to support common goals and approaches including:
 Interviewing managers and other stakeholders to understand their data needs. 
 Interviewing users to understand what they need from data systems to boost their performance. 
 Translating business goals, user needs and process improvements into data management functions and requirements. 
 Explaining the capabilities of data systems to business managers and users as the new solution is designed and configured. 
 Providing guidance on project scoping to meet stakeholders' requirements. 
 Assessing whether data is fit for use by performing initial validation of data delivered as part of the project. 
 Exhibiting an excellent understanding of how data are used within business processes and its impact. 
 In-depth exposure to data quality concepts, best practices, and tools. 
 Experience in designing, developing and deploying business analytics dashboards using PowerBI. 
 Experience in Cloud Data warehouses like Snowflake, Redshift. 
 Familiarity with Snowflake Data sharing, Snowpipe and Snowpark. 
 Experience in ETL tools such as Pentaho/Informatica and proficient in SQL. 
 Well-developed system analysis skills. 
 Understanding and knowledge of IT standards and controls. 
 Proficient written and verbal communication and interpersonal skills. 

Preferred Qualifications

Experience with Software as a Service cloud implementations particularly those in which legacy on premise applications have been migrated to cloud delivery options.

Requirements Entrance Minimum

 Bachelor's degree in computer science, system analysis or a related study, or equivalent experience. 

Minimum of 5 years of design and implementation experience in the areas of business intelligence/reporting and data warehousing.


Aptitudes y experiencia deseables
REPORTING","Almacenamiento de datos, Analítica de datos, Extraer, transformar y cargar (ETL), Gestión de datos, Herramientas ETL y Inteligencia empresarial, Administración del alcance, Modelo de datos, Necesidades empresariales y Pentaho",Solicitar
https://www.linkedin.com/jobs/view/3919260036/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=XNBrrqaxVbWbu9dx0evTFQ%3D%3D&trk=flagship3_search_srp_jobs,Engenheiro de Dados [Time Analytics],"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 meses,Estados Unidos,"Acerca del empleo
Job Description

 Nosso Engenheiro de Dados [Time Analytics] será responsável por desenvolver, otimizar e estruturar arquiteturas de código para melhorar a performance, manutenção e escalabilidade das soluções, utilizando práticas de Engenharia de Software. Contribuirá, assim, para a aplicação dos melhores modelos de desenvolvimento de soluções escaláveis, performáticas e de fácil manutenção.

Responsibilities and assignments

Como Engenheiro de Dados com atuação no Time de Analytics, esperamos que você possa:

Desenvolver, otimizar e estruturar arquiteturas de código para melhorar a performance, manutenção e escalabilidade das soluções, utilizando as melhores práticas de Engenharia de Software;
Movimentar, transformar e entregar dados de qualidade, que sejam facilmente utilizados por nossos stakeholders; 
Criação e sustentação de pipelines de dados de larga escala;
Garantir a qualidade dos dados, assegurando integridade, consistência e precisão;
Projetar e desenvolver modelos de dados complexos e otimizados, definindo padrões e estratégias de longo prazo; 
Modelar e gerenciar bases de dados SQL;
Criar e manter processos de governança de dados de acordo com as melhores práticas e seguindo a LGPD (Lei Geral de Proteção a Dados);
Entender do negócio, explorar dados e desenvolver análises que agreguem valor ao nossos produtos;
Automatizar processos, implementando melhorias em monitoramento e testes.

Requirements And Qualifications

Graduação completa em áreas como Computação, Sistema de Informação; Engenharias, Estatística, Tecnologias e correlatas;
Experiência com ambiente cloud, preferencialmente AWS (Athena, S3, EMR, Redshift, IAM); 
Conhecimentos e atuação prévia com Python, Airflow, SQL;
Gerenciamento de Cluster de Dados;
CI/CD;
Modelagem de Dados; 
Git e Catalogação de Dados;
Spark e Pyspark;
Compreensão de conceitos gerais de dados: ERM, Star Schema, Snowflake Schema; Data Lake; Data Warehouse; Data LakeHouse; Data Mesh; Data Mart; Modern Data Stack; Governança de dados;Lineage de dados; 
Será um diferencial: experiência prévia com engenharia de software. 

Additional information

Ambiente informal, de muita troca e amizade;
Pessoas apaixonadas pelo que fazem;
Um aprendizado novo por dia;
Local que está buscando seu melhor todos os dias;
Crescimento e mil oportunidades ao seu redor;
Salário competitivo;
Vale Refeição (Caju);
Auxílio Academia;
Ajuda de Custo Home Office;
Plano de Saúde;
Auxílio Creche - Mulheres em Retorno de Licença Maternidade
Vale-Educação Anual;
PLR;
Possibilidades de se tornar sócio/sócia da empresa :)

Process stages

Step 1: Registration1Registration
Step 2: Triagem2Triagem
Step 3: Entrevista RH3Entrevista RH
Step 4: Entrevista Técnica4Entrevista Técnica
Step 5: Entrevista Cultura5Entrevista Cultura
Step 6: Proposta6Proposta
Step 7: Hiring7Hiring

Somos a Big Data!

Somos líderes de mercado no segmento de Inteligência Artificial no Brasil, desde 2012!🚀

Transformamos dados em inteligência de negócio e impulsionamos resultados dos nossos clientes através de recomendações assertivas e otimização de preço de venda. Aqui, nossos desafios não somente são cumpridos, mas abraçados com entusiasmo para construir produtos que geram valor e um impacto real.

Valorizamos pessoas com mentalidade empreendedora, daquelas que não se contentam com o funcionalismo e buscam constantemente maneiras de fazer a diferença! Como dizemos por aqui, “saia dos máximos locais e alcance os melhores resultados possíveis!”

Nosso ambiente é para quem busca protagonismo, força e proatividade. Queremos embarcar em desafios juntos, crescendo e evoluindo como time, sempre com foco no que realmente importa: sermos donos do nosso destino e protagonistas da nossa história!

Junte-se a nós e faça parte de uma equipe que valoriza a iniciativa e a vontade de fazer a diferença.

Estamos empolgados para receber você e juntos alcançarmos novos horizontes! 💙

E olha só alguns dos clientes que já cresceram com a gente: Dexco, Haleon, Kimberly Clark, Eagle Rock, Liberty Coca-Cola, Nivea, Jonhson & Jonhson, Sanofi & Medley, Germed, Natura, Kraft Heinz, P&G, Nestlé, WP Lab, Colgate Palmolive, Banco Santander, Red Bull e muitos outros.","Almacenamiento de datos, Gobierno de datos, Ingeniería de datos y PySpark, Bases de datos, Esquema en estrella, Modelado de datos, Modelo de datos, Prácticas recomendadas en ingeniería de software y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3982757401/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=hF%2FDKSRYX%2BlfpTXMuSYNsA%3D%3D&trk=flagship3_search_srp_jobs,Blockchain Data Engineer,"150 US$K/año - 200 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,"Palo Alto, CA","Acerca del empleo
About Us

OpenBlock is a data-driven platform dedicated to fostering sustainable growth within decentralized protocols. Our product powers incentive recommendations for leading protocols in the space, including: EigenLayer, Lido, Arbitrum, Solana, Sui, Uniswap, Scroll, Fuel, Linea, Mode, and many others.

OpenBlock is backed by notable investors, including: Foundation Capital, Electric Capital, Circle Ventures, AlleyCorp, and others. Our team of 40+ has backgrounds from Stanford, a16z, Carnegie Mellon, Meta, Palantir, and other top-tier institutions.

We invite you to explore our work at www.openblocklabs.com

For the Blockchain Data Engineer position your responsibilities will center around collecting data from disparate data source.

This Includes

Develop and maintain automated ETL (Extract, Transform, Load) processes for ingesting and transforming raw blockchain data into usable formats.
Work across multiple L1s, L2s, and other blockchain ecosystems. Connect to RPC endpoints, index smart contracts, and quickly make sense of blockchain event data. 
Evaluate and implement tools and technologies for blockchain data processing, such as blockchain explorers, node APIs, and data streaming platforms.
Integrate and manage these workflows within Dagster for streamlined data processing workflows with robust monitoring and alerting.
Write clean, robust, and maintainable code that adheres to industry standards and best practices.
Manage and optimize databases for storing blockchain data, including schema design, indexing, and query optimization.

Qualifications

5+ years of experience as a data engineer. 
Understanding of blockchain event data, familiarity with smart contracts and blockchain data structures.
Proficiency in programming languages such as Python and SQL.
Experience with distributed storage systems (e.g. S3).
Expertise with any of ETL schedulers such as Apache Airflow, Dagster, Prefect or similar frameworks.
Excellent problem-solving skills and the ability to work independently as well as part of a team.
Strong communication skills to effectively collaborate with researchers, engineers, and data teams.
Experience with cloud platforms such as AWS, Azure, or Google Cloud.

Salary ranges provided for US based candidates.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Lenguajes de programación y SQL, Bases de datos, Comunicación, Lenguaje de consulta (query) y Resolución de problemas",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3984551006/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=LiovUf8rGw6vY%2FrUSrIuNg%3D%3D&trk=flagship3_search_srp_jobs,Azure Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,"Nuevo Brunswick, NJ","Acerca del empleo
Role: Azure Data Engineer

Location: New Brunswick, NJ

Duration: Long term

Rate: $Market All Inclusive

Job Description:

Looking for a strong Azure Data Engineer with 10 to 12 years of experience.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3985254405/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=DylReJ02EGXsxNZuVYC9ig%3D%3D&trk=flagship3_search_srp_jobs,Data Architect/Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Harrisburg, PA","Acerca del empleo
Location: This position will work a hybrid model (remote & office). The ideal candidate will live within 50 miles of one of our Elevance Health PulsePoint office locations.

Preferred Location: Indianapolis, IN

The Data Architect/Engineer would be working with business stakeholders, data team developers and administrators; building a dependable and scalable data orchestration architecture that allows for data storage, retrieval, processing and analysis. You will also be responsible for data security, compliance, and quality while staying up to date on emerging technologies and best practices.

How You Will Make An Impact

Creates data processing and orchestration processes and how data is formatted, stored, and retrieved inside the organization. This comprises conceptual, logical, and physical process and documentation development.
Creating and optimizing data processes and including the standardization of ETL and performance monitoring and tuning.
Creating and maintaining data integration processes, ETL (Extract, Transform, Load) workflows, and data pipelines to seamlessly transport data between systems.
Applies industry-accepted data architecture practices and techniques.
Applies data architecture principles, standards, guidelines and concepts.
Develops logical and physical data models.
Works with business users, development teams and database administrators to help develop, implement and maintain databases.
Upholds data quality standards and guidelines.
Upholds data sharing/integration standards and guidelines.
Upholds modeling tool standards and practices.

Minimum Requirements

Requires an BA/BS degree in Information Technology, Computer Science or related field of study and a minimum of 2 years experience in architecture/design in relevant technology disciplines and/or data modeling; or any combination of education and experience, which would provide an equivalent background.
This position is part of our NGS (National Government Services) division which, per CMS TDL 190275, requires foreign national applicants meet the residency requirement of living in the United States at least three of the past five years.

Preferred Skills, Capabilities And Experience

Experience in health care industry strongly preferred.
Experience in designing, developing, testing, and implementing enterprise ETL solutions and process automation and orchestration.
Experience with Data processing platforms and technologies such as Microsoft SSIS, Informatica, ActiveBatch, Snowflake, File Transfer platforms, and Azure technologies.
Experience with data process Orchestration, end-to-end design and build processes for Near-Real Time and Batch Data Pipelines.
Skilled in analyzing and automating manual processes to reduce manual interaction.
Experience with data virtualization/fabric platforms such as Denodo, CData, Talend, Data Virtuality.
Experience with developing data solutions utilizing skills such as Visual Studio, SQL, PL/SQL, T-SQL, Shell Scripting (PowerShell, Unix Shell, etc.), R or Python.
Ability to analyze, troubleshoot and tune SQL queries and develop recommendations.
Ability to understand and analyze existing code in data platforms such as Informatica, SSIS, SQL, Unix scripts, etc.","Arquitectura de datos y Extraer, transformar y cargar (ETL), ActiveBatch, Automatización de procesos, Denodo Platform, Logics, Modelado de datos, Modelo de datos, Next-Generation Sequencing (NGS) y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3946956499/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=DoORnEQ1NjWMMVY3ZlaL%2BQ%3D%3D&trk=flagship3_search_srp_jobs,Finance Data Engineer,"73,9 US$K/año - 170,3 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 4 días,"Lehi, UT","Acerca del empleo
Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

The Opportunity 

This is an exciting opportunity to join the Digital Media Finance team as we continue to propel the business through data-driven forecasts and provide influential insights. In this role, you will have the opportunity to help drive Adobe’s pivotal initiatives by data engineering solutions that connect systems to visualization platforms for management reporting. The position will report to the Group Manager, Finance Automation to drive our systems and automation effort. This position will provide an outstanding opportunity to drive timely insights across Digital Media (DMe) Business at Adobe and help drive the Finance Transformational efforts undertaken by the Finance Automation team.

The ideal candidate for this role is a well-rounded top performer with exceptional data engineering, analysis, and visualization skills, capable of driving growth in a fast-paced environment. You should possess a continuous learning mentality to explore and implement innovations in the team's reporting and transformational projects. Strong cross-functional collaboration within Adobe is essential. You will be responsible for automating complex financial models, developing engaging and interactive dashboards, streamlining processes, and enhancing efficiencies in management reporting

What you'll do:

Responsible for the strategy and execution of DMe lifecycle of financial and analytics dashboards and ensuring scalability of systems and processes. 
Manage data transitions and platform migrations between SAP HANA/DataBricks, Tableau/PowerBI, and related ETL processes. 
Develop data pipelines, connections, and infrastructure to better enable forecasting and data science modelling. 
Correct data errors via systematic, logical fixes and provide updates within the data pipelines, connections and infrastructures that are built to support the teams. 
Create documentation and support enablement for the teams who are interested and able to help themselves. 
Accountable for ad-hoc support & participation in critical business analytics and key project support as directed by management. 
Partner closely with IT, Finance Systems, Finance Transformation Office, and other Finance counterparts to continually improve, streamline & enhance planning and reporting processes. 

What you need to succeed:

BS/BA with preferred focuses in areas of Business, Finance, or Information Systems. Master’s in Data Science a plus. 
3+ years of demonstrated experience working with large and complex data structures and develop efficient queries to create calculated fields and data aggregates. 
2+ years of proven experience designing and developing dashboards using PowerBI or Tableau. 
3+ years of relevant experience in data science and analytics in creating/maintaining financial models for a subscription/SaaS business a plus. 
Proficient in data platforms/systems (such as SQL, Databricks), ETL tools (such as Python, SnapLogic), process automation, and standardization. 
Self-starter with high attention to details, excellent interpersonal skills, and ability to take charge, set objectives, and deliver results. 
Strong project management skills with ability to juggle multiple priorities. 
Strong team orientation and a learning mentality. 

Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $73,900 -- $170,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.

Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances.

Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.

Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.

Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, Automatización, Habilidades sociales, Panel de control y SAP HANA",Solicitar
https://www.linkedin.com/jobs/view/3866780801/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=Y%2FUMRRch7%2BQIrktu7tWcng%3D%3D&trk=flagship3_search_srp_jobs,Cloud Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 4 días,"Madison, WI","Acerca del empleo
Are you passionate about working with data? Do you thrive on strategic thinking and collaborating with cross-functional teams to deliver exceptional data solutions? Our client is seeking a skilled and innovative Cloud Data Engineer to come in and help them organize and combine their data groups. You will also be working with Informatica Cloud (IICS),Snowflake and MS-SQL. Work with their dynamic team. Your role will play a pivotal role in shaping our data strategy, enabling optimal performance, stability, and extensibility across the enterprise. This is a contract to hire role that is 100% remote.

Skills required:

Data background experience
Snowflake experience
Experience with Informatica Cloud
MS-SQL Server
Banking/Financials required
Must be US Citizen or GC holder
Creative solution solver


Why work here:

Remote

Make a difference

Benefits included

NRC is committed to providing our consultants with a better experience. We invest in relationships to better understand your career goals and aspirations. Through our Educate and Connect initiative, we offer multiple professional growth and thought leadership opportunities, including a comprehensive Consultant Education Reimbursement program for most roles, and actively connect you with our technical and charitable communities. Our efforts are acknowledged through our continued recognition as a Best Places to Work, Top Workplaces, and Best and Brightest award winner. is governed by our core values: Accountable, Passionate, Respectful, Innovative, and Collaborative. These fundamentals guide our business practices, our employment standards, and our continued willingness to give back.

3rd Party Disclaimer: New Resources Consulting does not accept unsolicited resumes or candidate submissions of any kind from third-party recruiting firms. Any resumes received from such agencies will be considered the property of New Resources Consulting.

We look forward to receiving your application!

#IND123_","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Informatica (empresa), Resolución creativa de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3982614584/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=AlbNhuTf%2FEcjNONZYZ3N8w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"ciudad de baltimore, MD","Acerca del empleo
About Authentica Solutions

Company Overview:

Authentica Solutions is a leading EdTech organization that aims to reimagine the education sector by providing innovative solutions that empower educators, students, and institutions. We are dedicated to creating a holistic and data-driven educational ecosystem, and we are seeking a skilled Data Engineer to play a pivotal role in advancing our Education Intelligence Platform. This platform leverages IPaaS (Integration Platform as a Service), complex data transformation, data cleansing, standards alignment, intricate privacy requirements, and Data Lake technologies to solve complex problems in the EdTech landscape, enhancing our partnerships and driving data-informed decision-making.

Our Customers

Architecting a platform that delivers capabilities as self-service and driven for needs known and unknown provide this role an opportunity to reimagine middleware making it miracle-ware that services EdTech Software Providers (ISVs), System Integrators, Resellers, Consulting Services Partners, Strategic Partners, Departments/Ministries of Education, K12 School Districts, and Higher Education organizations, globally. With a priority on third party purchase and use, our platform must solve the most complex data challenges in Education empowering partners to serve their customer’s needs fast, simple, and with consistency.

Responsibilities

As a Data Engineer, you will be an integral part of our engineering team, responsible for designing, developing, and maintaining the Education Intelligence Platform. Your expertise will be crucial in ensuring the scalability, reliability, and performance of the platform, enabling us to drive true impact for our partners and stakeholders.

You will play a key role in constructing, managing, and enhancing fault-tolerant data infrastructure while upholding the highest standards of data quality and integrity.

Collaborating with the engineering and product teams to execute on product goals.
Developing and managing fault-tolerant, scalable data pipelines capable of handling terabytes of data using distributed cloud technologies.
Develop data ingestion, processing, and transformation techniques to ensure data integrity and quality.
Assisting in the construction of control plane infrastructure using event-driven services for testing, code promotion, and job execution.
Conducting POCs to validate new tools and services that enhance our data engineering solutions and products.
Troubleshooting production data quality issues and ensuring data integrity.
Staying abreast of industry standards and technological advancements to continually improve our engineering output.

About You (or Here’s What We’re Looking For)

Growth mindset, insatiably curious, always learning, and welcoming challenges for the opportunity to grow.
You believe that you can only be successful when the whole team is successful, and you put your efforts towards it.
Has a keen interest in the educational sector and the impact technology can have on it.
Ability to bring innovative technical solutions ideas to solve real problems.
Strong verbal and written communication skills.

Required Skills And Experience

Must Have

Minimum of 2 years hands-on experience with Python and related data libraries (e.g. Pandas, Data Frames) Practical expertise in ETL/ELT technologies and methodologies.
Proven experience in data wrangling and cleaning across structured, semi-structured, and unstructured data formats.
Solid design and development background in modern technologies such as API management, REST/API integration, Containers, and Micro services.
Experience in designing or working with data warehouses, including an understanding of associated data flows.
Exceptional communication skills, both written and verbal.
English fluency is required to effectively communicate with our clients and other key stakeholders both internal and external.

Nice to Have

A background in Education and/or Ed Tech.
Familiarity or experience with orchestration tools.
Knowledge or hands-on experience with streaming platforms and real-time data pipeline systems such as Apache Kafka, Azure Event Hubs, Apache Spark, Databricks, and similar platforms.
Experience working with big data storage technologies like Azure Data Lake, Hadoop, BigQuery, Deltalake, Iceberg, and similar tools.
Knowledge of orchestration engines like Airflow, Dagster, Flyte, and integration with platforms like Snowflake or Databricks for data workflow management.

Join Authentica Solutions and be part of a dynamic team that's shaping the future of education through data-driven insights. If you are passionate about solving complex problems, building scalable data platforms, and making a positive impact on education, we encourage you to apply!

Authentica Solutions is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetics, disability, age, or veteran status. We provide a workplace free from discrimination and harassment, and where employees are treated with respect and dignity. Our employment decisions are based on business needs, job requirements, and individual qualifications.

We encourage candidates from all backgrounds to apply, as we believe a diverse workforce brings a variety of ideas, perspectives, and experiences that enhance our ability to meet the needs of our customers and drive innovation.

Applicants must be authorized to work for any employer in the United States. We are unable to provide sponsorship or assume responsibility for employment Visa sponsorship.","Almacenamiento de datos, Apache Kafka, Apache Spark, Extraer, transformar y cargar (ETL), Ingeniería de datos y Pandas (Software), Calidad de datos, Comunicación, Datos no estructurados y Disputas de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3833867276/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=5KHDNixIjeQR2c5v7cHJ2w%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist/Engineer(Entry Level),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Raleigh, NC","Acerca del empleo
At SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.

 Why Us ? 

SynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.

 REQUIRED SKILLS For Java /Software Programmers 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java , javascript , C++ or software programming 
 Spring boot, Microservices, Docker, Jenkins and REST API's experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis 

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 

Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://www.youtube.com/watch?v=EmO7NrWHkLM 

 https://www.youtube.com/watch?v=NVBU9RYZ6UI 

 https://www.youtube.com/watch?v=Yy74yvjatVg 

SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube

For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ 

 We are looking for the right matching candidates for our clients 

 Please apply via the job posting 

 REQUIRED SKILLS For Java /Full Stack/Software Programmer 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java , javascript , C++ or software programming 
 Spring boot, Microservices, Docker, Jenkins and REST API's experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning Positions 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow 

 If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

 No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Análisis predictivo, Ciencia de datos, Programación, Reconocimiento de patrones y Visualización de datos, Ciencias de la computación, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3975526978/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=28%2BxhQ6v5%2B2XgPoPbghvBg%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Scientist/Engineer (Remote),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 semanas,"Seattle, WA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3919922491/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=lju8txoNAzV7MBZQN5kwSw%3D%3D&trk=flagship3_search_srp_jobs,Remote Data Scientist/Engineer - Entry Level,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"Nueva York, NY","Acerca del empleo
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this market also our candidates get multiple job offers and $100k + salaries.

 please check the below links to see success outcomes of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 https://www.youtube.com/watch?v=OFoqPTNORew

 https://www.youtube.com/watch?v=-HkNN1ag6Zk

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

 https://youtu.be/bJJl27D8bh0

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3977202431/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=2OcPjkjthPSGUoJ5lLPOXQ%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist/Engineer(Entry Level),"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Berkeley, CA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates can achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergistic understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and are looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Positions 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernetes, and REST API experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who match client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or C2C candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3980243382/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=qU3uEZvGKFTtJ8qadxWzPA%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, Analytics","En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Meta Platforms, Inc. (f/k/a Facebook, Inc.), is seeking the following. Apply via Dice today!

Meta Platforms, Inc. (f/k/a Facebook, Inc.) has the following position in Menlo Park, CA:

Data Engineer, Analytics: Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. Telecommute from anywhere in the U.S. permitted. (ref. code REQ-2406-138424: $174,129 196,900).

Individual pay is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base salary only, and do not include bonus or equity or sales incentives, if applicable. In addition to base salary, Meta offers benefits. Learn more about benefits at Meta at this link: https://www.metacareers.com/facebook-life/benefits. For full information & to apply online, visit us at the following website http://www.metacareers.com/jobs & search using the ref code(s) above.","Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Solicitar
https://www.linkedin.com/jobs/view/3981649242/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=1br6o7FuDarLtAQhY%2FP7bw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Virginia, Estados Unidos","Acerca del empleo
Hybrid | Washington D.C. | 3-4 Days a Week Onsite

Active TS/SCI Clearance Required

Summary

Our client is an employee and Service-Disabled, Veteran-owned Small Business focused on providing niche technical services. They are a team of experienced cybersecurity professionals with a track record of success in the Federal, Commercial, and Academic workspaces. Additionally, our client designs, builds, operates, and secures scalable cloud and IT infrastructures to meet their customers’ near-term needs and fulfill their long-term requirements.

Responsibilities

Our client is seeking a Data Engineer to develop and design data pipelines to support end-to-end solutions in various settings. The ideal candidate will build systems that collect, manage, and convert raw data into usable information for purposes such as IT application usage, reporting, and applicant processing.

Develop and design data pipelines to support an end-to-end solution
Develop and maintain artifacts such as schemas, data dictionaries, and transforms related to ETL processes
Integrate data pipelines with AWS cloud services to extract meaningful insights
Manage production data within multiple datasets ensuring fault tolerance and redundancy
Design and develop robust and functional dataflows to support raw data and expected data
Provide Tier 3 technical support for deployed applications and dataflows
Collaborate with the data engineering team to design and launch new features, including coordination and documentation of dataflows, capabilities, etc.


Requirements

Three (3) to five (5) years of relevant experience is required
Expereince within
Amazon Web Services (AWS)
Database Administration
Data Engineering
ETL Architecture and Development
End-to-End Processes
API Development
Extract, Transform, and Load (ETL)
Data Pipeline
Tier 3 Technical Support
Preferred Requirements

Database administration and development experience
Experience with cloud message APIs and usage of push notifications
Keen interest in learning and using the latest software tools, methods, and technologies to solve real-world problem sets vital to national security


Education/Certification Requirements

A Bachelor's degree is required for this position


Clearance Requirements

Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; an active TS/SCI clearance is required.


Other Duties

Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

About Us

Northern Virginia-based Precision Solutions is an expert in staffing solutions for companies of any size that open the door to new opportunities and seek outstanding talent. We pride ourselves on being versatile enough to tailor our relationships to the needs of each individual client, being agile in the fast-paced marketplace, and being precise in meeting the needs of any company.

Equal Opportunity Employer Statement

Precision Solutions is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Administración de bases de datos, Bases de datos y Datasets",Solicitar
https://www.linkedin.com/jobs/view/3982183920/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=FXW%2FDMyTVv%2F5QGGSNmn5Eg%3D%3D&trackingId=y%2FM4gw7Efh5iDKuJgoWTnA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer-W2,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 6 días,"Bellevue, WA","Acerca del empleo
Job Description: 
  Are you interested in guiding key business decisions around one of Client's most significant customers facing teams? 
Do you want to build a cutting-edge highly scalable analytics big data platform using AWS technologies such as Redshift, RDS, S3, EMR, Glue, ADP, Hive, Kinesis, SNS/SQS, and NAWS supported streaming services? 
Do you want to collaborate with Business Intelligence Engineers (BIE) and Data Scientists (DS) to build ML/LLM models to support our major customer facing features and initiatives to drive team efficiency? 
We are seeking an experienced, self-driven, analytical, and strategic Sr. Data Engineer. 
In this role, you will be working in one of the world's largest and most complex data warehouse environments. 
You should be passionate about working with huge datasets and be someone who loves to bring data together to answer business questions. 
You should have deep expertise in creation and management of data lake and the proven ability to guide continuous enhancement of data architecture by identifying efficiency gaining solutions and leveraging evolving ne technologies. 
In this role, you will have ownership of end-to-end development of data engineering solutions to complex questions and you'll play an integral role in strategic decision-making. 
The right candidate will possess excellent business and communication skills, be able to work with business owners to tackle ambiguous business questions with creative data/science solution designs and be able to collaborate with BIEs and DSs to build those solutions or answer business questions. 
You should have a solid understanding of how to build efficient and scalable data infrastructure and data models and have the capability or the desire to learn and implement Elastic MapReduce (EMR)-based solutions where appropriate. 

 Business Group: 
  Delivery Experience 

 About the team 
  We consider ourselves a start-up team with a big vision and ""anything is possible"" mindset when faced with highly ambiguous and complex problems. 
We are a highly collaborative and high-energy team that isn't afraid to swing for the fences or exceed expectations even on tactical projects. 

 Primary responsibilities 
    In this role, you will have the opportunity to display your skills in the following areas: 
  Design, implement, and support an analytical data infrastructure providing ad hoc access to large datasets and computing power 
Managing AWS resources including EC2, RDS, Redshift, etc 
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies 
Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency 
Collaborate with BIEs to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation 
Collaborate with DS to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learning 
Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers 
AWS technologies such as Redshift, RDS, S3, EMR, Glue, ADP, Hive, Kinesis, SNS/SQS, and NAWS supported streaming services ","Almacenamiento de datos, Arquitectura de datos, Gestión de datos y Ingeniería de datos, Comunicación, Datasets, Informes y análisis, Lagos de datos, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3967427902/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=3rfDibBIappH3ue9zyoKTw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 semanas,Estados Unidos,"Acerca del empleo
Abrazo Group is a resource provider focused on gaming, entertainment, automotive, technology, healthcare, and marketing industries. We partner with both new and established businesses to help them reach their target audiences and achieve growth. Our client based in the Greater Chicago Area is seeking a full-time Data Engineer.

Role Description

We are looking for a skilled Data Engineer with a strong background in data management and optimization. We are a PostgreSQL/AWS environment with Redshift, RDS and Shopify. The ideal candidate will have experience in data warehousing and ETL. Data Analytics experience is also strongly desirable as well as experience with data visualization tools.. As a Data Engineer, you will play a crucial role in ensuring our data is efficiently managed, to help optimize the performance of our digital products.

Responsibilities

Design, deploy, and maintain data warehouses systems and data solutions.
Collaborate with cross-functional teams to understand data requirements and ensure data solutions align with the organization’s data strategy.
Develop and maintain documentation related to data platform design, architecture, and best practices.
Stay up-to-date with industry trends, tools, and best practices related to data analytics and data engineering.
Maintain scalable data pipelines and build out new API integrations to support continuing increases in data volume and complexity.
Provide clean, transformed data ready for analysis
Work with business users to understand data requirements
Train business users on how to use self-service data visualization tools (Tableau)
Provide deep insights work
Build critical dashboards

Qualifications

Experience with Redshift or other modern data warehousing platforms such as Snowflake and/or BigQuery. 
Extensive experience with dimensional data modeling. 
Data orchestration experience with Airflow/Python. Prefect, and/or Dagster are options. 
Knowledge of data modeling within a data warehouse: dimensional, data vault, etc. 
Experience with data integration tools such as Airbyte, Stich, FiveTran, etc. 
Experience with Data Analytics using data visualization tools (Tableau preferred).
Expert-level SQL skills, and familiarity with scripting languages.
Strong problem-solving and troubleshooting skills. 
Excellent communication and teamwork skills. 

Nice-To-Have

Experience with Django / Python 
Experience with customer data platforms (Segment preferred)","Airflow, Almacenamiento de datos, Desarrollo de base de datos, Extraer, transformar y cargar (ETL), Python y Tableau, Administración de bases de datos, Amazon Redshift, Diseño de bases de datos y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3977138365/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=rGqCCLBrug0G2wkhgAoymA%3D%3D&trk=flagship3_search_srp_jobs,DATA ENGINEER (REMOTE),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,Estados Unidos,"Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Job Title: Data Engineer

Duration: 12+ months

Location: Remote 

Description

Experience in manual and database testing along with proficiency in API testing methodologies.
Excellent verbal and written communication skills, including the ability to write comprehensive test cases and test plans.
Strong background in application and API testing, preferably with applications built on Java and J2EE.
Knowledge and hands-on experience in cloud environments (GCP or AWS) with the ability to perform end-to-end and integration testing effectively.","Buena práctica clínica y Google Cloud, Bases de datos, Casos de prueba, Comunicación, Comunicación escrita, Planificación de pruebas, Pruebas API, Pruebas de bases de datos y Pruebas de integración",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3975413664/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=IMu8E%2B0xTFAM2TOSKX19hQ%3D%3D&trk=flagship3_search_srp_jobs,Python Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
Python Data Engineer- Mid level
Charlotte NC
Current remote

Skills Needed:
Mandatory : Python with Object oriented concepts, Spark, Problem Solving ability, Effective communication
Good to have : AWS.
 Detailed Job Description:
 We are currently seeking a Senior Data Engineer with hands-on coding experience and a strong background in Python, PySpark, and Object-oriented programming. The ideal candidate will be responsible for designing, developing, and implementing new features to our existing framework using PySpark and Python. This position requires a deep understanding of data transformation and the ability to create standalone scripts based on given business logic.
 Responsibilities:
Design, develop and implement new features to our existing framework using PySpark and Python.
Write efficient and effective standalone scripts in PySpark with transformations as per the defined business logic.
Use your expertise in Python and Object-oriented concepts to solve complex problems and implement robust solutions.
Work closely with the team to understand the requirements and develop solutions that align with the company's objectives.
Test and debug code to ensure it produces the desired results.
Document all programming tasks and procedures for future reference and troubleshooting.
 Qualifications:
Proficient in Python, PySpark, and Object-oriented programming concepts.
Proven experience as a Senior Data Engineer or similar role.
Strong problem-solving techniques with an ability to troubleshoot complex software issues.
Experience with AWS is preferred, but not mandatory.
Excellent communication skills, both written and verbal.
Self-motivated and able to work independently with minimal supervision.","Amazon Web Services (AWS), PySpark y Python",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982835858/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=rDrOYE0%2F71iKPwogHU4V1w%3D%3D&trk=flagship3_search_srp_jobs,Big Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Red Bank, NJ","Acerca del empleo
Benefits:

401(k)
401(k) matching
Company parties
Competitive salary
Dental insurance
Flexible schedule
Free food & snacks
Health insurance
Opportunity for advancement
Paid time off
Parental leave
Training & development
Vision insurance


Big Data Engineer (Hybrid position)

Job Summary

 

As a Big Data Engineer at Throtle, you will be responsible for designing and developing complex software systems that integrate with Throtle big data processing solutions. You will work closely with cross-functional teams to architect scalable, secure, and maintainable software systems that meet the needs of our business.

Duties/Responsibilities

 

 Design and develop software architectures for large-scale data processing and analytics platforms
 Collaborate with data engineers to design and implement data pipelines and transformations using big data technologies such as Hadoop, Spark, Kafka, and related ecosystems
 Develop and maintain complex SQL queries for data extraction and reporting, and optimize query performance and scalability
 Design and develop software components that integrate with our identity graphs, client data, and keying processes
 Work with cross-functional teams to ensure software systems meet business requirements and are scalable for future growth
 Develop and maintain technical documentation for software architectures and designs
 Ensure data integrity and quality in development process
 Position will require that the individual understands all regulations and laws applicable to their assigned roles and responsibilities. Additionally, the individual will be responsible for the development, implementation, and regular maintenance of policies and procedures that govern the work of assigned roles and responsibilities, including compliance with the security requirements of ePHI.


Required Skill And Abilities

 Strong understanding of software design patterns, architecture principles, and big data technologies
 Proficiency in programming languages such as Java, Python, Scala, or similar languages
 Experience with large-scale data processing and analytics platforms, including Hadoop, Spark, Kafka, and related ecosystems
 Knowledge of distributed computing concepts and experience with cloud-based infrastructure (AWS)
 Strong analytical and problem-solving skills to address complex software architecture challenges
 Ability to work effectively with cross-functional teams and communicate technical designs and solutions to non-technical stakeholders


Education And Experience

 Bachelor's Degree in Computer Science, Engineering, or a related field
 Master's Degree or certification in Software Architecture or a related field preferred but not required
 5+ years of professional experience in software development, architecture, or a related field
 Bachelor’s Degree in related field such as computer science, Engineering, or related field.
 Experience in data modeling, relational and NoSQL databases, SQL and programming languages for building big data pipelines and transformations.


About Throtle:

Throtle is a leading identity company trusted by the world’s top brands and agencies located in Red Bank, NJ. At Throtle, we empower brands at scale with true individual-based marketing using a data-centric identity and onboarding approach.

Throtle is a company that truly values its employees and their work-life balance. We offer a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being:

Competitive compensation.
Comprehensive benefits include Medical, Dental, and Vision.
 Life insurance.
Long-Term Disability
A generous PTO program.
A 401k plan supported by a company match.
Half Day Summer Fridays (close at 1 p.m. Memorial Day to Labor Day).
Early Fridays (office closes at 3 p.m.). 
Hybrid Schedule (Mondays and Fridays WFH)
The office is closed between Christmas and New Year.
Company-sponsored lunch at least 1x a month. 
Professional Development Policy! And much MORE!


Throtle is an equal-opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws. 

Flexible work from home options available.","Apache Kafka, Canalizaciones de datos, Hadoop y Scala, Ciencias de la computación, Desarrollo de software, Factoring, Java, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982180097/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=Dq8%2FHFr%2BbHDY01%2FUfyAH2A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 6 días,Estados Unidos,"Acerca del empleo
Experience: 6-10 years

Location: Remote

Key Skills:
Proficiency in LAN/WAN, Cisco routing/switching
Experience in Telecom/VoIP is a plus
Familiarity with AWS, Salesforce, and APIs is a plus

Responsibilities:
Design, develop, and maintain robust data infrastructure
Collaborate with cross-functional teams to ensure seamless data integration and communication
Optimize and troubleshoot network systems

Qualifications:
Proven ability to work effectively in a remote team environment
Strong communication and collaboration skills

Preferred Qualifications:
Experience in Telecom/VoIP
Proficiency with AWS, Salesforce, and APIs","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Conmutación de paquetes, LAN-WAN, Red de área local y Routers de Cisco",Solicitud sencilla
https://www.linkedin.com/jobs/view/3975691429/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=WpU84K%2BYDUS9v97e6UOCnw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Kansas, Estados Unidos","Acerca del empleo
This inclusive employer is a member of myGwork – the largest global platform for the LGBTQ+ business community.  

Experience Level: Experienced Hire

Categories

Engineering & Technology

Location(s):

Remote - United States, US

At Moody's, we unite the brightest minds to turn today's risks into tomorrow's opportunities. We do this by striving to create an inclusive environment where everyone feels welcome to be who they are-with the freedom to exchange ideas, think innovatively, and listen to each other and customers in meaningful ways.

If you are excited about this opportunity but do not meet every single requirement, please apply! You still may be a great fit for this role or other open roles. We are seeking candidates who model our values: invest in every relationship, lead with curiosity, champion diverse perspectives, turn inputs into actions, and uphold trust through integrity.

he Compliance & 3rd-Party Risk unit at Moody's leverages data-driven insights and cutting-edge technologies to help businesses deliver prudent risk management in the KYC, Supply Chain, and Trade Credit space. We are currently seeking a Data Engineer to join our dynamic and motivated Data & Analytics team. The ideal candidate will have a strong foundation in data engineering concepts such as pipelines, orchestration and streaming; as well as a passion for working with cutting-edge technologies to ensure clean, quality, and usable data is democratized to the business.

In this role, you will be highly involved working on creating a robust data model supporting a multitude of business functions. In your day-to-day you will be working in a squad consisting of other data engineers but working closely with other data roles.

The Work

Develop and maintain product-specific data pipelines to ensure the timely and accurate flow of data from various sources to our Databricks platform.
With the guidance of technical peers, learn to work with business and other cross-functional team members to identify, integrate, and distribute data data from sources to destinations.
Work on project squads adhering to project management best practices and participating in scrum ceremonies
Gain an understanding of data usage patterns and requirements within the organization to design and implement appropriate data models in the data warehouse, ensuring compliance with industry regulations.
Utilize Databricks and AWS/GCP services to manage, store, and process data, ensuring optimal performance, security, and scalability while adhering to data protection standards.
Leverage orchestration tools, such as Databricks Workflows or Airflow, to orchestrate and monitor data pipelines, ensuring seamless integration, data quality, and compliance with regulatory requirements.
Implement data transformation and validation processes using dbt and SQL to ensure data accuracy, consistency, and regulatory compliance across the organization.
Continuously monitor, troubleshoot, and optimize data pipelines to ensure data reliability, performance, and adherence to best practices and industry regulations
Stay current with industry trends, emerging technologies, and regulatory changes to ensure the company remains at the forefront of data engineering best practices in the compliance and AML space.

Qualifications

1-2+ years of experience in data, or backend engineering roles.
Bachelor's degree in Computer Science, Engineering, or a related field.
Some exposure and understanding of data modeling concepts such as the snowflake/Kimball model and Data Vault.
Some understanding of data engineering concepts such as orchestration & pipelining, data modeling, streaming and warehousing
Excellent problem-solving skills and the ability to work independently or as part of a team.
Eagerness to learn new technologies and adapt to industry trends and changes.

For US-based roles only: the anticipated hiring base salary range for this position is $106,600.00 - $154,600.00, depending on factors such as experience, education, level, skills, and location. This range is based on a full-time position. In addition to base salary, this role is eligible for incentive compensation. Moody's also offers a competitive benefits package, including not but limited to medical, dental, vision, parental leave, paid time off, a 401(k) plan with employee and company contribution opportunities, life, disability, and accident insurance, a discounted employee stock purchase plan, and tuition reimbursement.

Moody's is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody's also provides reasonable accommodation to qualified individuals with disabilities or based on a sincerely held religious belief in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications.

For San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance.

This position may be considered a promotional opportunity, pursuant to the Colorado Equal Pay for Equal Work Act.

Click here to view our full EEO policy statement. Click here for more information on your EEO rights under the law. Click here to view our Pay Transparency Nondiscrimination statement. Click here to view our Notice to New York City Applicants.

Candidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody's Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.","Airflow, Almacenamiento de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Modelado de datos, Modelo de datos, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3966155212/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=ndl12iEin2niFYz6VkUDcw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,"Gilbert, AZ","Acerca del empleo
Description

The Data Engineer is responsible for developing and maintaining data solutions, including databases and BI pipelines. Their duties include collaborating with Data & Analytics leadership and IT colleagues on creating data infrastructure, assuring data dependability and uptime, and updating systems to accommodate changes in company needs.

Who We Are

Champions Funding is a nationwide wholesale lender that provides non-QM loan options for consumers and investors. As a community development financial institution (CDFI), Champions is uniquely poised to serve underserved borrowers. As designated by the U.S. Treasury, our flagship Ally program and other robust loan programs give our broker partners the tools to fulfill the dreams of homeownership for diverse homebuyers.

Requirements

5+ years of experience working as a Data Engineer, BI Developer, or similar.

Strong expertise in SQL, ETL development, and database optimization (SQL Server preferred).

Knowledge of data architecture and associated best practices.

Familiarity with BI system integration is preferred.

Experience working in a financial services environment is a plus but not required.

Sponsorship for work authorization is not available for this position.","Almacenamiento de datos, Analítica de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3979164956/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=VKNldRPqbqLuFxGYMXVRRw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 1 semana,"Nueva York, Estados Unidos","Acerca del empleo
Description

Data Engineer

Come build the future as a Data Engineer at Amazon, where you will be inspired working along best-in-class inventors and innovators! You will have the opportunity to create meaningful experiences that deliver on the ever-evolving needs of our customers, and your work will impact millions of people around the world.

As an Amazon Data Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. You will find that there is an unlimited number of opportunities within Amazon, where developing your career across a wide range of teams is highly supported. We are committed to making your work experience as enjoyable as the experiences you’ll be creating for our customers.

Apply now and you will be eligible for Amazon Data Engineer positions that are based on your preferred location, team, and more. We’re hiring across Amazon Stores in the United States and Canada.

Teams with available positions include, but are not limited to:

 Consumer Technology: Build new generation features and products for amazon.com, constantly improving the Customer and Seller experience for billions around the globe. Whether building site wide features such as reviews and recommendations, category specific software for the likes of Pharmacy, Electronics, Digital Software and Video Games or seller infrastructure, there are a variety of complex problems to tackle using a range of technologies in the design of your technical solutions.
 Operations Technology: Shape the future of transportation planning and execution on a global scale, that impacts hundreds of fulfillment centers, thousands of Amazonians, and millions of customers across the world. Your technology will support thousands of operators worldwide to design, build and run the best-in-class Amazon transportation network. We are building intelligent software to make transportation more reliable, faster, and less costly, providing a better and less expensive experience for our customers.
 Human Resources Technology: Create a seamless experience for millions of Amazonians and/or candidates. Whether supporting technologies for onboarding, time and attendance, compensation, amazon.jobs, or recruiting, you’ll deliver robust feature sets, elegant designs, intuitive user interfaces and systems that make it easy for Amazonians to excel at performing critical business functions.

About Us

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance be-tween your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign opportunities based on what will help each team member develop into a better-rounded contributor.

Inclusive Team Culture

Here at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Key job responsibilities

 Design, implement, and support a platform providing secured access to large datasets.
 Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
 Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
 Tune application and query performance using profiling tools and SQL.
 Analyze and solve problems at their root, stepping back to understand the broader context.
 Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
 Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
 Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
 Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com LLC - A03

Job ID: A2703397","AWS Lambda, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Datasets, Modelado de datos y Object Storage",Solicitar
https://www.linkedin.com/jobs/view/3921158910/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=%2FZXFFeBtGFHcg8L9sQYgcg%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 meses,"Fairfax, VA","Acerca del empleo
Secure our Nation, Ignite your Future

Job Description

Become an integral part of a diverse team while working at an Industry Leading Organization, where our employees come first. At ManTech International, you’ll help protect our national security while working on innovative projects that offer opportunities for advancement.

Currently, ManTech is seeking a motivated, career and customer-oriented Data Engineer to join our team in Fairfax, VA, Warrenton, VA , The Pentagon or Fort Washington, MD. 

We are seeking a passionate and skilled Junior Data Scientist to join our growing team. In this role, you will apply your expertise in data wrangling, analysis, and model building to tackle complex business challenges and derive actionable insights from diverse data sources. You will work collaboratively with stakeholders across our customer’s government organization to translate data into meaningful recommendations that drive positive change.

Responsibilities Include but are not limited to:

Design, implement, and maintain data processing pipelines for structured and unstructured data. 
Perform data cleaning, transformation, and feature engineering to prepare data for analysis. 
Conduct exploratory data analysis to identify patterns, trends, and anomalies. 
Develop and implement machine learning models for tasks such as prediction, classification, and clustering. 
Evaluate model performance and iterate to improve accuracy and generalizability. 
Communicate findings effectively through visualizations, reports, and presentations. 
Collaborate with cross-functional teams (e.g., product, engineering, marketing) to translate insights into actionable steps. 
Stay up to date on the latest data science tools, techniques, and industry trends. 
Contribute to the development and improvement of data science processes and best practices. 

Minimum Qualifications:

Bachelor's Degree in data science, Statistics, Computer Science, or a related field (bachelor’s degree with relevant experience considered). 
2 + years of experience as a Data Scientist or similar role. 
Strong proficiency in programming languages like Python, R, or SQL. 
Experience with machine learning libraries and frameworks (e.g., scikit-learn, TensorFlow, PyTorch). 
Solid understanding of statistical concepts and analysis methods. 

Preferred Qualifications: 

Experience with cloud computing platforms like AWS or Azure preferred. 
Excellent communication, collaboration, and problem-solving skills. 
Ability to work independently and manage multiple projects simultaneously. 

Clearance Requirements: 

TS/SCI

Physical Requirements:

The person in this position must be able to remain in a stationary position 50% of the time. 
Occasionally move about inside the office to access file cabinets, office machinery, or to communicate with co-workers, management, and customers, via email, phone, and or virtual communication, which may involve delivering presentations

For all positions requiring access to technology/software source code that is subject to export control laws, employment with the company is contingent on either verifying U.S.-person status or obtaining any necessary license. The applicant will be required to answer certain questions for export control purposes, and that information will be reviewed by compliance personnel to ensure compliance with federal law. ManTech may choose not to apply for a license for such individuals whose access to export-controlled technology or software source code may require authorization and may decline to proceed with an applicant on that basis alone.

ManTech International Corporation, as well as its subsidiaries proactively fulfills its role as an equal opportunity employer. We do not discriminate against any employee or applicant for employment because of race, color, sex, religion, age, sexual orientation, gender identity and expression, national origin, marital status, physical or mental disability, status as a Disabled Veteran, Recently Separated Veteran, Active Duty Wartime or Campaign Badge Veteran, Armed Forces Services Medal, or any other characteristic protected by law.

If you require a reasonable accommodation to apply for a position with ManTech through its online applicant system, please contact ManTech's Corporate EEO Department at (703) 218-6000. ManTech is an affirmative action/equal opportunity employer - minorities, females, disabled and protected veterans are urged to apply. ManTech's utilization of any external recruitment or job placement agency is predicated upon its full compliance with our equal opportunity/affirmative action policies. ManTech does not accept resumes from unsolicited recruiting firms. We pay no fees for unsolicited services.

If you are a qualified individual with a disability or a disabled veteran, you have the right to request an accommodation if you are unable or limited in your ability to use or access http://www.mantech.com/careers/Pages/careers.aspx as a result of your disability. To request an accommodation please click careers@mantech.com and provide your name and contact information.","Analítica de datos, Análisis exploratorio de datos, Ciencia de datos y Ingeniería de datos, Ciencias de la computación, Comunicación, Conceptos de estadística, Datos no estructurados, Limpieza de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3972708194/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=Nz7AfMh%2BN%2Bz%2BKnY2kVarPQ%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer I, PLEX-SIA","Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 2 semanas,"Nueva York, NY","Acerca del empleo
Description

Amazon is looking for a Data Engineer (DE) to join the North America Supply Chain (NASC)'s Science, Intelligence, & Analytics (NASC-SIA) team.

The successful candidate will be a self-starter, comfortable with ambiguity, and able to create and maintain automated processes efficiently. They will set the vision, define the roadmap, and work alongside stakeholders across organizations to deliver results. They know and love working with various AWS Cloud and Data Technologies, understand the principles and techniques of distributed computing, and are enthusiastic about working with data in diverse forms, types, volumes, velocities, etc. This role will have an immediate and lasting impact on both internal and external customers, and provides a unique opportunity to impact Amazon's Supply Chain business at scale for those willing to roll up their sleeves and dive deep to achieve results.

Key job responsibilities

 Implement & maintain an AWS Cloud Infrastructure to securely store, process, and deliver NASC planning data.
 Build and support a modern data architecture that integrates a wide range of available AWS data technologies.
 Ensure the accessibility, trustworthiness, usability, and security of NASC planning data.

About The Team

As part of North America Supply Chain Science Intelligence Analytics team (NASC-SIA), the NASC-SIA Engineering team is responsible for building and maintaining the critical data infrastructure, data management platform, and various planning tools that support NASC's planning and operational needs.

Basic Qualifications

 1+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines
 Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala)
 Experience with one or more scripting language (e.g., Python, KornShell)
 Experience as a data engineer or related specialty (e.g., software engineer, business intelligence engineer, data scientist) with a track record of manipulating, processing, and extracting value from large datasets

Preferred Qualifications

 Experience with big data technologies such as: Hadoop, Hive, Spark, EMR
 Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc.
 Exposure to Supply Chain business operations, particularly the labor planning processes.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com Services LLC

Job ID: A2695372","Apache Spark, Arquitectura de datos, Big data, Extraer, transformar y cargar (ETL), Hadoop, Hive, Ingeniería de datos y PL/SQL, Lenguaje de definición de datos (DDL) y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3980774332/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=Cv1%2FEnIlaIfKh7bC9cX%2BVA%3D%3D&trk=flagship3_search_srp_jobs,Data Analytics engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",hace 5 días,Estados Unidos,"Acerca del empleo
Urgent Need Of analytics engineer please share if your profile is suitable only.


Please call me at 470-312-7682

min 6 yrs in usa 



● Bachelor's degree, MS or greater in a quantitative field of study (Computer/Data Science, Engineering, Mathematics, Statistics, etc.)
● 5+ years of relevant experience in business intelligence/data engineering
● Expertise in writing SQL (clean, fast code is a must) and in data-warehousing concepts such as star schemas, slowly changing dimensions, ELT/ETL, and MPP databases
● Experience in transforming flawed/changing data into consistent, trustworthy datasets, and in developing DAGs to batch-process millions of records
● Experience with general-purpose programming (e.g. Python, Java, Go), dealing with a variety of data structures, algorithms, and serialization formats
● Experience with big-data technologies (e.g. Spark, Kafka, Hive)
● Advanced ability to build reports and dashboards with BI tools (such as Looker and Tableau)
● Experience with analytics tools such as Athena, Redshift/BigQuery, Splunk, etc.
● Proficiency with Git (or similar version control) and CI/CD best practices
● Experience in managing workflows using Agile practices
● Ability to write clear, concise documentation and to communicate generally with a high degree of precision
● Ability to solve ambiguous problems independently
● Ability to manage multiple projects and time constraints simultaneously
● Care for the quality of the input data and how the processed data is ultimately interpreted and used
● Experience with digital products, streaming services, or subscription products is preferred
● Strong written and verbal communication skills",Python,Solicitud sencilla
https://www.linkedin.com/jobs/view/3982098535/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=fji1o47%2FyYYTEGd7N3Evdg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"San Francisco, CA","Acerca del empleo
Description

Requires knowledge of Data quality management techniques and standards

Business metadata definitions and content data definitions;

Data profiling tools, data cleansing tools, data integration tools, and issues and event management tools;

Understanding of user's data consumption, data needs, and business implications

Data modeling, storage, integration, and warehousing

Data quality framework and metrics

User access best practices

Enterprise data architecture, modeling and design, storage, integration, and warehousing

Enterprise data quality framework and metrics;

Enterprise data strategy;

Enterprise data quality strategy;

Enterprise strategy to address regulatory and ethical requirements and policies around data privacy, security, storage, retention, and documentation.

To promote and educate others on data quality awareness.

Profile, analyze, and assess data quality. Test and validate data quality requirements.

Continuously measure and monitor data quality.

Deliver against data quality service level agreements. Manage operational Data Quality Management procedures.

Manage data quality issues and leads data cleansing activities to remove data quality defects, improve data quality, and eliminate unused data.

Determine user accessibility and removes or restricts user access as needed.

Interpret company and regulatory policies on data.

Educate others on data governance processes, practices, policies, and guidelines.

Education: Bachelors Degree","Almacenamiento de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Limpieza de datos, Metadatos, Modelado de datos y Perfiles de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3962587204/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=XPqMrvJZUiB0W6Ums53qUA%3D%3D&trk=flagship3_search_srp_jobs,"Specialist, Ticketing Data Engineer, FIFA World Cup 2026","Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Miami, FL","Acerca del empleo
Department: Ticketing, Hospitality & Stadium Revenue

Employment Type: Fixed Term - Full Time

Location: Miami

Description

Reporting organizationally to the Ticketing Sales Strategy and Requirements Manager, the Data Engineer Specialist will help serve Ticketing and Hospitality efforts for the FIFA World Cup 2026™ for the first time ever, is hosting 48 teams and 1,200+ players across 3 countries and 16 venues. This role will manage the planning and the definition of the process relative to the creation of all the customer's communications and collaterals across the ticket sales process, under the applicable sales strategy and ticketing requirements, and work closely with all the other sub-teams or Functional Areas.

The Data Engineer Specialist designs data pipelines and reports to analyze customer behavior using KPI dashboards and ensure data quality. They collaborate cross-functionally to gather and implement requirements for reports that enable the CRM team to monitor the FIFA World Cup 2026™ project

THE POSITION

The main responsibilities of the Data Engineer Specialist, for the FIFA World Cup 2026™ include: 

 Design, develop, and maintain efficient data pipelines to support the Sales Strategy team in setting up various reports for monitoring project evolution, including the ingestion, processing, and transformation of large volumes of data from various sources. 
 Collaborate with cross-functional teams, including Sales Strategy, Sales Collaterals and Communications, and Customer Care, to gather requirements and timelines, and translate them into technical solutions. 
 Define and track KPI dashboards with the Sales Strategy team to continuously review and report on customer behaviors, ensuring a comprehensive understanding of the information. 
 Liaise with internal and external stakeholders or providers to obtain the necessary data for building required reports. 
 Ensure the quality and availability of data, performing data profiling, cleansing, and validation processes. 

YOUR PROFILE

Education & Qualifications

 Bachelor or master’s degree or equivalent in a relevant area: Computer Science, Engineering, Mathematics, or another related field. 

Work Experience

 2-3 Years of experience in a Data Engineer or a Business analytics role. 
 Experience in data-driven roles or job functions preferred. 
 Proficiency in programming data languages such as Python, SQL, or similar. 
 Skilled in Microsoft Excel (Pivot Tables, Vlookups, etc.), and BI Tools (Microsoft Power BI). 
 Experience analyzing large data sets, from different sources, to generate reports. 
 Excellent verbal communication skills and the ability to manage numerous business relationships in a professional manner, including managing multiple concurrent projects across stakeholders. 
 Ability to execute non-conventional market research. 
 Keen attention to detail, strong time management, problem-solving, and decision-making skills. 
 Highly motivated with a desire to learn new programs and skill sets. 

Languages

 Fluent in English, spoken and written. 
 A native level of at least one of the following: French, Spanish, German, or Portuguese is nice to have. 

Technology

 Strong skills in data management and analysis using SQL for querying databases. 
 Experience with data pipeline tools and/or frameworks. 
 Familiarity with Microsoft Excel for advanced data analysis and reporting (e.g., Pivot Tables, Vlookups). 
 Proficiency in BI tools such as Microsoft Power BI for dashboard creation and data visualization. 

About FWC2026

The FIFA World Cup 26™ will mark the first time that the tournament features 48 teams and will be hosted by three countries: Canada, Mexico, and the United States.

This new format redefines excellence, generating unique opportunities for greater participation and engagement from fans and players in North America and all over the world. Now is your time to be a game changer and join the workforce that is planning and delivering this unique and unforgettable experience.","Analítica de datos, Analítica empresarial, Arquitectura de datos, Canalizaciones de datos, Ciencia de datos, Microsoft Power BI y Visualización de datos, Paneles KPI, Perfiles de datos y Portugués",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3962985640/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=Ba8UUSJYjMqrG09RXzu25g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,San Diego Metropolitan Area,"Acerca del empleo
At Jack Henry, we deliver technology solutions that are digitally transforming and empowering community banks and credit unions to provide enhanced and streamlined user experiences to their customers and members. Our best-in-class products are just the start as we lay the groundwork for the future of digital banking and payments. We hope you'll join us. We can't do it without you.

Join our Credit Union Solution team as a seasoned Data Engineer. As a key player in our modernization mission, you will be at the forefront, providing top-notch engineering support and development on building real-time data pipelines. You will work closely with our product team, researching and diagnosing data strategy to implement a performant data migration path for our customers.

At our core, we value more than just technical prowess. We prize strong internal and team relationships, fostering camaraderie, and injecting a heavy dose of fun into our day-to-day work environment. We're looking for naturally curious individuals, and self-starters, who possess the communication, organizational, and interpersonal skills to navigate complex challenges.

As you grow within the team, your talents and skills won't go unnoticed—we believe in recognizing and capitalizing on what you do best.

This position can be worked remotely located in the Allen, TX or San Diego, CA areas.

What you'll be responsible for: 
Data Pipeline Development: Design, develop, and maintain scalable and efficient data pipelines on Google Cloud Platform using tools such as Dataflow, Big Query, and Cloud Storage.
Database Management: Manage and optimize SQL Server databases, ensuring performance, reliability, and data integrity.
ETL Processes: Implement Extract, Transform, Load (ETL) processes to transform raw data into a usable format for analytics and reporting.
Streaming Data Processing: Leverage GCP streaming services like Pub/Sub to handle real-time data processing and integration.
Data Modeling: Design and implement effective data models to support business requirements and analytical needs.
Collaboration: Collaborate with cross-functional teams including data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions.
Performance Tuning: Conduct performance tuning, optimization, and troubleshooting for data pipelines and SQL Server databases.
Data Governance: Implement and enforce data governance policies, ensuring data quality, security, and compliance.
Documentation: Create and maintain documentation for data processes, data models, and system configurations.
May perform other job duties as assigned.

What you'll need to have: 
Bachelor’s degree in computer science, Engineering, or related field.
Minimum of 11 years of experience in Data Engineering
Proven experience as a Data Engineer with a focus on GCP and SQL Server technologies.
Strong proficiency in SQL and experience with query optimization.
Extensive experience with Data Extraction, Transformation, and Loading (ETL) from different data sources.
Providing support for existing data pipelines.
Monitoring and Reporting Power BI, Google Looker Studio.
Hands-on experience with GCP services such as Big Query, Dataflow, Pub/Sub, and Cloud Storage.
Experience managing and optimizing SQL Server databases.
Proficiency in data modeling and designing effective data structures.
Solid understanding of ETL processes and data integration strategies.
Familiarity with streaming data processing and real-time analytics.
Excellent problem-solving and analytical skills.
Strong communication and collaboration skills.

What would be nice for you to have: 
GCP Professional Data Engineer Certification.
Experience with additional cloud platforms (AWS, Azure).
Knowledge of version control systems (Git).
Familiarity with orchestration tools (Apache Airflow, Google Composer).
Understanding of containerization and orchestration (Docker, Kubernetes).

If you got this far, we hope you're feeling excited about this opportunity. Even if you don't feel you meet every single requirement on this positing, we still encourage you to apply. We're eager to meet motivated people who align with Jack Henry's mission and can contribute to our company in a variety of ways.
 Why Jack Henry?
At Jack Henry, we pride ourselves through our motto of, ""Do the right thing, do whatever it takes, and have fun."" We recognize the value of our associates and believe much of our company's strength and success depends on their well-being.
We demonstrate our commitment by offering outstanding benefit programs to ensure the physical, mental & financial well-being of our people is always met.

Culture of Commitment
Ask our associates why they love Jack Henry, and many will tell you it is because our culture is exceptional. We do great things together. Rising to meet challenges and seeking opportunities is part of who we are as an organization. Our culture has helped us stay strong through challenging times and we credit our dedicated associates for our success. Visit our Corporate Responsibility site to learn more about our culture and commitment to our people, customers, community, environment, and shareholders.

Equal Employment Opportunity
At Jack Henry, we know we are better together. We value, respect, and protect the uniqueness each of us brings. Innovation flourishes by including all voices and makes our business - and our society - stronger. Jack Henry is an equal opportunity employer and we are committed to providing equal opportunity in all of our employment practices, including selection, hiring, performance management, promotion, transfer, compensation, benefits, education, training, social, and recreational activities to all persons regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, genetic information, pregnancy, marital status, sex, gender, gender identity, gender expression, age, sexual orientation, and military and veteran status, or any other protected status protected by local, state or federal law.

No one will be subject to, and Jack Henry prohibits, any form of discipline, reprisal, intimidation, or retaliation for good faith reports or complaints of discrimination of any kind, pursuing any discrimination claim, or cooperating in related investigations.","Almacenamiento en la nube, Buena práctica clínica, Google BigQuery y Ingeniería de datos, Estrategias de integración, Lenguaje de consulta (query), Modelado de datos, Modelo de datos, Necesidades empresariales y Optimización de consultas",Solicitar
https://www.linkedin.com/jobs/view/3980345854/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=vrgm6njt%2BAXnAIAwQ5LqUA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"San Petersburgo, FL","Acerca del empleo
For more Job Opportunities follow FINTRUST CONNECT here: FinTrust Connect: Jobs | LinkedIn

**This job cannot sponsor H1B visas**

Fraud Intelligence Data Engineer - St. Petersburg, FL
FinTrust Connect is excited to announce that we have partnered with a prominent Broker Dealer firm in search of a Fraud Intelligence Data Engineer. This role is responsible for designing and implementing fraud detection solutions, performing complex data analysis, and collaborating with stakeholders to mitigate fraud risks effectively.

Why this Opportunity?
Culture: This firm values a strong team-oriented culture that prioritizes inclusivity and professional development. They believe in creating a supportive and collaborative work environment where all employees feel valued and empowered to reach their full potential.

Workplace (On-site, Hybrid, Remote): Hybrid in St. Petersburg, FL

Requirements:
Bachelor’s Degree in statistics, mathematics, physics, economics, or other analytical or quantitative disciplines. Master’s Degree or PhD preferred.
5-8 years of experience in data analytics, modeling, and analytical project implementation.
Experience working with Big Data environments with hands-on coding experience in tools such as SAS, SQL, Python, Impala, Hive.
Proficiency with data visualization tools, such as Tableau, PowerBI, or Qlik.
Strong quantitative and analytic skills; ability to derive patterns, trends, and insights, and perform risk/reward tradeoff analysis.
Excellent written and verbal communication skills, with the ability to connect analytics to business impacts and present to peers and management.
Creative problem-solving skills and attention to detail.
Deep understanding of data management, including data pipelines and data quality.

Description:
Lead internal fraud analytics, engaging and collaborating with business, fraud, and control stakeholders to use advanced analytics in proactively identifying fraud risk and trends.
Own the knowledge and quality of internal fraud detection data, collaborating with technology to implement the appropriate analytics platform, data structure, pipeline, and transformation within the Big Data ecosystem.
Extract knowledge and insights from data to design complex internal fraud detection solutions through various data preparation, modeling, and visualization techniques, including predictive analysis, pattern recognition, and machine learning.
Serve as the regional analytics point person for stakeholder management and communication, including internal/external audit reviews and compliance assurance reviews.
Support the Internal Fraud Governance team in designing and executing analytical processes, such as change management.
Provide analytic thought leadership and manage project planning effectively.","Análisis de datos, Análisis de datos estadísticos, Big data, Ciencia de datos, Ingeniería de datos y Tableau, Análisis de fraude, Corretaje, Detección de casos de fraude, Modelado de datos y Prevención del fraude",Solicitud sencilla
https://www.linkedin.com/jobs/view/3976343016/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=im9L%2Fzij%2Bi1yCDIcxNxSrg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"40 US$/h - 45 US$/h Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",hace 1 semana,"North Palm Beach, FL","Acerca del empleo
Responsibilities

Kforce has a client that is seeking a Data Engineer in Juno Beach, FL. Summary: We are looking for Data Engineers to work on creation, maintenance, and upgrades to a data pipeline. Daily tasks will include creating datasets from multiple sources, working on debugging, and polishing a real time data pipeline, python notebook dev time. Agile work methodology will be used within the team.

Requirements

 2 years of experience desired
 Proficiency in Python, Pandas, SQL, and large-scale data management and cleanup

Nice To Haves Include

 PostgreSQL
 AWS
 Power BI data flow pipelines
 Excel ETL
 API familiarity

The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.

We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.

Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.

This job is not eligible for bonuses, incentives or commissions.

Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Pandas (Software), Python y SQL, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3975263472/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=jDsUtG35p70HI2J3me6QVw%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Analyst/Scientist/Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Atlanta, GA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this market also our candidates get multiple job offers and $100k + salaries.

please check the below links to see success outcomes of our candidates .

(url removed)

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3971456932/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=zS5mwO8M%2FOHR9Dyul7mEXg%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist/Engineer - Junior,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Baltimore, MD","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

please check the below links to see success outcomes, salaries of our candidates .

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)(phone number removed)di

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación, Reconocimiento de patrones y Visualización de datos, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3984549335/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=5NySx%2FK6KPe%2BAu6DaLl5BQ%3D%3D&trk=flagship3_search_srp_jobs,Data Lakehouse Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,"Houston, TX","Acerca del empleo
Job Title: Data Lakehouse Engineer

Duration: 12+ Months

Location: Houston, TX

Job Description

We are currently seeking an experienced Data Engineer to join the Big Data and Advanced Analytics department. The Data Engineer will work closely with business domain experts to create an Enterprise Data Lakehouse to support data analytic use cases for Gathering, Processing, and Transformation business units.

Responsibilities include:

Design and implement reliable data pipelines to integrate disparate data sources into a single Data Lakehouse
Design and implement data quality pipelines to ensure data correctness and building trusted data sets
Design and implement a Data Lakehouse solution which accurately reflects business operations
Assist with data platform performance tuning and physical data model support including partitioning and compaction
Provide guidance in data visualizations and reporting efforts to ensure solutions are aligned to business objectives
The successful candidate will meet the following qualifications:
10+ years of experience as a Data Engineer designing and maintaining data pipeline architectures
Expert knowledge of ANSI SQL, PLSQL, TSQL, and Python
Experience in various data integration patterns including ETL, ELT, Pub/Sub, and Change Data Capture
Experience in data management practices including data catalog, data lineage, and master data management
Experience in business analysis and defining business performance metrics
Experience in software development practices such as Design Principles and Patterns, Testing, CI/CD, and version control
Experience in implementing a Data Lakehouse using Apache Iceberg or Delta Lake
Knowledgeable of common data visualization tools such as Power BI and Tibco Spotfire
Experience with Dremio is preferred","Ciencia de datos, Extraer, transformar y cargar (ETL), Integración de datos, PL/SQL y SQL, Captura de datos electrónica, Captura de datos modificados, Captura de imagen, Linaje de datos y Métricas de rendimiento",Solicitar
https://www.linkedin.com/jobs/view/3981618586/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=9fYp1DppyqbaXL988QAxtQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - 56813 - Remote | WFH,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Richardson, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Are you a skilled Data Engineer looking for an exciting remote opportunity? We have a long-term position available with one of our prime clients, where you'll be a key player in transforming and managing enterprise data.

About The Role

As a Data Engineer, you will be responsible for designing and developing data pipelines and ETL processes to transform pharmacy data from various sources, both on-premise and cloud-based. You will build and maintain large-scale, robust, and secure databases stored in data lakes or warehouses. You'll have the opportunity to automate data workflows and create engineering solutions that support machine learning and data science projects. Collaborating with data scientists and business partners, you'll play a vital role in deploying machine learning models to production.

Key Responsibilities

Develop CI/CD data pipelines and ETL processes.

Automate data workflows, including ingestion, cleaning, structuring, and formatting.

Build secure databases for storage in data lakes or data warehouses.

Collaborate with data scientists to deploy machine learning models.

Work with a variety of datasets, including structured, unstructured, text, voice, and images.

Required Skills

4+ years of experience in enterprise data warehouse development, preferably using SQL or Snowflake.

Hands-on experience with SQL, PL/SQL, Python, and/or Shell Scripting.

Proficiency with Azure Cloud, including Azure Data Factory and Databricks.

Experience with Kafka for setting up and integrating with Databricks.

Additional Skills

Experience migrating from RDBMS to Snowflake is a plus.

End-to-end dataflow design and development experience is a plus.

Why Join Us?

Work 100% remotely, enjoying a flexible work environment.

Engage in innovative projects with a focus on data engineering and machine learning.

Be part of a collaborative team working on cutting-edge technologies.

If you are passionate about data engineering and excited about remote work opportunities, we would love to hear from you!

Note: This position is available only to W2 candidates. No agencies, please.

Employment Type: Full-Time","Apache Kafka, Extraer, transformar y cargar (ETL), Lenguajes de programación, PL/SQL y SQL, Azure Databricks, Datos empresariales, Guiones shell, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3967449094/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=g3bcgCLxgTQdTCA29e1MyQ%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Junior,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 semanas,"Boyne City, MI","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Positions 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3841221638/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=AgKY7IRV%2B%2Fd640zb1FhC1Q%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Scientist/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 meses,"Colorado Springs, CO","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 If you applied for a job and got emails from our Job Placement Program team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

 please check the below links to see success outcomes of our candidates  and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/ 

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 https://www.youtube.com/watch?v=OFoqPTNORew 

 https://www.youtube.com/watch?v=-HkNN1ag6Zk 

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://youtu.be/bJJl27D8bh0 

For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/

 We are looking for the right matching candidates for our clients 

 Please apply via the job posting 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Programmer 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java , javascript , C++ or software programming 
 Spring boot, Microservices, Docker, Jenkins and REST API's experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning Positions 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow 

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements. 

 No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3958038732/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=GgL2dkxIKWTBDeESjmKI4A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 semana,"Dallas, TX","Acerca del empleo
Build and maintain ETL processes in SSIS and Azure Data Factory
Engineer scalable, reliable and performant systems to manage data
Develop, implement and optimize stored procedures and functions
Research and analyze data issues and provide automated solutions
Implement new technologies to enhance the optimization of current practices
Provide valuable suggestions regarding new ideas and technologies

Aptitudes y experiencia deseables
SSIS , SQL","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Procedimientos de almacenado",Solicitar
https://www.linkedin.com/jobs/view/3982798315/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=Hf59qG4HZa5OZtylXajZ4g%3D%3D&trk=flagship3_search_srp_jobs,Tableau/Power BI Developer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Allied Consultants, Inc., is seeking the following. Apply via Dice today!

Overview

Allied Consultants, Inc. is an Austin-based firm which has for 32 years been a premier provider of technical and business professionals to clients in Texas. We are currently seeking an experienced Tableau/Power BI Developer to be a key resource on a technical services team.

Allied Consultants offers its family of consultants excellent rates, a local support staff, and an attractive benefits package which includes medical insurance (Allied shares a percentage of the cost), life insurance, a matching 401(k) plan and a cafeteria plan.

Candidates selected for interview will be required to undergo criminal background checks and may be required to complete a drug screen in accordance with Federal and State Law. Offers of Employment are contingent on a successful background check

Allied Consultants is an equal opportunities employer.

Responsibilities

Level Description

8 or more years of experience, relies on experience and judgment to plan and accomplish goals, independently performs a variety of complicated tasks, a wide degree of creativity and latitude is expected.

Job Description

Understands business objectives and problems, identifies alternative solutions, performs studies and cost/benefit analysis of alternatives. Analyzes user requirements, procedures, and problems to automate processing or to improve existing computer system: Confers with personnel of organizational units involved to analyze current operational procedures, identify problems, and learn specific input and output requirements, such as forms of data input, how data is to be; summarized, and formats for reports. Writes detailed description of user needs, program functions, and steps required to develop or modify computer program. Reviews computer system capabilities, specifications, and scheduling limitations to determine if requested program or program change is possible within existing system.

Additional job details and special considerations

Contractor will support the State Health Analytics Reporting Platform (SHARP) by creating dashboards and reports using Tableau or PowerBI as determined by the business requirements. Contractor will translate business requirements into Tableau/PowerBI reports, and work with the program staff to ensure it meets the needs of the customer. Contractor will participate in testing or reports and dashboards to ensure proper function, and work with coworkers to publish and promote the reports as needed.

Qualifications

Minimum Requirements:

Candidates that do not meet or exceed the minimum stated requirements (skills/experience) will be displayed to customers but may not be chosen for this opportunity.

Years

Required/Preferred

Experience

8

Required

Experience Creating reports or data analysis, using Tableau, PowerBI, SQL, Cognos, Business Objects, QlikView or similar

4

Preferred

Experience working with very large scale data sets in Oracle, SQL Server, MySQL, Snowflake, or similar.

4

Preferred

Experience working with Tableau creating dashboards and reports

4

Preferred

Experience working with PowerBI creating dashboards and reports","Analítica de datos, Análisis de datos, SAP BusinessObjects y SQL, Expresiones de análisis de datos (DAX), Panel de control, QlikView, Servicios técnicos, Snowflake y Snowflake cloud",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982797505/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oLaVTPkEqI4%2BkOndHvPD0w%3D%3D&trackingId=NKAi5Sl1LPRDYgCz5YJ%2BbQ%3D%3D&trk=flagship3_search_srp_jobs,Tableau/PowerBI Developer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, McKinsol Consulting Inc, is seeking the following. Apply via Dice today!

Exp: Minimum 12+ Years Must

Note : Candidates Must Have Experience with Any State Projects At least 1 or 2

experience Creating reports or data analysis, using Tableau, PowerBI, SQL, Cognos, Business Objects, QlikView or similar

experience working with very large scale data sets in Oracle, SQL Server, MySQL, Snowflake, or similar.

experience working with Tableau creating dashboards and reports

experience working with PowerBI creating dashboards and reports

Job Details :

Contractor will support the State Health Analytics Reporting Platform (SHARP) by creating dashboards and reports using Tableau or PowerBI as determined by the business requirements. Contractor will translate business requirements into Tableau/PowerBI reports, and work with the program staff to ensure it meets the needs of the customer. Contractor will participate in testing or reports and dashboards to ensure proper function, and work with coworkers to publish and promote the reports as needed

--

--

Tableau/PowerBI Developer","Analítica de datos, Análisis de datos, SAP BusinessObjects y SQL, Expresiones de análisis de datos (DAX), Panel de control, QlikView, SQL Server Analysis Services (SSAS), Snowflake y Snowflake cloud",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3984576113/?eBP=BUDGET_EXHAUSTED_JOB&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=EM7JBTiSvHPAZb88cPQqwA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Remote),"114,5 US$K/año - 168,7 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 3 días,"Filadelfia, PA","Acerca del empleo
What You’ll Do

As a Data Architect, you will develop multi-functional relationships and work closely with data engineering and data science teams to drive near- and longer-term initiatives to improve our product, and customer experience. Common Services Platform Engineering provides the basic building blocks for the Cisco Security Cloud with a great emphasis on Data Platform.

This would be a perfect fit for an experienced software specialist with system-level understanding and expert-level knowledge of end-to-end cloud solutions

Who You’ll Work With

A Data Architect in the in Cisco Secure Common Services Engineering, you will work with a team of cybersecurity experts and innovative engineers who support the products and developers across Cisco Security. We put our people first, we take bold steps together, and we value transparency each step of the way.

We’re adding more talented members to our growing team who will help us take Platform and Security to the next level because we believe that there is always room for growth.

Who You Are

To be successful in this role, you’d be a role model who exemplifies our culture and embraces our product principles. These include:

 Make it secure by design, private by default 
 Simplify. To quote Colin Chapman, founder of Lotus Cars “Simplify, then add lightness” 
 Move fast 
 Constantly improve 
 Design products people [really] love 
 Obsessing over the customer 

Minimum Qualifications

 BS or MS in Computer Science or equivalent combination of graduate degree and work experience 
 3 + years of work experience building Data and ML Infra, machine learning & NLP problems involving some or all the following parts of the pipeline: data extraction, developing and training machine learning models, working the new model to scale production deployments 
 3 + years of experience programming skills in Python, Scala, PySpark or C++ 

Preferred Qualifications

 3 + years of work experience architecting software infrastructure, developing, and delivering software products/services with one or more core cloud provider services, preferably AWS, GCP 
 Strong background of building and leading cloud platform with a focus on security, reliability, scalability and performance 
 Confirmed understanding of SaaS and PaaS 
 Proficiency in Data Structures, Micro services, CI/CD, automation, containerization technologies, Kafka Streaming, Spark, GitHub, Jenkins, etc 
 Work experience with DevOps tools and technologies 
 Strong written and verbal communication skills and excellent attention to detail and accuracy 
 Have a passion for complex data/engineering challenges 
 Have the ability to communicate ideas and solutions to both technical and non-technical audiences 

Why Cisco

Secure 

We're global, we're adaptable, we're diverse, and our security portfolio is as extensive as it is groundbreaking. Have you heard of Threat, Detection & Response, Zero Trust by Duo, Common Services Engineering, or Cloud & Network Security? Those are only a few of our product teams! The only thing we're missing is YOU.

Join an enterprise security leader with a start-up culture, committed to driving innovation and giving you the opportunity to make an impact. We #InnovateToWin and we know we're better together, that's why we're dedicated to inclusivity, collaboration, and diversity in everything we do.

We're proud to be the Best Small and Mid-Size Enterprises Security Solution Cisco Secure continues to grow and evolve year after year with 100% of Fortune 100 Companies using our products, and we're excited to see the new heights we'll reach with your passion for security, your customer focus, and your desire to change things up!

There are so many amazing reasons to join Cisco. Learn more here !","Ciencia de datos, DevOps, Ingeniería de datos , PySpark, Python y Scala, Atención al detalle, Ciencias de la computación, Comunicación y Comunicación oral",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3963028182/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=CoXwN%2FuHXYLFektoKNK%2FHA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Orlando, FL","Acerca del empleo
Midtown Consulting Group’s Education Analytics practice is looking for a data engineer with the ability to own the data lifecycle from sourcing through consumption. This role may split time between our core product development and consulting projects, where we develop custom data solutions for both K-12 and higher-ed institutions.

This data engineer should be well-versed in Microsoft Azure suite of products, and have a long-standing history of working with Microsoft technology offerings. 

While a primarily remote, work-from-home opportunity this job does require once monthly travel to Orlando, FL to collaborate in person with the team and our clients. 

You Will
Source complex data, manage and prepare for consumption into PowerBI visualizations for K-12 & Higher-Ed data products, including early warnings, live progress monitoring, graduation & school-grade predictors
Be comfortable across the data lifecycle and be able to jump into tasks related to front-end prototyping and dashboard development as needed
Contribute to data architecture and strategy for our core education analytics product development. Build for the scalability of a turnkey solution.
Engineer custom data solutions for specific consulting clients, including contributing to data architecture and management. Work with cross-functional client stakeholders to gather, analyze, and understand the user requirements
Act as a liaison to the client stakeholders for the models, analysis or reports created; understand the impact of specific drivers and provide insights. Continually refine models and/or reports and analyze results.
Work with senior team members to successfully deliver to clients

You Have
2-6 years of data experience, preferably with consulting experience OR in an education institution
Exposure to a variety of areas of data & analytics, including cloud data storage, data visualizations, and statistical programming.
Experience with large enterprise level data environments and working with 1B+ record data sets
Advanced level skills with the Azure Tech Stack, including: Data Lake, Data Factory, Data Bricks or Analytics, and Azure Machine Learning
Knowledge of CoPilot and Fabric technologies and the ability to work in state-of-the-art technology developments into everyday practice
Reporting & Dashboard development experience with PowerBI
Understanding of management consulting functional skills including: business analysis, process improvement, solutions architecture, and project management

Extras we are looking for
Experience in Education Analytics, either in a research or professional role
Excellent written and verbal communication skills
Knowledgeable in back-testing, simulation, and statistical techniques (auto-regression, auto-correlation, and Principal Component Analysis)
Graduate degree in data & analytics field preferred","Azure Databricks, Microsoft Azure y Microsoft Fabric",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3945093540/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=K%2FYMKopTtsMlmePcfq%2F5uQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 mes,"Nueva York, Estados Unidos","Acerca del empleo
Job Title: Data Engineer

 Job Location: Denver, CO

Job Type: Fulltime and Longterm Contract

Job Mode: Remote

Required Skills

Leadership & Design Skills: team leadership, Agile/SAFe, tech mentor, cloud engineering

 Databases & Languages: AWS Stack (Aurora, S3, Redshift), PostgreSQL, SQL Server, Cloud SQL, Python
 Cloud Deployment: AWS certified, Pipeline development
 Integration Tools: AWS Glue, Terraform, Airflow, Github

About Us

 InterSources Inc, a Certified Diverse Supplier, was founded in 2007 and offers innovative solutions to help clients with Digital Transformations across various domains and industries. Our history spans over 16 years and today we are an Award-Winning Global Software Consultancy solving complex problems with technology. We recognize that our employees and our clients are our strengths as the diverse talents and opportunities they bring to the table enable us to grow as a global platform and they are causally linked with our success. We provide strategic and technical advice, and we have expertise in areas covering  Artificial Intelligence, Cloud Migration, Custom Software Development, Data Analytics Infrastructure & Cloud Solutions, Cyber Security Services, etc. We make reasonable accommodations for clients and employees and we do not discriminate based on any protected attribute including race, religion, color, national origin, gender sexual orientation, gender identity, age, or marital status. We also are a  Google Cloud partner company. We align strategy with execution and provide secure service solutions by developing and using the latest technologies that thrive our resources to deliver industry-leading capabilities to our clients and customers, making it convenient for our clients to do business with InterSources Inc. Our teams also drive growth by refining technology-driven client experiences that put the users first, providing an unparalleled experience. This results in strengthening the core technologies of clients, enabling them to scale with flexibility, create seamless digital experiences and build lifelong relationships.
Aptitudes y experiencia deseables
DATA ENGINEER, AGILE, SAFE, AWS, AWS STACK, AURORA, REDSHIFT, POSTGRESQL, CLOUD SQL, PYTHON, TERRAFORM, AIRFLOW, GITHUB, AWS GLUE","Airflow, Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift y Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3836840720/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=PHf1E9J9woXAPHYVfT6mXw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 meses,Estados Unidos,"Acerca del empleo
Job title: Data Engineer (Remote)

Job location: Juno Beach, FL

Visa: Any (candidate must be authorized to work in US)

Duration: 12 months with possible extension (s)

Must-have Skills: 2+ years as a Data engineer, with large scale data management exp, Python, Pandas, SQL. PostgreSQL, AWS, Power BI data flow pipelines, Excel ETL, energy exp;

Requirements

Experience working as Data Engineer (2+ years).
⁠Experience in Python, Pandas, SQL, and large scale data management and cleanup. Agile work experience.
Preferred: Experience in PostgreSQL, AWS, PowerBI data flow pipelines, Excel ETL, API.

Responsibilities

Work on creation, maintenance, and upgrades to a data pipeline.
Creating datasets from multiple sources, working on debugging and polishing a real time data pipeline, python notebook dev time.

Whopper Technologies, a minority women-owned enterprise, is at the forefront of digital transformation, technology excellence, and business growth solutions. Specializing in talent mobilization and innovation, we are dedicated to enhancing customer experiences across diverse sectors such as Information Technology, Telecommunications, Healthcare, Engineering, and the Public sector. With a focus on deploying top-tier talent and fostering innovation, we empower businesses to thrive and excel in a rapidly evolving digital landscape, helping them reach new heights of success.

Whopper Technologies is committed to fostering workforce diversity and is proud to be an equal opportunity employer.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Pandas (Software) y SQL, Bases de datos, Datasets, Flujo de datos y PostgreSQL",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983538237/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=FrwsgDXSWnFT2iMszI3h%2BA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"55 US$/h - 65 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Filadelfia, PA","Acerca del empleo
Description

We are offering a long-term contract employment opportunity for a Data Engineer in Philadelphia, Pennsylvania. The role primarily involves working in the veterinary services industry, with a focus on data integration for partner hospitals. The workplace environment is collaborative, with team members working together on multiple platforms, including mobile applications and internal systems.

Responsibilities

 Work on data integration projects for partner hospitals, ensuring smooth access to internal platforms.
 Utilize technologies such as Apache Kafka, Apache Spark, and Apache Airflow for optimal data processing.
 Employ cloud technologies such as Google Cloud Platform for data stream operations and project completion.
 Develop and maintain databases for efficient data management and retrieval.
 Use algorithm implementation to create effective solutions for data processing challenges.
 Work with Analytics, API Development, and AWS technologies to enhance data processing capabilities.
 Leverage skills in EO/IR systems for optimal data handling and processing.
 Be responsible for the completion of the data stream to ensure all data is processed accurately.
 Work collaboratively with internal team members to ensure knowledge transfer and project success.
 Actively engage in the monitoring and maintenance of customer accounts, taking appropriate action when needed.

Requirements

 Proficiency in Apache Kafka and Apache Spark is essential
 Demonstrated experience with Cloud Technologies such as AWS Technologies and Google Cloud Platform
 Strong understanding of Database management and Data Flow
 Familiarity with EO/IR systems
 Proven ability in Algorithm Implementation
 Previous experience in Analytics
 Proficiency in Apache Hadoop
 Experience in API Development
 Knowledge of Apache Airflow is preferable

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Airflow, Almacenamiento de datos, Apache Kafka, Apache Spark, Ciencia de datos, Desarrollo de API, Extraer, transformar y cargar (ETL), Hadoop y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3918191460/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=68AVv1X7EuSMjTWeVx7O7w%3D%3D&trk=flagship3_search_srp_jobs,Jr. Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"Charlotte, NC","Acerca del empleo
Role: Jr. Data Engineer

Duration: Long Term

Location: Charlotte, NC, Onsite 

Description: In this contingent resource assignment you may: Participate in low to moderately complex initiatives and identify opportunity for process improvements within Database Engineering. Review and analyze basic or tactical Database Engineering assignments or challenges that require research evaluation and selection of alternatives related to low-to-medium risk deliverables. Present recommendations for resolving low to moderately complex situations and exercise some independent judgment while developing understanding of function policies procedures and compliance requirements. Provide information to client personnel in Database Engineering. Required Qualifications: 2 years of Database Engineering experience or equivalent demonstrated through one or a combination of the following: work or consulting experience training military experience education.

Skills: Category: Areas of Expertise, Name: Data Warehousing, Required: Yes, Experience: 1; Category: Technical Skill, Name: ETL, Required: No, Experience: 2; Category: xms-USIT, Name: Data Analytics, Required: Yes, Experience: 1; Category: xms-USIT, Name: Data Engineer, Required: Yes, Experience: 1; Category: xms-USIT, Name: Data Integration, Required: Yes, Experience: 1; Category: xms-USIT, Name: Data Mapping, Required: Yes, Experience: 1; Category: xms-USIT, Name: Data Reporting, Required: Yes, Experience: 1; Category: xms-USIT, Name: Data Visualization, Required: Yes, Experience: 1; Category: xms-USIT, Name: Data Warehouse, Required: Yes, Experience: 1; Category: xms-USIT, Name: data pipelines, Required: No, Experience: 1

Comments: This position is critical for creating a consistent Operating model across Teradata ETL and Hadoop in the areas of roadmap creation and tracking technology refresh planning consumption analysis and future hardware and software forecasts. This will also help improve our Vulnerability Remediation and Tech Refresh cycles, hence improving our Risk Management. The additional resource is needed to expand the scope of roadmap creation and tracking beyond Teradata to ETL and Hadoop as well. As a result, the scalability and stability of our platforms will be better managed. Familiarity with Infrastructure Management Database Management and experience with platforms like Teradata Hadoop ETL Technologies Ab Initio Informatica is desired along with sound understanding of industry best practices.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Asignación de datos, Bases de datos, Elaboración de informes de datos y Research Evaluation",Solicitud sencilla
https://www.linkedin.com/jobs/view/3980255524/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=OAw5sQtHXRHPka%2F78UvHMA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,"Kentucky, Estados Unidos","Acerca del empleo
The Data Engineer will work with analytic teams across the enterprise to ensure work is accurate and complete and work closely with operations teams. Work assignments involve moderately complex to complex issues where the analysis of situations or data requires an in-depth evaluation of variable factors. Leverage advanced knowledge of data, SSIS, optimization, ETL processes, and data warehousing tools.

Responsibilities:
Designs, develops, and maintains ETL processes using SSIS to ensure data quality and integrity.
Manages and optimizes data warehousing solutions to support analytics and reporting needs.
Collaborates with data engineers and other stakeholders to ensure seamless data integration and transformation
Understands and analyzes complex data, articulates findings to various units within the company at the appropriate level, and impacts the business with mathematical concepts.
Influences the department’s strategy and makes decisions on moderately complex to complex issues regarding the technical approach for project components, with work performed without direction.
Exercises considerable latitude in determining objectives and approaches to assignments.

Required Qualifications:
5 or more years of data analytics experience
Proficiency in SSIS for ETL processes and data integration
Strong experience in data warehousing and management
Advanced experience working with big and complex data sets within large organizations
Advanced experience using SQL to analyze complex datasets, and can own complex data analysis and transformation processes with limited oversight
Able to articulate and present findings and insights to senior leadership
Bachelor's degree

Preferred Qualifications:
Proficiency in understanding related data
Experience with data visualization tools, Power BI preferred
Master's degree","Almacenamiento de datos, Analítica de datos, Extraer, transformar y cargar (ETL), Integración de datos, Microsoft SQL Server, SQL y SQL Server Integration Services (SSIS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3972613799/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=1DoXrDFuSDRA9C34yLyxTA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"55 US$/h - 63 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,"Freeport, ME","Acerca del empleo
Description
Are you a Data Engineer looking for a great new challenge?
Our Maine - based client is looking to bring on a Data Engineer to help them with ongoing cloud migration efforts.
This person will design, develop, implement, and test processes / workflows on GCP.
The successful candidate will be able to work independently and work collaboratively to help drive multiple projects of varying size to completion.
Don't miss a great opportunity to join a large, stable, and growing organization with a great work environment!

Must haves: 
4+ years Data Engineering experience
Google Pub / Sub
BigQuery
Google Dataform
Data ingestion into BigQuery
Cloud Composer
GitHub
SQL
Agile

Apply to learn more!","GitHub, Google BigQuery y SQL",Solicitud sencilla
https://www.linkedin.com/jobs/view/3954733831/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=SETF1awmyrqWuj%2FQO%2BA0%2FQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Media jornada
Coincide con tus preferencias de empleo. El tipo de empleo es Media jornada.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Des Moines, IA","Acerca del empleo
Title: Data Engineer – Remote

Location: Remote (Preferably in the Midwest)

Type: 2-month contract 40 hours per week OR part-time 5-20 hours per week until projects are complete

Pay: $52-57/hr.

Job Description

We are seeking a talented Data Engineer to join our dynamic team. You will be responsible for designing, developing, and maintaining our data pipeline architecture and data systems. You will work closely with cross-functional teams to understand and support their data needs. The ideal candidate is passionate about data and has strong analytical skills.

Responsibilities

Design, construct, install, test, and maintain highly scalable data management systems
Develop and implement ETL processes using Python, SQL, and AWS services
Integrate external APIs and data sources to enrich our data warehouse
Perform data modeling and schema design for SQL databases
Optimize and tune SQL queries for improved performance and reliability
Collaborate with analysts to understand data needs and deliver on their requirements
Ensure data quality and integrity across all data pipelines and systems
Monitor and troubleshoot data pipeline issues and implement solutions swiftly

Skills And Qualifications

Bachelor's degree in Computer Science, Engineering, or a related field; or equivalent work experience
Experience in an eCommerce environment (Amazon, Walmart, Shopify)
Proven experience as a Data Engineer or in a similar role
Strong programming skills in Python for data manipulation and ETL processes
Expertise in SQL and experience with SQL Server database management
Hands-on experience with API endpoint integration and RESTful services
Familiarity with cloud services, particularly AWS (Amazon Web Services), for data storage and computation
Solid understanding of ETL (Extract, Transform, Load) processes and best practices
Experience with data modeling, schema design, and data warehousing concepts

Required Skills

API end point integration
Python
SQL
Data modeling
ETL

Bonus/Soft Skills:

Experience working on an eCommerce platform and integrating API's
AWS (nice to have)
Snowflake (nice to have)

Welcome to ConsultNet and the family of companies, Tekne, SaltClick, TechBridge, and OmniMedia. As a premier national provider of technology talent and solutions, our expertise spans across project services, contract-to-hire, direct placement, and managed services both onshore and nearshore.

Celebrating more than 25 years of partnership with a diverse client base, we've crafted rewarding opportunities for our consultants, fostering high-performing teams that deliver impactful results.

Over the last few years thousands of consultants have found their calling with us in roles that have made a meaningful impact on their lives, enhanced their career, challenged them, and propelled them towards achieving their personal and professional goals. At the ConsultNet family of companies, we believe effective communication is crucial in aligning the right job with your unique skills and professional aspirations. To us, it's all about the personal approach we take and the values we uphold.

Our comprehensive service offerings cover a wide range of technology positions across key markets nationwide. Client more at  www.consultnet.com .

We champion equality and inclusivity, proudly supporting an Equal Opportunity Employer policy. We welcome applicants regardless of Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other status protected by law.","Almacenamiento de datos, Extraer, transformar y cargar (ETL) y SQL, Administración de bases de datos, Bases de datos, Ciencias de la computación, Manipulación de datos, Modelado de datos, Shopify y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3879642056/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=zl1nyYmgRhX4by8ivsjDRg%3D%3D&trk=flagship3_search_srp_jobs,Data engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 meses,Estados Unidos,"Acerca del empleo
location - Remote 

Skills

ETL , Data warehouse , SQL, Azure Data factory

healthcare Background

**We are an Equal Opportunity Employer**

https://primevcs.com/jobs","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984546785/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=4Hq9rj4flUozjAHPUEU4PA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,Estados Unidos,"Acerca del empleo
Role: Data Engineer

Location: Houston, TX (100% Remote)

Duration: Long term

Project description:

1 Data Engineer to support Cloudera IT Use Cases. This person will support existing 3 use cases that are currently in production for break/fix and minor enhancement. This role will also work on new use cases as they come in. Please keep in mind this environment will be moved from on-site Cloudera/Informatica to AWS in 6 to 8 months. Need a Strong Data Engineer with Cloudera, Python and Informatica that has a strong interest in moving to AWS.

Location: Client prefers candidates in the following locations: They need to live in Texas, Louisiana, Arkansas states. It will be 100% remote for now.

Thank you and Regards,","Almacenamiento de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3958935293/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=rXKJbZd6x0kQpeoddayUfg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer(Remote)- W2,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
Position: Data Engineer

Location: 250 E 500 S, Salt Lake City, UT 84111

Projected Duration: 6 Months from projected start date (with possible extension)

Visa: Only USC/GC (need W2 candidate)

Interview Mode: Webcam

 Remote position available. Local preferred.
 Preferred that contractor come in and pick up equipment if local
 Annual review to renew contract yearly.

Summary

The Utah State Board of Education IT department is working on a comprehensive modernization initiative referred to as the Utah Schools Information Management System (USIMS). We are replacing the current legacy system data collection and associated reporting applications and reports from the ground up. We are looking for a high‐energy, adaptive team player with a great work ethic to join our Information Technology (IT) team as a Data Engineer, reporting to the USIMS Program Manager/Chief Product Owner. The Data Engineer is accountable for the architecting and solutioning of our data pipeline.

The Ideal Candidate

The ideal candidate for this position has experience in Azure, data pipelines, data architecture and warehousing, and has dabbled in machine learning. They are a Data Engineer with a passion for data, and for making peoples’ lives easier. They enjoy both philosophical discussions and physical deployments and are comfortable working with multiple teams.

Principal Duties

As a Data Engineer, your responsibilities will include:

Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
Designs efficient data structures and database schemas, as well as working with distributed system architecture and non-relational databases.
Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
Works closely with a team of engineers, product managers, and analysts.
Defines company data assets (data models), jobs to populate data models.
Designs data integrations and data quality framework.
Designs and evaluates open source and vendor tools for data lineage.
Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Other duties as assigned.

Requirements

Degree – Candidate must possess a bachelor’s degree from an accredited college in Computer Science, Information Technology, Business, or equivalent experience.
Functional Experience – Candidates must have 5+ years of experience with BI reporting tools like Power BI or Tableau.
Functional Experience – Candidates must have 3+ years of experience with large sets of regulated data, designing efficient data structures, and database schemas, as well as working with distributed systems architecture and CosmosDB.
Functional Experience – Candidates must have 3+ years of experience with SQL and Python, with a focus on building reusable frameworks.
Functional Experience – Candidates must have 3+ years’ experience with integration technologies and protocols such as SOAP, REST, JSON and APIs, with experience in handling streaming data.
Functional Experience - Data Lake and/or Date Warehousing – Experience with planning and implementing a data warehouse, data lake, or other data repositories is required.
Functional Experience - Hands-on experience in Azure Data Factory, DataBricks, Azure Storage, SQL Pools, CI/CD Pipeline Design along with other Azure services like functions and Logic apps.
Interpersonal Relationships – Candidates must demonstrate the ability to work well with others of all personality types while demonstrating problem-solving and the ability to prioritize tasks.
Communication – Candidates must demonstrate the ability to communicate in verbal and written form with both technical and non-technical personnel.
Initiative – Candidates must demonstrate success as a self-starting, hardworking and inquisitive worker.
Teamwork – Candidates must demonstrate the ability to work with cross-functional teams to deliver on a common goal.
Agile/Scrum - Candidates must demonstrate experience in an agile product environment to include deep understanding and experience with agile methodologies.

Bonus Skills

Certifications: Azure Data Engineering, Azure DBA, Azure Developer or others related to the position
Domain Driven Design – Understanding and experience with Domain Driven Design
Financial or Education Domains and their data standards – Ed-Fi, CEDS, SIF, GAAP, etc.
Knowledge and Experience with Data Privacy, Data Governance, and Data Cataloguing principles.
Machine Learning – Knowledge of best practices and experience with an LLM.","Analítica de datos, Gobierno de datos, JSON, Python y Tableau, Arquitectura de sistemas, Ciencias de la computación, Relaciones interpersonales, SOAP y Schemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982279151/?eBP=BUDGET_EXHAUSTED_JOB&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=NkqdD10u%2FAwS%2FhvHlZ74zg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 3 días,"Atlanta, GA","Acerca del empleo
Job Number: R0201661

Data Engineer

Key Role:

Apply broad understanding of the principles of computer and information science to build data pipelines using Python and PySpark. Import and sync to various data sources, clean and standardize data elements, and fix data quality issues. Collect, manage, and convert raw data into usable information for clients, data scientists, and business analysts to interpret. Use code authoring tools in designated platforms to make data accessible so that organizations can use it to evaluate and optimize policy. Build processes for users and data owners to interact with their data in platform, using a variety of technologies, including Python, TypeScript, and JavaScript. Develop innovative solutions to complex data-driven problems and operate with substantial latitude for unreviewed action or decision.

Basic Qualifications:

Experience with Python
Experience with data integration and management
Knowledge of fundamental principles of programming or web development
Ability to obtain and maintain a Public Trust or Suitability/Fitness determination based on client requirements
Bachelor's degree


Additional Qualifications:

Experience with Palantir Foundry
Experience with R
Experience with JavaScript, TypeScript, HTML, and CSS
Experience with version control systems, including GIT
Experience working in a public health environment
Knowledge of PySpark
Knowledge of Agile and Scrum processes
Master's degree


Vetting:

Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client.

Create Your Career:

Grow With Us

Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.

A Place Where You Belong

Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll build your community in no time.

Support Your Well-Being

Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.

Your Candidate Journey

At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.

Compensation

At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.

Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $51,600.00 to $105,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees. This posting will close within 90 days from the Posting Date.

Work Model

Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.

If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.


EEO Commitment

We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Integración de datos y PySpark, Control de versiones, Hojas de estilos en cascada (CSS) y TypeScript",Solicitar
https://www.linkedin.com/jobs/view/3980098443/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=hbV18mjKAgkd7lqDzU1XwA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - 100% Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 6 días,"Kentucky, Estados Unidos","Acerca del empleo
Data Architect /Sr Data Engineer (100% Remote)
Primary Location: 100% Remote
V-Soft Consulting is currently hiring for a Data Architect /Sr Data Engineer (100% Remote) for our premier client.
What You’ll Need
Technical Requirements and Certifications »
OHC.
Oracle training classes, WSM, RPD, BI.
Education And Experience »
CIS, MIS, General Business, or equivalent. Flexibility based on candidate's ability to handle tasks.
Requires 2-3 years of experience with OC Cloud, On-Prem Version, OPID IC, RPD, Admin tools, and Web Semantic tools.
If there is a more junior resource with 1 year, that could work as long as their role as been working directly with the above technologies.
Must-Have Skills
RPD Admin tool and/or Web Semantic Modeler (OC, ORS, OBIDC12).
Generalized SQL or Data Modeling experience.
Nice-to-Have Skills
OBIA and Pre-Built Models (legacy conversion experience is a plus).
Business and manufacturing experience, particularly in the supply chain.
What You’ll Do
Job Responsibilities:
Helping create good data that allows for the upskilling of business analysts.
Improving the current state of data across 20 different departments.
Enhancing internal efficiency.
Focusing on Oracle and BI.
Interested?
Qualified candidates should send their resumes to mtaher@vsoftconsulting.com

V-Soft Consulting Group is recognized among the top 100 fastest growing staffing companies in North America, V-Soft Consulting Group is headquartered in Louisville, KY with strategic locations in India, Canada and the U.S. V-Soft is known as an agile, innovative technology services company holding several awards and distinctions and has a wide variety of partnerships across diverse technology stacks.
As a valued V-Soft Consultant, you’re eligible for full benefits (Medical, Dental, Vision), a 401(k) plan, competitive compensation and more. V-Soft is partnered with numerous Fortune 500 companies, exceptionally positioned to advance your career growth.
V-Soft Consulting provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
For more information or to view all our open jobs, please visit www.vsoftconsulting.com or call (844) 425-8425.","Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Circuitos integrados, Modelado de datos, RPD, Requisitos técnicos y Sistemas de gestión de información (MIS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3943518881/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=IuRlXuJWwfqW%2BYcmiaRrrQ%3D%3D&trk=flagship3_search_srp_jobs,Remote Work - Need Data Analytics Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,Estados Unidos,"Acerca del empleo
100% Remote

Must be W2 or 1099

Qualifications

Experience with cloud platform AWS and their respective data services.
AWS experience:
Data Storage Tools: Redshift, S3
Automation Tools: Batch, EventBridge, Lambda, Step Functions, Glue, etc.
MadPlotLib for Python
Sagemaker
Experience working closely with a Data Science team

Key Responsibilities

Design, develop, and maintain robust analytics solutions to support data-driven decision-making across the organization.

Collaborate with data scientists, analysts, and business stakeholders to understand analytical requirements and translate them into technical specifications.

Develop and maintain ETL (Extract, Transform, Load) processes to ingest and process large volumes of structured and unstructured data from various sources.

Implement and manage data models and architecture to ensure the integrity, security, and performance of analytics systems.

Optimize and troubleshoot analytics workflows to ensure data quality and system reliability.

Perform data wrangling and cleansing to prepare data for analysis and reporting.

Implement and manage data governance practices to ensure data accuracy, consistency, and compliance with regulatory requirements.

Develop and maintain dashboards, reports, and visualizations to communicate insights to business stakeholders.

Monitor and maintain analytics infrastructure, including databases, data warehouses, and cloud-based storage solutions.","Ciencia de datos y Ingeniería de datos, Amazon Redshift, Comunicación, Disputas de datos , Especificaciones técnicas, Form W-2, Modelo de datos, Panel de control y Servicios de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3913454471/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=qYR3prcXdtaDXwbSbr9UjQ%3D%3D&trk=flagship3_search_srp_jobs,IT Developer (IT Data Engineer (ETL),Presencial Sin experiencia,hace 2 meses,"Vienna, VA","Acerca del empleo
Basic Purpose
To design, develop, implement and maintain new IT solutions, or changes/enhancements to existing solutions that align with business initiatives and corporate strategies. This role will work with product owners and multiple internal and external teams to develop automated data pipelines from heterogeneous sources, including file and data validation, exception handling and reporting, and applying business rules in ETL to populate a repository in alignment with client's standards and guidelines. Recognized as an expert with specialized depth and breadth of expertise in their discipline. Solves complex problems, taking a broad perspective to identify solutions. Requires advanced proficiency in Python and SQL programming, as well as several years of hands-on experience creating automated data pipelines using modern technology stacks to integrate diverse data sources into data repositories with exception handling and monitoring.","Extraer, transformar y cargar (ETL), Programación y SQL, Bases de datos, Desarrollo de software, Hojas de estilos en cascada (CSS), Lenguaje unificado de modelado (UML), Reglas de empresa y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3984558747/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=MM3bADWA%2BKnDW5UhwuLrcQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Developer Needed,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,Estados Unidos,"Acerca del empleo
Must have Schwab Experience (Must have been out of Schwab for at least 6 month to be considered)

Data Engineer/Developer Needed

Informatica Powercenter
Control M
Strong batch Processing
PowerShell Scripting
Python Scripting
IICS Experience
Investment Experience (Asset Management)","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, BMC Control-M, Bases de datos, Procesamiento por lotes y Secuencia de comandos",Solicitar
https://www.linkedin.com/jobs/view/3984253406/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=EBIADmK4FoWFkf3ijjYXWA%3D%3D&trk=flagship3_search_srp_jobs,BI Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Redmond, WA","Acerca del empleo
BE/B.Tech in Computer Science or related field

5+ years of enterprise software design and development experience

Proficiency in SQL, with experience in writing and optimizing complex queries

Professional, practical programming using Spark, SQL, Scala/Python

Strong technical expertise in Azure, Big Data, Apache Nifi, Hadoop, HDInsights, ADF, ADW etc.

Experience working with reporting and visualization tools like Power BI, Tableau, or similar.

Hands-on experience with scripting languages like Powershell/Bash for automation and data manipulation tasks.

Extensive knowledge and experience in data warehousing, Streaming data processing (ETL), e-metrics/measurement, business intelligence, information retrieval, parallel and distributed computation

Experience in implementation of Cloud Computing concepts and platforms

Experience in analyzing very large real world datasets and hands-on approach in data analytics

Experience with Test Driven Development, Continuous Integration, Continuous Deployment, Telemetry etc.

Great design and problem-solving skills, with a strong bias for quality and engineering excellence

Excellent verbal and written communications skills

Excellent problem-solving and debugging skills with a solid understanding of testing practices

Proven sense of high accountability and self-drive to take on and see through big challenges

Experience working in a global delivery model

Experience with SCRUM, Devops or similar Agile development/implementation methodologies

Familiarity with automation tools Visual Studio etc

Talteam Inc. is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability.**","Analítica, Analítica de datos, Extraer, transformar y cargar (ETL) y SQL, Buenas prácticas de pruebas, Ciencias de la computación, Comunicación, Comunicación escrita, Manipulación de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3977455328/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=29c2hzJqSM84qAD5Q2oy3g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Finance,"118 US$K/año - 157,5 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Dublin, CA","Acerca del empleo
Build the future of data. Join the Snowflake team.

We’re looking for a talented Data Engineer to join our Analytics Engineering team. In this role, you will work closely with Finance, Treasury, and IT stakeholders to build best-in-class datasets supporting analytics across the company. This high-impact role will also help shape the future of Snowflake products and services.

RESPONSIBILITIES:

Design, build, and own the core data models and key infrastructure to manage data and usage across our Snowflake account with an emphasis on Finance & Treasury.
Design and implement a data quality monitoring system, including source-to-target data validations and anomaly detection.
Collaborate closely with Engineering, Product Management, IT, Finance, and Treasury to inform product decision-making with data and identify opportunities for system improvements.
Collaborate with Engineering, Finance, Treasury, and IT Applications teams to meet data governance requirements.
Build dashboards to help IT Leadership monitor the performance and availability of our systems.
Answer questions from the executive team for board reporting, publications, and industry reports.
Think creatively to find optimal solutions to complex, often unstructured and ambiguous problems.


OUR IDEAL CANDIDATE WILL HAVE: 

3-5 years of experience including:
Domain knowledge in areas such as: Finance, Treasury, and IT.
Designing and building complex data pipelines in a large-scale data environment.
Experience integrating data across multiple business applications.
Familiarity with developing data products to drive data science and machine learning initiatives.
Using big data to drive business impact.
Experience using Snowflake is a plus.
BS in a technical field (CS, Physics, Math, etc.), MS/PhD is a plus.
Proficiency in Python and SQL.
Ability to clearly present learnings to business leaders and technical stakeholders.
The ability to thrive in a dynamic environment. That means being flexible and willing to jump in and do whatever it takes to be successful.

Every Snowflake employee is expected to follow the company’s confidentiality and security standards for handling sensitive data. Snowflake employees must abide by the company’s data security plan as an essential part of their duties. It is every employee's duty to keep customer information secure and confidential.

The following represents the expected range of compensation for this role:

 The estimated base salary range for this role is $118,000 - $157,500.
Additionally, this role is eligible to participate in Snowflake’s bonus and equity plan.


The successful candidate’s starting salary will be determined based on permissible, non-discriminatory factors such as skills, experience, and geographic location. This role is also eligible for a competitive benefits package that includes: medical, dental, vision, life, and disability insurance; 401(k) retirement plan; flexible spending & health savings account; at least 12 paid holidays; paid time off; parental leave; employee assistance program; and other company benefits.

Snowflake is growing fast, and we’re scaling our team to help enable and accelerate our growth. We are looking for people who share our values, challenge ordinary thinking, and push the pace of innovation while building a future for themselves and Snowflake.

How do you want to make your impact?","Ciencia de datos, Gobierno de datos, Ingeniería de datos , Integración de datos y SQL, Aplicaciones empresariales, Core Data, Modelo de datos, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3984126083/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=6MSrwx8LSITCIPTqCiqe%2Fw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 4 días,"Minnesota, Estados Unidos","Acerca del empleo
About Mortenson

As a Top 25 builder, developer, and EPC, our expertise spans markets like sports, renewable energy, data centers, healthcare, and more. We are builders at heart, working to ensure the built environment has a lasting positive impact. Let’s Redefine Possible®

Summary

As a member of the Data Solutions team, you will contribute to developing solutions for our data engineering practice, collaborating with other groups to design, document, develop, test, debug and implement solutions that support companywide BI, advanced analytics and other strategic data initiatives. You will use your skills and knowledge of data engineering tools and practices, database analysis tools and techniques, descriptive and statistical techniques, and applications to generate strategic insights that drive business goals. You will have many responsibilities in this role, but much of your work will involve building ETL processes for BI solutions and reports, pipelines for data ingestion for analytics purposes and working in SQL. You will need to have excellent communication skills, problem-solving skills, and working knowledge of data engineering methodologies and tools, as well as analytics and data science best practices to succeed in this role.

LOCATION: Minneapolis, MN preferred location.

Responsibilities

Build data pipelines that clean, transform, and aggregate data from disparate sources.
Work closely with business subject matter experts to identify, design, and develop datasets for complex experiments or solutions.
Coordinate data resource requirements between business subject matter experts and data engineering experts within data solutions team to solve analytics problems.
Work with business stakeholders, data engineers, data scientists and other team members to translate prototypes into production.
Assist in the development of data management policies and procedures as assigned.
Analyze large, noisy datasets and identify meaningful patterns that provide actionable results.
Create informative visualizations that intuitively display large amounts of data and/or complex relationships.
Develop, implement, and maintain change control and testing processes for modifications to algorithms and data analytics.
All other duties as assigned.

QULIFICATIONS AND EDUCATION

Fundamental knowledge of the software development lifecycle including common documentation and testing methods.
Comfort manipulating and analyzing complex, high-volume, high-dimensionality data from varying sources.
Prior experience with a cloud-based integration platform like Informatica strongly preferred.
Expert knowledge of Python and associated libraries for data engineering and data science.
Familiarity with relational SQL, and NoSQL databases.
Experience with large datasets a must.
Prior experience with data lakes and related platforms such as Data Bricks or Microsoft Fabric is a plus.
Experience in programming using programming languages such as JAVA, PLSQL.
Good understanding of the organization’s goals and objectives.
Knowledge of applicable data privacy practices and laws.
Understanding of project management methodologies
Understanding of scripting languages preferred.
A passion for analyzing and answering hard questions with data.
A flexible analytic approach that allows for results at varying levels of precision.
Ability to communicate complex quantitative analysis in a clear, precise, and actionable manner.
Open to change based on diverse input and feedback.
Effectively work with those of diverse backgrounds and organizational levels.
Adaptive, effective communication and active listening skills. 
Strong cross-cultural relationship building and influencing at all organizational levels.
Minimum of 5 years technical education or equivalent experience developing software or data solutions
Hands-on experience in a technical environment either in an analyst or development role.
Graduate or post graduate university degree in the field of computer science, mathematics, or statistics preferred.

Benefits

Mortenson provides a comprehensive benefits program to team members and their families to support their health, build long-term financial security, and provide the opportunity for work and life balance.

Benefits offered to non-craft; non-union, full-time team members include:

Medical and prescription drug plans – the choice between two affordable HSA-eligible medical plans that include vision coverage.
Dental plan 
401k retirement plan with generous matching and profit-sharing contributions 
Paid time off, holidays, and other paid leaves 
Life, AD&D, and disability insurance 
Employee assistance program online mental health tool and concierge 
Tuition reimbursement 
Adoption Assistance 
Gym Membership Discount Program 
Identity Theft Protection 

Mortenson is an EOE/Affirmative Action/M/F/Veteran/Disabled employer.

Visa sponsorship is not available for this position.

Mortenson reserves the right to hire any individual without legal or financial obligation on unwanted solicitations. 

No agency emails, calls, or solicitations are accepted without a valid agreement.","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y PL/SQL, Bases de datos, Ciencias de la computación, Comunicación, Datasets y Expertos en la materia",Solicitar
https://www.linkedin.com/jobs/view/3977449986/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=i%2BKWNi2qXgocvC2nustICA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Finance,"118 US$K/año - 157,5 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Bellevue, WA","Acerca del empleo
Build the future of data. Join the Snowflake team.

We’re looking for a talented Data Engineer to join our Analytics Engineering team. In this role, you will work closely with Finance, Treasury, and IT stakeholders to build best-in-class datasets supporting analytics across the company. This high-impact role will also help shape the future of Snowflake products and services.

RESPONSIBILITIES:

Design, build, and own the core data models and key infrastructure to manage data and usage across our Snowflake account with an emphasis on Finance & Treasury.
Design and implement a data quality monitoring system, including source-to-target data validations and anomaly detection.
Collaborate closely with Engineering, Product Management, IT, Finance, and Treasury to inform product decision-making with data and identify opportunities for system improvements.
Collaborate with Engineering, Finance, Treasury, and IT Applications teams to meet data governance requirements.
Build dashboards to help IT Leadership monitor the performance and availability of our systems.
Answer questions from the executive team for board reporting, publications, and industry reports.
Think creatively to find optimal solutions to complex, often unstructured and ambiguous problems.


OUR IDEAL CANDIDATE WILL HAVE: 

3-5 years of experience including:
Domain knowledge in areas such as: Finance, Treasury, and IT.
Designing and building complex data pipelines in a large-scale data environment.
Experience integrating data across multiple business applications.
Familiarity with developing data products to drive data science and machine learning initiatives.
Using big data to drive business impact.
Experience using Snowflake is a plus.
BS in a technical field (CS, Physics, Math, etc.), MS/PhD is a plus.
Proficiency in Python and SQL.
Ability to clearly present learnings to business leaders and technical stakeholders.
The ability to thrive in a dynamic environment. That means being flexible and willing to jump in and do whatever it takes to be successful.

Every Snowflake employee is expected to follow the company’s confidentiality and security standards for handling sensitive data. Snowflake employees must abide by the company’s data security plan as an essential part of their duties. It is every employee's duty to keep customer information secure and confidential.

The following represents the expected range of compensation for this role:

 The estimated base salary range for this role is $118,000 - $157,500.
Additionally, this role is eligible to participate in Snowflake’s bonus and equity plan.


The successful candidate’s starting salary will be determined based on permissible, non-discriminatory factors such as skills, experience, and geographic location. This role is also eligible for a competitive benefits package that includes: medical, dental, vision, life, and disability insurance; 401(k) retirement plan; flexible spending & health savings account; at least 12 paid holidays; paid time off; parental leave; employee assistance program; and other company benefits.

Snowflake is growing fast, and we’re scaling our team to help enable and accelerate our growth. We are looking for people who share our values, challenge ordinary thinking, and push the pace of innovation while building a future for themselves and Snowflake.

How do you want to make your impact?","Ciencia de datos, Gobierno de datos, Ingeniería de datos , Integración de datos y SQL, Aplicaciones empresariales, Core Data, Modelo de datos, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3965814701/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=CVhCBLVvU8z0JvbryEk%2Fuw%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 semanas,"Dallas, TX","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries. 

please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

https://synergisticit.wistia.com/medias/n8487768di 

https://synergisticit.wistia.com/medias/o5gmv7i9eu 

https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3833864739/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=ZuvJ%2FEbpZEzBf89XaNqQQg%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist/Analyst/Engineer - Entry Level,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Dallas, TX","Acerca del empleo
2024 is finally here and we hope the Job market improves however as per a resume builder survey based on response from more than 900 companies 4 out of 10 companies are planning to have layoffs in 2024 or have a hiring freeze.  Almost 390,000 tech employees have been laid off since 2022 and it's still ongoing. The effect of this has led hundreds of thousands of laid off Tech employees competing with existing Jobseekers.

 AI is replacing many normal jobs which were done by people. As per news reports Google is planning to Client off 30,000 employees in its ad sales who will be replaed by AI ad technology.

 Entry level Job seekers struggle to get responses to their applications, are getting ghosted after interviews.  In such a scenario the Job seekers need  to differentiate themselves by ensuring to obtain exceptional skills and technologies so that they can wear multiple roles at a client as clients now would want to expand roles and responsibilities assigned to a particular job to save costs.

Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs as Clients now post covid can also hire remote workers which increases even more competition for jobseekers.

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We also offer Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If a Jobseeker is qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection are totally based on clients discretion not ours.

 If you applied for a job and got emails from our skill enhancement team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

 please check the below links to see success outcomes of our candidates and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrn

 https://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Q

 https://www.youtube.com/watch?v=NVBU9RYZ6UI

 https://www.youtube.com/watch?v=EmO7NrWHkLM

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/

 We are looking for the right matching candidates for our clients

 Please apply via the job posting

REQUIRED SKILLS For Java /Full Stack/Software Programmer

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Project work on the skills
 Knowledge of Core Java , javascript , C+
or software programming
 Spring boot, Microservices, Docker, Jenkins and REST API's experience
 Excellent written and verbal communication skills

 For data Science/Machine learning Positions

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Project work on the technologies needed
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
 Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow

 If you get emails from our skill enhancement team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements. 

 No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación, Reconocimiento de patrones y Visualización de datos, Ciencias de la computación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3956578783/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=rx9Rdp1LLiEUBTVL59EFCw%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Entry Level,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Henderson, NV","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3983974194/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=XKBIC2d5Yji74OBVXHFNbw%3D%3D&trackingId=vaGlTguyTb6CQ%2B7JPA3b1A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer- Hybrid / Remote,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Arlington, VA","Acerca del empleo
Overview:

BigBear.ai is seeking a Data Engineer to support a program in Crystal City, VA (Hybrid 2-3 days in office). As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our Advana data infrastructure and systems. Your expertise in ETL, Databricks, Python, Spark, Scala, JavaScript/JSON, SQL, and Jupyter Notebooks will be essential in ensuring efficient data processing and analysis.

This is an ideal opportunity to be part of one of the fastest growing AI/ML companies in the industry. At BigBear.ai, we're in this business together. We own it, we make it thrive, and we enjoy the challenges of our work. We know that our employees play the largest role in our continual success. That is why we foster an environment of growth and development, with an emphasis on opportunity, recognition, and work-life balance. We give the same high level of commitment to our employees that we give to our clients. If BigBear.ai sounds like the place where you want to be, we'd enjoy speaking with you.

This position requires an active Secret Clearance.

What you will do:

Design, develop, and implement end-to-end data pipelines, utilizing ETL processes and technologies such as Databricks, Python, Spark, Scala, JavaScript/JSON, SQL, and Jupyter Notebooks.

Create and optimize data pipelines from scratch, ensuring scalability, reliability, and high-performance processing.

Perform data cleansing, data integration, and data quality assurance activities to maintain the accuracy and integrity of large datasets.

Leverage big data technologies to efficiently process and analyze large datasets, particularly those encountered in a federal agency.

Troubleshoot data-related problems and provide innovative solutions to address complex data challenges.

Implement and enforce data governance policies and procedures, ensuring compliance with regulatory requirements and industry best practices.

Work closely with cross-functional teams to understand data requirements and design optimal data models and architectures.

Collaborate with data scientists, analysts, and stakeholders to provide timely and accurate data insights and support decision-making processes.

Maintain documentation for software applications, workflows, and processes.

Stay updated with emerging trends and advancements in data engineering and recommend suitable tools and technologies for continuous improvement.

What you need to have:

Secret clearance is required

Minimum of 5+ years of experience as a Data Engineer, with demonstrated experience creating data pipelines from scratch.

High level of proficiency in ETL processes and demonstrated, hands-on experience with technologies such as Databricks, Python, Spark, Scala, JavaScript/JSON, SQL, and Jupyter Notebooks.

Strong problem-solving skills and ability to solve complex data-related issues.

Demonstrated experience working with large datasets and leveraging big data technologies to process and analyze data efficiently.

Understanding of data modeling/visualization, database design principles, and data governance practices.

Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.

Detail-oriented mindset with a commitment to delivering high-quality results.

Must be in the DC Metro area and available to work onsite in Crystla City, VA hybrid/remote.

DOD or IC-related experience.

What we'd like you to have:

Knowledge of Qlik/Qlik Sense, QVD/QlikView, and Qlik Production Application Standards (QPAS) is a significant plus.

Recent DoD or IC-related experience.

Previous experience with Advana is a plus

About BigBear.ai:

BigBear.ai delivers AI-powered analytics and cyber engineering solutions to support mission-critical operations and decision-making in complex, real-world environments. BigBear.ai’s customers, which include the US Intelligence Community, Department of Defense, the US Federal Government, as well as customers in manufacturing, healthcare, commercial space, and other sectors, rely on BigBear.ai’s solutions to see and shape their world through reliable, predictive insights and goal-oriented advice. Headquartered in Columbia, Maryland, BigBear.ai is a global, public company traded on the NYSE under the symbol BBAI. For more information, please visit: and follow BigBear.ai on Twitter: .","Apache Spark, Extraer, transformar y cargar (ETL), JSON, Jupyter y Scala, Aseguramiento de la calidad de los datos, Limpieza de datos, Modelo de datos, Ordenador portátil y SC Clearance",Solicitar
https://www.linkedin.com/jobs/view/3978251541/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=5uCvRiJh%2Fjt2D8%2FUNZVcxQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Washington, DC","Acerca del empleo
DEPARTMENTDIRECT REPORT(S)ITn/aTRAVEL REQUIREDLOCATIONn/aHybrid – DC AreaTERMSALARYFull-Time, Permanent$85,000 – 100,000

About Shatterproof

Shatterproof was founded in 2013 to fill the gap for a well-funded, national, coordinated effort to reverse the course of the addiction health crisis, with the singular goal of ending the devastation addiction is causing our families. We know there are science-based solutions that can prevent and treat this disease, and we are committed to what research has proven to be effective. We have identified three priority pillars: transforming addiction treatment, ending addiction stigma, and educating and empowering communities.

Transforming Addiction Treatment: Shatterproof is transforming the U.S. healthcare system for the more than 49 million Americans living with a substance use disorder (SUD). Like anyone suffering from a disease, those with this disease deserve access to treatment that is based on science, complemented by a supportive recovery community, both free of shame and stigma. Removing barriers to lifesaving treatment is a core mission focus of Shatterproof.

Ending Addiction Stigma: Shatterproof’s National Stigma Initiative replicates the success of other transformational social movements—such as HIV/AIDS activism, cancer awareness, and marriage equality—in catalyzing the change needed to create a more tolerant, more compassionate, and healthier future. We are committed to creating solutions that are adept at changing attitudes and behaviors and ultimately closing SUD treatment gaps and health inequities for historically disadvantaged populations who experience compounded discrimination and bias.

Supporting and Empowering Communities: Shatterproof is committed to educating and empowering our families and communities nationwide by providing supportive and evidence-based resources related to prevention, treatment, and recovery from addiction.

You can learn more in our 2022/2023 Return on Investment Report here.

About The Platform

ATLAS® is the first and only web-based platform that allows the public searching for high-quality addiction treatment to locate and compare facilities, including trustworthy, standardized quality data on the services available, and feedback on the services reported by other patients. ATLAS helps the public navigate to better quality care and supports providers in quality improvement.

The core mission of Shatterproof is to reverse the addiction crisis in the United States and to serve patients and families. ATLAS fulfills Shatterproof’s goal of leveraging quality measures to increase transparency in and encourage improvements to addiction treatment. It is based upon Shatterproof’s National Principles of Care©.

ATLAS collects facility-level data from multiple, validated sources. Data from these sources is displayed in publicly facing dashboards that allow for easy comparison between facilities. Moreover, ATLAS promotes quality improvement by offering password-protected portals for facilities, payers, and states to view and use the data to drive quality improvement and innovation.

ATLAS is currently live in fourteen (14) states (DE, LA, MA, NC, NY, WV, FL, NJ, OK, PA, CA, WI, IN, CT). These 14 states represent close to 50% of the population in the United States, and ATLAS is in conversations with additional states as well. Shatterproof works closely with many addiction treatment stakeholders, including states health departments, provider and medical organizations, payers, and recovery advocates, to ensure successful and collaborative implementation.

ATLAS is an initiative overseen by a cross-functional group, composed of operational, research, and stakeholder engagement staff, who work remotely. Preference will be given to candidates in a major metropolitan area with access to a major airport.

THE ROLE

Shatterproof seeks a self-motivated and detail-oriented Data Engineer. We are seeking an individual passionate about contributing to the operation, support, and enhancement of our mission-driven data operations platform. This key role will support, monitor, and develop core database tables, processes, pipelines and interfaces responsible for the storage, collection, processing, and analysis of data gathered across the Shatterproof organization. This encompasses the development, management, testing and operation of our facility and patient data hubs, including data intake, data quality assessment/testing, data curation and enrichment processes on cloud-based data architecture and modern system integration with various software and web applications.

This position will work collaboratively with Shatterproof Information Technology team as well as external data vendors and will be supervised by the Vice President, IT.

This person will be an individual contributor, with no direct reports.

Duties And Responsibilities

Develop, implement, and support scalable data integration (ETL/ELT) processes, covering storage, ingestion, cleansing, curation, unification, presentation, using new or current technologies (Azure SQL Server, Data Factory, Salesforce, Qualtrics, Drupal, Python, Power BI, Power Apps)
Develop and enhance data architecture to directly improve operational efficiency for the business, continuing to build out an enterprise-level data ecosystem
Manage and ensure the success of repeatable reliable daily data pipeline routines using continuous delivery (CD) and Software Development Life Cycle (SDLC) best practices
Develop and update dynamic documentation for data pipelines, environments, releases, operational processes, and system support for applications and integration layers
Other tasks assigned by the Vice President, IT

Education And Experience

Bachelor’s degree in Computer Science, Engineering, Information Systems, Data Analytics or applicable experience
Expertise with SQL, database design, data manipulation methodologies, and data integration via batch file exchange and/or REST API
Experience with ETL/ELT and the development of automated validation, data pipelines, Python/Power Shell scripting
Experience and comfort using Microsoft Azure/Data Factory
Excellent verbal and written communication/
Self-motivated, strong critical thinking skills, with the ability to quickly problem solve and adopt new strategies/technologies as needed
Passionate about learning and innovating
Ability and authorization to work in the US required
Ability to work Central/Eastern Time Zone hours

Requirements

Must show proof of authorization to work in the United States
Must complete a background check once hired
Shatterproof participates in E-Verify

Shatterproof values diverse perspectives and is committed to building an inclusive workplace. We are proud to be an equal opportunity workplace and do not discriminate on the basis of sex, race, color, age, sexual orientation, gender identity, pregnancy, religion, national origin, citizenship, marital status, veteran status, disability status, or prior arrest history. We strongly encourage people from underrepresented groups to apply.

Powered by JazzHR

SqKzoQKWnN","Analítica de datos, Arquitectura de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Comunicación, Comunicación escrita, Manipulación de datos y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3967788821/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=Hs3PQGMD32MRUx2GAE5Bew%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"120 US$K/año - 145 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
dv01 is lifting the curtain on the largest financial market in the world: structured finance. The $16+ trillion market is the backbone of everyday activities that empower financial freedom, from consolidating credit card debt and refinancing student loans, to buying a home and starting a small business. dv01’s data analytics platform brings unparalleled transparency into investment performance and risk for lenders and Wall Street investors in structured products. As a data-first company, we wrangle critical loan data and build modern analytical tools that enable strategic decision-making for responsible lending. In a nutshell, we're helping prevent a repeat of the 2008 global financial crisis by offering the data and tools required to make smarter data-driven decisions resulting in a safer world for all of us.

More than 400 of the largest financial institutions use dv01 for our coverage of over 75 million loans spanning mortgages, personal loans, auto, buy-now-pay-later programs, small business, and student loans. dv01 continues to expand coverage of new markets, adding loans monthly, and developing new technologies for the structured products universe.

To get a better idea of what a year at dv01 looks like, check out our 2022 Year in Review page here: https://dv01.co/year-in-review/2022/ If that looks like fun to you, get in touch because we'd love to hear from you.

YOU WILL:
Be at the heart of dv01. You will operate as the bridge between the engineering and finance teams, contributing to a variety of integral processes that drive dv01 on a daily basis. Every new dataset that gets integrated within dv01 will have your fingerprints all over it.
Be an owner of dv01's most valuable asset. You'll own the business logic in our data pipeline, encapsulating all the knowledge we've accumulated across hundreds of datasets. The output from the pipeline powers all of dv01's customer offerings and is critical to the success of our business.
Be customer-facing. You will have direct exposure to high-level contacts at hedge funds, banks, and asset originators, providing valuable insights to help them answer complex questions.
Work with state-of-the-art technology. You'll work with popular, modern, and exciting open source technologies like Apache Spark, Airflow and DBT. Everything we do is cloud-native on Google Cloud Platform, utilizing tools including BigQuery and Dataproc. The skills you develop here will serve you well beyond dv01.

YOU ARE:
A well-rounded engineer. You have 3+ years of professional experience writing production-ready code in a language such as Python, Scala, Java or R. You are able to write well thought-out code while accounting for resource and performance constraints.
Passionate about working with data. You should have 2+ years of professional experience working directly with data pipelines with exposure to datasets related to loans an added plus. Your typical day-to-day involves tasks such as wrangling messy raw data into a usable state, expressing complex business logic as code, and configuring new and debugging existing pipelines.
Excited about big data tools. You work frequently with processing frameworks and databases designed to handle large datasets and have spent time optimizing how you store and compute data within them.
Knowledgeable about relational data concepts. You are able to understand and explain the relationship between various tables and how and why the needs of stakeholders have been captured in a particular data model. You are very comfortable performing ad-hoc data exploration using SQL.
Interested and experienced in both engineering and finance. You're looking to grow your skills in both disciplines and are excited about the synergies between finance and technology. You’re capable of understanding how investors evaluate loan portfolios and the complexities of amortization, prepay, and default.
A first-rate collaborator and communicator. You’re comfortable working alongside analysts and subject matter experts and translating their requirements into code. You thrive on interacting with clients to best understand and satisfy their needs.

In good faith our salary range for this role is $120,000 - $145,000 but are not tied to it. Final offer amount will be at the company’s sole discretion and determined by multiple factors, including years and depth of experience, expertise, and other business considerations.
Our community is fueled by diverse people who welcome differing points of view and the opportunity to learn from each other. Our team is passionate about building a product people love and a culture where everyone can innovate and thrive.

BENEFITS & PERKS:
Unlimited PTO. Unplug and rejuvenate, however you want—whether that’s vacationing on the beach or at home on a mental-health day.
$1,000 Learning & Development Fund. No matter where you are in your career, always invest in your future. We encourage you to attend conferences, take classes, and lead workshops. We also host hackathons, brunch & learns, and other employee-led learning opportunities.
Remote-First Environment. People thrive in a flexible and supportive environment that best invigorates them. You can work from your home, cafe, or hotel. You decide.
Health Care and Financial Planning. We offer a comprehensive medical, dental, and vision insurance package for you and your family. We also offer a 401(k) for you to contribute.
Free Equinox Membership or $1,650 Annual Fitness Fund. Regular exercise offers a plethora of mental and physical health benefits. You can either enroll in an all-access Equinox membership or at your preferred gym. Or take advantage of our fitness fund, which can be used toward at-home workout equipment (yes, including a Peloton).
New Family Bonding. Primary caregivers can take 12 weeks off 100% paid leave, while secondary caregivers can take 3 weeks. Returning to work after bringing home a new child isn’t easy, which is why we’re flexible and empathetic to the needs of new parents.

dv01 is an equal opportunity employer and all qualified applicants and employees will receive consideration for employment opportunities without regard to race, color, religion, creed, sex, sexual orientation, gender identity or expression, age, national origin or ancestry, citizenship, veteran status, membership in the uniformed services, disability, genetic information or any other basis protected by applicable law.","Análisis de datos, Canalizaciones de datos, Google BigQuery, Google Cloud , Python, SQL y Scala, Data Build Tool (DBT), Datasets y Lógica de negocios",Solicitar
https://www.linkedin.com/jobs/view/3974782495/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=DrJEhSRmH4LG9khzj8%2F19Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"75 US$/h - 85 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Position: Data Engineer
Location: Preferably local to Atlanta, GA or St. Louis, MO, but open to fully remote candidates (EST or CST time zone)
Contract Duration: Through the end of the year
Important Details:
We are not able to work with 3rd party vendors OR provide sponsorship opportunities.
Candidates must be able to work on a W2 basis, no C2C arrangements or visa transfers.

Key Responsibilities:
Participate in a critical migration project, importing data from SQL Server into BigQuery and consolidating it into a single data warehouse.
Ensure seamless data migration, consolidation, and reconciliation with a focus on accuracy and reliability.
Collaborate with cross-functional teams to maintain and enhance data quality throughout the project lifecycle.
Interpret the complexities of SQL and transfer them into Google BigQuery SELECTs.
Understand Tableau to ensure results of the SELECTs are available via API calls in Tableau.

Must Haves:
Advanced MS SQL Server experience.
Advanced Google BigQuery skills.
Experience converting MS SQL Server PROCs to Google BigQuery SELECTs.
Tableau experience

Nice to Haves:
Python experience.
Experience with Dataflow and/or Composer

Join our client in this exciting opportunity to contribute to a high-impact data migration project. If you meet the above qualifications and are eager to apply your expertise in SQL and BigQuery in a collaborative environment, we encourage you to apply!","Google BigQuery, Microsoft SQL Server y Tableau",Solicitud sencilla
https://www.linkedin.com/jobs/view/3956118990/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=vl3mVT01KVY3GQXxoHtD3Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,"San Francisco, CA","Acerca del empleo
Overview

Working at Atlassian

Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.Atlassian is looking for a Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.

You'll have flexibility in where you work – whether in an office, from home (remote), or a combination of the two.

Responsibilities

A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.

You'll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time.

Qualifications

On your first day, we'll expect you to have:

BS in Computer Science or equivalent experience with 3+ years as a Data Engineer or a similar role
Programming skills in Python & Java (good to have)
Design data models for storage and retrieval to meet product and requirements
Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)
Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring


We'd Be Super Excited If You Have

Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System


Compensation

At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:

Zone A: $140,100 - $186,800

Zone B: $126,100 - $168,100

Zone C: $116,300 - $155,000

This role may also be eligible for benefits, bonuses, commissions, and equity.

Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.

Our Perks & Benefits

Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more.

About Atlassian

At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

To learn more about our culture and hiring process, visit go.atlassian.com/crh .","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Bases de datos, Calidad de datos, Ciencias de la computación, Lenguaje de consulta (query) y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3978077068/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=U%2FyeGiYg6bCCfUMhnU8dXQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Washington, DC","Acerca del empleo
Description

Data Engineer, Data Services

Company Overview

Over the next ten years, there will be at least 4.6 million hospitalizations from the misuse of prescription drugs in people 65 or older, resulting in $528 billion in annual avoidable costs. RxAnte is on a mission to improve people’s health by helping them get more from medicines. A rapidly growing, tech-enabled healthcare services company with over 30 million lives under management, RxAnte has become a leading provider of value-based pharmacy care management solutions for health plans.

RxAnte launched Mosaic Pharmacy Service in 2019, a wholly owned subsidiary designed to offer pharmacy and chronic care management services for our clients’ most medically complex and vulnerable members. Using data, advanced analytics, specialized software and pharmacy automation, Mosaic is transforming the pharmacy experience for medically complex seniors while also helping payers achieve their quality improvement and cost savings objectives.

Job Profile

The Data Engineer of Data Services reports directly to the VP, Data Services and is responsible for taking part in the managing, designing, and building of systems required to deliver Mosaic and RxAnte's analytic products in a scalable manner using cloud data warehouse/lake technology. Strong analytic, communication, and AWS cloud experience is required. The Data Engineer will have data architectural and system engineering skills. In addition to taking part in the design and development of the systems, the Data Engineer will contribute to overall future cloud data warehouse/lake vision. The Data Engineer will be responsible for helping to assess and gather project requirements and assess work effort. Additionally, the role will interact with both technical and non-technical internal stakeholders. We are seeking someone who loves to set the vision in a new environment as a trailblazer, educate internal team members, and then work within the environment to apply best practices. This is a remote work position.

Specific Responsibilities Include

Work with Data Services leadership to architect, develop, and maintain processes and programs
Establish and maintain project-deliverable processes with an eye toward full scalability and automation
Support and establish cloud data environment design and development
Engineer within the AWS cloud data environment using Glue, EMR Serverless, Databricks, Snowflake, or similar technologies
Collaborate with internal IT team to establish best practices
Collaborate with product, analytical, business intelligence, and data teams to establish best practices utilizing the cloud data environment
Gather business requirements and work on system design frameworks documentation using Atlassian tools
A strong desire to be able to lead projects, set vision of data governance, establish CI/CD pipelines, and contribute to ongoing development
Ability to successful manage individual projects through the entire project lifecycle
Other activities as needed

Requirements

5+ years of relevant/related experience in similar role
Experience with SQL and/or NoSQL databases including coding and system design
Experience with clouds (ideally AWS)
Experience with Java, Spark, and/or Python
Experience with ETL/ELT tool set
Experience with CI/CD pipeline
Experience designing, constructing, and using data databases/lakes for product delivery
Experience working with administrative health care data (e.g., commercial medical, hospital, and pharmacy claims, and Medicare or Medicaid data), healthcare informatics, or health care claims processing a plus
Experience with SAS programming a plus
Strong communication, analytical, and data quality skills
Ability to document and work through requirements gathering
Experience working in an environment which utilizes project management tools such as Atlassian
Willingness to travel as needed

We strongly encourage candidates from all backgrounds and every walk of life to apply. We are committed to creating an inclusive and diverse workforce. Every person on our team brings their own unique perspective, and it’s what makes our products better and our work more rewarding.","Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Calidad de datos, Comunicación, Diseño de sistemas, Necesidades empresariales y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3975498338/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=7ZuyxBE1jAdyGIfV1eGT5w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
What We Do

Care Access is a unique, multi-specialty network of research sites which operates as one connected team of physician investigators, nurse coordinators, and operations managers. Our goal is to engage every healthcare professional in clinical research and to make clinical trials a care option for every patient. By removing this bottleneck, Care Access is helping accelerate the approval and delivery of critical and life-saving therapies.

Who We Are

We care. Our people are the engines behind our mission: to revolutionize access to clinical trials for the benefit of patients everywhere. We care for one another, find new ideas to accelerate medicine, and seed a long-term impact for generations.

Position Overview

Our technology team is searching for a data engineer to continue building and maintaining our data architecture. Your central responsibility as the data engineer will be to maintain and optimize our organizational data platform. Your duties may include understanding and framing data requirements, building platform and interfaces with various applications, supporting migration, troubleshooting data issues, and supporting governance and maintenance efforts. To succeed in this role, you should know how to examine new data system requirements, implement pipelines, and attend end-to-end enterprise data needs. The ideal candidate will also have proven experience constructing data architectures leveraging data lakes/marts, warehouses, databases.

What You'll Be Working On

Duties include but not limited to:

Assist in the design, configuration, and implementation of effective data platform solutions and pipelines to store and retrieve company data across diverse applications, particularly with Azure stack, integrating with AWS (Amazon Web Services) and Google BigQuery.
Support database implementation procedures to ensure they comply with internal and external regulations (21 CFR Part 11).
Help install and organize information systems to guarantee company functionality, producing detailed topology. Assist in the migration of data from legacy systems to innovative solutions.
Monitor system performance by performing regular tests, troubleshooting, and integrating new features.
Address requirements from stakeholders to ensure data systems meet organizational needs.
Provide support and training to staff members. Respond to system problems in a timely manner.

Physical And Travel Requirements

This is a remote position with less than 10% travel requirements. Occasional planned travel may be required as part of the role.

What You Bring

Knowledge, Skills, and Abilities:

Training and Education: Undergraduate education in Information Systems, Computer Science, or Engineering; OR relevant certifications for database administration or big data systems.
Experience: 2-4 years of experience in building, developing, deploying, and monitoring data systems to perform ETL/ELT processes.
Technical Proficiency:

○ Strong knowledge of Azure data products and standard languages: Data Factory, Data Lake, Azure SQL Databases, Power BI, SQL, Python.

○ Working experience with Databricks and Spark.

○ Comfortable transforming data in standard formats: JSON, Parquet, HL7.

○ Working knowledge of API connections.

Soft Skills: Excellent communication and technical writing skills.

Certifications/Licenses, Education, And Experience

2-4 years of experience working in the role of a data engineer or similar roles.
Experience using software development lifecycle, cloud infrastructure, database query, ETL, and continuous deployment.

Benefits (US Full-Time Employees Only)

PTO/vacation days, sick days, holidays.
100% paid medical, dental, and vision Insurance. 75% for dependents.
HSA plan
Short-term disability, long-term disability, and life Insurance.
Culture of growth and equality
401k retirement plan

Diversity & Inclusion

We serve patients and researchers from diverse cultures and communities around the world. We are stronger and better when we build a team representing the people we aim to support. We maintain an inclusive culture where people from a broad range of backgrounds feel valued and respected as they contribute to our mission. We value diversity and believe that unique contributions drive our success.

At Care Access, every day, we are advancing medical breakthroughs. We’re uniting standard patient care with cutting-edge treatments and research. Our work brings life-changing therapies to those in need and paves the way for newer and greater treatments to reach the world. We’re proud to advance these breakthroughs and work with the big players while engaging with the

physicians and caring for patients.

We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status, or any other characteristic protected by law.

Care Access is unable to sponsor work visas at this time.

Employment Statement

Care Access complies with all employment laws and regulations with respect to its employment practices, terms and conditions of employment, and pay equity and wages. Care Access does not engage in any unfair or forced labor practice and does not tolerate, under any circumstances, the use of any form of forced or involuntary labor, child labor, or human trafficking. This extends to suppliers, partners, or other third parties with whom Care Access does business. Care Access values and promotes the protection of human rights everywhere.","Apache Spark, Extraer, transformar y cargar (ETL), Google BigQuery y Ingeniería de datos, Bases de datos, Ciencias de la computación, Comunicación, Lenguaje de consulta (query), Sistemas de datos y Technical Proficiency",Solicitar
https://www.linkedin.com/jobs/view/3952964269/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=ZgKaeLsjeJarQ91XIza%2FHw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Nueva York, NY","Acerca del empleo
About GiveDirectly

GiveDirectly (GD) provides cash grants directly to people living in extreme poverty. Since launching in 2011, GD has raised over $1B, delivered cash to more than 1.5 million recipients, launched operations in 15 countries, and continues to expand its reach across the Global South. GD has also grown the research base supporting unconditional cash with 20 randomized control trials from its programs, generating rigorous evidence across countries and contexts. As a result, GD has been celebrated as one of the most innovative non-profit companies by Fast Company, while the growing cash transfer movement (and GD’s leading role within it) has been featured in the New York Times Magazine, This American Life, Foreign Affairs, and The Economist.

Across our global offices, our culture is candid, analytical, non-hierarchical, and agile. We work alongside 750+ individuals who come from 21 different countries and speak 69 different languages. Team members at GiveDirectly attest that diversity, equity, and inclusion are not just buzzwords, but a fundamental part of our culture and values. We actively seek to recruit individuals from the communities we serve, and use DEI as a lens in our hiring practices, programs, and initiatives. Our goal is to maintain a workplace where everyone can bring their authentic selves to work, and feel valued and respected for who they are. We strive to be inclusive of all cultures and experiences while upholding our values globally. In the spirit of our ""Know Yourself and Grow"" value, we recognize there is always room to improve our team's working experience. But day to day, we aim to ""Create Positive Energy"" - we take care of one another, have fun, aim to maximize flexibility and accessibility in roles, and pursue professional development opportunities to stay challenged & engaged in our work.

We are proud to be an equal opportunity employer, and we do not discriminate on the basis of race, color, religion, gender, sexual orientation, national origin, age, disability, veteran status, or any other status protected by law.

Location: This role is fully remote but must overlap with an East Africa timezone by at least 4 hours. We are unable to sponsor or take over sponsorship of employment Visas in the U.S. or U.K. at this time.

About This Role

The mission of the Central Data team is to empower GiveDirectly with a rigorous, data-driven culture to maximize our efficiency, effectiveness, and scale of delivering dollars to recipients. Access to accurate, timely, actionable financial data is key to this mission but currently a pain point for the organization. As the seventh member of Central Data, you will collaborate with the Finance team to build, maintain, and improve data pipelines, tools, and dashboards to address this need. Additionally, you will work closely with the Senior Data Architect to maintain and enhance our current infrastructure (AWS, Databricks, Tableau) to ensure data accessibility for data scientists and analysts.

Reports to: Graham Tyler (Director of Data)

Level: Manager

Travel Requirement: Must be able to travel ~1-2 times per year to one of our countries of operation for team retreats or field visits

What You’ll Do

Success in this role is determined by meeting these key objectives: 

[30%] Build data pipelines and tools that reduce the time and risk associated with monthly financial reporting and other finance processes. 
Reconciling data across accounts and systems is currently a time consuming, mostly manual process. You will combine data from multiple sources in Databricks and automate reconciliation tasks.
 

[20%] Build datasets and dashboards that make financial data more transparent, actionable, and accurate. 
Only top level financial metrics are currently available in dashboards. You will make detailed financial data easily accessible for decisions and analysis, as well as automate data quality tests to validate key financial metrics.
 

[20%] Increase data available for analysis and ML models by ingesting new data sources.
Our data lakehouse does not currently include data from all internal data sources, such as our HR system. You will build pipelines to make this data available and useful. 
 

[30%] Increase speed of development, reduce infrastructure failures and cost. 
On-call rotation duties: monitor pipeline jobs, review and test new pipeline PRs, track usage and cost.
Add new data quality checks and alerting.
Proactively identify and implement improvements to data infrastructure, as well as the tools used by data scientists and analysts to develop, deploy, and share their work.
Leave our systems and processes better than you found them.
What You’ll Bring

Exceptional alignment with GiveDirectly Values and active demonstration of our core competencies: emotional intelligence, problem solving, project management, follow-through, and fostering inclusivity. We welcome and strongly encourage applications from candidates who have personal or professional experience in the low-income and/or historically marginalized communities that we serve.
Language Requirement: English
Language Preferences: No additional language preferences
Critical thinking and analytical approach necessary to develop technical solutions that scale and are resilient to changes over time
Entrepreneurial mindset and stakeholder management skills required to identify, design, and execute technical solutions that solve important, ambiguous organizational problems
Python, SQL, and spark expertise, along with core competencies required to ship high quality data pipelines and data tools fast
Extensive experience with Databricks preferred; experience with Tableau is a plus
Intellectual humility, curiosity, and a commitment to being part of an exceptional team

Compensation

At GiveDirectly, we strive to pay our employees generously and equitably. We use an accredited third party salary aggregator to ensure that staff’s total compensation package (base compensation + bonus) falls within the 75th percentile of similar roles, at similar organizations. We also have a no negotiation policy to ensure we are paying staff equitably across roles.

The United States base salary for this role is $130,000.
The Kenya base salary for this role is $74,000.

This role is fully remote, so if you are not based in the US or Kenya, we will share an estimated salary benchmark for the country you are based in during the hiring process.

Why work at GiveDirectly?

Role

At GiveDirectly, we work to ensure that you have everything you need to excel in your role and on your team, including:

A positive and supportive team with opportunities for advancement 
A demonstrated commitment to helping all staff develop and grow
A competitive salary, including bonus
A robust health benefits plan (exact details will vary by country)
Unlimited PTO (that we encourage staff to take!)
Desk allowance and flexible work location

Read more about our ongoing diversity, equity, and inclusion efforts here and about our decision to move our central support teams to remote first here.

GiveDirectly is an Equal Opportunity Employer that values the strength diversity brings to the workplace. All qualified applicants are considered for employment without regard to the person’s race, color, religion, national origin, sex, sexual orientation, age, marital status, veteran status, disability, or any other characteristic protected by applicable law.

US applicants only: We invite you to ""Know Your Rights"" as an applicant.

About The Hiring Process

Format: The hiring process follows the same general outline for all open roles:

First interview (30 mins)

Take home skills assignment (~2 hours)

Second interview (1 hour)*

Third interview (1 hour)*

Final interview (1 hour)

Reference checks (30 mins each)

For some roles, second & third interviews are combined into a panel interview. If there are adjustments or variations on this process, those changes will be communicated during the first interview.

Venue: We conduct interviews over Google Meet with camera on (unless communicated otherwise).

Accessibility: Closed captioning is available during all Google Meet interviews, and interviewers will also post interview questions in the chat box throughout the call. If you need assistance accessing either of these features, please let your interviewer know at the start of your interview!

We’re committed to running an inclusive and accessible application process for all of our open roles. If there are questions or concerns you have about the accessibility of our hiring process, we warmly invite you to reach out to careers@givedirectly.org. Please include the word ""Accessibility"" in the email title.

**GD is committed to observing all local, national and international laws that protect children, vulnerable adults, and basic human rights of all. GD is committed to a policy of “zero tolerance for sexual exploitation, abuse, and harassment (SEAH)” and expects anyone who works for GD to uphold the protection and safeguarding of our recipients as a priority.**



Want to put your best foot forward on your GiveDirectly application? Take a look at our Candidate Application Prep Guide!","Apache Spark, Arquitectura de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Automatización, Calidad de datos, Datasets y Panel de control",Solicitar
https://www.linkedin.com/jobs/view/3946603771/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=KKnMKJhch8VPern8GYwq6A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Healthcare,"130 US$K/año - 180 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 4 días,Estados Unidos,"Acerca del empleo
Data Engineer - Healthcare

Our client is a highly innovative health tech company operating in stealth mode, focused on revolutionizing healthcare delivery through cutting-edge AI. As they enter an exciting phase of growth and transformation, they are seeking a skilled and experienced Data Engineer to contribute to the company's future success.

You will play a critical role in building and managing data pipelines that pull and process data from various hospitals to support applications in healthcare.

Responsibilities:
Developing data pipelines to pull data from hospital systems
Managing data adhering to standards and regulations
Using orchestration tools to manage data flow and processing at scale

Qualifications:
4 years’ experience in Data Engineering
Must have experience working in Healthcare data – FHIR, HL7
Experience working on EHR systems
Azure experience – beneficial
Python programming skills
Excellent communication and problem-solving skills

If you are interested, please apply or send a resume to kieran@alldus.com","Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, Comunicación, Fast Healthcare Interoperability Resources (FHIR), Flujo de datos, HL7 y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3934976021/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=ee8pToObARSWTDo3%2BaH2Qw%3D%3D&trk=flagship3_search_srp_jobs,Remote Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 meses,"Fairfax, VA","Acerca del empleo
Required Skills And Experience

Google Cloud Platform: Experience with GCP services, particularly Cloud Functions, Cloud SQL, BigQuery, and database management.
Python Scripting: Capable of automating tasks or developing utilities using Python.
ETL Pipelines: Comfortable with data cleaning, transformation, and integration workflows.
Ability to design, create, and manage APIs on GCP.
Desired Skills: Geospatial Technologies Mapping Libraries: Proficiency with Leaflet and/or Esri technologies.
React: Proficiency in React and JavaScript.
Frontend: Familiarity with Material-UI (MUI) or U.S. Web Design System (USWDS).","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud , Ingeniería de datos y Python, Bases de datos, JavaScript, Limpieza de datos y React.js",Solicitar
https://www.linkedin.com/jobs/view/3961441154/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=i%2Bb4tWY3dLNEdyqLLFnG7Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"60 US$/h - 63 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 semanas,Estados Unidos,"Acerca del empleo
Data Engineer, AI Solution Implementation (GCP - Gemini/VertexAI)
Remote (USA, EST)
Contract

Join our team and play a key role in implementing cutting-edge AI solutions for our Fortune 20 Retail customer using GCP Gemini and VertexAI technologies! Apply now to drive innovation and deliver impactful business outcomes.

Responsibilities:
Architect, design, and implement data pipelines and infrastructure to support the implementation of an AI solution at a large retail customer, leveraging Google Cloud Platform (GCP) technologies, including Gemini and VertexAI.
Collaborate with cross-functional teams to understand project requirements, translate them into technical specifications, and develop scalable and efficient data solutions.
Build and maintain data ingestion, transformation, and storage systems using GCP services such as BigQuery, Dataflow, and Cloud Storage, ensuring data quality, reliability, and security.
Develop and optimize machine learning pipelines for model training and inference, integrating with Gemini and VertexAI for seamless AI model deployment and management.
Work closely with AI engineers and data scientists to preprocess and prepare data for machine learning models, performing feature engineering, data augmentation, and exploratory data analysis as needed.
Implement monitoring, logging, and alerting mechanisms to track data pipeline performance and ensure timely identification and resolution of issues.
Collaborate with GCP technical experts and consultants to leverage best practices and optimize the use of Gemini and VertexAI tools and services for AI solution implementation.
Document data engineering processes, workflows, and best practices, and provide guidance and support to project team members as needed.

Requirements:
Bachelor's or Master's degree in Computer Science, Engineering, or a related technical field.
Proven experience in data engineering, with a focus on building data pipelines and infrastructure in cloud environments, preferably on Google Cloud Platform (GCP).
Strong proficiency in GCP services such as BigQuery, Dataflow, Cloud Storage, and AI Platform, as well as related technologies like Apache Beam and TensorFlow.
Experience with data preprocessing, feature engineering, and data modeling techniques, particularly in the context of machine learning and AI applications.
Proficiency in programming languages such as Python or Java, with experience in developing scalable and efficient data processing code.
Familiarity with data governance, compliance, and security best practices, especially in regulated industries such as retail.
Excellent problem-solving and analytical skills, with the ability to troubleshoot complex data engineering issues and optimize performance.
Effective communication and collaboration skills, with the ability to work in cross-functional teams and interact with stakeholders at all levels.","Google BigQuery, Google Cloud , Ingeniería de datos , Inteligencia artificial y Python, Ingeniería de características, Java y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3985076387/?eBP=BUDGET_EXHAUSTED_JOB&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=rXy90Sf3DMzxcuRPQODOTg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Nueva York, Estados Unidos","Acerca del empleo
Role Description

Data Engineer

Data Architect I

Who We Are

Born digital, UST transforms lives through the power of technology. We walk alongside our clients and partners, embedding innovation and agility into everything they do. We help them create transformative experiences and human-centered solutions for a better world.

UST is a mission-driven group of 29,000+ practical problem solvers and creative thinkers in more than 30 countries. Our entrepreneurial teams are empowered to innovate, act nimbly, and create a lasting and sustainable impact for our clients, their customers, and the communities in which we live.

With us, you’ll create a boundless impact that transforms your career—and the lives of people across the world.

Visit us at UST.com.

You Are

We need an experienced data engineer with a proven track record of driving product success from an engineering perspective. We are looking for someone who has strong experience in building scalable and reliable data pipelines using Databricks and Spark. You will be working with various data sources and formats and transforming them into valuable insights for our business. Cultural Fit: Join our collaborative, respectful, and intellectually curious team. We are persistent problem solvers, communicative, results-driven, and positive—bringing accountability and ownership to everything we do.

The Opportunity

 Optimize data pipelines for performance, scalability, and reliability
 Ensure data quality and integrity throughout the data lifecycle
 Collaborate with data scientists, analysts, and other stakeholders to understand and meet their data needs
 Troubleshoot and resolve data-related issues, and provide root cause analysis and recommendations
 Document data pipeline specifications, requirements, and enhancements, and communicate them effectively to the team and management
 Create new data validation methods and data analysis tools, and share best practices and learnings with the data engineering community
 Implement ETL processes and data warehouse solutions, and ensure compliance with data governance and security policies
 Holistic Learning: Dive into the intricacies of our platform, mastering its ins and outs.
 Strategic Contributions: Design, develop, and maintain data pipelines using Databricks and Spark, and other cloud technologies as needed
 Hands-On Contribution: Tackle challenging tasks as a hands-on contributor, ensuring a seamless transition between various facets of this dynamic role throughout the workday.

This position description identifies the responsibilities and tasks typically associated with the performance of the position. Other relevant essential functions may be required.

What You Need

 Use knowledge of technology trends to provide inputs on potential areas of opportunity for UST,
 Use knowledge of Architecture Concepts and Principles to create architecture catering to functional and non-functional requirements under guidance of the specialist. Re-engineer existing architecture solutions under the guidance of the specialist. Provide training on best practices in architecture under guidance.
 Use independent knowledge of Design Patterns, Tools and Principles to create high level design for the given requirements.

Compensation can differ depending on factors including but not limited to the specific office location, role, skill set, education, and level of experience. UST provides a reasonable range of compensation for roles that may be hired in various U.S. markets as set forth below.

Role Location: Remote

Compensation Range: $ 112,000-$168,000

Benefits

Full-time, regular employees accrue a minimum of 10 days of paid vacation per year, receive 6 days of paid sick leave each year (pro-rated for new hires throughout the year), 10 paid holidays, and are eligible for paid bereavement leave and jury duty. They are eligible to participate in the Company’s 401(k) Retirement Plan with employer matching. They and their dependents residing in the US are eligible for medical, dental, and vision insurance, as well as the following Company-paid Employee Only benefits: basic life insurance, accidental death and disability insurance, and short- and long-term disability benefits. Regular employees may purchase additional voluntary short-term disability benefits, and participate in a Health Savings Account (HSA) as well as a Flexible Spending Account (FSA) for healthcare, dependent child care, and/or commuting expenses as allowable under IRS guidelines. Benefits offerings vary in Puerto Rico.

Part-time employees receive 6 days of paid sick leave each year (pro-rated for new hires throughout the year) and are eligible to participate in the Company’s 401(k) Retirement Plan with employer matching.

Full-time temporary employees receive 6 days of paid sick leave each year (pro-rated for new hires throughout the year) and are eligible to participate in the Company’s 401(k) program with employer matching. They and their dependents residing in the US are eligible for medical, dental, and vision insurance.

Part-time temporary employees receive 6 days of paid sick leave each year (pro-rated for new hires throughout the year).

All US employees who work in a state or locality with more generous paid sick leave benefits than specified here will receive the benefit of those sick leave laws.

What We Believe

We proudly embrace the values that have shaped UST since day one. We build our culture of Humility, Humanity, and Integrity. These values inspire us to nurture a people-first, human centric culture that fosters diversity, prioritizes sustainable solutions, and keeps our people and clients at the forefront of all decisions.

Humility

We will listen, learn, be empathetic and help selflessly in our interactions with everyone.

Humanity

Through business, we will better the lives of those less fortunate than ourselves.

Integrity

We honor our commitments and act with responsibility in all our relationships.

Equal Employment Opportunity Statement

UST is an Equal Opportunity Employer.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other applicable characteristics protected by law. We will consider qualified applicants with arrest or conviction records in accordance with state and local laws and “fair chance” ordinances.

UST reserves the right to periodically redefine your roles and responsibilities based on the requirements of the organization and/or your performance.

#UST

#CB

Skills

Azure Data Factory,Python,Databricks,Data Lake Concepts,Data Lake


Aptitudes y experiencia deseables
Azure Data Factory,Python,Databricks,Data Lake Concepts,Data Lake","Analítica de datos, Apache Spark, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Comunicación, Requerimientos no funcionales y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3983410350/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=Mr%2Bx2FCUr3Z9b%2B2AaOpEYA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,Estados Unidos,"Acerca del empleo
Job Description

 Data Engineer - Remote 

Job Purpose

Resource will be part of Data Analytics Team (Operations Analytics group) and involved in Database development, Big Data development and ETL activities contributing to Analytics Projects and Paid POVs for Infosys BPO analytics.

Duties And Responsibilities

 Understanding requirements provided by client 
 Analyzing the requirements and document the requirement analysis 
 Come up with Development plan and individual task estimation 
 Design and develop Data flow pipelines using RDBMS or Big Data platform 
 Development and Implement Database design, ETL and prepare database for Visualization and analytics. 

Qualifications

 Basic 

 Bachelor’s degree in computer science, engineering, or equivalent work experience 
 4 years of professional experience in related field 

 Skills 

 Hands on experience with Data warehouse and database development (5-8 years) using any of the leading database systems. 
 Hands on experience with Database development including ETL (min 5 years), Sql and Analytical Sql Queries with any of the leading databases e.g. Postgres, Oracle, Sql Server etc. 
 Hands on with Cloud ecosystem and tools 
 Advanced working knowledge of Python 
 Fair working knowledge of Python, R programming for Data analysis 
 Familiar with Reporting tools like Tableau, PowerBI etc. 
 Team Player, Good Communication and Analytics skills 

About

 About Us 

Infosys BPM Limited, a wholly owned subsidiary of Infosys Limited (NYSE: INFY), provides end-to-end transformative business process management (BPM) services for its clients across the globe. The company’s integrated IT and approach enables it to unlock business value across industries and service lines, and address business challenges for its clients. Utilizing innovative business excellence frameworks, ongoing productivity improvements, , , and cutting-edge technology platforms, Infosys BPM enables its clients to achieve their cost reduction objectives, improve process efficiencies, enhance effectiveness, and deliver superior customer experience.

Infosys BPM has 42 delivery centers in 16 countries spread across 5 continents, with 57,908 employees from 124 nationalities, as of June 2023.

The company has been consistently ranked among the leading BPM companies globally and has received over 60 awards and recognitions in the last 5 years, from key industry bodies and associations like the Outsourcing Center, SSON, and GSA, among others. Infosys BPM also has very robust people practices, as substantiated by the various HR-specific awards it has won over the years. The company has consistently been ranked among the top employers of choice, based on its industry leading HR best practices. The company’s senior leaders contribute widely to industry forums as BPM strategists.

 EOE/Minority/Female/Veteran/Disabled/Sexual Orientation/Gender Identity/Nationality","Almacenamiento de datos, Analítica de datos, Arquitectura de datos, Desarrollo de base de datos y Extraer, transformar y cargar (ETL), Bases de datos, Ciencias de la computación, Comunicación, Modelado de datos y Sistemas de bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3907676644/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=YH5O15KPzD%2Buo%2BwBeXfohA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer with Testing experience,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Director",hace 3 meses,Estados Unidos,"Acerca del empleo
Data Engineer with Testing Knowledge

Remote 

Long term

12+ years only

Here are the key skills required for the Data Engineer with QE experience.

 Data engineer with testing knowledge
 PySpark/Python
 Snowflake or other data warehouse experience
 AWS","Almacenamiento de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y PySpark, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3974902089/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=KM2txvIc5K5GPoCZOxuVIw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Software Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Who we are
We are software artisans passionate about what we do: help companies build awesome solutions. With an agile process that is built on top of the best engineering practices.
Our team is comprised of full-stack developers and architects, who are versed in the very latest technologies and love what they do!
We believe transparent, honest and fluent communication, both remotely and on-site is a key factor to the success of any project.

What are we looking for?
We are looking for a seasoned Software Engineer oriented to a Data & Analitycs ecosystem (ADF, Databricks, ADLS, Synapse, Python, etc...). Our ideal candidate is the one who has a wide experience in infrastructure enterprise, On-Premise and Cloud.

Responsibilities
Develop ETL pipelines in Python and Azure Data Factory as well as their DevOps CICD pipelines.
Software engineering and systems integration through REST APIs and other standard interfaces.
Work together with a team of professional engineers with the objective of automating processes, deploying and building infrastructure as code and managing the architecture of multicloud systems.
To Participate in agile ceremonies, weekly demos and such.
To Communicate your daily commitments.

Qualifications
4+ years of relevant work experience.
Solid understanding of DevOps CI CD and monitoring.
Experience with Agile ceremonies.
Passionate about good engineering practices and testing.
Ability to organize, prioritize and communicate daily/weekly goals.
You like to learn, are curious, humble and like to get things done.
Passion for working in a customer facing setting.

Required Skills
Proficient with Python, Java or .NET C#.
Proficient with SQL.

Nice to have:
Proficient with ETL products (Spark, Databricks, Snowflake, Azure Data Factory, etc.)
Proficient with Azure Data Factory.
Proficient with Databricks/Snowflake and PySpark.
Proficient developing DevOps/CICD pipelines.
Proficient with Azure DevOps YAML Pipelines.
Proficient with Azure cloud services: ARM templates, API management, App Service, VMs, AKS, ACR, Gateways.
Terraform, Docker & Kubernetes.
Bash/Powershell.

Our solutions support enterprise information management, business intelligence, machine learning and data science. You should excel in working remotely and have outstanding communication skills (Transparency is one of our core values).","Base de datos SQL, Extraer, transformar y cargar (ETL) y Python",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984787871/?eBP=BUDGET_EXHAUSTED_JOB&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=JsFDFUfxHqxQUpwfTQF9kA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Manhattan, NY","Acerca del empleo
⭐Data Analytics Developer | Hybrid | Manhattan, NYC | $140,000 - $170,000 ⭐

Are you looking to take the next step in your career?
Are you looking to move into a professional service environment that's purely in Azure?
Would you like to be a part of a culture that rewards self-starters and team players while expanding your technical knowledge?

If so, this role may be for you.

The company:
A large global legal firm in New York is looking to hire a Data Analytics Developer to join its growing Data team to support its technology roadmap, technology strategy, and global analytics You will be working within an environment that invests heavily in IT and only works with the latest technology.

The role:
This is an exciting role within a prestigious IT team, you will have the chance to be their SME in PowerBI. You will be working closely with their Data Architect and be responsible for developing their Power BI dashboards and creating SQL scripts in Azure.

This role is full-time direct hire and is located in Manhattan. This role does not offer sponsorship.

What you need:
Experience working within an MS Azure environment.
Excellent developing SQL scripts.
Experience developing PowerBI dashboards and reports.
Excellent communication skills.
Experience collaborating with other technical teams.

What’s in it for you:
Fantastic company culture
Great work/life balance
A rewarding career path with a growing company
401k match, paid certifications

Apply directly to be considered or show your interest to j.ellis@franklinfitch.com!

⭐Data Analytics Developer | Hybrid | Manhattan, NYC | $140,000 - $170,000 ⭐","Analítica de datos y SQL, Microsoft Azure y Panel de control",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982701504/?eBP=BUDGET_EXHAUSTED_JOB&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=1L7RsqH7EIyD2uEyjyf1fA%3D%3D&trk=flagship3_search_srp_jobs,Cloud Data Engineer- Alteryx - 100% Remote - 1921,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 días,"Wilmington, DE","Acerca del empleo
 Location: 100% Remote

Term: 12 to 36 months

Conract Type: Must be W2 only (no c2c, no 1099)

Salary: Upto $128/hr DOE

Project Details

Data management and analytics visualization are growing areas of emphasis across the client. As a data software engineer (visual analytics support facilitator) within Enterprise Data Services, you will be an integral part of a dynamic team responsible for the effective utilization and daily support of Alteryx Designer and Alteryx Server.
This position is responsible for contributing to the success of the System's Alteryx Shared Service by leading process improvement initiatives related to content management, platform utilization, customer onboarding, and end user technical support. This role requires versatility and a drive to learn and pursue continuous improvement with the ability to work well in a diverse and cross-functional team.
Projects will range in duration and complexity, and they will require direct consultation with business partners, technical subject matter experts, and IT service providers.

Key Activities Include But Are Not Limited To

Work with the team to manage and support an Alteryx ecosystem including Alteryx Server and Alteryx Designer.
Alteryx Designer directly involved supporting users with designing workflows, schedules and troubleshooting simple and complex technical problems.
Recognize and fill gaps as necessary to resolve tickets and address inquiries when service requests spike.
Create and maintain detailed internal process and self-service knowledge base documentation.
Selected candidate will support the Alteryx user community by building relationships and communicating frequently.

 

Qualifications & Job Requirements

Minimum of 2+ years experience with Alteryx Server & Alteryx Designer.
Alteryx certifications preferred.
Moderate to high skills in data visualization and data management required.
Moderate skill with scripting and query languages such as SQL, Python, and PowerShell required.
Experience with AWS or other public cloud providers strongly preferred.
Strong customer service and communication skills, including the ability to patiently explain technical concepts to end-users of varying expertise via formal presentations and informal coaching/consultation.
Strong customer-focus, using their expectations and user experience as a basis to define what success looks like for the ecosystem
Strong organization and time management skills required.
Associate degree from a two-year college or technical school, or equivalent combination of education and/or work-related experience.
Bachelor's degree from a four-year college or university preferred.
May require some weekend or evening hours.
Ability to work independently and with general supervision and direction. May consult with more senior staff in decision making.

Note

No 3rd party vendors or candidates 
US Citizenship Required - Federal requirement","Analítica, Analítica de datos, Ciencia de datos, Minería de datos, Query Languages, Visualización y Visualización de datos, Alteryx, Expertos en la materia y Lenguaje de consulta (query)",Solicitar
https://www.linkedin.com/jobs/view/3973209107/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=ZIV%2FsqB5upfRzaMo%2BVDXPg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Contractor,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.",hace 2 semanas,San Francisco y alrededores,"Acerca del empleo
Contract Description

EY GigNow has an immediate opening for a Data Engineer

Develop and enhance data processing systems to ensure seamless data integration across multiple databases including Oracle, SQL Server, and proprietary databases.
Implement and optimize NiFi processors and process groups to streamline data workflows and enhance performance.
Design, develop, and maintain ETL pipelines using Azure Datalake with Delta Lake file format (Parquet) to support data analytics and reporting.
Perform SQL queries to extract, transform, and load data from various sources into target systems ensuring data integrity and consistency.
Collaborate with cross-functional teams to identify and resolve system issues, ensuring data quality and reliability.

Qualifications

Proficiency in NiFi with experience in creating and optimizing processors and process groups.
Strong knowledge of Azure Datalake with experience in handling Delta Lake file formats (Parquet).
Expertise in Python and PySpark for data manipulation and analysis.
Advanced SQL skills with the ability to work across multiple databases including Oracle, SQL Server, and proprietary systems.
Experience with Databricks for data engineering tasks, including building and managing data pipelines.
Experience in building pipeline strategies for data loading and ETL processes, including setting up and managing workflows.

The expected pay rate range for this contract assignment is $82 to $92 per hour. Exact pay rate will vary based on skills, experience, and location.

Equal Employment Opportunity Information

EY provides equal opportunities to applicants, employees, contractors, vendors, and stakeholders without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, pregnancy, genetic information, national origin, protected veteran status, disability status, or any other legally protected basis, including arrest and conviction records, in accordance with applicable law. If you are an individual with a disability and either need assistance applying online or need to request an accommodation during any part of the application process, please email gignow.recruiting@ey.com.","Azure Data Lake, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y PySpark, Apache NiFi, Azure Databricks, Bases de datos, Data Loading y Manipulación de datos",Solicitar
https://www.linkedin.com/jobs/view/3978169134/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=5jN9vthSp9dumem8tI1JIA%3D%3D&trk=flagship3_search_srp_jobs,ENGENHEIRO DE DADOS,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Home, KS","Acerca del empleo
Mandatory Requirements Extensive experience in analyzing and understanding diverse data sources. Strong skills in data mapping from source data to internal data structures. Proficient in designing and building ETL pipelines using tools such as Apache Airflow, Talend, or similar. Strong programming skills in Python, SQL, and relevant scripting languages. Experience with data warehousing solutions like Amazon Redshift, Google BigQuery, or Snowflake. Familiarity with big data technologies such as Hadoop, Spark, or Kafka. Experience with cloud platforms like AWS, Azure, or GCP. Excellent analytical and problem-solving abilities. Strong verbal and written communication skills. Desirable Requirements Advanced knowledge in machine learning implementation and optimization. Familiarity with real-time data processing. Experience with data security and privacy standards. Summary of Requirements:  ETL, Apache Airflow, Talend, Python, SQL, Amazon Redshift, Google BigQuery, Snowflake, Hadoop, Spark, Kafka, AWS, Azure, GCP","Almacenamiento de datos, Google BigQuery, Google Cloud y Ingeniería de datos, Amazon Redshift, Asignación de datos, Comunicación escrita, Estructuras de datos, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3983287900/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=e9IMKiEMadxdOMmnWKyOAQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"120 US$K/año - 165 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 días,"Alexandría, VA","Acerca del empleo
We are seeking a skilled and experienced Data Engineer to join our team. The ideal candidate will be responsible for designing, implementing, and maintaining ETL processes for both structured and unstructured data. Experience with Microsoft Dynamics 365 and the Dataverse is highly desirable, as well as proficiency in Python scripting. The successful candidate will play a crucial role in ensuring the efficient and secure handling of data, contributing to the optimization of our data infrastructure.

Pay Range: $120,000 - $165,000

Travel Up To - 20%

You Will:

Design, develop, and maintain ETL processes for structured and unstructured data. 
Integrate data from various sources into our data infrastructure. 
Optimize and tune ETL processes for performance and reliability. 
Collaborate with data analysts, data scientists, and other stakeholders to understand data requirements and provide robust solutions. 
Implement data validation and quality checks to ensure data accuracy and integrity. 
Develop and maintain documentation for ETL processes and data workflows. 
Monitor ETL jobs and data pipelines, addressing issues promptly. 
Ensure data security and compliance with relevant regulations. 
Work with Microsoft Dynamics 365 and Dataverse to integrate and manage data. 
Utilize Python scripting for data manipulation, automation, and integration tasks. 
Stay up-to-date with emerging technologies and best practices in data engineering. 
Support data migration projects and contribute to cloud data integration initiatives. 

You Have:

Bachelor’s degree in Computer Science, Information Technology, or a related field. 
Proven experience as a Data Engineer or in a similar role. 
Experience with Microsoft Azure Data Factory, Data Lake Storage Gen2, Azure Blob Storage. 
Experience migrating data from Microsoft SQL Server Databases into Microsoft Dataverse. 
Experience with Cloud Platforms to just what the CG has (Microsoft Azure Gov Cloud or Amazon Web Service Gov Cloud). 
Experience migrating multiple data formats from and to Microsoft SharePoint Online. 
Experience in Oracle Web Logic Server
Expertise in ETL processes for both structured and unstructured data. 
Experience with Microsoft Dynamics 365 Customer Service and Dataverse. 
Proficiency in Python scripting. 
Strong understanding of data architecture, data modeling, and database design principles. 
Familiarity with SQL and noSQL databases. 
Experience with data integration tools and platforms. 
Knowledge of data security and privacy best practices. 
Excellent problem-solving and analytical skills. 
Effective communication and collaboration skills. 
Ability to work independently and as part of a team. 

This position offers a unique opportunity to contribute to the optimization and security of our data infrastructure. If you are passionate about data engineering and have the required skills and experience, we encourage you to apply.

Want to hear more? Read on to see what our Team Members say about their experience:

“Atlas Tech has the culture, trust, and fairness that you expect of the best of communities. I feel lucky to be part of their amazing journey of growth as a standout organization.” - Syed

“You are not just a number here at Atlas. I have been with Atlas for over 13 years now I can say that Atlas displays a genuine interest in its employees. This is evidenced in its commitment to mentoring every employee and providing educational paths for each employee’s personal and professional growth within the company.” - Ian

Atlas Tech offers a competitive salary, generous benefits package, and an opportunity to make a positive impact in your own community.","Arquitectura de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Ciencias de la computación, Manipulación de datos, Microsoft Dynamics 365, Modelado de datos, Validación de datos y WebLogic",Solicitar
https://www.linkedin.com/jobs/view/3977777945/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=USkeeO%2BLz2p2Z6gbsCbyzg%3D%3D&trk=flagship3_search_srp_jobs,Data Science Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 semana,"Salt Lake City, UT","Acerca del empleo
About Wipro:

Wipro Limited (NYSE: WIT, BSE: 507685, NSE: WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. We leverage our holistic portfolio of capabilities in consulting, design, engineering, operations, and emerging technologies to help clients realize their boldest ambitions and build future-ready, sustainable businesses. A company recognized globally for its comprehensive portfolio of services, strong commitment to sustainability and good corporate citizenship, we have over 250,000 dedicated employees serving clients across 66 countries. We deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world.

A PROUD HISTORY OF OVER 75 YEARS
FY22 REVENUE 10.4 BN USD
WE’RE PRESENT IN 66 COUNTRIES
OVER 1,400 ACTIVE GLOBAL CLIENTS

Job Title: Data Science Engineer

Location: Salty Lake City, Utah (Onsite)

Job Description:

Responsibilities

Performs business analytics, which includes data analysis, trend identification, and pattern recognition, using advanced techniques (e.g., machine learning, text mining, statistical analysis, etc.) to support decision-making and drive data-driven insights
Build dashboards in PowerBI, and other technologies to facilitate business decisions
Applies agile practices for project management, solution development, deployment, and maintenance
Creates and maintains technical documentation, capturing the business requirements and specifications related to the developed analytical solution and its implementation in production
Manages multiple priorities and maintains quality and timeliness of work deliverables such as quantitative models, data science products, data analysis reports, or data visualizations, while exhibiting the ability to work independently and in a team environment
Delivers engaging presentations and engages in both in-person and virtual conversations that effectively communicate technical concepts and analysis results to a diverse set of internal stakeholders, and develops professional relationships to foster collaboration on work deliverables
Mitigates risk by identifying potential issues and developing controls
Researches the latest advances in the fields of data science and artificial intelligence to support business analytics

Requirements:

2-6+ years experience in Business Intelligence Engineering, Data Engineering, Data Analysis or Data Science roles, building data pipelines, and analyzing large datasets to solve problems
Proficiency in PowerBI, SQL, and other Data Lake interactions
Strong statistical knowledge
Expertise in visualization and using data insights to make recommendations and achieve goals
Proven ability to manage and deliver on multiple projects with great attention to detail
Ability to clearly communicate results and drive impact
Comfortable collaborating across functions to identify data analytics problems and execute solutions with technical rigor and data-driven insights.

We are an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, caste, creed, religion, gender, marital status, age, ethnic and national origin, gender identity, gender expression, sexual orientation, political orientation, disability status, protected veteran status, or any other characteristic protected by law.

Internet of Things - IoT","Analítica de datos, Análisis de datos, Canalizaciones de datos, Ciencia de datos y Ingeniería de datos, Comunicación, Datasets, Documentación técnica, Lagos de datos y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3984581660/?eBP=BUDGET_EXHAUSTED_JOB&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=vaAyJEhukq7IEpcNNhes7w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer I,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Edina, MN","Acerca del empleo
Job Type

Full-time

Description

Who are we?

Western National Insurance Group is a private mutual insurance company with over 120 years of experience serving customers' property-and-casualty insurance needs in the Midwestern, Northwestern, and Southwestern United States. Known as “The Relationship Company®,” we define success as a measure of the relationships we’ve built over time. In everything that we do, we know that delivering a friendly and helpful interaction makes for a better experience for everyone involved. That’s the power of “nice”. At Western National, nice is something we work to bring to every person and organization with whom we partner and serve.

Does this opportunity interest you?

Western National is seeking a Data Engineer I to join our team!

The individual in this role will have the opportunity to collaboratively design, implement, maintain, test, and deploy data-centric software applications.

What are the responsibilities and opportunities of this role?

Collaborates in troubleshooting and resolving application and data quality issues.
Participates in work refinement and estimation processes with product team.
Prepares and executes unit testing on all new and modified code per team standards.
Works with product team and Change Control Team to ensure quality deliveries.
Creates and maintains documentation that the product team or company has defined as important.
Ensures timely delivery of projects and tasks by communicating delays and roadblocks with team and leadership.
Flexes to non-core responsibilities as needed to ensure timely work delivery to the customer.
Collaborates with product team to develop, test, and deploy routine application modifications based on collaboratively defined requirements.
Reviews and contributes to technical specifications, flow charts, and data flow diagrams to effectively communicate application design, delivery, and updates.
Solicits code review per team standards.
Identifies opportunities for process improvement and shares ideas during process improvement conversations at product team level.
Follows along with troubleshooting, resolution, and communication of urgent production issues.
Utilizes beginner-level knowledge, experience, and available resources to find acceptable solutions.
Identifies opportunities to improve application efficiency and effectiveness. 
Participates in problem identification and process improvement conversations at the product team level.
Consistently acts according to our customer experience standards, including responding quickly, maintaining a positive attitude, building rapport, demonstrating empathy, managing the customer’s expectations, using the proper communication channel for the situation, and taking ownership to ensure the customer’s issue is resolved.
Performs special projects and other duties as assigned.


Requirements

What are the must-have qualifications for a candidate?

Good mathematical, analytical, and problem-solving skills.
Ability to work independently and carry out assignments to completion.
Strong attention to detail and quality.
Passion for data.
Ability to communicate clearly both verbally and in writing with technical and non-technical audiences.
Beginner to intermediate knowledge and use of various office and computer equipment and software packages.
Fundamental knowledge of data structures, databases, and data warehousing concepts.
Fundamental effectiveness with Python, SQL, and accessing / managing data in relational databases.
Fundamental effectiveness in managing and merging software changes in tools like Git / Gitlab.
Awareness of software containerization and deployment tools.
Bachelor’s degree in computer science or related technical field; relevant experience (one to two years) in lieu of education acceptable.


What will our ideal candidate have?

Demonstrated basic organizational and prioritization skills.
Ability to onboard rapidly to multiple, and potentially new, responsibilities and to accomplish multiple low-impact tasks within specified timeframes. 
Demonstrated basic knowledge of the Agile framework. 
Experience with Microsoft SQL Server and ETL development using SSIS packages and stored procedures.


Compensation Overview

The targeted hiring range for this role is $80,100 - $97,900, annually. However, the base pay offered may vary depending on the job-related knowledge, skills, credentials, and experience of each candidate as well as other factors such as the scope and location of the role. Candidates looking for compensation outside of the posted range are encouraged to apply and will be considered based on their individual qualifications and/or may be considered for other positions.

Culture and Total Rewards

Western National has long been known as “The Relationship Company®” and caring for our employees is part of that relationship commitment. We value connectiveness, empowerment, and accountability, and we believe that our employees are our biggest asset.

Currently ranked as the 41st largest private company by revenue in Minnesota (Minneapolis/St. Paul Business Journal), Western National has earned accolades year-over-year as an employer of choice and garnered multiple awards for wellness in the workplace. Western National has also been named a Top Workplace by the Star Tribune for the past four consecutive years. In addition, the Group is consistently recognized as a Ward’s 50 property-and-casualty insurance company for its outstanding financial results.

Western National offers full-time employees a significant Total Rewards Package, including:

Medical insurance plan options and other standard employee benefits, including dental insurance, vision benefits, life insurance, disability insurance, and more!
Health Savings Accounts (HSA) and Flexible Spending Accounts (FSA)
401(k) Plan (participants are eligible for 100% matching on the first 6% of their contributions)
Wellbeing Program, including onsite fitness studio
Paid Time Off – including holiday, vacation, and volunteer
100% company-paid tuition reimbursement for approved job-relevant coursework and access to The Institutes (Risk and insurance education)
Paid parental leave
Bonus opportunities


Western National believes in supporting balance between work and life by providing a flexible work environment, which includes a variety of hybrid and remote work arrangements designed to balance individual, job, department, and company needs.

Western National provides employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.

Salary Description

$80,100 - $97,900","Almacenamiento de datos, Extraer, transformar y cargar (ETL) y SQL Server Integration Services (SSIS), Bases de datos, Ciencias de la computación, Diagramas de flujo, Flow Diagrams, Procedimientos de almacenado, Resolución de problemas y Revisión de código",Solicitar
https://www.linkedin.com/jobs/view/3976856771/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=MWu7eX8EMdNCylX330uYsw%3D%3D&trk=flagship3_search_srp_jobs,"Program Data Analyst/BI Engineer_W2_only local to Boston, MA","Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",hace 1 semana,"Boston, MA","Acerca del empleo
Position: Program Data Analyst/BI Engineer

Location: Boston, MA (only local)

Required Skills

 In-depth exposure to data quality concepts, best practices, and tools.
 Experience in designing, developing and deploying business analytics dashboards using PowerBI.
 Experience in Cloud Data warehouses like Snowflake, Redshift.
 Familiarity with Snowflake Data sharing, Snowpipe and Snowpark.
 Experience in ETL tools such as Pentaho/Informatica and proficient in SQL.
 Experience with Software as a Service cloud implementations particularly those in which legacy on premise applications have been migrated to cloud delivery options.","Analítica, Analítica de datos, Analítica empresarial, Extraer, transformar y cargar (ETL), Herramientas ETL y SQL, Calidad de datos, Informatica (empresa), Panel de control y Pentaho",Solicitud sencilla
https://www.linkedin.com/jobs/view/3868056748/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=HW%2F5IvQHskZGWnJwdnWB6g%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Scientist/Analyst/Engineer - Remote,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 meses,"Provo, UT","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, walmart lab s, etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry?

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 If you applied for a job and got emails from our Job Placement Program team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

 please check the below links to see the successful outcomes of our candidates our  participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/ 

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage see us exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and Gartner Data Analytics Summit (Florida)-2023 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 https://www.youtube.com/watch?v=OFoqPTNORew 

 https://www.youtube.com/watch?v=-HkNN1ag6Zk 

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://youtu.be/bJJl27D8bh0 

To prepare for interviews please visit  https://www.synergisticit.com/interview-questions/

 We are looking for the right matching candidates for our clients 

 Please apply via the job posting 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Programmer 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java, Javascript, C++, or software programming 
 Spring boot, Microservices, Docker, Jenkins, and REST API experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning Positions 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, and data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow 

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates.","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3958863589/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=3fX79Ol9EtBUke%2Fh5KpaSg%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Atlanta, GA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3980378131/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5mBpkD%2FnRKSUjL%2BykHawgg%3D%3D&trackingId=f%2BlNQF9xmnT7UZXjHMXB5w%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Junior/Entry,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 6 días,"Tulsa, OK","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this market also our candidates get multiple job offers and $100k + salaries.

please check the below links to see success outcomes of our candidates .

(url removed)

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3961302328/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=IeWVpaiNqdc%2Fs5egZhG7Uw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,Estados Unidos,"Acerca del empleo
About the company:

Avenue Code is the leading software consultancy focused on delivering end-to-end development solutions for digital transformation across every vertical. We’re privately held, profitable, and have been on a solid growth trajectory since day one. We care deeply about our clients, our partners, and our people. We prefer the word ‘partner’ over ‘vendor’, and our investment in professional relationships is a reflection of that philosophy. We pride ourselves on our technical acumen, our collaborative problem-solving ability, and the warm professionalism of our teams.

Avenue Code has been believing in and promoting plurality actions for over 10 years, understanding that recognizing differences and fostering a safe environment, employment opportunities, representation, and support are the best ways to promote an increasingly equitable culture.

About the opportunity:

We are seeking a highly experienced Data Engineer with exposure to lead or help other data engineers. The ideal candidate will possess a deep knowledge of Python, Java, Kafka, Spark, Flink, Elasticsearch, and graph data technologies. This role requires strong debugging and systems knowledge to ensure high availability and performance of our data systems and to efficiently address production issues. This is a hybrid position 2 days a week in San Francisco/San José – CA. There will be on-call rotations to address high-priority production issues and maintain system uptime.

Responsibilities:

Help other data engineers in designing, building, and maintaining advanced data pipelines and architectures.
Act as a subject matter expert in production system operations, including monitoring, debugging, and resolving production issues efficiently.
Implement real-time and batch data processing solutions using Kafka, Spark, Flink, and other relevant technologies.
Leverage Elasticsearch to enhance search and analytics capabilities, ensuring optimal performance and scalability.
Utilize graph and vector databases and fine-tune language models (LLMs) to support innovative data applications.
Collaborate with global engineering teams, particularly in the India Time Zone, to manage cross-functional projects and initiatives.
Provide technical guidance and mentorship, fostering a culture of continuous improvement and learning within the team.
Develop and enforce best practices for data quality, reliability, and systems security.
Participate in on-call rotations to address high-priority production issues and maintain system uptime.

Required Qualifications:

Strong experience in data engineering.
Expert proficiency in programming languages such as Python or Java, with a strong background in data processing and analytics.
Deep understanding of Kafka, Spark, Flink, and Elasticsearch, with hands-on experience in production environments.
Experience with graph databases, graph algorithms, and NLP techniques, including LLM fine-tuning.
Strong debugging skills and experience with production system operations, including performance tuning and troubleshooting.
Excellent communication and collaboration skills, with the ability to work effectively across time zones and cultures.

Nice to Have:

Hybrid – 2 days a week in San Francisco or San Jose, CA. Avenue Code discloses salary range information in compliance with state and local pay transparency obligations. It considers a wide range of factors such as internal equity, geographic location, relevant education, qualifications, certifications, experience, skills, seniority, business or organizational needs and others. At Avenue Code it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for US Based Data Engineer is $90.000 USD to $110.000 USD per year. 

Avenue Code reinforces its commitment to privacy and to all the principles guaranteed by the most accurate global data protection laws, such as GDPR, LGPD, CCPA and CPRA. The Candidate data shared with Avenue Code will be kept confidential and will not be transmitted to disinterested third parties, nor will it be used for purposes other than the application for open positions. As a Consultancy company, Avenue Code may share your information with its clients and other Companies from the CompassUol Group to which Avenue Code’s consultants are allocated to perform its services.","Apache Kafka, Apache Spark y Ingeniería de datos, Ajuste de rendimiento, Base de datos orientada a grafos, Bases de datos, Calidad de datos, Comunicación, Graph Algorithms y Operaciones de sistemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3860725646/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=9NFHYjGaGj1e0duR%2Fm476w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"120 US$K/año - 160 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 meses,"Nueva York, NY","Acerca del empleo
We're building Al thought partners to make people smarter and more creative, accelerating the creation and sharing of knowledge in financial services. We're unabashedly ambitious, and we're dead set on building the biggest Financial AI company in the world. Our team is lean, smart, and enormously ambitious. We're growing fast out of our beautiful office in NYC.

WHY JOIN ROGO?

Exceptional traction: strong PMF with the world's largest investment banks, hedge funds, and private equity firms. 
World-class team: we take talent density seriously. We like working with incredibly smart, driven people. 
Velocity: we work fast, which means you learn a lot and constantly take on new challenges. 
Frontier technology: we're developing cutting-edge AI systems, pushing the boundaries of published research, redefining what's possible, and inventing the future. 
Cutting Edge Product: Our platform is state-of-the-art and crazily powerful. We're creating tools that make people smarter, reinventing how you discover, create, and share knowledge. 

About The Role

As a Distributed Systems Engineer at Rogo, you will help build out our real-time data pipelines for millions of unstructured financial documents to feed our financial LLM. It’s cutting-edge data engineering at the AI frontier.

Responsibilities

Architect distributed systems with AI to handle petabyte-scale content, reliably, quickly, and fuel our underlying LLM infrastructure. 
Build REST APIs that are backed by stable, scalable server side implementations and maximize web client flexibility for rapidly meeting evolving product requirements
Ship secure and compliant code: implement security concepts to develop software dealing with sensitive data, work with the security teams choosing what to build vs buy. 
Write robust code that’s easy to read, maintain and test
Raise the bar for code quality, reliability and product velocity. Collaboratively, you'll push yourself and peers to develop technically and interpersonally. 

Hard Requirements:

4+ years of industry experience as a data engineer
You love scaling workloads amongst many machines to handle petabyte-scale tasks
Highly proficient with Python and SQL, and an intuitive understanding of multi-threading, multi-processing, asyncio, and other concurrency primitives
Mastery of: Postgres, Snowflake or Elasticsearch
2+ years of experience with Apache Airflow
Experience deploying and monitoring mission-critical ETL pipelines with large and heterogenous datasources
Experience with Distributed systems 
Experience with AWS or other cloud environment

Bonus Requirements:

Experience with a strongly typed language (e.g., Rust)
Experience at a hypergrowth startup
Financial Services work experience
Experience with stream processing
Knowledge of Datadog and other Telemetry tooling

Who You Are

You thrive in fast-paced environments. You are high-intensity and care a lot about what you do, and you're ecstatic to work at a start-up 
You are ambitious. You have fun solving problems that others think are impossible. 
You are curious. You find joy in learning about AI, technology, and finance
You are an owner. You are autonomous, self-directed, and comfortable working with ambiguity
You are collaborative, organized, and thoughtful.","Airflow, Almacenamiento de datos, Apache, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Apache Airflow, Bases de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3959004745/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=8EnUmTcRNko%2FVzqL8goR8w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 semana,Estados Unidos,"Acerca del empleo
You Belong at Greenway

Bring your best and truest self. We celebrate what makes us different and what brings us all together. At Greenway Health, we are committed to an inclusive environment and a culture of belonging as we pursue our purpose of healthier communities, successful providers, and empowered patients. We are united in our goal to build the future of healthcare technology. Join us.

The Data Engineer is responsible for building, maintaining, and ensuring data production pipelines run smoothly. This role will monitor data pipelines for our data analytics platform, proactively diagnose and resolve issues, enhance pipeline components as needed, and develop support tools to enable non-technical support staff to troubleshoot common issues over time. In addition, this role will implement methods to improve data reliability and quality as well as develop and test architectures that enable data extraction and transformation for predictive or prescriptive modeling. This role may perform these duties in Cloud, non-Cloud, and hybrid environments.

Essential Duties & Responsibilities

Analyze and organize raw data sets to meet both functional and non-functional requirements
Develop and maintain data sets
Improve data quality and efficiency
Create scalable, production-ready pipelines for data movement and transformation
Work with various stakeholders, including data, design, product, and executive teams, to assist with data-related technical issues
Implement and enhance support tools for monitoring and acting on data pipeline issues
Interpret trends and patterns
Conduct data analysis and report on results
Prepare data for prescriptive and predictive modeling
Build algorithms and prototypes
Combine raw information from different sources
Explore ways to enhance data quality and reliability
Identify opportunities for data acquisition
Develop analytical tools and programs

Education And Experience 

Bachelor’s degree in Computer Science, Information Technology, Mathematics, or another related field.
A Master’s degree is a plus.
Two (2) to four (4) years of software development experience with a focus on data engineering.
Experience with Cloud-based deployment and data processing technologies (AWS Lambda, Step Functions, S3 storage, Simple Service (SQS), EMR clusters, CloudWatch monitoring, and CloudWatch insights).
Experience with supporting data transformation and analytics solutions preferred.
Experience in Healthcare Information Technology (HIT) is highly preferred.
Working experience with the following languages: Spark, Python (2 years preferred), SQL (2 years), Kafka (preferred)
Working experience with relational databases, queries, and optimization (DBA or database developer with proven hands-on application and understanding) with a high emphasis on Postgres.
Data engineering certification is a plus.

Skills, Knowledge And Abilities

Technical expertise in data models, data mining, segmentation techniques, and software architecture
High-level understanding of data schemas for structured and unstructured data (scheme on write vs. schema on read, columnar vs. row, relational vs. NoSQL)
Big Data experience (HDFS, Hive, HBase, M/R, Impala, Spark, HUE) preferred
Understanding of software architecture
Simple Notification Service (AWS SNS)
Change Data Capture
Working knowledge of data lake architecture (batch vs. streaming, Lambda architecture, Delta Lake, Apache Hudi)
Pipeline orchestration (Apache Airflow, Apache NiFi)
Strong verbal and written communication skills
Excellent attention to detail
Customer service skills with a high level of professionalism
Strong desire to learn
Basic computer skills, including Microsoft Outlook, Word, and Excel
Able to manage a variety of tasks concurrently

Work Environment/Physical Demands

While at work, this position is primarily a sedentary job and requires that the associate can work in an environment where they will consistently be seated for the majority of the workday 
This role requires that one can sit and regularly type on a keyboard the majority of their workday 
This position requires the ability to observe a computer screen for long periods of time to observe their own and others’ work, as well as incoming and outgoing communications via the computer and/ or mobile devices. 
The role necessitates the ability to listen and speak clearly to customers and other associates 
The work environment is an open room with other associates and noise from others will be part of the regular workday 

Here’s what we can offer you in exchange for your amazing work:

Competitive pay
Medical, dental and vision benefits
Matching 401(k) 
Generous paid time-off programs
Education reimbursement
Growth potential for your career
Corporate discounts

At Greenway, we strive to imagine, empower, engage, and inspire. Join us!

To learn more about Greenway, take a video tour of our office, and meet our employees, visit us at www.GreenwayHealth.com/careers.

Disclaimer: This Job Summary indicates the general nature and level of work expected of the incumbent(s). It is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities required of the incumbent. Incumbent(s) may be asked to perform other duties as requested. Greenway Health, LLC is an Equal Opportunity Employer. We do not discriminate on the basis of race, religion, age, gender, national origin, sexual orientation, disability, or veteran status.

While this position is primarily remote, please note that if you reside within a 26-mile radius of our corporate office, you will be required to work in a hybrid capacity. This means you will be expected to work on-site at the corporate office for part of the week and remotely for the remainder. This hybrid arrangement is designed to foster team collaboration and engagement. Our corporate office is located at 4301 Boy Scout Blvd, Tampa, FL 33607. Please consider your proximity to this location when applying.

If you are a Colorado resident, please email us at recruiting@greenwayhealth.com to receive compensation and benefits information for this role. Please include the Job ID in the subject line of the email.","Analítica de datos, Ciencia de datos y Ingeniería de datos, Bases de datos, Calidad de datos, Captura de datos modificados, Ciencias de la computación, Comunicación, Conocimientos informáticos y Requerimientos no funcionales",Solicitar
https://www.linkedin.com/jobs/view/3953824808/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=IIDGfvCbZsQlQ82rDkqRzQ%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"McLean, VA","Acerca del empleo
Overview

As a Junior Data Engineer you will help develop and deploy technical solutions to solve our customers’ hardest problems, using various platforms to integrate data, transform insights, and build first-class applications for operational decisions. You will leverage everything around you: core customer products, open-source technologies, and anything you and your team can build to drive real impact. In this role, you work with customers around the globe, where you gain rare insight into the world’s most important industries and institutions. Each mission presents different challenges, from the regulatory environment to the nature of the data to the user population. You will work to accommodate all aspects of an environment to drive real technical outcomes for our customers.

LMI is a consultancy dedicated to powering a future-ready, high-performing government, drawing from expertise in digital and analytic solutions, logistics , and management advisory services. We deliver integrated capabilities that incorporate emerging technologies and are tailored to customers’ unique mission needs, backed by objective research and data analysis. Founded in 1961 to help the Department of Defense resolve complex logistics management challenges, LMI continues to enable growth and transformation, enhance operational readiness and resiliency, and ensure mission success for federal civilian and defense agencies.

LMI has been named a 2024 Best Places to Work by BuiltIn! We are honored to be recognized as a company that values a people-centered culture, and we are grateful to our employees for making this possible! 

Responsibilities

 Perform various ETL functions 
 Debug issues related to delayed or missing data feeds 
 Write transformations and derive new datasets 
 Monitor build progress and debug build problems 
 Rapid development and iteration cycles with SME’s including testing and troubleshooting application issues 
 Containerize workflows 
 Data modeling and metadata tagging 
 Create and maintain data access and sharing pipelines 
 Work with technical and non-technical team members to derive solutions 
 Develop architecture diagrams to proposed problems 
 Configuring cloud resources for data pipelining, storage, and retrieval 

Qualifications

 Bachelor’s degree in data science, mathematics, statistics, economics, computer science, engineering, or a related business or quantitative discipline ( Masters degree preferred) 
 3 + years of work experience in data engineering, data science, software engineering or relevant other hands-on-keyboard experience 
 Proficiency with programming languages such as Python , Java or similar languages. 
 Working knowledge of databases and SQL; preferred qualifications include linking analytic and data visualization products to database connections 
 Experience with Test Driven Development (TDD) 
 Experience with Open-Source Tools, such as: Superset, Airflow, Kafka , Postgres, Docker, Helm (or similar) , Kubernetes, Spark, etc 
 Experience building production-ready data pipelines 
 Experience developing commercial data products and services 
 Experience with streaming data is preferred 
 Experience with building solutions on both cloud services and on-prem deployments 
 Experience with the restrictions of working in classified engineering environments is preferred 
 Knowledge or experience with federal ATO/RMF processes 
 Knowledge or familiarity with IDAM/ICAM/IAM 
 Ability to work effectively in teams of technical and non-technical individuals. 
 Skill and comfort working in a rapidly changing environment with dynamic objectives and iteration with users. 
 Demonstrated ability to continuously learn, work independently, and make decisions with minimal supervision. 
 Proven track-record of strong communications including feedback gathering, execution updates, and troubleshooting. 
 Minimum clearance requirement: Ability and willingness to obtain a Secret clearance.","Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Comunicación, Datasets, Modelado de datos, Resolución de incidencias y SC Clearance",Solicitar
https://www.linkedin.com/jobs/view/3980797638/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=dLl0SNH0sUbkgiIoOvs9fg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer-Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,Estados Unidos,"Acerca del empleo
Iron EagleX is a veteran owned defense contracting company based in Tampa, FL. 

It is our mission to provide solutions to the most challenging technical problems facing the Department of Defense while simultaneously making a positive impact on our employees and community.

Responsibilities

 Job Description: 

Iron EagleX is looking for a Remote Data Engineer who knows how hardware and software components fit together and integrate with cloud-based resources, to help us modernize a DoD client's data architecture. We are seeking an engineer who is comfortable stepping outside of their comfort zone and learning new technologies to propel our command into the future.

 Job Duties Include (but not limited to): 

 Leverage expertise in cloud-based data architectures to support mission-critical operations. 
 Recommend tools and capabilities based on your research of the current environment and knowledge of various on-premises, cloud-based, and hybrid resources. 
 Work with the client's migration teams to implement strategy and architecture design. 

Qualifications

 Required Skills & Experience: 

 SQL Database experience 
 Advana (or generally DataBricks) experience 
 S3/Blog Storage integration 
 Python scripting 
 PowerBI Ecosystem (primarily concerned with establishing & maintaining DataFlows) 
 3-5 years of experience with non-proprietary big data technologies (Presto, Hive, Spark, Flink etc.) - Presto/Trino is preferred 
 3-5 years of experience with demonstrated strength in data lake/warehouse technical architecture, infrastructure components, and ETL/ELT pipelines 
 3-5 years of experience with workflow management engines (i.e., Airflow, AWS Step Functions, Prefect) 
 Experience understanding requirements, analyzing data, discovering opportunities, addressing gaps, and communicating them to multiple individuals and stakeholders 
 3-5 years of experience in a software development, infrastructure, or Cloud operations role 
 Experience with multiple operating systems, including LINUX, UNIX, and Windows-based operating systems 
 Experience with Amazon Web Services (AWS) 
 Experience with deploying containerized microservices in Kubernetes infrastructure 
 Knowledgeable about data security methods (ABAC, etc.) and data lakes 
 Required travel to Jordan for 3 weeks at a time, up to 3 times over the course of a year 
 Due to U.S. Government contract requirements, only U.S. citizens are eligible for this role. 

 Desired Skills: 

 Databricks experience is a plus but not required. 
 Experience with applying DoD security technical implementation guides (STIGs) and automating that process 
 Experience with working in defense or intelligence community cloud environments is a plus 
 Experience with developing and managing machine images or templates to automate cloud deployments 
 Experience with optimizing cloud environments, including leveraging native cloud capabilities, right-sizing servers, expanding storage, and tracking cost efficiencies at the microservice level 
 Experience with configuring and aggregating logs for data analysis using Splunk or ELK solutions 
 Experience with Delta Lake is highly desirable 

 Education & Certifications: 

 Bachelor’s degree in a STEM field with preference towards Computer Science and Software Engineering. 
 A Certified Data Management Professional certification is preferred. 

 Security Clearance: 

 An active TS/SCI security clearance is Required 

 Benefits: 

 National health, vision, and dental plans 
 20 days of PTO and 11 paid holidays 
 Life Insurance 
 Short and long term disability plans 
 401(K) retirement plan 
 Incentive and recognition programs 
 Relocation opportunities 

Iron EagleX is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, among other things, or status as a qualified individual with disability.","Airflow, Amazon Web Services (AWS) y Extraer, transformar y cargar (ETL), AWS Step Functions, Arquitectura técnica, Gestión de flujos de trabajo, Inglés como lengua extranjera, Lagos de datos, Operaciones en la nube y Prefect Dataflow Automation",Solicitar
https://www.linkedin.com/jobs/view/3952454005/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=FIiavDoI7jXSaJciPAPfsw%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,"Scott AFB, IL","Acerca del empleo
Here at UNCOMN, our mission is to empower systems thinkers to create elegant solutions to complex problems – to improve the systems that improve our communities. Our team members apply their natural curiosity and grit to discover elegant solutions for our clients’ most complex organizational, logistics, process, data, and technical challenges, with the overall goal of building great businesses that contribute to great communities.

We’re an award-winning firm, one of the country’s fastest-growing and—more importantly—a consistent ‘Top Workplace’ as evaluated by our own employees. We are a values-driven organization (see the Core Values section of our website) and we’re looking for new Uncommon Geniuses to join our growing team, so if you’re a person who likes to solve problems, fix things, build things, tweak things, or otherwise show creative flair, you might just be an ""UNCOMN Genius"" and we encourage you to check out the specifics of this position below!

UNCOMN is seeking a Junior Data Engineer to:

Automate routine tasks, maintenance, and monitoring. 
Accomplish basic data retrieval, manipulation, and reporting tasks (e.g., aggregations, joins).
Accomplish basic data processing tasks.
Accomplish basic database administration and tuning tasks (e.g., Schemas, Tables, Indexes, optimize performance).
Ensure data integrity and availability in case of system failures or data corruption. 
Apply skills to an advanced understanding and support the completion of client activities. 
Under supervision, develop and complete quality project deliverables. 
Apply applicable project-related tools and processes with limited direction. 
Independently utilize critical thinking skills. 
Learn to assess and estimate the required level of effort to a task/project based on experience. 
Collaborate closely with data engineers and team lead, to understand their data needs, provide timely support, understand business requirements, and translate them into effective data structures. 


Requirements

At least 1 year of experience with exposure to Oracle databases (Oracle RDS on AWS preferred) and SQL. 
Prior exposure to project management and documentation tools. 
Must hold an active an IAT Level II certification; CompTIA Security+ CE highly preferred.
Excellent communication skills, with the ability to communicate technical concepts to non-technical stakeholders.
Strong problem-solving skills and the ability to think strategically about complex technical challenges.
Ability to work independently and as part of a team in a fast-paced, deadline-driven environment.
Must be eligible to obtain a Secret clearance, granted by the US Government, which requires US citizenship. The government also uses 13 adjudicative guidelines to determine an individual's eligibility.


Preferred

Hands-on experience with Jira for project management and issue tracking, Confluence for documentation and collaboration, Oracle RDS on AWS, and/or Toad for Oracle. 
Bachelor’s degree or higher in Computer Science, Engineering, Information Technology, or another related field.
Active Secret clearance granted by the US Government.


NOTE: This position is contingent upon contract award.

Why UNCOMN?

Flexible PTO effective on day 1*
7 Paid Holidays & up to 3 Floating Holidays*
Eligible for Health Benefits on day 1*
401K Safe Harbor Match Program*
Training and Education Assistance
*Must be a full-time employee


Don’t meet every single requirement? We’re dedicated to building an uncommon, inclusive, and authentic workplace, so if you’re excited about this role but your experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.","Ingeniería de datos y Pensamiento crítico, Amazon RDS, Ciencias de la computación, Comunicación, Necesidades empresariales, Procesamiento de datos, Project Based, Reflexión estratégica y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3947504615/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=jfDDPD8W4%2FbIVwYbfStyYA%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer - scikit-learn,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Nueva York, NY","Acerca del empleo
Job Title: Data Engineer

Job Location: New York, NY (Hybrid)

Job Type: Full Time

Expected Start Date: ASAP

Job Description

Must be strong with Python for ML pipelines specifically with Py torch and scikit-learn

AWS is required, building pipelines within

Should have a background in LLM (langchain, agents, extensive prompt engineering)

The ""strong additional requirements"" below are required.

Responsibilities

Ingesting, structuring and analyzing a wide range of unstructured datasources 
Designing, maintaining and orchestrating data pipelines in an AWS environment for production processing and training flows 
Continuously evaluate, analyze, test and improve the quality, privacy and performance of our data systems 
Contribute across the product, where - from front-end UX and product design, API/systems architecture and ML processing/training 

Minimum Qualifications

3+ years of experience ingesting, analyzing and structuring a wide variety of datasources
Significant experience building and maintaining data pipelines in a production environment
Strong database/SQL, python, pandas (or equivalent) experience 
Prior experience working in fast paced environments and tackling problems across the stack with quick iterations while maintaining a high quality bar. 

Strong Additional Qualifications

Significant healthcare data experience 
LLM experience (langchain, agents, extensive prompt engineering) 
MLE Experience - pytorch, scikit-learn, etc.. 
Extensive production AWS, container and/or data orchestration experience 
Full stack development experience (JS/TS/Node in particular)","Aprendizaje automático, Pandas (Software), PyTorch, Python, SQL y Scikit-learn, JavaScript, Node.js, Stack y TS",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979171094/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=2ABcApM8w824bw46lLVCxg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 1 semana,"Austin, TX","Acerca del empleo
Description

Data Engineer

Come build the future as a Data Engineer at Amazon, where you will be inspired working along best-in-class inventors and innovators! You will have the opportunity to create meaningful experiences that deliver on the ever-evolving needs of our customers, and your work will impact millions of people around the world.

As an Amazon Data Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. You will find that there is an unlimited number of opportunities within Amazon, where developing your career across a wide range of teams is highly supported. We are committed to making your work experience as enjoyable as the experiences you’ll be creating for our customers.

Apply now and you will be eligible for Amazon Data Engineer positions that are based on your preferred location, team, and more. We’re hiring across Amazon Stores in the United States and Canada.

Teams with available positions include, but are not limited to:

 Consumer Technology: Build new generation features and products for amazon.com, constantly improving the Customer and Seller experience for billions around the globe. Whether building site wide features such as reviews and recommendations, category specific software for the likes of Pharmacy, Electronics, Digital Software and Video Games or seller infrastructure, there are a variety of complex problems to tackle using a range of technologies in the design of your technical solutions.
 Operations Technology: Shape the future of transportation planning and execution on a global scale, that impacts hundreds of fulfillment centers, thousands of Amazonians, and millions of customers across the world. Your technology will support thousands of operators worldwide to design, build and run the best-in-class Amazon transportation network. We are building intelligent software to make transportation more reliable, faster, and less costly, providing a better and less expensive experience for our customers.
 Human Resources Technology: Create a seamless experience for millions of Amazonians and/or candidates. Whether supporting technologies for onboarding, time and attendance, compensation, amazon.jobs, or recruiting, you’ll deliver robust feature sets, elegant designs, intuitive user interfaces and systems that make it easy for Amazonians to excel at performing critical business functions.

About Us

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance be-tween your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign opportunities based on what will help each team member develop into a better-rounded contributor.

Inclusive Team Culture

Here at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Key job responsibilities

 Design, implement, and support a platform providing secured access to large datasets.
 Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
 Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
 Tune application and query performance using profiling tools and SQL.
 Analyze and solve problems at their root, stepping back to understand the broader context.
 Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
 Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
 Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
 Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com LLC - A03

Job ID: A2703397","AWS Lambda, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Datasets, Modelado de datos y Object Storage",Solicitar
https://www.linkedin.com/jobs/view/3984547410/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=SZyZFTYJyN%2BK6%2Bf2vjfjlw%3D%3D&trk=flagship3_search_srp_jobs,Data engineer,"78 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,Estados Unidos,"Acerca del empleo
Data Engineer - c2c Fully Remote

The Job:

Collaborate as part of a cross-functional Agile team to create and enhance software that enables state of the art, next generation Big Data & Fast Data applications. 
Build software and frameworks to automate high-volume and real-time data delivery between our cloud based data platforms and applications. 
Build data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners
Leverage DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of software utilizing tools like Jenkins, Maven, Nexus, Chef, Terraform, Ruby, Git and Docker
Perform unit tests and conduct reviews with other team members to make sure the code is rigorously designed, elegantly coded, and effectively tuned for performance
Develop and deploy distributed computing Data applications using Spark or Pyspark
Utilize programming languages like Python and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Sql experience
ETL experience 

Basic Qualifications:

Bachelor's Degree or military experience
At least 3 years of professional work experience in data engineering
At least 3 years of experience with Python
At least 3 years of experience with Sql
At least 3 years of experience with Spark or Pyspark
At least 3 years of experience with ETL development 
At least 2 year of experience working with cloud data capabilities - AWS

Python Developer - 76-78/hr c2c - 2 roles one can sit remotely, one will eventually sit in Plano TX

Responsibilities:

Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions

Collaborate across teams to deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment

Work with cloud native stack, build on AWS, using technologies like Serverless/Lambdas, Kubernetes, etc

Work with Product Owners or downstream teams to understand business requirements and desired application capabilities

Work with frontend-teams to establish API contracts to realize features

Write unit and integration tests

Collaborate with other backend teams to design/build solutions across a large enterprise

Write extensible, thoughtful, well-structured, maintainable code

Follow software engineering best practices to build modular and sustainable software

Basic Qualifications:

3+ years of software development

Agile development

Source code versioning with Git

Strong communication skills

2+ years of experience in at least one of the following: Python

2+ years of experience working on high-volume scalable applications

1+ years of experience in Agile practices

1+ years of experience with AWS, GCP, Microsoft Azure, or another cloud service

2+ years of experience working with AWS managed services (e.g. Lambda, DynamoDB, Kinesis)

2+ years of experience working with CICD, Jenkins

Preferred Qualifications:

Advanced degree in software engineering related field

Mentorship of junior developers

Object Oriented and knowledge of classic Design Patterns

Experience in open source frameworks

Working in an multi-team/organizational level collaborative environment","Amazon Web Services (AWS), Apache Spark, Extraer, transformar y cargar (ETL), Google Cloud , Ingeniería de datos , PySpark y Python, Amazon Redshift, Comunicación y Microsoft Azure",Solicitar
https://www.linkedin.com/jobs/view/3982297915/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=cW0cWxM%2Ft%2B7QJOnG2uWzWQ%3D%3D&trk=flagship3_search_srp_jobs,Cloud Data Engineer- Alteryx - 100% Remote - 1921,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 días,"East Rutherford, NJ","Acerca del empleo
 Location: 100% Remote

Term: 12 to 36 months

Conract Type: Must be W2 only (no c2c, no 1099)

Salary: Upto $128/hr DOE

Project Details

Data management and analytics visualization are growing areas of emphasis across the client. As a data software engineer (visual analytics support facilitator) within Enterprise Data Services, you will be an integral part of a dynamic team responsible for the effective utilization and daily support of Alteryx Designer and Alteryx Server.
This position is responsible for contributing to the success of the System's Alteryx Shared Service by leading process improvement initiatives related to content management, platform utilization, customer onboarding, and end user technical support. This role requires versatility and a drive to learn and pursue continuous improvement with the ability to work well in a diverse and cross-functional team.
Projects will range in duration and complexity, and they will require direct consultation with business partners, technical subject matter experts, and IT service providers.

Key Activities Include But Are Not Limited To

Work with the team to manage and support an Alteryx ecosystem including Alteryx Server and Alteryx Designer.
Alteryx Designer directly involved supporting users with designing workflows, schedules and troubleshooting simple and complex technical problems.
Recognize and fill gaps as necessary to resolve tickets and address inquiries when service requests spike.
Create and maintain detailed internal process and self-service knowledge base documentation.
Selected candidate will support the Alteryx user community by building relationships and communicating frequently.

 

Qualifications & Job Requirements

Minimum of 2+ years experience with Alteryx Server & Alteryx Designer.
Alteryx certifications preferred.
Moderate to high skills in data visualization and data management required.
Moderate skill with scripting and query languages such as SQL, Python, and PowerShell required.
Experience with AWS or other public cloud providers strongly preferred.
Strong customer service and communication skills, including the ability to patiently explain technical concepts to end-users of varying expertise via formal presentations and informal coaching/consultation.
Strong customer-focus, using their expectations and user experience as a basis to define what success looks like for the ecosystem
Strong organization and time management skills required.
Associate degree from a two-year college or technical school, or equivalent combination of education and/or work-related experience.
Bachelor's degree from a four-year college or university preferred.
May require some weekend or evening hours.
Ability to work independently and with general supervision and direction. May consult with more senior staff in decision making.

Note

No 3rd party vendors or candidates 
US Citizenship Required - Federal requirement","Analítica, Analítica de datos, Ciencia de datos, Minería de datos, Query Languages, Visualización y Visualización de datos, Alteryx, Expertos en la materia y Lenguaje de consulta (query)",Solicitar
https://www.linkedin.com/jobs/view/3849047395/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=OogKchPSbMUn3dDbTTymeQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer 1,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Hoover, AL","Acerca del empleo
Apply Now

<< Return to Search Results

Elevate Data Engineer

For over 16 years, Brooksource has established and maintained relationships that are designed to meet your IT staffing needs. Whether it’s contract, contract-to-hire, or permanent placement work, we customise our search based upon your company’s unique initiatives, culture and technologies. With our national team of recruiters placed at 21 major hubs around the nation Brooksource finds the people best-suited for your business. When you work with us, we work with you. That’s the Brooksource promise.

Brooksource is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression, sexual orientation, marital status, military service and veteran status, physical or mental disability, protected medical condition as defined by applicable state or local law, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances.

Benefits & Perks

Brooksource offers competitive medical, dental, vision, Health Savings Account, Dependent Care FSA, and supplemental coverage with plans that can fit each employee’s needs. We offer a 401k plan that includes a company match and is fully vested after you become eligible, paid time off, sick time, and paid company holidays. We also offer an Employee Assistance Program (EAP) that provides services like virtual counseling, financial services, legal services, life coaching, etc.

Pay Disclaimer

The pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.

JO-2402-143536

Apply Now","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3974306998/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=JBkvR4j3KKHSN2W6MGwlKw%3D%3D&trk=flagship3_search_srp_jobs,Azure Data Engineer w/ Python/C#,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
Experienced using Databricks & Apache Spark
 Experienced using Azure Data Factory or Synapse Analytics","Apache, Apache Kafka, Apache Spark, Extraer, transformar y cargar (ETL), Herramientas ETL, Hive, Ingeniería de datos , Microsoft Power BI y Procedural Programming, Azure Databricks",Solicitud sencilla
https://www.linkedin.com/jobs/view/3981629691/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=JpqQ0NW8pD0VGtbdY6SrkA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Contractor),"65 US$/h - 75 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,Estados Unidos,"Acerca del empleo
Data Engineer 

Remote

Hourly Rate: $65 to $75/hour 

Overview: Looking for a skilled data engineer to join the team, focusing on building out data pipelines in AWS. This role is pivotal in supporting upcoming product launch. 

Key Responsibilities:
Collaborate closely with the team to build and optimize data pipelines in AWS using Python, Step Functions, AWS Lambda, Databricks, and DBT.
Implement bug fixes, code enhancements, and contribute to overall pipeline stability and efficiency.
Work with AWS Step Functions for orchestrating workflows and AWS Lambda for executing data processes.
Interface with data scientists to understand requirements and integrate their models and analytics into production pipelines.
Required Skills: 
Proficiency in Python; experience with libraries like Pandas and familiarity with Jenkins.
Strong experience as a data engineer (pref using databricks or spark) 
Ability to conduct unit & integration testing to ensure robust data pipelines
Nice-to-have Skills: 
Experience with DBT
AWS SAM for application deployment
Experience working on government projects, or compliance standards such as HIPAA and SOC2","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Pandas (Software), Azure Databricks, Bases de datos, Pruebas de integración y Terapia dialéctica conductual",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982968713/?eBP=BUDGET_EXHAUSTED_JOB&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=1U0jw2k4SjrnquSDogt5jA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 3 días,"Birmingham, AL","Acerca del empleo
Job Description

 Johnson Service Group (JSG) is a nationally recognized professional staffing and recruiting firm that is looking for an experienced Data Engineer to fill a one- year assignment in the Birmingham, AL area. 

NO CORP TO CORP APPLICANTS

Green Card holders accepted

This position may require some bending, lifting, climbing, prolonged sitting, prolonged walking, walking on gravel or crawling when necessary. 

Job Title: Data Engineer

Work Location: Birmingham, AL

Requirements

Bachelor’s degree in Computer Science, Information Technology, or related field
3+ years of experience in data engineering or related roles, preferably in a cloud environment
Working closely with data scientists and within a cloud environment, this role requires expertise in data engineering, cloud technologies, and collaborative problem-solving
Assist TO in designing, building, and maintaining scalable data pipelines to ingest, transform, and store large volumes of AMI data from various sources
Collaborate with data scientists to ensure seamless integration of data pipelines with analytical models and AI processes
Proficiency in programming languages such as Python, SQL, and Scala
Proficiency with Power BI for data visualization and reporting
Experience with cloud-based data platforms (e.g., Azure Databricks, AWS EMR)
Strong understanding of data modeling, ETL processes, and data warehousing concepts
Familiarity with big data technologies
Develop and implement data models and architectures optimized for AMI data analytics, ensuring efficiency, scalability, and data integrity
Implement best practices for data storage, partitioning, and indexing to optimize performance and facilitate analysis
Provide technical support and troubleshooting for data-related issues, ensuring the reliability and availability of data infrastructure

Is this position supporting a government-related project? No

Does this position require driving (excluding commute)? No

Does this position require personal protective equipment (PPE)? No

Estimated Start Date: 08/19/2024

Estimated Duration of Job Assignment: One Year

 Johnson Service Group (JSG) is an Equal Opportunity Employer. JSG provides equal employment opportunities to all applicants and employees without regard to race, color, religion, sex, age, sexual orientation, gender identity, national origin, disability, marital status, protected veteran status, or any other characteristic protected by law. #D650

Job Requirements

Data engineering, Cloud technologies, Data pipelines, AMI data, Scala, Power BI, Azure, Databricks, AWS EMR, Technical support","Almacenamiento de datos, Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Ciencias de la computación, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3980036449/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=NBaKDCEBCVYsyZ8HaIFiNg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Oakbrook Terrace, IL","Acerca del empleo
Darwill is a nationally recognized print and marketing communications firm based in the west suburbs of Chicago. As a premier provider of complex marketing products, including direct mail, employee communications and marketing collateral, we influence excellent results for CMO’s, Directors of Marketing and Print Production Professionals by providing ideas, workflow solutions, cutting edge production technologies and a seamless execution process. Our diverse product offering includes data acquisition, email appends, integrated marketing services, production workflows, custom print production, direct mail solutions, fulfillment and complete lettershop, bindery and mailing services.

Location: Oakbrook, IL
*Remote option on Tuesdays and Thursday other days work in the office for collaboration.

Responsibilities/Essential Functions:
Design, develop, and maintain robust data integration pipelines for Darwill's growing data sets and product offerings
Build data validation testing frameworks to ensure high data quality and integrity
Collaborate with sales, product, and application teams to take requirements from idea to production
Develop, maintain, and manage reports and dashboards in Tableau
Write and maintain documentation
Stay current with modern data trends, experimenting, and learning new tools, and mentoring members of the team

Qualifications:
Advanced degree in Computer Science
5+ years designing and developing ETL processes
Expert in problem-solving, testing, debugging, and troubleshooting
Expert command writing SQL, as well as optimizing SQL
Expert command of modern ETL development (e.g. SSIS, Talend, Altreyx)
Hands-on experience in AWS (Redshift, S3, Aurora, Glue)
Hands-on experience with BI Reporting and Dashboard development (e.g. Tableau, Looker, Power BI or Qlikview)
Ability to build architecture diagrams, data dictionaries, ER diagrams and source to target mappings
Comfortable working with non-standardized or unstructured data
Excited to tackle ambiguous business problems in a fast-paced environment

Work Environment/Physical Demands:
This job requires you to sit in a cubicle or office, on a computer (1 or more monitors) for the majority of the day.
May be required to work extended/evening hours as needed.","Amazon Web Services (AWS), Extraer, transformar y cargar (ETL), SQL, SQL Server Integration Services (SSIS), Tableau y Talend, Amazon Redshift, Amazon S3, Ciencias de la computación, QlikView y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982846960/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=iAIHPbCXP5ls1NiN5nT5Lg%3D%3D&trk=flagship3_search_srp_jobs,Maximo Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",Publicado de nuevo hace 3 días,Estados Unidos,"Acerca del empleo
· Complex Project delivery via multiple teams with inter-dependency of work products.
· Leadership of multiple team projects.
· Experience with multiple vendor teams with differing project management approaches and resource allocation.
· Direct experience working with Product Managers, PMO, and Stakeholders to provide project status and to translate leadership directives into actionable tasks.
· Leadership of database centric conversion projects.
· Leadership or design experience in Application System conversions/migrations.
· Experience with system integrations as well as bulk data movement design patterns.
· Depth of knowledge in areas enabling the ability to mentor and assist developers.
· System Architecture design and analysis.
· Data Architecture design and analysis.
· Experience with Enterprise Asset Management systems.","Ingeniería de datos, Maximo",Solicitud sencilla
https://www.linkedin.com/jobs/view/3829316747/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=IRj9KeTZ%2B4MNMp9QYfMkeA%3D%3D&trk=flagship3_search_srp_jobs,Hybrid Work - Need Data Engineer/Analyst in Piscataway NJ,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 5 meses,"Municipio de Piscataway, NJ","Acerca del empleo
Piscataway, NJ – 3 days onsite a week, must be local to NJ and under 60 mins

need Valid LinkedIn

Must be W2 or 1099

FInal interview most likely will be F2F

Looking for someone with 5+ years of exp. This is a mid junior role

Do they have a minimum of 5 years of IT experience?
Do they have experience with Tableau reporting?
Experience with Alteryx?
Experience with Marketo?

Job Description

Position Overview:

As a Junior Data Engineer with a specialization in Marketo and Alteryx, you will play a crucial role in extracting, transforming, and delivering valuable data insights to support our business initiatives. You will work closely with cross-functional teams, serving as a bridge between technical data operations and business stakeholders.

Key Responsibilities

Data Integration: Collaborate with the IT and Marketing teams to design and implement data integration pipelines that connect Marketo with our data infrastructure.

Data Manipulation: Leverage Alteryx to transform, cleanse, and manipulate raw data from various sources into formats suitable for analysis, reporting, and visualization.

Data Quality: Ensure data accuracy, consistency, and completeness through data profiling, validation, and data cleansing techniques.

ETL (Extract, Transform, Load): Develop ETL processes to move, transform, and load data from source systems to data warehouses.

Reporting and Visualization: Create and maintain dashboards and reports to provide business stakeholders with actionable insights derived from Marketo data.

Collaboration: Work closely with marketing and sales teams to understand their data requirements and help them make data-driven decisions.

Documentation: Maintain clear and comprehensive documentation for data processes, data models, and transformation workflows.

Data Security: Ensure compliance with data privacy and security regulations and best practices.

Adaptability: Stay current with industry trends and new data technologies, providing suggestions for improvements and optimization.","Arquitectura de datos, Ingeniería de datos , Tableau y Visualización de datos, Alteryx, Limpieza de datos, Manipulación de datos, Marketo, Modelo de datos y Perfiles de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3975664606/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=UpGp7STCNO9r5CFQ8y7Jwg%3D%3D&trk=flagship3_search_srp_jobs,Data & Infrastructure Engineer,"80 US$K/año - 105 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 semana,"Coral Gables, FL","Acerca del empleo
Overview

About Bayview Asset Management, LLC

Bayview Asset Management, LLC (“Bayview”), initially founded in 1993, is an investment management firm focused on investments in mortgage and consumer credit, including whole loans, asset backed securities, mortgage servicing rights, and other credit-related assets.

With over 1,550 employees, our corporate headquarters is uniquely situated in the beautiful community of Coral Gables, Florida, located near the shores of Biscayne Bay and minutes from Miami Beach. The firm has additional asset management offices in New York, London, and Geneva, and loan servicing and origination affiliates in seven U.S. states and Milan, Italy. As of February 29, 2024, Bayview managed approximately $18 billion in assets under management.

About MSR Portfolio Management

The Bayview Mortgage Servicing Rights (“MSR”) Fund along with its wholly owned subsidiary Lakeview Loan Servicing (“Lakeview”) is the largest of Bayview’s funds. Lakeview is the top servicer of agency MSRs in the United States, and actively sources new MSRs across its origination platform and bulk and co-issue MSR businesses.

MSRs are essentially interest-only (IO) cash flow strips on underlying pools of mortgages. Like IOs on any other callable bond, MSRs are susceptible to prepayment and credit risk. Yet it is so much more than that - the MSR owner is entitled to many different income and expense cash flows. On a single MSR asset, we model and monitor over 60 types of cash flows.

Your Role: MSR Financial Analyst - Data & Infrastructure

In this position, the analyst will be responsible for providing high-quality analytics to support MSR bidding strategies and portfolio management functions. The analyst will have the opportunity to gain exposure to various aspects of the mortgage industry, collaborate with different departments inside Bayview and contribute to important investment decisions in quantitative strategies. The ideal candidate will be highly analytical with strong technical skills and a desire to learn and grow within the MSR Portfolio Management team.

Responsibilities include, but are not limited to:

Prepare in-depth analytics for MSR sourcing strategies & business development;
Support securitization & trading of other mortgage-related products, such as XS IOs, TBAs, specs,etc;
Integrate data from multiple sources to meet business requirements;
Conduct independent research and analysis using cash flow modeling, regression techniques, and other applicable quantitative and qualitative methods;
Improve data surveillance and develop data visualization tools;
Contribute to the automation initiatives of MSR Portfolio Management team;
Participate in MSR team meeting & training sessions to gain exposure to mortgage industry;
Other duties as needed or required.

What We Look For

0 to 3 years of work experience
Bachelor in Finance/ Business Analytics/ Computer Science/ Math/ Statistics/ Engineering or related fields. Master’s degree is a plus.
Strong experience with Microsoft SQL Server, database management & data visualization using Tableau/Power BI.
Advanced proficiency in Excel and VBA
Intermediate proficiency in Python
Great attention to detail and the ability to meet strict deadlines
Ability to multi-task in a fast-paced environment
Strong verbal and written communication skills
Exceptional analytical, problem-solving, and strategic thinking skills

LOCATION & COMPENSATION:

This role will be based in Bayview’s Coral Gables, FL office.
Base compensation is expected to be $80-105k with the opportunity for incentive compensation including bonus compensation. 

CERTIFICATIONS, LICENSES, AND/OR REGISTRATION

N/A. 

PHYSICAL DEMANDS AND WORK ENVIRONMENT 

The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

While performing the duties of this job, the employee is regularly required to sit and use hands to handle, touch or feel objects, tools, or controls. The employee frequently is required to talk and hear. The noise level in the work environment is usually moderate. The employee is occasionally required to stand; walk; reach with hands and arms. The employee is rarely required to stoop, kneel, crouch, or crawl. The employee must regularly lift and/or move up to 10 pounds. Specific vision abilities required by this job include close vision, color vision, and the ability to adjust focus.

EEOC 

Bayview Asset Management is an Equal Employment Opportunity employer. All aspects of consideration for employment and employment with the Company are governed on the basis of merit, competence and qualifications without regard to race, color, religion, sex, national origin, age, disability, veteran status, sexual orientation, or any other category protected by federal, state, or local law.","Analítica, Ciencia de datos, Visual Basic for Applications (VBA) y Visualización de datos, Administración de bases de datos, Comunicación, Comunicación escrita, Necesidades empresariales, Reflexión estratégica y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3943328709/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=Orl3LYa8u1y7%2BZOA%2Bj3C%2Fg%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Analyst/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Colorado Springs, CO","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3907983785/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=3jCJMGOcfsMvJUNX4Rg9ig%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Scientist/Engineer - Entry/Junior Level,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 meses,"Fresno, CA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this market also we have been able to help our candidates get multiple job offers and $100k + salaries.

please check the below links to see the success outcomes of our candidates  our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

We regularly interact with the Top Tech companies to give our candidates a competitive advantage see us exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, Google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry?

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

https://www.youtube.com/watch?v=OFoqPTNORew 

https://www.youtube.com/watch?v=-HkNN1ag6Zk 

https://www.youtube.com/watch?v=OAFOhcGy9Z8 

https://youtu.be/bJJl27D8bh0 

We are looking for the right matching candidates for our clients 

REQUIRED SKILLS For Java /Full Stack/Software Programmer

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
Highly motivated, self-learner, and technically inquisitive 
Experience in programming language Java and understanding of the software development life cycle 
Project work on the skills 
Knowledge of Core Java , javascript, C++, or software programming 
Spring boot, Microservices, Docker, Jenkins, and REST API experience 
Excellent written and verbal communication skills 

For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
Project work on the technologies needed 
Highly motivated, self-learner, and technically inquisitive 
Experience in programming language Java and understanding of the software development life cycle 
Knowledge of Statistics, SAS, Python, Computer Vision, and data visualization tools 
Excellent written and verbal communication skills 

Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow 

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

No phone calls please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3964479139/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=vO4Oys3NxSPQR80oZfkYsA%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Programmer/Coder/Developer/Data Scientist/Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 semanas,"San Rafael, CA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates can achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergistic understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and are looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Positions 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernetes, and REST API experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who match client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or C2C candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación y Programación, Ciclo de vida de desarrollo de software (SDLC), Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3818361624/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=c45pnw95WN5IFyP%2BHCc1Gw%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Scientist/Engineer - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Tampa, FL","Acerca del empleo
The Job Market is Challenging due to almost 300,000 Tech Layoffs since October 2022 due to which thousands of laid off Techies are competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications forget about getting client interviews. As the Saying goes “when the Going gets tough the Tough get going” Candidates who want to make a tech career they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.

Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don’t focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$’s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs. Clients now post covid can also hire remote workers which increases even more competition for jobseekers.

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000’s of candidates get jobs at technology clients like apple, google, Paypal, western union, bank of america, visa, walmart labs etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT Industry

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.

Please check the below links to see success outcomes of our candidates

https://www.synergisticit.com/candidate-outcomes/

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

Oracle CloudWorld Event (OCW) Las Vegas 2023/ 2022 | SynergisticIT - YouTube

https://youtu.be/-HkNN1ag6Zk

https://youtu.be/Rfn8Y0gnfL8

https://www.youtube.com/watch?v=OAFOhcGy9Z8

https://www.youtube.com/watch?v=EmO7NrWHkLM

https://www.youtube.com/watch?v=NVBU9RYZ6UI

https://www.youtube.com/watch?v=Yy74yvjatVg

For preparing for interviews please visit https://www.synergisticit.com/interview-questions/

We are looking for the right matching candidates for our clients

Please apply via the job posting

REQUIRED SKILLS For Java /Full Stack/Software Programmer

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Project work on the skills
Knowledge of Core Java , javascript , C++ or software programming
Spring boot, Microservices, Docker, Jenkins and REST API's experience
Excellent written and verbal communication skills

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Project work on the technologies needed
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciencias de la computación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3985206125/?eBP=BUDGET_EXHAUSTED_JOB&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=DPTRE1gVPX%2BUxWI6acE1HA%3D%3D&trk=flagship3_search_srp_jobs,Sr. Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 días,"Chicago, IL","Acerca del empleo
Apply Now

<< Return to Search Results

Sr. Data Engineer 

Contract 

100% Remote

About the role: 

You, as a Senior Data Engineer, will join a small team to design, develop, and maintain data sourcing, ingestion, and transformation solutions that will be used in reporting & analytics, data science, and other consuming applications. You will collaborate on building and maintaining the foundation of a world class data and analytics platform.

What You'll Do

Design and lead development of data ingestion and transformation pipelines in Databricks 
Design, develop, and maintain automation pipelines for CI/CD, infrastructure-as-code, observability and alerting, and all other aspects of a world-class Databricks platform 
Initiate documentation, knowledge-sharing, and creating of operating procedures 
Mentor junior developers and help them achieve more advanced skills 

What We're Looking For

5+ years of experience in the design and development of data pipelines, data marts, and data warehouses. 
2+ years of experience writing Databricks spark jobs and/or DLT pipelines 
Excellent analytical and problem-solving ability with strong attention to detail and accuracy. 
Knowledge of AWS or another cloud providers offered capabilities 
Excellent understanding of data warehousing concepts and dimensional data modeling. 
Hands-on experience with troubleshooting performance issues and fine-tuning queries. 
Strong ability to handle multiple tasks and adapt to evolving business and technical environments. 
Self-starter with the ability to work independently, take initiative, and learn new skills. 
Excellent written and oral communication skills, with the ability to articulate complex processes to individuals of varying technical abilities. 
Firm knowledge of data engineering tools including version control systems using Git, Bitbucket, SVN, or Team Foundation. 
Excellent Python skills with experience with other languages a big plus 
Experience in Microsoft SQL Server and SSIS 

For over 16 years, Brooksource has established and maintained relationships that are designed to meet your IT staffing needs. Whether it’s contract, contract-to-hire, or permanent placement work, we customise our search based upon your company’s unique initiatives, culture and technologies. With our national team of recruiters placed at 21 major hubs around the nation Brooksource finds the people best-suited for your business. When you work with us, we work with you. That’s the Brooksource promise.

Brooksource is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression, sexual orientation, marital status, military service and veteran status, physical or mental disability, protected medical condition as defined by applicable state or local law, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances.

Benefits & Perks

Brooksource offers competitive medical, dental, vision, Health Savings Account, Dependent Care FSA, and supplemental coverage with plans that can fit each employee’s needs. We offer a 401k plan that includes a company match and is fully vested after you become eligible, paid time off, sick time, and paid company holidays. We also offer an Employee Assistance Program (EAP) that provides services like virtual counseling, financial services, legal services, life coaching, etc.

Pay Disclaimer

The pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.

JO-2407-148358

Apply Now

Tagged as: Yes","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Data Marts, Ingeniería de datos y SQL Server Integration Services (SSIS), Atención al detalle, Comunicación, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3944123571/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=gngmg52xz3p4hFZ93lTQYQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 semana,"Dallas, TX","Acerca del empleo
Build and maintain ETL processes in SSIS and Azure Data Factory
Engineer scalable, reliable and performant systems to manage data
Develop, implement and optimize stored procedures and functions
Research and analyze data issues and provide automated solutions
Implement new technologies to enhance the optimization of current practices
Provide valuable suggestions regarding new ideas and technologies

Aptitudes y experiencia deseables
SQL","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Procedimientos de almacenado",Solicitar
https://www.linkedin.com/jobs/view/3983435376/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=tktVNIjssvpNRt3bFPCbDg%3D%3D&trackingId=wXwnSaX7NNirnB4H053tWw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,"Boston, MA","Acerca del empleo
Job Description- Primary Skills
SCALA, SPARK, SQL, HADOOP, AWS, HIVE, Spark SQL, HIVE QL, CICD (Continuous Integration/Continuous Delivery), VCS (GIT HUB)
Coding in Scala
Designing in of HADOOP ecosystem
Hands-on experience on AWS tools like EMR, EC2
Hands-on experience of SQL in Big Data: SQL, Spark SQL, Hive QL
Proficient in working with large data sets and pipelines
Proficient with workflow scheduling / orchestration tools
Well versed with CICD process and VCS
Databricks is a big PLUS","SQL y Scala, Historia clínica electrónica (HCE)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3973546998/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=65TFejP9d0jLs9HiBHzkRw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"85 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 2 semanas,Washington DC-Baltimore y alrededores,"Acerca del empleo
About BlueLabs
BlueLabs is a leading provider of analytics services and technology dedicated to helping our partners do the most good with their data. Our team of analysts, scientists, engineers, and strategists hail from diverse backgrounds yet share a passion for using data to solve the world’s greatest social and analytical challenges. Since our inception we’ve worked with more than 400 organizations ranging from government agencies, advocacy groups, unions, political campaigns, and international groups. In addition, we service an ever-expanding portfolio of commercial clients in the automotive, travel, CPG, entertainment, healthcare, media, and telecom industries. Along the way, we’ve developed some of the most innovative tools available in analytics, media optimization, reporting, and influencer outreach.

About the team
The BlueLabs Civic Tech practice revolutionizes the way government agencies use data to reduce the friction between residents and the essential services they use. Our team develops deep expertise within the areas our clients care about most, then builds data programs that create impact at scale. We work closely with internal government innovation groups, and build on the analytics methodology pioneered in e-commerce, advocacy, politics, and consumer finance.

About the role:
As a Data Engineer, you will play a critical role in our database upgrade roadmap, in support of our work for a large federal healthcare program. The Data Engineer will work with complex and nuanced data pipelines and will be responsible for the continuous development, review, and documentation of data wrangling and pipeline solutions. You should have experience working in a rapid development team in the past in which you were responsible for significant contributions to client data pipeline solutions.

In this position you will:
Develop, test, and operationalize the data pipelines that power our analytics.
Provide visibility into data transformations by designing and implementing data tests throughout existing pipelines.
Extract business logic (ETL/ELT, metrics, metadata) from current data systems into portable cloud-agnostic layers.
Analyze, build, and deploy data models, including relational models for data warehousing.
Plan and maintain data architectures that are aligned with business requirements.
Produce scalable, replicable code and engineering solutions that help automate repetitive data management tasks.
Work closely with data analysts to understand, identify and effectively respond to their specific needs.
Partner with other engineers to deploy and troubleshoot pipeline-related tools, automations, and integrations.
Work as part of a team to maintain a well documented, consistent codebase.
Support our analysts with support requests accessing data in our snowflake cluster.

What we are seeking:
3+ years of experience as a contributor to technical projects, such as working with complex data pipelines or software applications.
Experience working with production grade data warehouses using SQL queries and scripting languages (e.g., Python).
Experience with SQL transformation tools like dbt.
Strong background in database design and data modeling.
Experience with Snowflake, implementing a modern data stack, and contributing to large scale data migration efforts.
Effective communication skills when working with team members of varied backgrounds, roles, and functions.
Ability to manage your individual priorities and comfortably context-switch between active development, client discussion, and issue response.
Experience delivering on client priorities that operate on a regular deployment schedule.
Passion in applying your skills to our social mission to problem-solve and collaborate within a cross-functional, client-facing team environment.
The ability to successfully attain and maintain a Federal Public Trust background investigation that our government clients require; this includes a requirement that the individual has U.S. Citizenship or U.S. residency for three of the past five years.
Ability to hold a Public Trust certification (including residency requirement of 3+ Years in the USA)

What We Offer:
BlueLabs offers a friendly work environment and competitive compensation and benefits package including:
Salary: $85,000 annually
Premier health insurance plan
401K matching
Unlimited vacation leave
Paid sick, personal, and volunteer leave
13 paid holidays
15 weeks paid parental leave
Professional development stipend & tuition reimbursement
Employee Assistance Program (EAP)
Supportive & collaborative culture
Flexible working hours
Remote friendly (within the U.S.)
And more!

The salary range for candidates who meet the minimum posted qualifications reflects the Company’s good faith understanding and belief as to the wage range, and is accurate as of the date of this job posting.

To protect the health and safety of our workforce, as a company policy, BlueLabs strongly encourages all employees to be fully vaccinated against COVID-19 prior to beginning employment. BlueLabs adheres to all federal, state and local COVID-19 vaccination regulations. Except where prohibited by law, applicants who receive a conditional offer of employment will be required to produce proof of vaccination status prior to their first day of employment; if not the offer may be rescinded or employment terminated. BlueLabs will evaluate requests for reasonable accommodations for applicants unable to be vaccinated due to a religious belief, disability, pregnancy, or on an individualized basis in accordance with applicable laws.

At BlueLabs, we celebrate, support and thrive on differences. Not only do they benefit our services, products, and community, but most importantly, they are to the benefit of our team. Qualified people of all races, ethnicities, ages, sex, genders, sexual orientations, national origins, gender identities, marital status, religions, veterans statuses, disabilities and any other protected classes are strongly encouraged to apply. As an equal opportunity workplace and an affirmative action employer, BlueLabs is committed to creating an inclusive environment for all employees. BlueLabs endeavors to make reasonable accommodations to the known physical or mental limitations of qualified applicants with a disability unless the accommodation would impose an undue hardship on the operation of our business. If an applicant believes they require such assistance to complete the application or to participate in an interview, or has any questions or concerns, they should contact the Director, People Operations. BlueLabs participates in E-verify. EEO is the Law.

Collection of Personal Information Notice:
As you are likely aware, by submitting your job application, you are submitting personal information to our company. We collect various categories of personal information, including identifiers, protected classifications, professional or employment related information and sensitive personal information. We may retain and use this information for up to three years, in order to come to a decision on whether or not you are a good fit for our company. We may also retain or use some of this information to comply with any requirements under law, or for purposes of defending ourselves in any litigation. We do not use this information for any other purpose, or share it with third parties, unless you become an employee. To learn more, or to see our fully Notice to Job Applicants, please click here.","Arquitectura de datos, Python y SQL, Diseño de bases de datos, Experiencia en codificación, Ingeniería, Modelado de datos y Snowflake",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3892472435/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=ljOxst2GdA8uaaWeWi%2BtQw%3D%3D&trk=flagship3_search_srp_jobs,Data Science Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,Estados Unidos,"Acerca del empleo
Anywhere (Remote)

We are seeking a talented and experienced Data Science Engineer to join our team. As a Data Science Engineer, you will be responsible for developing and implementing data-driven solutions that leverage advanced analytics and machine learning techniques. You will collaborate closely with data scientists, software engineers, and business stakeholders to design, build, and deploy scalable and robust data science models and applications. Your work will play a crucial role in driving data-driven decision-making and delivering actionable insights to improve business outcomes.

Responsiblities

Work closely with cross-functional teams to understand business requirements and identify opportunities for applying data science and machine learning techniques
Design and develop data science models and algorithms that solve complex business problems and deliver actionable insights
Implement end-to-end data science solutions, including data collection, preprocessing, feature engineering, model training, evaluation, and deployment
Collaborate with data engineers to ensure the availability, quality, and reliability of data for analysis and modeling
Conduct exploratory data analysis to gain insights and identify patterns, trends, and anomalies in the data
Develop and deploy scalable machine learning models and algorithms that can handle large-scale datasets and real-time data streams
Optimize and fine-tune models for performance, accuracy, and scalability
Collaborate with software engineers to integrate data science models and algorithms into production systems and applications
Stay up to date with the latest advancements in data science, machine learning, and artificial intelligence and apply them to solve business challenges
Communicate and present complex technical concepts and findings to both technical and non-technical stakeholders in a clear and understandable manner
Collaborate with data scientists and domain experts to understand the domain-specific challenges and requirements and translate them into data science solutions

Requirements

Bachelor's or master's degree in computer science, data science, statistics, or a related field
Proven experience as a Data Science Engineer or in a similar role, with a focus on developing and deploying data science models and applications
Strong programming skills in languages such as Python or R, with experience in data manipulation, statistical analysis, and machine learning libraries (e.g., Pandas, NumPy, scikit-learn, TensorFlow, PyTorch)
Proficiency in SQL and experience with relational databases for data retrieval and manipulation
Solid understanding of data preprocessing, feature engineering, and model evaluation techniques
Experience with big data technologies and distributed computing frameworks such as Hadoop and Spark
Familiarity with cloud platforms and services, such as AWS, Azure, or Google Cloud, and their data-related services (e.g., S3, EMR, ML services)
Strong knowledge of machine learning algorithms and techniques, including supervised and unsupervised learning, deep learning, and natural language processing
Familiarity with software development practices and version control systems (e.g., Git)
Excellent problem-solving and analytical abilities, with keen attention to detail
Strong communication and collaboration skills, with the ability to work effectively in cross-functional teams and present complex concepts to non-technical stakeholders

Powered by JazzHR

8gD67zYcbL","Analítica de datos, Análisis exploratorio de datos, Base de datos relacional, Ciencia de datos y Ingeniería de datos, Bases de datos, Ciencias de la computación, Comunicación, Manipulación de datos y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3941692465/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=bsWV2B8Ut8shymoti2fQQQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
MISSION

Missions are accomplished with people, driving towards a greater purpose. Foley has been a trusted partner to motor carriers of all sizes. Our technologically advanced approach to DOT compliance automates many of the background screening and compliance requirements motor carriers must comply with- making it easier for them to onboard and retain the right driver for their business. At Foley we just don’t help our customers stay compliant – we help them thrive.

A positive attitude is everything. At Foley, we’ve created a culture that rewards kindness, enthusiasm, and a can-do attitude. Whether we’re working together internally or helping a customer solve a problem, we approach every challenge with a sense of humor, optimism, and the determination to succeed.

Data Engineer

Foley is looking for an exceptional Data Engineer with extensive experience in ETL/ELT Tools and Processes, General Database Development, Data Warehousing, Data Delivery Automation and Testing, and foundational engineering patterns and practices to be a key member of Foley’s engineering team.

As a Data Engineer at Foley, you will be a key member of the team that drives direction of our data analytics capabilities, evaluating current and prospective tools and technologies, and generally helping to grow Foley’s data product line! You are really excited about joining a team with massive potential for growth.

This is a REMOTE option- Those residing in CT, MA, SC, GA, FL, TX are welcome to apply!

Who You Will Work With

At Foley, we value our employees and treat them with respect while providing them with opportunities to make a difference. You will work with Foley teammates across the organization.

Because our departments work so closely together, we are always looking to improve our current process. We welcome new ways to work collaboratively with roles and departments. We cannot wait to hear your ideas!

What You Will Do

At first, you will

Learn how Foley’s products and services work and how Foley can have a positive impact on our customers’ business. 
Meet and get to know your teammates and stakeholders of our engineering organization. 
Understand our processes, procedures, and tools for development and deployment. 
Attend scrum events for all our development teams to understand how our technology and team works. 
Get up to speed on value of data products at Foley, understanding both internal and external customer needs. 

Once you get settled, you will

Maintain and evolve existing data pipelines using SQL Server Integration Services (SSIS) packages built using Visual Studio. 
Implement transformation logic in AWS Redshift data warehouse. 
Work with views and stored procedures to stage data from source systems for new and existing data pipelines. 
Design, build, test and deploy solutions to connect to data sources such as SQL Server databases, web services/APIs (from government or public databases, service vendors, and software partners), flat files on network and cloud storage, and other data source as needed. 
Build data extraction solutions using browser automation, robotic process automation, or other technology, as needed. 
Critically assess current technologies and processes and provide recommendations for improvement. 
Help establish and grow data governance policies and practice. 
Help foster a culture of safety, collaboration, and inquisitiveness. 

What We’d Like You To Have

Excellence in teamwork – sharing ownership and inviting criticism. 
Excellent grasp of fundamental engineering concepts. 
Experience in and extensive knowledge of Scrum or Agile development. 
Strong analytical and diagnostic skills, as well as excellent written and verbal communication skills. 
Desire to continually improve and a strong appetite for change. 
Ability to quickly adjust to changing priorities. 
Strong documentation, organizational, and planning skills. 
Extensive experience building and running data workflows using SQL Server Integration Services (SSIS). 
Extensive experience developing in a Microsoft SQL Server environment. 
Experience working within a version control system (branching, merging, pull requests, et al) 
Knowledge of cloud native technologies and patterns. 
Experience with infrastructure-as-code tools and products. 
Working knowledge of data workflow deployment automation and testing technologies. 
Diverse knowledge of database products and concepts, including Microsoft SQL, redshift, database versioning and deployment models, et al. 
Experience with Python, C#, or other programming languages. 
Experience working with web services/APIs to extract data from vendor solutions 
Experience in non-SSIS ETL/ELT tooling is a huge plus (Airbyte, Fivetran, et al) 

If you are an analytical and critical thinker with excellent organizational skills, attention to detail, and the ability to self-organize and meet deadlines, we encourage you to apply.

What You Will Love About Foley

The people! Our employees and customers consistently express the best thing about Foley is our close-knit, exceptionally talented teams. Check out our customers feedback – on Trustpilot 
Outstanding benefits. 3 medical plans to choose from, 2 level dental, and 2 level vision plans. Generous vacation, sick, and personal time off. 401K plan with a match. We’ve got your back so you can live your best life. 
It’s about ideas over egos. You will have the freedom to explore new ideas and approaches in an entrepreneurial environment, supported by a collaborative team. 
Professional growth. We open our roles to our employees first and encourage them to apply for growth opportunities. Our People Operations team is available to discuss career growth and help put a plan in place, helping employees achieve the growth they crave. 
Our environment! We celebrate success and believe in transparency and teamwork to get us there. We invest in collaboration tools so you can meet with your team face to face. Many of our roles are remote and we want to ensure our employees are engaged and can interact with their peers in a virtual space. 

What We Do, How We Do It

Too often, companies use a piecemeal approach when it comes to screening drivers. They might use one vendor to help recruit. Another to screen. And still, another to address the complex world that is compliance. This approach is inefficient, expensive, and redundant. Not to mention, it makes it too easy for things to fall through the cracks.

At Foley, we've built a company that effortlessly manages these three areas under one roof: recruit, screen, comply. Thanks to powerful technology combined with our compliance expertise, we're able to deliver a comprehensive solution to our customers and, as a result, a better overall customer experience.

Where We're Headed

We're always developing new solutions to slay tomorrow's recruitment, screening, and compliance monsters. At the core of these solutions is our vast collection of data—and the many ways to leverage it, whether that's developing software to calculate a company's compliance risk or implementing predictive analytics to identify the best drivers.

What It's Like To Work With Us

Diving deep into a niche industry and becoming an expert . . .

Continually growing and advancing . . .

Making lifelong friends during the process . . .

That Sums Up What It's Like To Work For Us. We're a 250+ Person Company On The Verge Of Explosive Growth Thanks To Our AI-powered Technology—and Where It's Headed With Predictive Analytics. If You'd Like To Board Our Rocket Ship, Check Us Out

www.foleyservices.com

Keywords

Data Architect

Database Engineer

Big Data Engineer

Data Systems Engineer

ETL Developer

Data Warehouse Engineer

Business Intelligence Developer

Data Specialist

Data Platform Engineer","Almacenamiento de datos, Analítica de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL Server Integration Services (SSIS), Amazon Redshift, Bases de datos, Comunicación y Efectos foley",Solicitud sencilla
https://www.linkedin.com/jobs/view/3961465052/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=leottltMys6YN9xoYLmAuw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Kansas, Estados Unidos","Acerca del empleo
This inclusive employer is a member of myGwork – the largest global platform for the LGBTQ+ business community.  

Experience Level: Experienced Hire

Categories

Engineering & Technology

Location(s):

Remote - United States, US
Remote - United States, US
Remote - United States, US
7 World Trade Center, 250 Greenwich Street, New York, New York, 10007, US
7575 Gateway Blvd., Suite 300, Newark, California, 94560, US
Remote - United States, US
Remote - United States, US

At Moody's, we unite the brightest minds to turn today's risks into tomorrow's opportunities. We do this by striving to create an inclusive environment where everyone feels welcome to be who they are-with the freedom to exchange ideas, think innovatively, and listen to each other and customers in meaningful ways.

If you are excited about this opportunity but do not meet every single requirement, please apply! You still may be a great fit for this role or other open roles. We are seeking candidates who model our values: invest in every relationship, lead with curiosity, champion diverse perspectives, turn inputs into actions, and uphold trust through integrity.

The Compliance & 3rd-Party Risk unit at Moody's leverages data-driven insights and cutting-edge technologies to help businesses deliver prudent risk management in the KYC, Supply Chain, and Trade Credit space. We are currently seeking a Data Engineer to join our dynamic and motivated Data & Analytics team. The ideal candidate will have a strong foundation in data engineering concepts such as pipelines, orchestration and streaming; as well as a passion for working with cutting-edge technologies to ensure clean, quality, and usable data is democratized to the business.

In this role, you will be highly involved working on creating foundational data models and data products within our data warehouse which supports various business and product reporting needs. In your day-to-day you will be working in a squad consisting of data engineers, analysts, software engineers, and product managers.

The Work

Develop and maintain product-specific data pipelines to ensure the timely and accurate flow of data from various sources to our Databricks platform.
With the guidance of technical peers, learn to work with business and other cross-functional team members to identify, integrate, and distribute data data from sources to destinations.
Work on project squads adhering to project management best practices and participating in scrum ceremonies
Gain an understanding of data usage patterns and requirements within the organization to design and implement appropriate data models in the data warehouse, ensuring compliance with industry regulations.
Utilize Databricks and AWS/GCP services to manage, store, and process data, ensuring optimal performance, security, and scalability while adhering to data protection standards.
Leverage orchestration tools, such as Databricks Workflows or Airflow, to orchestrate and monitor data pipelines, ensuring seamless integration, data quality, and compliance with regulatory requirements.
Implement data transformation and validation processes using dbt and SQL to ensure data accuracy, consistency, and regulatory compliance across the organization.
Continuously monitor, troubleshoot, and optimize data pipelines to ensure data reliability, performance, and adherence to best practices and industry regulations
Stay current with industry trends, emerging technologies, and regulatory changes to ensure the company remains at the forefront of data engineering best practices in the compliance and AML space.

Qualifications

0-2+ years of experience in data, or backend engineering roles
Bachelor's degree in Computer Science, Engineering, or a related field.
Some understanding of data engineering concepts such as orchestration & pipelining, data modeling, streaming and warehousing
Excellent problem-solving skills and the ability to work independently or as part of a team.
Eagerness to learn new technologies and adapt to industry trends and changes

For US-based roles only: the anticipated hiring base salary range for this position is $106,600.00 - $154,600.00, depending on factors such as experience, education, level, skills, and location. This range is based on a full-time position. In addition to base salary, this role is eligible for incentive compensation. Moody's also offers a competitive benefits package, including not but limited to medical, dental, vision, parental leave, paid time off, a 401(k) plan with employee and company contribution opportunities, life, disability, and accident insurance, a discounted employee stock purchase plan, and tuition reimbursement.

Moody's is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody's also provides reasonable accommodation to qualified individuals with disabilities or based on a sincerely held religious belief in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications.

For San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance.

This position may be considered a promotional opportunity, pursuant to the Colorado Equal Pay for Equal Work Act.

Click here to view our full EEO policy statement. Click here for more information on your EEO rights under the law. Click here to view our Pay Transparency Nondiscrimination statement. Click here to view our Notice to New York City Applicants.

Candidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody's Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.","Airflow, Almacenamiento de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Modelado de datos, Modelo de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3973393880/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=TDDOGDRG%2FfLs8w1Woc41Kw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,Estados Unidos,"Acerca del empleo
Experience Level: Experienced Hire

Categories:

Engineering & Technology

Location(s):

Remote - United States, US

At Moody's, we unite the brightest minds to turn today’s risks into tomorrow’s opportunities. We do this by striving to create an inclusive environment where everyone feels welcome to be who they are-with the freedom to exchange ideas, think innovatively, and listen to each other and customers in meaningful ways.

If you are excited about this opportunity but do not meet every single requirement, please apply! You still may be a great fit for this role or other open roles. We are seeking candidates who model our values: invest in every relationship, lead with curiosity, champion diverse perspectives, turn inputs into actions, and uphold trust through integrity.

he Compliance & 3rd-Party Risk unit at Moody’s leverages data-driven insights and cutting-edge technologies to help businesses deliver prudent risk management in the KYC, Supply Chain, and Trade Credit space. We are currently seeking a Data Engineer to join our dynamic and motivated Data & Analytics team. The ideal candidate will have a strong foundation in data engineering concepts such as pipelines, orchestration and streaming; as well as a passion for working with cutting-edge technologies to ensure clean, quality, and usable data is democratized to the business.

In this role, you will be highly involved working on creating a robust data model supporting a multitude of business functions. In your day-to-day you will be working in a squad consisting of other data engineers but working closely with other data roles.

The Work:

Develop and maintain product-specific data pipelines to ensure the timely and accurate flow of data from various sources to our Databricks platform.
With the guidance of technical peers, learn to work with business and other cross-functional team members to identify, integrate, and distribute data data from sources to destinations.
Work on project squads adhering to project management best practices and participating in scrum ceremonies
Gain an understanding of data usage patterns and requirements within the organization to design and implement appropriate data models in the data warehouse, ensuring compliance with industry regulations.
Utilize Databricks and AWS/GCP services to manage, store, and process data, ensuring optimal performance, security, and scalability while adhering to data protection standards.
Leverage orchestration tools, such as Databricks Workflows or Airflow, to orchestrate and monitor data pipelines, ensuring seamless integration, data quality, and compliance with regulatory requirements.
Implement data transformation and validation processes using dbt and SQL to ensure data accuracy, consistency, and regulatory compliance across the organization.
Continuously monitor, troubleshoot, and optimize data pipelines to ensure data reliability, performance, and adherence to best practices and industry regulations
Stay current with industry trends, emerging technologies, and regulatory changes to ensure the company remains at the forefront of data engineering best practices in the compliance and AML space.

Qualifications

1-2+ years of experience in data, or backend engineering roles.
Bachelor’s degree in Computer Science, Engineering, or a related field.
Some exposure and understanding of data modeling concepts such as the snowflake/Kimball model and Data Vault.
Some understanding of data engineering concepts such as orchestration & pipelining, data modeling, streaming and warehousing
Excellent problem-solving skills and the ability to work independently or as part of a team.
Eagerness to learn new technologies and adapt to industry trends and changes.

For US-based roles only: the anticipated hiring base salary range for this position is $106,600.00 - $154,600.00, depending on factors such as experience, education, level, skills, and location. This range is based on a full-time position. In addition to base salary, this role is eligible for incentive compensation. Moody’s also offers a competitive benefits package, including not but limited to medical, dental, vision, parental leave, paid time off, a 401(k) plan with employee and company contribution opportunities, life, disability, and accident insurance, a discounted employee stock purchase plan, and tuition reimbursement.

Moody’s is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody’s also provides reasonable accommodation to qualified individuals with disabilities or based on a sincerely held religious belief in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications.

For San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance.

This position may be considered a promotional opportunity, pursuant to the Colorado Equal Pay for Equal Work Act.

Click here to view our full EEO policy statement. Click here for more information on your EEO rights under the law. Click here to view our Pay Transparency Nondiscrimination statement. Click here to view our Notice to New York City Applicants.

Candidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody’s Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.","Airflow, Almacenamiento de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Modelado de datos, Modelo de datos, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3888456856/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=%2B8Dfc46Ou7jEsg94KE%2BGxw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Remote),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,Estados Unidos,"Acerca del empleo
Role: Data Engineer 

Location: 100% Remote

Duration: Full Time

Requirements

4+ years' experience developing data centric applications

Knowledge on Scala, DataBricks

4+ years' experience creating data transformation and validation logic

Strong understanding of streaming and batch data processing best practices.

Advanced knowledge of data formats (JSON, XML, Parquet, etc.) and data modeling practices.

Data modelling experience shaping and transforming data into third normal form and dimensional models.

Advanced understanding of best practices for structuring and organizing Data Lake file systems for large volumes of data.

Skills

Number of years' experience

Rating out of 5

Developing data centric apps

Scala and Databricks

Creating data transformation and validation logic

Data formats (JSON, XML, Parquet)

Data Modeling

Data Lake file systems

Thanks & Regards,

Arvind Kumar Bind

PH Number:- 469-750-0607

Email : Arvind.B@sparinfosys.com","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Azure Databricks, Lagos de datos, Modelado de datos, Procesamiento por lotes y Transformación de datos",Solicitar
https://www.linkedin.com/jobs/view/3914117516/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=AYOfRTAZ4LJvsKhAqDs8bg%3D%3D&trk=flagship3_search_srp_jobs,Junior AWS Data Engineer,"70 US$K/año - 90 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 mes,"Torrance, CA","Acerca del empleo
About Us:
LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.
 Job Title: 
Junior AWS Cloud Data Engineer

Work Location
Torrance, CA

Job Description:
We are seeking a highly skilled and motivated AWS Cloud Data Engineer to join our dynamic team. 
As a key member of our cloud engineering group, you will play a crucial role in designing, implementing, and maintaining data pipelines and workflows on AWS. Your expertise in S3, AWS Glue / EMR, AWS Redshift, Athena, SQL, and optionally Python/Java/Scala will be essential in ensuring the success of our data-driven projects.
Responsibilities:
Data Pipeline Development:
Design, develop, and maintain ETL processes using AWS Glue/EMR to extract, transform, and load data from various sources (including S3, ORC/Parquet/Text files) into AWS Redshift.
Optimize data pipelines for performance, scalability, and reliability.
Implement data aggregation, consolidation, and enrichment.
3+ years of experience with AWS tools and technologies (S3, EMR, Glue, Athena, RedShift)
Workflow Automation:
Utilize AWS Managed Airflow (Apache Airflow) to orchestrate complex data workflows.
Schedule, monitor, and manage data processing tasks efficiently.

Database Management:
Leverage your strong SQL skills to create and optimize database queries.
Strong knowledge of data storage and processing technologies, including databases (SQL and NoSQL), data lakes, and distributed computing frameworks (e.g., Hadoop, Spark).
5+ years of experience in data engineering, database design, ETL processes, and data warehousing.
Support Project Processes:
Work closely with cross-functional teams to understand project requirements and deliverables.
Provide technical support during project execution and troubleshoot issues as needed.
Optional Programming Skills:
Familiarity with Python/Java/Scala is a plus.

Qualifications:
Bachelor’s or Master’s degree in Computer Science, Information Technology, or related field.
AWS certifications (e.g., AWS Certified Data Analytics - Specialty) are advantageous.
Strong problem-solving skills and ability to work independently.
Excellent communication and collaboration skills.""



Benefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):

Benefits and Perks:
Medical Plan Covering Medical, Dental, Vision
Term and Long-Term Disability Coverage
Plan with Company match
Insurance
Time, Sick Leave, Paid Holidays
Paternity and Maternity Leave

The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.

Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting.

LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
 Safe return to office:
In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.","Amazon Web Services (AWS), Ingeniería de datos , Python y Scala",Solicitud sencilla
https://www.linkedin.com/jobs/view/3966335240/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=B%2F6qE12RgsOfI%2BbH2Sd6sQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - ETL Data Warehouse,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 semanas,"Sun Prairie, WI","Acerca del empleo
Position:  Data Engineer - ETL Data Warehouse (Cloud T ransformation + Datawarehouse)

Location: Remote Role

Job Description

Strong SQL and Complex queries 
Strong SQL Experience and query writing ability. 
Expertise in developing applications using BI and Data warehouse technologies 
Strong Unix, Shell Scripting, Autosys knowhow 
An extensive grounding in software development with solid data management skills. 
Well versed with DDL, DMLs, DCLs and Data Storage and Data Retrieval mechanism for in premises (Data Warehouse / Data Mart) and Cloud based environments. 
Experience working with large scale Data Warehouses. 
Extensive experience PL/SQL Development, Database design, SQL performance tuning 
Good understanding of Data Warehouse and Data Mart concepts with proven experience in both 
Experience with Unix (preferably Linux) 
Hands on experience with Source Code control tools. 
Familiar with scheduling management technologies e.g. Autosys

Aptitudes y experiencia deseables
DATAWAREHOUSE, DATA WAREHOUSE, ETL","Almacenamiento de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL), Minería de datos, PL/SQL y SQL, Ajuste de rendimiento, Autosys, Lenguaje de consulta (query) y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3980652064/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=8Ezm%2BPZFJw2%2Bopvp18pj%2Fg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 6 días,Estados Unidos,"Acerca del empleo
Please be aware that for this position, the candidate is required to be located in Brazil.

Data Engineer II

Personal development and becoming the best you are all about growth and exploring new skills and opportunities – both in and out of the office. At Accruent, we call this Grow Without Limits, and we are proud to offer each of our employees the resources, coaching and support vital to achieving Growth Without Limits in their personal and professional lives. Explore where the path takes you.

Why You Want To Work For Our Engineering Team:

The Engineering team is vital to the success of our current and emerging software developments. As part of this team, you will be able to deliver enhancements and customizations on new and existing products to make an impact on the business and our customers. As we continue to expand our product suite, we look for innovative team members to persevere in the ever-changing technology environment.

The Data Engineer II is responsible for coding, testing, and implementing solutions for Accruent’s applications. As a Data Engineer II, you will be critical in developing robust and scalable data gathering and distribution tools. Your primary focus will be designing and implementing a standardized data lake, collaborating with cross-functional teams, and ensuring our applications' high performance and responsiveness.

The ideal candidate will be highly motivated and passionate about technology and creative problem-solving. If you are dedicated, enjoy working in a team environment, have an entrepreneurial spirit and enjoy creating innovative solutions to complex problems, Accruent may be a fit for you.

How You Will Make a Difference:

Develop tools that help to support a common data lake and common data model for consumption and use by Accruent products.
Collaborate with product managers, designers, and backend engineers to define and implement innovative solutions.
Design and implement reusable and efficient components.
Write clean, maintainable, and well-documented code while adhering to coding standards and best practices.
Conduct thorough testing and debugging to identify and resolve software defects and performance issues.
Continuously research and stay up to date with the latest industry trends, tools, and technologies.
Collaborate with the QA team to perform code reviews and ensure high code quality and software reliability.
Participate in Agile development processes, including sprint planning, task estimation, and regular team meetings.
Provide technical guidance and mentorship to junior team members, sharing knowledge and fostering a collaborative work environment.

What You Bring To The Table: 

Bachelor’s degree in computer science, Data Engineering, or a related field (or equivalent industry experience).
Fluent/Advanced English and Portuguese.
Solid understanding and experience with Test Driven Development.
Proven experience as a Data Engineer with a strong focus on modern design patterns.
Proficiency in Python and Azure Services.
Experience with containerization and Kubernetes.
Experience with RDBMS products like Oracle or SQL Server.
Experience with Snowflake, Terraform, Prefect, and DBT is a plus.
Familiarity with RESTful APIs, GraphQL, and asynchronous request handling.
Strong problem-solving skills and ability to debug complex issues in a timely manner.
Excellent communication and collaboration skills, with the ability to work effectively in a team environment.
Familiarity with version control systems (e.g., Git) and code collaboration tools (e.g., GitHub, Bitbucket).
Experience with Agile development methodologies and tools (e.g., Jira, Scrum) is preferred.

Benefits

Annual Bonus
Meal Vouchers - Ticket Refeição
Life Insurance
Health and Dental Plan - extended to your partner and children without any discount.
Mentorship and Leadership Program 

Fortive Corporation Overview

Fortive’s essential technology makes the world stronger, safer, and smarter. We accelerate transformation across a broad range of applications including environmental, health and safety compliance, industrial condition monitoring, next-generation product design, and healthcare safety solutions.

We are a global industrial technology innovator with a startup spirit. Our forward-looking companies lead the way in software-powered workflow solutions, data-driven intelligence, AI-powered automation, and other disruptive technologies. We’re a force for progress, working alongside our customers and partners to solve challenges on a global scale, from workplace safety in the most demanding conditions to groundbreaking sustainability solutions.

We are a diverse team 18,000 strong, united by a dynamic, inclusive culture and energized by limitless learning and growth. We use the proven Fortive Business System (FBS) to accelerate our positive impact.

At Fortive, we believe in you. We believe in your potential—your ability to learn, grow, and make a difference.

At Fortive, we believe in us. We believe in the power of people working together to solve problems no one could solve alone.

Fortive: For you, for us, for growth.

Operating Company: Accruent

Personal development and becoming the best you is all about growth and exploring new skills and opportunities – both in and out of the office. At Accruent, we call this Grow Without Limits, and we’re proud to offer each of our employees the resources, coaching and support necessary to achieve Growth Without Limits in their personal and professional lives. Explore where the path takes you.

About Accruent At Accruent (a subsidiary or affiliate of Fortive Corporation), we strive to be on the cutting edge of the software world, providing purpose-built intelligent solutions that raise customer expectations, shift paradigms and transform the way businesses operate and achieve success.We aim to provide the same transformational growth for our 1,000+ employees which includes a vibrant office culture in major cities like Austin, London, and Amsterdam – and 10,000 customers in more than 150 countries – we know you’ll gain new experiences along the way. In our continued effort to help our teams Grow Without Limits, we provide all employees with the resources, coaching and support they need to reach new heights and experience true professional and personal development – and we do this because we believe it will help us grow as a global company in return.Every person can bring something incredible to the table, and we can always achieve more together. So, if you are courageous, adaptable, collaborative and interested in becoming the best you, we encourage you to join us for the ride – even if you don’t believe you have the exact experience to fill a particular role.Explore the path. Join Accruent. We Are an Equal Opportunity Employer. Fortive Corporation and all Fortive Companies are proud to be equal opportunity employers. We value and encourage diversity and solicit applications from all qualified applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity or expression, or other characteristics protected by law. Fortive and all Fortive Companies are also committed to providing reasonable accommodations for applicants with disabilities. Individuals who need a reasonable accommodation because of a disability for any part of the employment application process, please contact us at applyassistance@fortive.com.

Bonus or Equity

This position is also eligible for bonus as part of the total compensation package.

Fortive Corporation Overview

Fortive’s essential technology makes the world stronger, safer, and smarter. We accelerate transformation across a broad range of applications including environmental, health and safety compliance, industrial condition monitoring, next-generation product design, and healthcare safety solutions.

We are a global industrial technology innovator with a startup spirit. Our forward-looking companies lead the way in software-powered workflow solutions, data-driven intelligence, AI-powered automation, and other disruptive technologies. We’re a force for progress, working alongside our customers and partners to solve challenges on a global scale, from workplace safety in the most demanding conditions to groundbreaking sustainability solutions.

We are a diverse team 18,000 strong, united by a dynamic, inclusive culture and energized by limitless learning and growth. We use the proven Fortive Business System (FBS) to accelerate our positive impact.

At Fortive, we believe in you. We believe in your potential—your ability to learn, grow, and make a difference.

At Fortive, we believe in us. We believe in the power of people working together to solve problems no one could solve alone.

Fortive: For you, for us, for growth.

Personal development and becoming the best you is all about growth and exploring new skills and opportunities – both in and out of the office. At Accruent, we call this Grow Without Limits, and we’re proud to offer each of our employees the resources, coaching and support necessary to achieve Growth Without Limits in their personal and professional lives. Explore where the path takes you. About Accruent At Accruent (a subsidiary or affiliate of Fortive Corporation), we strive to be on the cutting edge of the software world, providing purpose-built intelligent solutions that raise customer expectations, shift paradigms and transform the way businesses operate and achieve success.We aim to provide the same transformational growth for our 1,000+ employees which includes a vibrant office culture in major cities like Austin, London, and Amsterdam – and 10,000 customers in more than 150 countries – we know you’ll gain new experiences along the way. In our continued effort to help our teams Grow Without Limits, we provide all employees with the resources, coaching and support they need to reach new heights and experience true professional and personal development – and we do this because we believe it will help us grow as a global company in return.Every person can bring something incredible to the table, and we can always achieve more together. So, if you are courageous, adaptable, collaborative and interested in becoming the best you, we encourage you to join us for the ride – even if you don’t believe you have the exact experience to fill a particular role.Explore the path. Join Accruent. We Are an Equal Opportunity Employer. Fortive Corporation and all Fortive Companies are proud to be equal opportunity employers. We value and encourage diversity and solicit applications from all qualified applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity or expression, or other characteristics protected by law. Fortive and all Fortive Companies are also committed to providing reasonable accommodations for applicants with disabilities. Individuals who need a reasonable accommodation because of a disability for any part of the employment application process, please contact us at applyassistance@fortive.com.","Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Sistema de gestión de bases de datos relacionales, Ciencias de la computación, Comunicación, Resolución creativa de problemas, Resolución de problemas, Revisión de código y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3982005655/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=JFvdA9%2BG6Bsxu2rCWNIf%2BQ%3D%3D&trk=flagship3_search_srp_jobs,Junior Analytics Engineer - CAG,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Austin, TX","Acerca del empleo
Do you enjoy using code to solve a good challenge? Do you strive for constant improvement? Are you uncommonly good with spreadsheets? If so, you may be the candidate we’re looking for!

As Junior Analytics Engineer, you will help build tools, dashboards, and reports to empower our employees with the data they need to make important decisions. You will learn the nuances of our data and how to use it to answer business questions. You will write scripts to automate repetitive tasks, allowing our employees to better focus their efforts. Most importantly, you will strive to uphold our core values by following through on commitments, encouraging and supporting other employees, and being committed to continuous growth!

Responsibilities include but are not limited to:

Design, build, and maintain scalable tools, dashboards, and reports.

Extract raw data from websites, databases, etc. and turn it into usable datasets for our employees.

Meet with employees throughout project lifecycles to gain an understanding of their data needs.

Troubleshoot and improve upon existing scripts and reports as needed.

Collaborate on large projects with other members of the Data Science team.

Perform other ad-hoc data-related tasks as needed.

Required Qualifications:

A B.S. in Data Science, Computer Science, Mathematics, or a related field.

Must be located in the Austin area, or planning to relocate to the Austin area within the next 60 days.

At least 1 year of experience in a related position.

Proficient in Python and at least one spreadsheet application (Google Sheets preferred).

An assessment will be conducted as part of the interview process to help gauge your skill level in these areas.

Experienced With SQL (PostgreSQL Preferred).

Fast-learning critical thinker and problem-solver.

High attention to detail.

Strong communication skills.

Must have held a valid driver's license for at least 1 year

Must be at least 18 years of age to be considered

Nice-to-Have Qualifications:

Experience with database design.

Experience working with Linux-based systems, APIs, and Git.

Experience working with Google Apps Script or JavaScript.

An eye for design.

Experience with web-scraping.

Familiarity with front or back-end development.

Compensation:

$26-$30/hour based on experience.

Monthly paid time towards professional skills growth and self-study.

Schedule and Hours:

Monday-Friday, full-time position.

We're extremely flexible on granting our employees days off when advance notice is given. Overtime occasionally available.

At this time, we are currently only accepting candidates located in the Austin area, or planning to relocate to the Austin area within the next 60 days.

This is an in-office position with the potential for remote work after the first 90 days, based on employee performance.

CAG is proud to be an equal opportunity employer. We are committed to building a diverse, equal, and inclusive workplace and our recruiting process reflects this commitment.

As a forward-thinking, adaptive, and supportive company, we seek others who care about providing a transformational environment where everyone has a voice and opportunities to succeed. We encourage all interested candidates to submit an application.

Core Values:

Transformational Not Transactional - Be Transformational

We do what we say we're going to do

We encourage and support each other

We're committed to continuous growth

Benefits

The Continental Automotive Group is an equal opportunity employer and a drug free workplace.

All Continental Automotive Group Full-Time Employees Receive:

Employer Paid Dental Insurance

Employer Paid Life Insurance

Employer Paid Medical Insurance

Employer Paid Health Savings Account Contribution

Employer Paid Wellness Clinic

Employer Paid Flu Vaccinations Every Fall

Employer Percentage Matching for 401k

Employer Paid Parental Leave

5 Paid Bereavement Leave Days/Year for immediate family members (after 60 days)

14 Paid Time Off Vacation Days/Year (60 days - 3 years)

19 Paid Time Off Vacation Days/Year (3 - 7 years)

24 Paid Time Off Vacation Days/Year (7+ years)

Annual Christmas Bonus Based on Tenure

Annual Christmas Party

Annual Employee Appreciation Dinner

Employee Vehicle Purchase Program

Employee Discounts on Collision Repair, Parts, and Service

Bi-Weekly Pay Periods, Paid on Fridays

Holidays: Thanksgiving, Christmas & New Years

Employer Paid Lunches Every Saturday

Employer Paid Lunches/Dinners - Special Occasions (Blood Drives, Birthdays, Awards, etc.)

Access to In-House Insurance Agency

Access to In-House Marketing Agency

Voluntary Life Term and Whole Insurance

Voluntary AFLAC Supplemental Coverages, Voluntary Vision plan, Pet Insurance, and Legal Shield

$200 Donation to Charity of Your Choice with Approval

We strive to take excellent care of our employees, so that they will strive to take excellent care of our clients!

CAG is committed to compliance with the American Disabilities Act. If you require reasonable accommodation during the application process or have a question regarding an essential job function, please call (512) 220-0988.","Arquitectura de datos, Ciencia de datos y Python, Bases de datos, Ciencias de la computación, Diseño de bases de datos, Google Apps Script, Hojas de cálculo de Google, Modelado de datos y Professional Skills",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3945598148/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=sQ7uI7KVLGjUbq5VXFqD3A%3D%3D&trk=flagship3_search_srp_jobs,Databricks data engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
CapGemini,

Databricks data engineer and data architect roles.. To all involved in resourcing, please provide the most experienced, most knowledgeable Databricks resources available based either in the US. needs senior very experienced Databricks guys.","Almacenamiento de datos, Analítica de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3974316570/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=pmpHnwqXFeYcY15MF0%2FlHQ%3D%3D&trk=flagship3_search_srp_jobs,Staff Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
Job Title: Staff Data Engineer
Job Type: Full-Time
Location: Remote Flexibility, preference for local candidates willing to work a Hybrid schedule in Westwood, MA.

About Intentsify:
Intentsify.io is a forward-thinking technology company specializing in AI-driven intent analytics for empowering businesses with actionable insights.

WHAT WE DO - Intentsify provides B2B organizations with the most accurate, comprehensive buyer-intent intelligence in the market today, and solutions to act on that intelligence.

HOW WE DO IT - Delivering and activating proprietary, next-generation precision intent data, Intentsify’s Intelligence Activation Platform identifies companies exhibiting research behaviors directly related to your business solutions, pinpoints where they are in the buying process, surfaces the issues they care about most, and enables you to engage identified companies, quickly and effectively.??

WHY WE DO IT - To transform the way B2B organizations consume, interpret, and activate intent—enabling exceptional full-funnel buying experiences that drive revenue.

About the job:
We are seeking a talented and experienced Data Engineer to join our team in Westwood, MA. This role provides a unique opportunity to lead the design and implementation of robust data engineering solutions for our AI-based, data-intensive web application specializing in intent analytics.

As the Staff Data Engineer at Intentsify.io, you will be instrumental in shaping the data architecture, engineering processes, and analytics infrastructure for our intent analytics platform. Your role involves leading a team of data engineers, collaborating with cross-functional teams, and ensuring the scalability, efficiency, and reliability of our data-intensive applications. If you have a proven track record in data engineering technology and are passionate about leveraging data for actionable insights, this position is tailored for you.

Responsibilities:
Architectural Leadership: Lead the design and implementation of the data engineering architecture and ecosystem for our AI-based, data-intensive web application, ensuring scalability, reliability, while ensuring readability/usability of work products.
Data Processing: Design and optimize data processing pipelines to handle large volumes of diverse data sources, focusing on data quality, integrity, and real-time analytics capabilities.
Integration with AI: Collaborate with AI and machine learning teams to integrate data into predictive models and algorithms, ensuring seamless interoperability between data engineering and analytics components.
Technology Stack: Evaluate, select, and implement appropriate data technologies, frameworks, and tools to support the development of intent analytics applications.
Collaboration: Work closely with software architects, data scientists, and other cross-functional teams to align data engineering efforts with overall application architecture and business goals.
Performance Optimization: Conduct performance tuning, monitoring, and troubleshooting of data pipelines to ensure optimal functionality and efficiency.
Team Leadership: Lead and mentor a team of data engineers, fostering a culture of innovation, collaboration, and continuous improvement.
Data Governance: Implement and enforce data governance and compliance measures to ensure the security and integrity of data.

Requirements:

Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
Proven experience as a Data Engineering Architect, with a focus on building resilient and usable data systems utilizing large web scale data and integrating AI-based models.
Proficiency in programming languages such as Python, Java, or similar.
Expertise in big data technologies (e.g., Spark, Beam, Kafka, Airflow), databases and data store technologies (e.g., Hadoop, Snowflake, PostgreSQL), and cloud platforms (e.g., AWS, Azure, Google Cloud).
Strong leadership skills with experience in leading and mentoring data engineering teams.
Excellent problem-solving skills, attention to detail, and a strategic mindset.
Effective communication and collaboration skills to effectively work with cross-functional teams.

Benefits:
Competitive Medical, Dental, and Vision plans
401k with Company Match
Flex-time and open vacation policy
Tremendous growth opportunity within a fast-growing start up
Short term and long term disability
Life Insurance

Intentsify is focused on promoting an inclusive environment and is proud to be an equal opportunity employer. We celebrate the different viewpoints and experiences our diverse group of team members bring to Intentsify. Intentsify does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, physical or mental disability, national origin, veteran status, or any other status protected under federal, state, or local law.","Extraer, transformar y cargar (ETL), Gobierno de datos, Ingeniería de datos y Lenguajes de programación, Calidad de datos, Ciencias de la computación, Comunicación, Java, Optimización y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3977159358/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=EXd9bUuS4rnUbpkkVrl3ww%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer LLB #24-007,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",hace 1 semana,"Freeport, ME","Acerca del empleo
Contract Freeport, Maine

 Location: Freeport, Maine 
 Type: Contract 
 Job #19585 

Job Title: IT Data Engineer

Contract Duration: 6 months

Location: Remote (EST hours)

Position Purpose: Join our client’s team to design, implement, and manage data ingestion and pipeline processes on Google Cloud Platform. This role involves developing, testing, and troubleshooting these processes based on business needs and technical specifications.

Key Responsibilities:

Design and implement data workflows 
Collaborate with project teams 
Develop and execute technical solutions 
Extract and analyze data using SQL 
Conduct and document testing 
Participate in project planning and manage small projects 
Estimate and track project time 
Conduct solution design reviews 

Required Skills:

Google Pub/Sub 
BigQuery 
Google Dataform 
Data ingestion to BigQuery 
Google Cloud Storage 
Cloud Composer 
GitHub 
SQL 

Preferred Skills:

Cloud Data Fusion 
DBMS connectivity (SQL Server, DB2, Oracle) 
RESTful APIs and SOAP Webservices 
Integration with Google resources 
Jira and Confluence 

Education: Bachelor's Degree

Experience: 3+ years

Additional Requirements:

Reliable high-speed internet 
Laptop/desktop for Virtual Desktop Infrastructure (VDI) access 
Additional monitor and headset 

Join our client’s innovative team and elevate your career as a Data Engineer. Apply now!

 See All Jobs","Google BigQuery y Ingeniería de datos, Diseño de soluciones empresariales, Diseño de soluciones técnicas, Ingesta de datos, Representational State Transfer, Revisión de diseños, SOAP, Sistema de gestión de bases de datos (SGBD) y Transferencia de Estado Representacional (REST)",Solicitar
https://www.linkedin.com/jobs/view/3982341212/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=MXtKV8YxnhjG%2B5OzWvni0w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Tampa, FL","Acerca del empleo
Job Title: Data Engineer

Job Duration: 12 Months contract on W2

Job Location: Tampa, Florida, 33608 (Hybrid)

Note: US Citizenship or Permanent Resident is required.

Job Description

Let’s do this. Let’s change the world. In this vital role you will be part of the established technical/engineering team, develop web UI interface, plus data flow pipelines to extract, transform, and load data from various data sources in various data format to enterprise data lake and data warehouse system in three regions in AWS. Provide data analytics and predictive analysis to business users.

Responsibilities

Be a key team member assisting in design and development of the data pipeline for Global Data and Analytics team
Collaborate with Data Architects, Business SME’s, and Data Scientists to design and develop end-to-end data pipeline to meet fast paced business need across geographic regions
Serve as system admin to manage AWS and Databricks platform;
Adhere to best practices for coding, testing and designing reusable code/component
Able to explore new tools, technologies that will help to improve ETL platform performance and machine learning operations
Participate in sprint planning meetings and provide estimations on technical implementation; Collaborate and communicate effectively with the product teams
We are all different, yet we all use our unique contributions to serve patients. The professional we seek will have these qualifications.

Basic Qualifications

Master’s Degree OR Bachelor’s degree with 2 years Data Engineering and/or and Software Engineering experience Or Associate’s degree 6 years of Data Engineering and/or Software Engineering experience Or High school diploma and 8 years of Data Engineering and/or Software Engineering experience.

Preferred Qualifications

Familiar with PySpark dataframe and data processing libraries, machine learning frameworks (like Tensorflow, Keras or PyTorch), and other machine learning libraries
Familiar with machine learning operations
Experience with software development (Java, Python preferred), end-to-end system design
Experience with data modeling for both OLAP and OLTP databases, hands-on experience with SQL, preferred Oracle, PostgreSQL, and Hive SQL; SQL performance tuning
Experience with web development, java script, html, CSS, any web framework or microservice architecture
Experience with software DevOps CI/CD tools, such Git, Jenkins
Experience on AWS, familiar with EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, DynamoDB, and API gateway
Experience with docker container, Kubernetes container orchestration
Experience with Apache Airflow and Apache Spark; Spark performance turning
Experience with Tableau Dashboard and Tableau Server
Experience with Pharmaceutical industry, commercial operations
Ability to learn quickly, be organized and detail oriented.

https://www.linkedin.com/company/trispoke-managed-services-pvt-ltd/jobs

For more jobs kindly click on above link.

#Hiring #JobSearch #NowHiring #Recruitment #JobOpening #Career #JobOpportunities #JobHunt #JobSeeker #Employment #W2jobs #DataEngineer #TampaJobs #FloridaJobs #EngineeringJobs #DataJobs #DataScience #BigData","Airflow, Apache Spark, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Comunicación, Hojas de estilos en cascada (CSS), Microservicios y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3828059042/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=tjKMmnUgVbdfSqpcU0Rqfg%3D%3D&trk=flagship3_search_srp_jobs,Hybrid Work - Need Jr - Mid Data Engineer/Analyst in Piscataway NJ,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 5 meses,"Municipio de Piscataway, NJ","Acerca del empleo
Piscataway, NJ – 3 days onsite a week, must be local to NJ and under 60 mins

Must be W2 or 1099

Looking for someone with 5+ years of exp. This is a mid junior role

Do they have a minimum of 5 years of IT experience?
Do they have experience with Tableau reporting?
Experience with Alteryx?
Experience with Marketo?

Job Description

Position Overview:

As a Junior Data Engineer with a specialization in Marketo and Alteryx, you will play a crucial role in extracting, transforming, and delivering valuable data insights to support our business initiatives. You will work closely with cross-functional teams, serving as a bridge between technical data operations and business stakeholders.

Key Responsibilities

Data Integration: Collaborate with the IT and Marketing teams to design and implement data integration pipelines that connect Marketo with our data infrastructure.

Data Manipulation: Leverage Alteryx to transform, cleanse, and manipulate raw data from various sources into formats suitable for analysis, reporting, and visualization.

Data Quality: Ensure data accuracy, consistency, and completeness through data profiling, validation, and data cleansing techniques.

ETL (Extract, Transform, Load): Develop ETL processes to move, transform, and load data from source systems to data warehouses.

Reporting and Visualization: Create and maintain dashboards and reports to provide business stakeholders with actionable insights derived from Marketo data.

Collaboration: Work closely with marketing and sales teams to understand their data requirements and help them make data-driven decisions.

Documentation: Maintain clear and comprehensive documentation for data processes, data models, and transformation workflows.

Data Security: Ensure compliance with data privacy and security regulations and best practices.

Adaptability: Stay current with industry trends and new data technologies, providing suggestions for improvements and optimization.","Arquitectura de datos, Ingeniería de datos , Tableau y Visualización de datos, Alteryx, Calidad de datos, Manipulación de datos, Marketo, Modelo de datos y Perfiles de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3985006335/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=HZ%2BvXJDgFOwr3vNiJjyQqA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Littleton, CO","Acerca del empleo
** An on-site presence is required for this role. Please do not apply if you are not living in or planning to relocate to the Denver-metro area in the next few weeks.**

GENERAL PURPOSE

The Data Engineer’s main responsibility is to improve the performance and operational efficiency of Colorado Credit Union. The data architect collects, reviews, and analyzes data to solve problems, address challenges, and improve processes to help the organization achieve and maintain a competitive advantage.

Essential Duties And Responsibilities

Help CCU achieve its core values: Be the Advocate. Embrace Curiosity. Be Compassionate.
Using quantitative techniques, defines a problem, collects and analyzes data pertaining to it, evaluates processes, and surfaces information to the organization for process improvement opportunities.
Works across multiple departments to aid in data discovery.
Maintain and create reports for a variety of audiences.
Completes detailed product and member profitability analysis for use in strategic decision making. 
Prepares monthly and quarterly reports accurately and within deadlines.
Accurately presents analysis and recommendations to the organization using effective written and verbal communication skills.
Create logic to gather data from various sources and audiences.
Create alerts based on key thresholds within the data.
Data structuring from multiple sources into a centralized database.
Design strategies for databases and data warehouse systems.
Designing and implementing machine learning algorithms to support predictive analytics efforts.
Utilize external data sets to augment current capacity.
Manage banking applications and databases housed either on-site or on third-party premises.
Manage programming/development, systems analysis, and design functions.
Support business process changes through application upgrades and change control management.
Lead deployment, modification, integration, upgrades, and changes to business systems and applications.
Ensure efficient and accurate development and deployment of all systems changes while maintaining data integrity.
Administer servers, software deployment, and code review.
Coordinate ongoing change management of critical application platforms.
Execute planned maintenance.
Develop and enhance technical operating procedures and documentation to improve operational efficiency and effectiveness.
Participate in selecting new digital solutions that benefit the organization.
Participate in short and long-range planning to select and utilize appropriate software, enhancements, and tools.
Establish network specifications by conferring with users; analyze workflow, access, information, and security requirements.
Use process improvement, automation, and consistent development practices to further the organization’s mission.
Act as an escalation point for issues and requests, working issues to resolution and improving processes to reduce future escalations.
Periodically conduct training with other groups to increase capabilities, efficiencies, and effectiveness.
Provide visual, technical, and editorial guidance.
Identify and own problems or opportunities for improvement, developing and deploying temporary and permanent fixes.
Adheres to all service standards as outlined in the credit union’s policies and procedures.
Performs other job-related duties as assigned.

KNOWLEDGE, SKILL AND ABILITY

Advanced skills in Access, and expert skills in Excel. 
Highly proficient in SQL and modern business intelligence tools such as PowerBi.
Experience with data ETL workflows.
Comfortable with a variety of data structures.
Strong mathematical and analytical aptitude for solving problems.
Ability to analyze and document complex business processes.
Excellent written and verbal communication skills.
Ability to work well with employees at all levels of the organization.
Experience with Snowflake and Symitar a plus.
Experience with data automation workflows a plus.

EDUCATION OR FORMAL TRAINING

Bachelor’s degree with a major in Finance, Accounting, Economics, or Business Administration from an accredited institution is required.

Experience

Five years of similar or related experience or an advanced degree/ certifications is required.

WORK ENVIRONMENT/PHYSICAL ACTIVITES 

Work Environment

Office environment. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. The noise level in the work environment is usually moderate.

Physical Demands

The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. The employee must occasionally lift and/or move up to 25 pounds. Specific vision and hearing abilities are required by this job and include: close vision and ability to adjust focus, ability to listen closely and speak clearly. The employee is frequently required to sit; and reach with hands and arms. The employee is occasionally required to stoop, kneel or crouch.

Benefits

This position is eligible for the following benefits:

Medical/Dental/Vision Insurance
Company Paid Life, Short-term Disability, and Long-term Disability Insurance
401k Employer Contributions & Match
Annual Profit Sharing funded into the 401k
Tuition Reimbursement (Max $5,250.00/annually)
Paid Time Off 
Company Paid Holidays
Work-Life Balance and Flexible Work Schedules
Cross-Training and Career Development Opportunities
Employee Recognition Program
Monthly Commission Opportunities
Annual Bonus Based on Company Performance","Almacenamiento de datos, Analítica de datos, Capacidad de análisis, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Comunicación oral, Matemáticas y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3949481677/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=7LCdZRCuiTJ7sblPET7Zhw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 3 semanas,"Lakeland, FL","Acerca del empleo
Onebridge is a Consulting firm with an HQ in Indianapolis, and clients dispersed throughout North America and beyond. We have an exciting opportunity for a highly skilled Data Engineer to join one of our clients.

Employment: Direct Hire with Client
Location: Lakeland, FL - ONSITE
Industry: IT Consulting & Nonprofit

Candidates MUST be located in Florida. 

Data Engineer | About You
As a Data Engineer, you are responsible for engineering data infrastructure, building data pipelines, and shaping master data tables. You have a deep understanding of how data is produced and consumed. Your role involves working with disparate data sources, exploring and implementing tools to support operational and analytical needs, and driving change by identifying areas for improvement through data analysis. You'll also play a strategic role in decision-making on the analytics roadmap, supporting various programs including Counseling, Mental Health & Substance Use Services, Child Welfare & Family Support, and more.

Data Engineer | Day-to-Day
Develop and implement a comprehensive data strategy and roadmap, collaborating with third-party contractors to build and enhance data infrastructure.
Spearhead the design and construction of robust data models, creating and maintaining master data tables and data pipelines tailored for insightful analysis.
Work closely with subject matter experts and stakeholders to develop and implement analytics pipelines, ensuring data flows smoothly from source to insights.
Collaborate across teams to ensure data is leveraged effectively and its value is maximized, navigating and integrating disparate data from various systems and sources.
Identify opportunities to streamline operations and reduce technical debt through automation, freeing up resources for more strategic endeavors.
Engineer data solutions including data scraping, and transitioning between structured and unstructured data, while creating and managing analytics frameworks and relationships between data sets.

Data Engineer | Skills & Experience
7+ years of hands-on analytics experience navigating complex data landscapes.
Proficiency in analytical thinking and problem-solving, with a keen eye for detail, ensuring accuracy and reliability in data analysis.
Exceptional communication skills for effective collaboration with internal stakeholders, ensuring alignment on project objectives and outcomes.
Proven ability to manage multiple responsibilities concurrently, effectively prioritizing tasks to meet project deadlines and organizational goals.
Expertise in database query languages such as SQL, code versioning tools like Git, and experience with cloud data warehouses and BI tools, ensuring efficient data management and analysis processes.
Experience with Snowflake, DBT, FiveTran, Open Beta Data, and Power BI (support available for Power BI) with Databricks experience acceptable as a substitute for Snowflake.

A Best Place to Work in Indiana since 2015.","Analítica de datos y Análisis de datos, Atención al detalle, Fivetran ETL Tool y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3969244621/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=0Xr9i2lUsZIaBzs5h7y6Rw%3D%3D&trk=flagship3_search_srp_jobs,Data Management Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 4 días,"Manassas, VA","Acerca del empleo
Job Details

Job Location

VA Corp - Manassas, VA

Remote Type

Fully Remote

Position Type

Full Time

Education Level

4 Year Degree

Job Category

Information Technology

Description

Echelon is seeking to hire a Data Management Engineer to help support our customer’s mission critical needs.

Essential Functions

 Designing and implementing data storage solutions 
 Migration of data from MS SQL and Oracle databases into Azure SQL Service 
 Building data systems and pipelines, maintaining them through application integration and business need changes 
 Develop artificial intelligence, machine learning, deep learning, and natural language processing algorithms 
 Ensuring data accuracy and consistency through validation and cleansing 
 Develop analytical tools and programs to utilize crested data pipelines 

Requirements

 US Citizen 
 7 years of relevant experience, and a bachelor's degree in data science, mathematics, engineering, computer science or other relevant discipline. 
 Experience working with DoD data systems (Army Vantage, DoD Advana QLIK) 
 Experience creating and implementing machine learning processes to large data sets 

Salary at Echelon Services is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $155,000 - $195,000 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Echelon’s total compensation package for employees.

Other Information

EEO Employer F/M/Vet/Disabled","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Ciencias de la computación",Solicitar
https://www.linkedin.com/jobs/view/3985503319/?eBP=BUDGET_EXHAUSTED_JOB&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=E%2BG8BE0wdXYJQEI%2Fa6Qu3Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"70 US$K/año - 85 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 3 días,"Houston, TX","Acerca del empleo
Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

100% Remote, Silicon Valley Health & Wellness start-up, Salary + Equity

This Jobot Job is hosted by Duran Workman

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $70,000 - $85,000 per year

A Bit About Us

We are currently seeking a highly skilled and motivated Data Engineer to join our dynamic team. This is a permanent position, offering an exciting opportunity to work with a diverse array of technologies. The successful candidate will be responsible for designing, developing, and maintaining databases, data systems, and analytic solutions that turn data into knowledge. This position promises an exciting work environment with plenty of opportunities for professional growth and development.



Why join us?


 Remote Work
 Significant Equity package
 Great benefits & 401k

Job Details

Responsibilities

 Develop, construct, test, and maintain architectures such as databases and large-scale data processing systems.
 Develop database solutions to store and retrieve company information.
 Install, configure, and maintain data engineering platforms.
 Perform data analysis to ensure all data systems meet company requirements.
 Collaborate with data architects, modelers, and IT team members to identify opportunities for data acquisition.
 Develop data set processes for data modeling, mining, and production.
 Implement data flow to connect operational systems, data for analytics, and BI systems.
 Monitor data performance and modify infrastructure as needed.
 Conduct research for industry and business questions.
 Use large data sets to address business issues.
 Deploy sophisticated analytics programs, machine learning, and statistical methods.
 Prepare data for predictive and prescriptive modeling.
 Find hidden patterns using data.
 Use data to discover tasks that can be automated.
 Deliver updates to stakeholders based on analytics.

Qualifications

 Bachelor’s degree in Computer Science, Software Engineering, or a related field.
 A minimum of 5 years of experience in a data engineer or similar role.
 Expertise in SQL including coding, optimization, and troubleshooting.
 Experience with data architecture, data modeling, schema design, and software development.
 Knowledge of various ETL techniques and frameworks.
 Experience with big data tools Hadoop, Spark, Kafka, etc.
 Experience with relational SQL and NoSQL databases.
 Experience with data pipeline and workflow management tools.
 Experience with AWS cloud services.
 Experience with object-oriented/object function scripting languages Python, Java, C++, etc.
 Strong problem-solving skills and attention to detail.
 Excellent communication and teamwork skills.
 Ability to work independently and manage multiple tasks simultaneously.
 Strong understanding of data management fundamentals and data storage principles.
 Knowledge of distributed systems as it pertains to data storage and computing.
 Proven ability to work in a fast-paced environment and meet deadlines.

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!","Analítica de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Memoria de datos, Bases de datos, Ciencias de la computación, Gestión de flujos de trabajo, Modelado de datos y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3981608306/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=75LWI5yz3a4FF4mMCpRPrw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"60 US$/h - 70 US$/h Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,"Acworth, GA","Acerca del empleo
Job Description

This company provides energy management solutions to their clients within the manufacturing industry. Their applications allow for a more efficient energy usage for refrigeration, heating, and AC units.

This company is looking for a Data Engineer who also has strong experience with cloud architecture. You will be working with the engineering team to maintain their database and seek new ways to utilize their data. This role is a 6-month contract to hire, and you will be required to be on-site 2 days a week in the office in Acworth.

Required Skills & Experience

4+ Years of Experience in a Data Engineering role 
Strong Experience with Python 
Prior Professional Experience working with Azure or AWS 
Strong Experience with Flask API 
Big Data Management Experience 
Bachelor Degree in Computer Science or Related Field 

The Offer

You Will Receive The Following Benefits

Medical Insurance 
Dental Benefits 
Vision Benefits 
Paid Time Off (PTO) 
401(k) 

Applicants must be currently authorized to work in the US on a full-time basis now and in the future.

Posted By: Andrew Bond","Almacenamiento de datos, Analítica, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Flask y Ingeniería de datos, Bases de datos y Ciencias de la computación",Solicitar
https://www.linkedin.com/jobs/view/3948466534/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=UB6AALAKZUZhinMj1O5Wgw%3D%3D&trk=flagship3_search_srp_jobs,Remote-Data Engineer(US citizen/GC Only),"60 US$/h - 462 US$/h Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",Publicado de nuevo hace 1 mes,"Iowa, Estados Unidos","Acerca del empleo
Job Title Data Engineer
Duration 12+ Months (possible extensions/ conversion)
Location Remote

100% remote- Cross border work is not permitted
Monday-Friday- 40 hours a week.

***Glider Assessment John_Deere-65501001 Data Scientist will be used for this role***

Visa sponsorship is not available, now or in the near future, for this position.

Required Skill Set
• SQL Query experience (1-3 years)
• Dashboard design experience (3-5 years)
• Experience translating complex data into business visualizations
• Experience with Tableau (1-3 years)

Education Degree- BA in Data Science, Data Engineering, Computer Science, Business Analytics


Top skillset/must haves
• SQL Query experience (1-3 years)
• Dashboard design experience (3-5 years)
• Experience with Tableau (1-3 years)
Pre-Identified Candidate","SQL, Tableau y Visualización de datos, Panel de control",Solicitud sencilla
https://www.linkedin.com/jobs/view/3980105646/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=YzsNAXe9sg1Mxkk5SoO1tw%3D%3D&trk=flagship3_search_srp_jobs,Junior data analyst/AI engineer/java Devops programmer,"81 US$K/año - 128 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Davis, CA","Acerca del empleo
For Almost 14 years Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients.

The Job market is Hyper Competitive. For 1 position maybe 500-1000 candidates or more are applying and even experience candidates are taking entry level Job positions due to recent layoffs.

If you apply to 20 positions each day with no result, are not getting interviews or are ghosted by clients then maybe rethink your strategy.

School Projects or experience is not enough anymore. For Entry Level positions also Candidates need to have Projects with the in demand skills which Clients require to get Interviews and finally a Job Offer

In this market also our candidates are able to achieve multiple job offers and $100k + salaries once they have the required skills..

please check the below links to see success outcomes, salaries of our candidates.
https://www.synergisticit.com/candidate-outcomes/
https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

https://synergisticit.wistia.com/medias/tmwjwchxz5
https://synergisticit.wistia.com/medias/n8487768di
https://synergisticit.wistia.com/medias/o5gmv7i9eu
https://synergisticit.wistia.com/medias/k6t6a1n4kb

https://synergisticit.wistia.com/medias/pgrvq4fgni
https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens
We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000’s of candidates get jobs at technology clients like apple, google, Paypal, western union, bank of america, visa, walmart labs etc to name a few.
Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.
Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.
We assist in filing for STEM extension and also for H1b and Green card filing to Candidates
We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Data Analyst/AI/Machine learning Positions
REQUIRED SKILLS
Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Highly motivated, self-learner, and technically inquisitive
Knowledge of Statistics, Gen AI, LLM, Sagemaker, Python, Computer Vision, data visualization tools
Excellent written and verbal communication skills
Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full stack/Devops Positions
Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Knowledge of Core Java , javascript , C++ or software programming
Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience
Excellent written and verbal communication skills
If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Análisis de datos, Lenguajes de programación y Python, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, HTML, Java, JavaScript, Plataforma Java y React.js",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982955919/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=S6vCefcbVrCFLyccOpFviQ%3D%3D&trk=flagship3_search_srp_jobs,REMOTE - PERMANENT JOB - Need Data Engineer - 5 to 7 years experienced only,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,Estados Unidos,"Acerca del empleo
100% Remote

The ideal candidate will live in or near Nashville, TN, Scranton, PA, Kalamazoo, MI or St. Paul, MN but it is not required.

Need valid LinkedIn

need only 1 candidate on this and must be under 5-7 years exp range

Job Purpose/Summary: We are seeking a highly skilled Data Engineer to join our dynamic team. This fully remote position involves designing, building, and maintaining scalable data pipelines and architectures to support our healthcare analytics initiatives. The ideal candidate will have extensive experience in data engineering, particularly within the Azure ecosystem, and a passion for developing robust data solutions. You will also play a crucial role in building our brand-new data warehouse, helping to shape the future of our data infrastructure.

Key Responsibilities

 Design and implement scalable data pipelines for both streaming and batch processing, leveraging Azure Data Factory and other relevant technologies.
 Develop and maintain data lake architectures, ensuring efficient load strategies, appropriate file formats, and robust security measures.
 Utilize data platforms such as Synapse, Demio, Fabric, and Snowflake to manage and optimize data storage and retrieval.
 Play a key role in building and architecting a new data warehouse to support advanced analytics and reporting.
 Apply DevOps principles to data engineering tasks, utilizing GitHub or Azure DevOps for version control and CI/CD tooling.
 Employ Infrastructure as Code (IaC) tools such as Terraform, Open Tofu, or Bicep to automate and manage infrastructure deployments.
 Integrate and manage data from various sources, including relational databases (Oracle, SQL Server) and unstructured sources/files/NoSQL.
 Implement data normalization and denormalization practices, including modeling and developing Kimball-style star schemas.
 Collaborate with stakeholders to gather requirements, understand business needs, and develop data solutions that meet those needs.
 Work with healthcare data and adhere to healthcare data standards such as HL7.
 Support MLOps specialists and data scientists in operationalizing machine learning models.

Required Qualifications

 5+ years of experience in a Data Engineer role, with 3+ years of experience with Azure.
 Proficiency with data platforms like Synapse, Demio, Fabric, or Snowflake.
 Strong knowledge of data lake architectures, load strategies, file formats, and security.
 Expertise in building and monitoring streaming and batch data pipelines with CDC.
 Hands-on experience with DevOps tools and practices, specifically GitHub or Azure DevOps and CI/CD tooling.
 Proficiency with Infrastructure as Code (IaC) tools such as Terraform, Open Tofu, or Bicep.
 Experience with a variety of data sources, including relational databases (Oracle, SQL Server) and unstructured sources/files/NoSQL.
 Understanding of data normalization, denormalization practices, modeling, and Kimball-style star schemas.
 Proven ability to collaborate with stakeholders to gather requirements and develop solutions.
 Familiarity with healthcare data and standards such as HL7, preferably at least 1 year.

Desired Qualifications

 AZ-900 certification.
 Experience working with or operationalizing machine learning models.

Education

 Bachelor’s Degree or higher in Computer Science, Information Systems, or a related field preferred but not required.

Other Requirements

 Background Check
 Physical that will include a drug screen and TB test","Almacenamiento de datos, Analítica de datos, Base de datos relacional y Ingeniería de datos, Arquitectura técnica, Bases de datos, Estándares de datos, Microsoft Azure, Obtención de requisitos y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979922609/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=F0Qq7wPMbHcZ7IsPJo9Ceg%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Mineápolis, MN","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Positions 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3974881200/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y4kfaFPFuI02nvmkwttCBQ%3D%3D&trackingId=u6TEDYEMwdpaXRs8je6WIQ%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Entry Level,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Houston, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

please check the below links to see success outcomes, salaries of our candidates .

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)(phone number removed)di

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3833869536/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=e2wFIhr55FccD1uxMJ%2F9nA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 meses,Estados Unidos,"Acerca del empleo
Data Engineer.

The role is Remote.

The rate is flexible .

No H!B, CPT

To be submitted, 1) Please fill-out completely the skills matrix attached, 2) forward updated resume and 3) complete the demographic questionnaire below.

Note: If non-USC, we will need copy of the work authorization.

Demographic Questionnaire

Contact Name (as Per Work Authorization)

Current Location:

LInkedIN

Phone number:

Email

Skype ID:

Availability

Willing to relocate:

Work Authorization (If not USC, please share copy of your work authorization and photo ID):

Highest Degree/Education

Employer contact info/details:

Currently on project? If so, completion date:

___________________________________________________

Job Title: Data Engineer - onshore

Remote position

Contract Length: 6 months contract

Job Description

Looking for an onshore Data Engineer with the following skills:

Strong proficiency in SQL and 
Strong proficiency in Python. 
Must have experience in the financial sector.

�","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3956126028/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=nX1k4I1YDQd0%2Bv42LmCMYQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"126,1 US$K/año - 186,8 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,Estados Unidos,"Acerca del empleo
Overview

Working at Atlassian

Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.Atlassian is looking for a Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.

You'll have flexibility in where you work – whether in an office, from home (remote), or a combination of the two.

Responsibilities

A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.

You'll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time.

Qualifications

On your first day, we'll expect you to have:

BS in Computer Science or equivalent experience with 3+ years as a Data Engineer or a similar role
Programming skills in Python & Java (good to have)
Design data models for storage and retrieval to meet product and requirements
Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)
Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring


We'd Be Super Excited If You Have

Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System


Compensation

At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:

Zone A: $140,100 - $186,800

Zone B: $126,100 - $168,100

Zone C: $116,300 - $155,000

This role may also be eligible for benefits, bonuses, commissions, and equity.

Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.

Our Perks & Benefits

Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more.

About Atlassian

At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

To learn more about our culture and hiring process, visit go.atlassian.com/crh .","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Bases de datos, Calidad de datos, Ciencias de la computación, Lenguaje de consulta (query) y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3955293508/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=rjQvXl%2BTVRC%2BP0ygO3AzDQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 mes,"West Point, PA","Acerca del empleo
Databricks Need Data Engineer having good experience in Databricks .

Strong Data Analytics experience Knowledge of data modeling and Schema design Build processes supporting data transformation, data structures, metadata .

Strong troubleshooting and problem-solving skills in an AWS environment.

Excellent communication and collaboration skills.

AWS certifications (e.g., AWS Certified Solutions Architect, AWS Certified DevOps Engineer).","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Azure Databricks, Comunicación, Modelado de datos, Resolución de incidencias y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3943490159/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=jkBJu%2FhdOH%2BSx3Qjq1zwwQ%3D%3D&trk=flagship3_search_srp_jobs,Data Analytics Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,Estados Unidos,"Acerca del empleo
Job Title: Data Analytics Engineer

Job Location: Fully Remote – in the US, Core Working hours – 9AM to 5PM EST

Duration: Contract-to-hire

Work Authorization: GC/US Only (no EAD’s)

W2 only (No C2C)

Day to Day: Take complex business request / questions.. create a data product or dashboard for users to visualize these business requests/questions that arise.

Top skills/ How are they applied?

 Strong complex SQL skills
 BI Tools: Tableau is strongly preferred, open to others
 Data Analysis Skills, End User Design, Data Modeling (ETL or SSIS)
 VERY Strong Communicator that can work with Stakeholders

Minimum Qualifications

Education

Bachelor's degree in the healthcare field, analytics, biostatistics, informatics, computer science, business, or engineering required or
Master's degree in a related field preferred

Experience

3-4 years in data collection, management, analysis, and interpretation. Experience supporting customer groups through data analysis, report creation, and graphical presentation. required and
Tableau Desktop experience preferred and
Database development and maintenance. Metrics implementation, performance benchmarking, and data/metrics governance required and
Mathematical process modeling and simulation.. Operational and/or clinical data in healthcare field and knowledge of the healthcare industry preferred","Analítica de datos, Análisis de datos, Desarrollo de base de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL Server Integration Services (SSIS), Comunicación, Modelado de datos, Presentaciones y Recogida de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983294260/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=%2FaXW4WdaTKQ5%2BCIPLIkUbg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"55 US$/h - 65 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,Estados Unidos,"Acerca del empleo
Job Title: Data Engineer (Contract)
Location: United States
Work site: Fully Remote
LHH Recruitment Solutions is seeking an skilled Data Engineer for a large client of ours providing cutting edge software solutions to those in the legal industry. In this role, you will build a highly reliable, trustworthy data lakehouse that is leveraged across the organization to derive insights and make real-time decisions. You will be bringing your skills to drive new data projects, build reliable data pipelines, set up strong monitoring and alerting systems, and build data solutions to support a diverse set of use cases.

Responsibilities
Design and develop data pipelines using tools like Apache Airflow or by leveraging Snowflake's external tables functionality.
Translate HiveQL queries to Snowflake SQL, ensuring compatibility and efficient data processing.
Utilize DBT to model and transform data within Snowflake, adhering to best practices for data governance and maintainability.
Configure and manage data pipelines in GCP to orchestrate data movement and processing tasks.
Collaborate with data analysts and stakeholders to understand data requirements and ensure a successful migration outcome.
Monitor and optimize data pipelines for performance and scalability.
Develop and implement automated testing procedures to validate data quality and integrity after migration.
Document the migration process and provide ongoing support for the migrated data warehouse on Snowflake.

Requirements
Minimum 5+ years of experience as a Data Engineer with a proven track record of successful data warehouse/data lake implementation and management.
Deep understanding of leveraging Snowflake to build highly performant and resilient data warehouse.
Strong expertise in HiveQL, SQL, and experience with data warehousing/lake house concepts (dimensional modeling, data quality, etc.).
Strong programming knowledge in python.
Experience with Apache Spark for large-scale data processing (a plus).
Proficiency in DBT for data modeling and transformation in Snowflake preferred.
Experience working with Google Cloud Platform (GCP) and its data storage services (GCS, BigQuery - a plus).
Excellent written and verbal communication skills with the ability to collaborate effectively with cross-functional teams.
Strong problem-solving skills and a passion for building efficient and scalable data solutions.
Snowflake certification preferred.

Preferred Qualifications:
Strong understanding of data architectures and patterns.
Experience in DataOps implementation and support
Experience in MLOps implementation and support.
Experience in building and supporting AI/ML platform.

Benefit offerings: medical, dental, vision, life insurance, short-term disability, additional voluntary benefits, EAP program, commuter benefits and 401K plan. Our program provides employees the flexibility to choose the type of coverage that meets their individual needs. Available paid leave may include Paid Sick Leave, where required by law; any other paid leave required by Federal, State, or local law; and Holiday pay upon meeting eligibility criteria.

Pay Rate: $55-65/hr

Applicants must be authorized to work for any employer in the U.S. Our client is unable to sponsor or take over sponsorship of an employment Visa at this time.","Almacenamiento de datos, Apache Spark, Google BigQuery, Google Cloud y SQL, Data Build Tool (DBT), HiveQL, Lagos de datos, MLOps y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3944225065/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=zRIUKH97V1sxnnxx%2B4U2qg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer /Data tester,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,Estados Unidos,"Acerca del empleo
Position: Data Engineer /Data tester

Location: Tempe, AZ (Onsite)

Duration: W2 Only Contract

Job Description:

Minimum Qualifications- Education & Prior Job Experience

Bachelor's degree in Computer Science, Information Systems, Engineering, Technology, or related field or equivalent experience/training
2 years of relevant system development experience
2 years of experience working with automated testing, testable requirements, and continuous integration
2 years of experience working in an Agile environment

Preferred Qualifications- Education & Prior Job Experience

5 years of relevant system development experience
5 years of experience working in an Agile environment
Airline industry experience

Skills, Licenses & Certifications

Knowledge of systems flows, engineering documentation, tools, and architecture concepts
Understanding of business systems and industry requirements, and full technical knowledge of systems analysis
Understands automated testing, testable requirements, and continuous integration Knowledge of relational databases, SQL, HTML, Java Script, C#, and Python preferred
Ability to work closely with developers and quality assurance teams to create doman models, sequence diagrams, use case diagrams, and operation contracts
Experience with application administration, systems support, or development preferred
Experience managing user stories within an Agile software development process
Experience defining unstructured situations in terms of results and establishing a plan to reach them
Ability to adapt to unexpected events, new facts, and rapidly changing circumstances
Ability to thrive in a sense-of-urgency environment and leverage best practices, with a can-do attitude
Proficient in MS Office applications
Proficient in technical documentation","Análisis de sistemas, Automatización de pruebas, Desarrollo de sistemas, Diagramas de casos prácticos, Diagramas de secuencia, Documentación de ingeniería, Documentación técnica, Entorno Agile, Integración continua y Sistemas de negocios",Solicitud sencilla
https://www.linkedin.com/jobs/view/3933806017/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=5%2B0blj2mlI4SD0QE9nGBWw%3D%3D&trk=flagship3_search_srp_jobs,Python Data Engineer,"80 US$K/año - 160 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",Publicado de nuevo hace 1 semana,"Columbus, OH","Acerca del empleo
Duration: 6+ months

Location: OH-Columbus, 100% Remote

Overview: Every Turnberry consultant belongs to a practice, an internal group of consultants and leaders with shared experience and expertise. Each of these practices aligns to one of the core services Turnberry offers to clients. As a Python Data Engineer, you will join Turnberry's Data Strategy and Intelligence practice and service. This service provides insights into company data, advanced analytics solutions, robust data governance, and tailored solutions for specific data challenges.

Responsibilities

Assist the IT Data Engineering Team in developing the next generation data and analytics infrastructure
Write high quality SQL code to retrieve and analyze data from database tables (primarily Databricks) 
Develop high quality SQL models for ad-hoc requests, as well as ongoing reporting/dashboarding
Work directly with business stakeholders to translate between data and business needs
Continually improve SQL models through automating or simplifying self-service support for datasets

Qualifications

Bachelor's degree in Computer Science, Information Systems, Engineering, Data Science, or other similarly technical related field Mathematics, or specialized training/certification or equivalent work experience
2-3-years minimum professional experience in cloud data engineering and science and associated technologies utilizing cloud data platforms such as Databricks, AWS RedShift, and Snowflake
2-3-years minimum experience with advanced SQL data modeling and query optimization and Python
Proficiency in using visualization tools such as Tableau, Domo, or Power BI
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong verbal, written & presentation skills with the ability to effectively communicate complex technical information to personnel at all levels of the organization
Experienced in Python (requests library)
Proficient in extracting data from APIs and landing it in a data lake)

Preferred Qualifications

Specific experience with Data Warehouse/Data Lake configuration and development using Databricks platform
Experience with Tableau/Sigma Computing
Experience operating in an Agile development environment
Familiarity with usage of Agile tools (JIRA/Confluence)
Understanding of CI/CD deployment models and release strategy as well as SCM tools (Git preferred) and code management best practices
Experience in AWS environment
Experience with cloud ELT platforms such as AWS Glue, Talend Stitch, or FiveTran

The salary range for this role is $80,000 to $160,000 or the hourly equivalent. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Turnberry Solutions offers benefits such as a comprehensive healthcare package (medical, dental, vision), disability and group term life insurance, health and flexible spending accounts, a utilization bonus, 401(k) with match, flexible time off for salaried employees, parental leave for salaried employees, and flexible work arrangements (all benefits are subject to eligibility requirements). No matter where or when you begin a career with Turnberry, you'll find a far-reaching choice of benefits and incentives.

At Turnberry, inclusion is one of our core values. We are fully invested in and focused on hiring and growing a diverse team of high performers. We're committed to creating a positive and connected work environment for all. We believe that uniqueness in ideas, experiences, and backgrounds make us a better Turnberry: Turnberry is an Equal Employment Opportunity/Affirmative Action employer, and recruits, employs, trains, compensates, and promotes regardless of age, ancestry, family medical or genetic information, gender identity and expression, marital, military, or veteran status; national and ethnic origin; physical or mental disability; political affiliation; pregnancy; race; religion; sex; sexual orientation; and any other protected characteristics.","Almacenamiento de datos y Ingeniería de datos, Amazon Redshift, Azure Databricks, Lagos de datos, Lenguaje de consulta (query), Modelado de datos, Optimización de consultas, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3979168647/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=lbDqcXjv8x5MZIw2tgdWRw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 1 semana,"Boston, MA","Acerca del empleo
Description

Data Engineer

Come build the future as a Data Engineer at Amazon, where you will be inspired working along best-in-class inventors and innovators! You will have the opportunity to create meaningful experiences that deliver on the ever-evolving needs of our customers, and your work will impact millions of people around the world.

As an Amazon Data Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. You will find that there is an unlimited number of opportunities within Amazon, where developing your career across a wide range of teams is highly supported. We are committed to making your work experience as enjoyable as the experiences you’ll be creating for our customers.

Apply now and you will be eligible for Amazon Data Engineer positions that are based on your preferred location, team, and more. We’re hiring across Amazon Stores in the United States and Canada.

Teams with available positions include, but are not limited to:

 Consumer Technology: Build new generation features and products for amazon.com, constantly improving the Customer and Seller experience for billions around the globe. Whether building site wide features such as reviews and recommendations, category specific software for the likes of Pharmacy, Electronics, Digital Software and Video Games or seller infrastructure, there are a variety of complex problems to tackle using a range of technologies in the design of your technical solutions.
 Operations Technology: Shape the future of transportation planning and execution on a global scale, that impacts hundreds of fulfillment centers, thousands of Amazonians, and millions of customers across the world. Your technology will support thousands of operators worldwide to design, build and run the best-in-class Amazon transportation network. We are building intelligent software to make transportation more reliable, faster, and less costly, providing a better and less expensive experience for our customers.
 Human Resources Technology: Create a seamless experience for millions of Amazonians and/or candidates. Whether supporting technologies for onboarding, time and attendance, compensation, amazon.jobs, or recruiting, you’ll deliver robust feature sets, elegant designs, intuitive user interfaces and systems that make it easy for Amazonians to excel at performing critical business functions.

About Us

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance be-tween your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign opportunities based on what will help each team member develop into a better-rounded contributor.

Inclusive Team Culture

Here at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Key job responsibilities

 Design, implement, and support a platform providing secured access to large datasets.
 Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
 Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
 Tune application and query performance using profiling tools and SQL.
 Analyze and solve problems at their root, stepping back to understand the broader context.
 Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
 Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
 Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
 Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com LLC - A03

Job ID: A2703397","AWS Lambda, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Datasets, Modelado de datos y Object Storage",Solicitar
https://www.linkedin.com/jobs/view/3984111687/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=XSfhhLjm9WglSlgQlkzFKg%3D%3D&trk=flagship3_search_srp_jobs,AWS Python Data Engineer (31666),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 4 días,"Chicago, IL","Acerca del empleo
Myticas Consulting is currently seeking a 100% Remote - AWS Data Engineer for a 1 Year contract position with our direct client.

Job Description: 
We are looking for a versatile and experienced professional to fill the role of Data Engineer with a strong expertise in utilizing AWS tools and services. The ideal candidate will be adept at both data analysis and engineering, with a deep understanding of AWS technologies to drive data-driven insights and solutions.

Responsibilities:
Data Exploration and Analysis: Leverage AWS tools such as Amazon Redshift, Amazon Athena, and Amazon Quicksight to extract, transform, and analyze data, providing actionable insights to support business decisions.
Machine Learning and Predictive Analytics: Develop and deploy machine learning models using AWS SageMaker, utilizing algorithms to generate predictive and prescriptive insights from large datasets.
Data Pipeline Development: Design, build, and maintain end-to-end data pipelines using AWS services like AWS Glue, ensuring efficient data movement and transformation from various sources to target destinations.
Real-time Data Processing: Implement real-time data processing using AWS Kinesis, Lambda, and other relevant tools, enabling near-instantaneous data insights and actions.
Big Data Technologies: Work with AWS EMR and related tools to process and analyze large-scale datasets using technologies like Apache Spark and Hadoop.
Data Warehousing: Design and optimize data warehousing solutions using AWS Redshift, ensuring high-performance querying, and reporting capabilities.
Data Quality and Governance: Implement data validation, cleansing, and quality checks using AWS services to ensure the accuracy and reliability of data.
Dashboard Creation: Develop interactive and visually appealing dashboards using Amazon QuickSight or other relevant tools, enabling stakeholders to easily consume and interpret data.
Collaboration: Collaborate with cross-functional teams to understand business requirements, provide data-driven insights, and develop solutions that align with organizational goals.
Cloud Cost Optimization: Monitor and optimize AWS resource utilization to manage costs effectively while maintaining high-performance data processing and analysis capabilities.

Qualifications:
Bachelor's or advanced degree in Computer Science, Data Science, Engineering, or a related field.
Strong proficiency in AWS services, including but not limited to Redshift, Athena, Glue, SageMaker, Kinesis, Lambda, EMR, and Quicksight.
Demonstrated experience in both data analysis and engineering, with a portfolio showcasing impactful data-driven projects.
Proficiency in programming languages such as Python, R, or Java for data manipulation, analysis, and model development and component deployment through Terraform.
Experience with machine learning frameworks and liparies (e.g., TensorFlow, Scikit-learn) and their integration with AWS tools.
Solid understanding of data warehousing concepts, data modeling, and SQL proficiency.
Strong problem-solving skills and the ability to work in a fast-paced, collaborative environment.
Excellent communication skills to translate technical insights into actionable business recommendations.","Almacenamiento de datos, Amazon QuickSight y Analítica de datos, Almacenamiento, Amazon Redshift, Conocimientos comerciales, Desarrollo de modelos, Manipulación de datos, Modelado de datos y Terraform",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982756467/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=P7ymkvnDcvJ%2FWRoGWaxOYw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Austin, TX","Acerca del empleo
Citizen Data seeks a highly organized and entrepreneurial Data Engineer to manage Citizen Data’s growing cloud database and infrastructure, aggregate and process big and disparate datasets, and optimize performance to ensure analytics are accurate and available on-demand.

The ideal candidate will have experience with cloud architecture or data engineering experience as well as strong data manipulation and data analysis skills. The Data Engineer collaborates with Citizen Data Analysts and Scientists to help manage and organize Citizen’s databases and to prepare, clean, and generate insights from incoming and existing data.

The Data Engineer will be an early, founding member of the Citizen product capability, and will be expected to contribute at a strategic level as much as a technical one.

Citizen is a remote company, and we are happy to consider candidates nationwide for this position.

Role Responsibilities

Ingest, organize and clean first-party, research, and external datasets
Oversee and grow Citizen’s database built on AWS cloud infrastructure, including access control and governance
Innovate data management solutions that help Citizen grow and scale
Manage and integrate API-based data streams to our existing storage infrastructure
Monitor ETL scripts for errors and find areas for improvements
Review new data processing, storage, and analysis tools to recommend to the broader team as Citizen pilots new products and methodologies
Coordinate with Data Analysts and Scientists in weighting and preparation of proprietary research data for analysis, storage, modeling
Fielding internal and external data requests and questions
Perform various data science, analysis, and/or research tasks as needed

Requirements

4-6 experience in a data-related role, preferably in cloud database engineering or management, or data cleansing/processing
Excellent communication and time management skills
Proficiency with data analysis packages in Python
Proficiency and experience with SQL
Experience with AWS or cloud computing services preferred
Experience with R a plus
Experience with data visualization tools such as Tableau a plus
Familiarity with a wide variety of data sources, including databases, public or private APIs and standard data formats a plus
Experience working with voter and/or consumer data preferred
A problem solver with great data intuition
Extremely well-organized
Competency in the application of G-Suite
Demonstrated commitment to using data for the public interest

Benefits

Competitive Compensation
Employer-Sponsored Health Care Plan (Medical, Dental & Vision)
Unlimited Paid Time Off (vacation, sick days & public holidays)
Family Leave (Maternity, Paternity)
Training & Career Development
Work From Home

Our greatest strength as a company is the team building it. As we strive to use data to bridge divides, advance meaningful change, and strengthen American democracy, we are committed to growing a team that reflects the diversity of this country. Citizen is an equal opportunity employer and we welcome applicants from any and all backgrounds, experiences, ideologies, and perspectives.","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Comunicación, Datasets, Limpieza de datos y Manipulación de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3980079280/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=kIUMtqoj6dEFtG12ZHfDhg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 6 días,"St. Louis Park, MN","Acerca del empleo
Who we are...
SafeBasements is a basement waterproofing and foundation repair service provider and product manufacturer, and an innovator in both client satisfaction and product dependability. Acquired by Percheron Capital in 2023, we are looking for talented team members to be a part of our exciting growth plans!

What we need...
This position is responsible for designing, building, and maintaining scalable data pipelines and systems to support the organization’s data needs. This role requires a combination of technical expertise, analytical skills, and a passion for data-driven decision making.

While data engineering is the primary responsibility of this role, we are looking for a wide range of experience in overall IT fields such as developer experience and CRM system configuration/maintenance. 

What you'd do...

Design and Development
Develop and maintain scalable data pipelines and ETL processes to collect, clean, and transform data from various sources.
Design and implement data models, databases, and data warehouses to support business analytics and reporting needs.
Work with data analysts, and other stakeholders to understand data requirements and deliver solutions.
Data Integration
Integrate data from multiple data sources, ensuring data quality and integrity.
Implement data validation and monitoring systems to ensure data accuracy and reliability.
Performance Optimization
Optimize data processes and systems for performance, scalability, and reliability.
Identify and resolve bottlenecks in data processing workflows.
Collaboration and Communication
Collaborate with cross-functional teams to define and prioritize data engineering projects.
Communicate complex technical concepts to non-technical stakeholders in a clear and concise manner.
Documentation and Best Practices
Create and maintain documentation for data pipelines, systems, and processes.
Establish and enforce best practices for data engineering, including coding standards, testing, and version control.

What you'd need...
Bachelor’s degree in Computer Science, Engineering, Information Technology, or a related field. Master’s degree preferred; relevant experience may be substituted for a degree. 
Proven experience as a Data Engineer or in a similar role.
Proficiency in programming languages such as Python, Java, or Scala.
Experience with multiple SQL databases.
Hands-on experience with data pipeline and workflow management tools (e.g., Apache Airflow, Luigi, Hevo, AWS Glue).
Experience with cloud platforms (e.g., AWS, GCP, Azure) and their data services.
Excellent problem-solving skills and attention to detail.
Strong communication and teamwork abilities.

What will really make your candidacy stand out...
Knowledge of data visualization tools (e.g., Tableau, Power BI).
Understanding of data governance and security principles.

This position is based in our office in St Louis Park, MN. Because we are in the early stages of building our Corporate team, we have found it valuable to be together in the office Monday-Thursday, and most of us work remotely on Fridays. We all have flexibility to work remotely on ad hoc days as personal demands arise.","Analítica de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Modelo de datos, Resolución de problemas y Toma de decisiones basadas en datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983549291/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=p0RiQx%2F%2Fz19spsGILzKw%2Bw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Cyrpto/Python),"71 US$/h - 79 US$/h Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",hace 4 días,"Boston, MA","Acerca del empleo
Responsibilities

Kforce has a client that is seeking a Data Engineer (Crypto/Python) in Boston, MA. Responsibilities:

 Working closely with the analysts/business partners to understand the business problem
 Designing, developing, and supporting quality solutions that aligns with the technology blueprint and standard methodologies to solve business problems
 Actively participating in activities like innovation days, code, and design reviews, exploring emerging technologies etc.

Requirements

 B.S. degree or higher in Computer Science or equivalent experience
 2+ years of recent experience with relational database technology, data Engineering and ETL/ELT skills
 Working experience with Linux shell scripts and job scheduling tools like Control-M/Autosys
 Experience with data analysis and database design
 Experience with AWS services, Snowflake and Python
 Conceptual understanding of Blockchain and crypto based analytics gained in a financial services enterprise environment
 Ability to develop Data APIs to support varied application requirements
 Exposure to GitHub, and Jenkins, is desirable
 Knowledge of Finance and Investing domains is a plus

The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.

We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.

Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.

This job is not eligible for bonuses, incentives or commissions.

Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.","Analítica de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Autosys, Bases de datos, Inglés como lengua extranjera, Modelado de datos, Programación del trabajo y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984917465/?eBP=BUDGET_EXHAUSTED_JOB&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=7Y8FkYCUIL0Nm4zBViy9JQ%3D%3D&trk=flagship3_search_srp_jobs,Sr. Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 días,"Richmond, VA","Acerca del empleo
Role: Sr. Data Engineer

Location: Richmond, VA – Hybrid Onsite (Must be local & Face to Face Interview)

Job Description

Candidate will be expected to create and modify pipelines with a cloud-based Enterprise Resource Planning (ERP) system. To manage these pipelines the candidate will need experience with Azure Data Factory (ADF) and Sql Server (SSIS) pipelines. Candidate will be creating data pipelines that support data marts and will be expected to code processes according to our standards. Candidate will be well-versed in current methodologies for synchronizing data mart datasets from source systems via file-based extracts. Working with our business partners to develop data mapping from one system to another is desirable but not strictly required.

Required Skills

Candidate will need to be experienced with Azure Data Factory Pipeline development for both Azure and local SQL Server environments.
Candidate will need to be experienced with SSIS development for both Azure and local SQL Server environments.
Candidate will ideally have experience with Azure file storage data sources (data lake, blob, Hadoop)
Candidate will be expected to successfully complete training on HIPAA and PII
Candidate should have excellent communication skills (written and verbal)
Candidate will need to be self-motivated, independent, and willing to work flexible hours.
Candidate will be expected to work from home on occasion and therefore required to have a dependable high speed internet connection.
Candidate will be expected to pass CJIS background checks.
Candidate will have experience authoring Workday reports and outbound integrations
Candidate should be comfortable mentoring other personnel regarding Workday technologies.
Ideally, candidate would have experience working with Workday Studio","Extraer, transformar y cargar (ETL), Hadoop, Ingeniería de datos y SQL Server Integration Services (SSIS), Acceso de banda ancha, Bases de datos, Comunicación, Lagos de datos, Ley de portabilidad y responsabilidad del seguro médico en EE. UU. (HIPAA) y Microsoft Azure",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982763981/?eBP=BUDGET_EXHAUSTED_JOB&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=yyWGF3g%2BUNMPUrl7b%2B8AHw%3D%3D&trk=flagship3_search_srp_jobs,Composable Data Stack Python Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 días,Estados Unidos,"Acerca del empleo
Who We Are

We are looking for a Software or Data Engineer that is experienced in high performance Python data processing libraries (often referred to as the Composable Data Stack). You will collaborate directly with our CTO and be part of the core product team.

dlt is an open-source library that automatically creates datasets from messy, unstructured data sources. You can use the library to move data from about anywhere into the most well-known SQL and vector stores, data lakes, storage buckets, or local engines such as DuckDB, Arrow or delta-rs. The library automates many cumbersome data engineering tasks and can be handled by anyone who knows Python. You can see more details in this Hacker News article.

dltHub is based in Berlin and New York City. It was founded by data and machine learning veterans. We are backed by Foundation Capital, Dig Ventures, and many technical founders from companies such as Datadog, Instana, Hugging Face, MotherDuck, Mesosphere, Matillion, Miro, and Rasa.

Your Tasks and Responsibilities:

dlt is a missing part between traditional Modern Data Stack and the emerging Pythonic Composable Data Stack: a gateway that creates datasets which the other components can then process. Our mission is to integrate dlt fully with this new, emerging ecosystem in a way that our users love. This means we respect their time, effort and previous investments in modern data stack when designing features. To support this mission your tasks and responsibilities include:

You design and implement OSS features that make dlt a gateway to composable data stack: integrating query engines, transformation frameworks, table formats with our library
You listen to our users, always paying attention to what they need to go to production with dlt
You work with our customers in commercial projects where dlt is combined with existing ""modern data stack"" infrastructure
You maintain the open source project with the team (e.g., review PRs, resolve issues, talk with community contributors, etc.)


Requirements

Who You Are

If you are fascinated by the emerging ecosystem of data libraries in Python, which allows you to do on a single machine what until recently was possibly only in the cloud - then you'll enjoy working with us.

You know what duckdb, arrow, datafusion, lancedb, delta-rs, ibis, pyiceberg, sqlglot, kedro, hamilton and similar Python libraries / pip installable components do and know when to apply them
You have experience in building data apps or product based on composable data stack
... or you were contributing code to any (or similar) of projects above
You know what so called ""Modern Data Stack"" is and appreciate certain aspects of it (ie. maturity, fitting into enterprise workflows etc.)
... and in fact you are interested in combining both worlds
You really like Python and are fluent in writing Python code (e.g., Python typing, unit testing, writing docstrings, etc.)
You have a degree in computer science, data science, or other equivalent experience
You are familiar with GitHub workflows (e.g., pull requests, code reviews, CI/CD services, etc.)

Nice to Have:

You are based in Berlin and willing to work in our office regularly
You have a hacker nature and you love to make things optimized
Experience with DevOps (e.g., CI systems like GitHub Actions, Docker, Kubernetes, AWS/GCP/Digital Ocean, etc.)
Experience with machine learning (e.g., the toolset, the workflows, practical applications, etc.)


Benefits

What do we offer

In our work culture, we value each other's autonomy and efficiency. We have set hours for communication and deep work. We like automation, so we automate our work before we automate the work of others.

We are an office-first company but give you plenty of opportunities for deep work and work from home. Dedicated ""no meeting days"" to help the team focus on their most impactful work
As we work often from the Berlin office, we cover your public transportation ticket
We are deeply committed to your personal and professional growth, so we have an annual budget for learning and development
We offer regular subsidized team lunches and Urban Sports Club membership
We also have an ESOP plan for employees, depending on their role and dedication. We provide an option to increase your ESOP if you grow with us.","Ciencia de datos, Desarrollo web back end, Django, JSON y Python, Bases de datos, Ciencias de la computación, Hojas de estilos en cascada (CSS), Informática de alto rendimiento y Stack",Solicitud sencilla
https://www.linkedin.com/jobs/view/3971681924/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=Q6O8NUALcY%2BGNUWYI7tTLQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer Data Fabric,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Colorado, Estados Unidos","Acerca del empleo
En EY, tendrás la oportunidad de construir una carrera tan única como tú, con alcance global, apoyo, una cultura de inclusión y tecnología para convertirte en la mejor versión de ti. Contamos con tu talento y perspectiva para que EY también sea mucho mejor. Únete a nosotros y construye una experiencia excepcional para ti, y un mejor mundo de negocios para todos.

Aquí en EY centrarás tu inteligencia e imaginación en los temas más cruciales y complejos a los que se enfrentan las empresas, el gobierno y la sociedad actual. A través de retos podrás llenarte de conocimientos y experiencia que desarrollen tus habilidades para contribuir al desarrollo de tu carrera profesional.

La oportunidad

Únete a nuestro equipo y participa en proyectos en proyectos con uno de los clientes más importantes a nivel global, liderados por equipos de Estados Unidos pero interactuando con líderes y compañeros de varios países de América

Principales responsabilidades

Los servicios de Consultoría de EY tienen la amplitud y profundidad para proporcionar asesoría estratégica para ayudar a lograr resultados óptimos y sostenibles. Nuestro equipo global de consultores aporta una gran experiencia y un profundo rigor analítico a cada cliente. Desde el desarrollo de las primeras ideas hasta la implementación de estrategias con acciones concretas, estamos dispuestos a trabajar para definir estrategias que inspiren a sus clientes y empleados a alcanzar el gran potencial de su negocio.

Habilidades y atributos para postularte exitosamente

Para calificar al rol, debes contar con:

Estudiantes con disponibilidad de tiempo completo, Técnico, Tecnólogo o Profesional en Ingeniería de Sistemas, Electrónica o áreas afines a tecnología.
3 a 4 años de experiencia en
 Experiencia práctica demostrada trabajando con Synapse Pyspark 
Experiencia práctica demostrada trabajando con PowerBI (visualizaciones, modelado) ETLEngineering -Azure Stack
Experiencia en codificación con Spark y procesamiento distribuido es una plus
La experiencia de trabajo con Microsoft Fabric es un plus
Inglés avanzado indispensable. (EXCLUYENTE)

Lo que buscamos:

Colaborar en EY es un reto, necesitamos que adoptes nuevas tecnologías todos los días además de ser progresivo e innovador, sin perder de vista los estándares de calidad a los que nuestros clientes están acostumbrados. Además, debes visualizar siempre cómo contribuir a la sociedad, analizando y pensando siempre en cómo tu labor beneficia al mundo, siendo socialmente responsable y procurando el medio ambiente.

Queremos que en EY seas la mejor versión de ti mismo, por eso buscamos que veas por tu bienestar, seas curioso y abraces el cambio.

Te ofrecemos

Trabajar con tecnologías emergentes. Buscar nuevas oportunidades. Reinventarte cada día. Nuestra cultura de innovación en EY significa aceptar el cambio en todo lo que haces, desde la aplicación de nuevas tecnologías hasta la mejora de los procesos existentes. Tus ideas poderosas desbloquearán todo tu potencial, y el nuestro.

Además, podrás tener una carrera personalizada. Cuando tú prosperas, nosotros prosperamos. El paquete de beneficios de EY va más allá, enfocándose en tu bienestar físico, emocional, financiero y social.

Aprendizaje continuo: desarrollarás una mentalidad y habilidades que te lleven a navegar ante el futuro.
Éxito definido por ti: te daremos herramientas y flexibilidad, de esa manera tendrás un impacto significativo de la manera que lo deseas.
Liderazgo transformacional: te daremos las ideas, coaching y la confianza para ser el líder que el mundo necesita.
Una cultura diversa e incluyente: te valoraremos por lo que eres y te empoderaremos para que uses tu voz y así, ayudes a otros a alzar la suya.

Acerca de EY

Como líder global en servicios de aseguramiento, impuestos, transacciones y asesoría, estamos utilizando los productos financieros, experiencia y sistemas que hemos desarrollado para construir un mejor entorno de negocios. Esto comienza con una cultura que te ofrece la capacitación, las oportunidades y la libertad creativa que necesitas para mejorar las cosas. Sin importar el momento en que te unas a la firma, y sin importar cuánto tiempo te quedes, la experiencia excepcional de trabajar en EY dura para toda la vida. Y con nuestro compromiso por contratar y preparar a la gente más apasionada.

Si puedes demostrar que cumples con los criterios anteriores, contáctanos lo más pronto posible.

Envía tu solicitud ahora.","Almacenamiento de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y PySpark, Bases de datos, Hojas de estilos en cascada (CSS) y Microsoft Azure",Solicitar
https://www.linkedin.com/jobs/view/3952740070/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=mOZqjievynTYz8L%2F9UeP7A%3D%3D&trk=flagship3_search_srp_jobs,Associate Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Chicago, IL","Acerca del empleo
About Northern Trust

Northern Trust, a Fortune 500 company, is a globally recognized, award-winning financial institution that has been in continuous operation since 1889.

Northern Trust is proud to provide innovative financial services and guidance to the world’s most successful individuals, families, and institutions by remaining true to our enduring principles of service, expertise, and integrity. With more than 130 years of financial experience and over 22,000 partners, we serve the world’s most sophisticated clients using leading technology and exceptional service.

Major Duties :

 Uses existing procedures to solve standard problems; analyzes information and standard practices to make judgments
 Has limited impact on own work team
 Works within standardized procedures and practices to achieve objectives and meet deadlines

Knowledge :

 Requires conceptual knowledge of theories, practices and procedures within a job discipline
 Applies general knowledge of business developed through education or past experience
 Exchanges straightforward information, asks questions and checks for understanding

Experience :

 Accountable for own contributions

Working With Us

As a Northern Trust partner, greater achievements await. You will be part of a flexible and collaborative work culture in an organization where financial strength and stability is an asset that emboldens us to explore new ideas.

Movement within the organization is encouraged, senior leaders are accessible, and you can take pride in working for a company committed to assisting the communities we serve! Join a workplace with a greater purpose.

We’d love to learn more about how your interests and experience could be a fit with one of the world’s most admired and sustainable companies! Build your career with us and apply today. #MadeForGreater

Reasonable accommodation

Northern Trust is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation for any part of the employment process, please email our HR Service Center at MyHRHelp@ntrs.com.

We hope you’re excited about the role and the opportunity to work with us. We value an inclusive workplace and understand flexibility means different things to different people.

Apply today and talk to us about your flexible working requirements and together we can achieve greater.","Almacenamiento de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3971683811/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=skLNcIicIBHOKPayiI1qMg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - SQL & SSAS,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Colorado, Estados Unidos","Acerca del empleo
En EY, tendrás la oportunidad de construir una carrera tan única como tú, con alcance global, apoyo, una cultura de inclusión y tecnología para convertirte en la mejor versión de ti. Contamos con tu talento y perspectiva para que EY también sea mucho mejor. Únete a nosotros y construye una experiencia excepcional para ti, y un mejor mundo de negocios para todos.

Aquí en EY centrarás tu inteligencia e imaginación en los temas más cruciales y complejos a los que se enfrentan las empresas, el gobierno y la sociedad actual. A través de retos podrás llenarte de conocimientos y experiencia que desarrollen tus habilidades para contribuir al desarrollo de tu carrera profesional.

La oportunidad

Únete a nuestro equipo y participa en proyectos en proyectos con uno de los clientes más importantes a nivel global, liderados por equipos de Estados Unidos, pero interactuando con líderes y compañeros de varios países de América

Principales responsabilidades

Los servicios de Consultoría de EY tienen la amplitud y profundidad para proporcionar asesoría estratégica para ayudar a lograr resultados óptimos y sostenibles. Nuestro equipo global de consultores aporta una gran experiencia y un profundo rigor analítico a cada cliente. Desde el desarrollo de las primeras ideas hasta la implementación de estrategias con acciones concretas, estamos dispuestos a trabajar para definir estrategias que inspiren a sus clientes y empleados a alcanzar el gran potencial de su negocio.

Habilidades y atributos para postularte exitosamente

Para calificar al rol, debes contar con:

Estudiantes con disponibilidad de tiempo completo, Técnico, Tecnólogo o Profesional en Ingeniería de Sistemas, Electrónica o áreas afines a tecnología.
2 a 3 años de experiencia en SQL, SSAS (SQL Server Analysis Services) tabular, DAX, PowerBI, Python pyspark.
La experiencia de trabajo con Azure data lake store, Synapse y C# es un plus.
Inglés avanzado indispensable. (EXCLUYENTE).

Lo que buscamos:

Colaborar en EY es un reto, necesitamos que adoptes nuevas tecnologías todos los días además de ser progresivo e innovador, sin perder de vista los estándares de calidad a los que nuestros clientes están acostumbrados. Además, debes visualizar siempre cómo contribuir a la sociedad, analizando y pensando siempre en cómo tu labor beneficia al mundo, siendo socialmente responsable y procurando el medio ambiente.

Queremos que en EY seas la mejor versión de ti mismo, por eso buscamos que veas por tu bienestar, seas curioso y abraces el cambio.

Te ofrecemos

Trabajar con tecnologías emergentes. Buscar nuevas oportunidades. Reinventarte cada día. Nuestra cultura de innovación en EY significa aceptar el cambio en todo lo que haces, desde la aplicación de nuevas tecnologías hasta la mejora de los procesos existentes. Tus ideas poderosas desbloquearán todo tu potencial, y el nuestro.

Además, podrás tener una carrera personalizada. Cuando tú prosperas, nosotros prosperamos. El paquete de beneficios de EY va más allá, enfocándose en tu bienestar físico, emocional, financiero y social.

Aprendizaje continuo: desarrollarás una mentalidad y habilidades que te lleven a navegar ante el futuro.
Éxito definido por ti: te daremos herramientas y flexibilidad, de esa manera tendrás un impacto significativo de la manera que lo deseas.
Liderazgo transformacional: te daremos las ideas, coaching y la confianza para ser el líder que el mundo necesita.
Una cultura diversa e incluyente: te valoraremos por lo que eres y te empoderaremos para que uses tu voz y así, ayudes a otros a alzar la suya.

Acerca de EY

Como líder global en servicios de aseguramiento, impuestos, transacciones y asesoría, estamos utilizando los productos financieros, experiencia y sistemas que hemos desarrollado para construir un mejor entorno de negocios. Esto comienza con una cultura que te ofrece la capacitación, las oportunidades y la libertad creativa que necesitas para mejorar las cosas. Sin importar el momento en que te unas a la firma, y sin importar cuánto tiempo te quedes, la experiencia excepcional de trabajar en EY dura para toda la vida. Y con nuestro compromiso por contratar y preparar a la gente más apasionada.

Si puedes demostrar que cumples con los criterios anteriores, contáctanos lo más pronto posible.

Envía tu solicitud ahora.","Analítica de datos, Apache Kafka, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , PySpark, Python y SQL, Expresiones de análisis de datos (DAX) y SQL Server Analysis Services (SSAS)",Solicitar
https://www.linkedin.com/jobs/view/3969276813/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=r9EfXKqYGKOCksXSQ0Ppxw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 semanas,"Milwaukee, WI","Acerca del empleo
Codeworks is an IT Services firm headquartered in SE Wisconsin, known for our strong commitment to quality and for our direct client relationships.

Who We’re Looking For: A Data Engineer to support enhancement and stabilization of our data ingestion platform for cost and usage data across various business platforms. As part of our team of engineers and FinOps practitioners, you will play a pivotal role in maturing our current data systems. This is a long term contract position on site in Milwaukee WI!

Responsibilities:

Design, develop, and maintain AWS Aurora PostgreSQL data schemas.

Implement data transformations using AWS Glue for seamless integration with downstream databases via AWS Step Functions.

Optimize SQL queries for performance and efficiency.

Develop and maintain ETL processes to ensure timely and accurate data delivery.

Collaborate closely with team members to understand data requirements and translate them into technical solutions.

Troubleshoot and resolve data issues in a timely manner.

Document processes and procedures related to data engineering.

Qualifications:

Hands-on experience with AWS Glue for ETL processes.

Proficiency in PostgreSQL database administration and schema design.

Strong SQL skills with a focus on query optimization.

Experience With AWS Services Such As Step Functions.

Understanding of ETL best practices and data integration techniques.

Familiarity with Athena for interactive query analysis.

Knowledge of AWS billing concepts and experience with AWS Cost and Usage Report.

Background in FinOps or financial operations.

Proficiency in Python scripting for automation and data manipulation tasks.

Experience with Apache Spark as an alternative to AWS Glue.

Strong problem-solving skills and attention to detail.

Ability to work independently and collaboratively in a team environment.

Excellent communication skills to interact effectively with stakeholders.

Adaptable and willing to learn new technologies and tools as needed.

About Codeworks: Codeworks has over 25 years of experience serving Fortune 1000 companies in Wisconsin as well as our client's national locations. Our recruiting team excels at evaluating, advising, and connecting IT professionals with new opportunities that will satisfy their expectations regarding income and opportunity for growth. At Codeworks, we're committed to diversity, equity, and inclusion in our workforce and beyond. We believe in equal opportunities and value the unique perspectives that every individual brings to our team. Join us in creating an inclusive, innovative, and collaborative workplace where your talents can thrive.

Codeworks is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, disability, or national origin.","Apache Spark, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Administración de bases de datos, Bases de datos, Comunicación, Lenguaje de consulta (query), Manipulación de datos, Optimización de consultas y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3974882224/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=67Q4O43mw%2BNsjA%2B00vepag%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer/Scientist - Junior,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Lansing, MI","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

please check the below links to see success outcomes, salaries of our candidates .

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)(phone number removed)di

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3982357809/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=4GKsOYM4nN5aoQJgc8M%2BZw%3D%3D&trk=flagship3_search_srp_jobs,Data Analytics & Engineering - Data Engineer IV Data Engineer IV,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Nueva York, NY","Acerca del empleo
Job Description: Summary:

The main function of the Data Engineer is to develop, evaluate, test and maintain architectures and data solutions within our organization. The typical Data Engineer executes plans, policies, and practices that control, protect, deliver, and enhance the value of the organization’s data assets.

Job Responsibilities:

 Manage data engineering projects through the full cycle. 
 Identify and underline business initiatives from a data engineering perspective
 Design, construct, install, test and maintain highly scalable data management systems. 
 Ensure systems meet business requirements and industry practices. 
 Design, implement, automate and maintain large scale enterprise data ETL processes. 
 Build high-performance algorithms, prototypes, predictive models and proof of concepts. 

Skills:

 Ability to work as part of a team, as well as work independently or with minimal direction. 
 Excellent written, presentation, and verbal communication skills. 
 Collaborate with data architects, modelers and IT team members on project goals. 
 Strong PC skills including knowledge of Microsoft SharePoint. 

Education/Experience:

 Bachelor's degree in a technical field such as computer science, computer engineering or related field required. 
 Process certification, such as, Six Sigma, CBPP, BPM, ISO 20000, ITIL, CMMI. 

Comments for Suppliers:

EEO:

“Mindlance is an Equal Opportunity Employer and does not discriminate in employment on the basis of – Minority/Gender/Disability/Religion/LGBTQI/Age/Veterans.”
Aptitudes y experiencia deseables
SQL","Extraer, transformar y cargar (ETL), Gestión de datos y Ingeniería de datos, Ciencias de la computación, Comunicación, Comunicación oral, ISO 20000, Necesidades empresariales, Presentaciones y SharePoint",Solicitar
https://www.linkedin.com/jobs/view/3959997276/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=MtEL2VKRKKCjWQhIEFtlzQ%3D%3D&trk=flagship3_search_srp_jobs,Expression of Interest - Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,Estados Unidos,"Acerca del empleo
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The Marlee Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavours.
Help individuals understand, and bring out the best in themselves and each other.
The Marlee Talent Pool process:
Once you express your interest, you will be asked to complete the Marlee work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Marlee (Fingerprint For Success)
Backed by 20+ years of research, Marlee’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviours, and performance.
Ultimately, we help people find purpose and fulfillment at work, and help build and scale high-performing teams.
Keep in mind that joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your Marlee work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via: hello@getmarlee.com to help co-create the future of recruitment, together.","Almacenamiento de datos, Analítica, Analítica de datos, Análisis de datos, Big data, Extraer, transformar y cargar (ETL), Ingeniería de datos y Transact-SQL (T-SQL), Bases de datos y Desarrollo de software",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982515266/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=9yy68E84KG2mEpjeKsSSVQ%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Seattle, WA","Acerca del empleo
Why Jerry?

Join a pre-IPO startup with capital, traction and runway ($240M funded | 40X revenue growth in 4 years | $2T market size)
Work closely with brilliant leaders and team members who have scaled companies like Nvidia, Better, Nerdwallet, Wayfair, Sofi, etc. 
Disrupt a massive market and take us to a $5B business in the next few years
Be immersed in a talent-dense environment and greatly accelerate your career growth

About the opportunity: 

Jerry is building the first AllCar™ super app to help people optimize all aspects of owning a car – insurance, buy/sell, registration, loans, safety, repairs, parking, etc – a $2T market in the U.S. We started with insurance in 2019, and since then we’ve launched loan refinancing, real-time driving insights, a repair marketplace, car diagnostics, and a GenAI-powered chatbot. We have amassed over 5M customers, raised $240MM in funding, scaled our revenue 40X and our team to 250 across 7 countries.

The Data team at Jerry is responsible for supporting intelligent business decision making through analytical insights and advanced models. As a senior data engineer, you bring your previous industry experience in infrastructure scaling to Jerry and lead the development of a new infrastructure solution that will continue to perform with great stability and efficiency even when the company grows 10x bigger. You will join a team of data engineers, data analysts, and data scientists who operate with urgency and passion for excellence in pursuit of our mission to build the first AllCar™ super app.

How you will make an impact:

Partner with data scientists, software engineers and cross-functional stakeholders (Marketing, Operations, Finance) to build data pipelines that will provide critical data and insights to drive growth for the business, while ensuring data availability and accuracy
Partner with data scientists and machine learning engineers to evolve, optimize, and integrate critical predictive models
Partner with software engineers and data scientists to build data pipelines to consume and refine application data
Define data engineering standards and best practices and drive operational excellence
Perform regular maintenance and optimization of our data infrastructure

Ideal profile:

Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related technical discipline. Master’s degree in Computer Science or Engineering preferred
5+ years of experience in data engineer or similar role
Proven success in communication with database users, software engineers and senior management to collect requirements, discuss data modeling decisions and define data engineering strategies
Outstanding communication and problem solving skills, eager to work on the most pressing business problems and drive impact for the company
Experience with data modeling, data warehousing, and ETL pipeline development
Experience with column-based databases (Amazon Redshift, ClickHouse, etc.) and infrastructure integration with analytical platforms (Tableau, Qlik, Power BI, etc.)
Hands-on experience and advanced knowledge of SQL
Experience with big data technologies (Spark, Clickhouse, Redshift, Snowflake etc.)
Experience in process automation and deployment of advanced machine learning models

Jerry is proud to be an Equal Employment Opportunity employer. We prohibit discrimination based on race, religion, color, national origin, sex, pregnancy, reproductive health decisions or related medical conditions, sexual orientation, gender identity, gender expression, age, veteran status, disability, genetic information, or other characteristics protected by applicable local, state or federal laws. 

Jerry is committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, please contact us at recruiting@getjerry.com

About Jerry:

Jerry is America’s first and only AllCar™ app. We are redefining and radically improving how people manage owning a car, one of their most expensive and time-consuming assets.

Backed by artificial intelligence and machine learning, Jerry simplifies and automates owning and maintaining a car while providing personalized services for all car owners' needs. We spend every day innovating and improving our AI-powered app to provide the best possible experience for our customers. From car insurance and financing to maintenance and safety, Jerry does it all.

We are the #1 rated and most downloaded app in our category with a 4.7 star rating in the App Store. We have more than 6 million customers — and we’re just getting started.

Jerry was founded in 2017 by serial entrepreneurs and has raised more than $242 million in financing.

Join our team and work with passionate, curious and egoless people who love solving real-world problems. Help us build a revolutionary product that’s disrupting a massive market.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Automatización de procesos, Bases de datos, Comunicación, Modelado de datos, Oil Pipeline Development y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3977518068/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=UeUhTz6MCCOb8sDRipSBbg%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Junior/Entry,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Saint Louis, MO","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates can achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergistic understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and are looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Positions 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernetes, and REST API experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who match client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or C2C candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3965813699/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=iuedueqMTqpNLwhjJ%2BeJtg%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer - Entry Level,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 semanas,"Seattle, WA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates can achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergistic understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and are looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Positions 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernetes, and REST API experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who match client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or C2C candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3979094932/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MyBtvhPV32Www3DkQIDQlg%3D%3D&trackingId=T2OEc4ov9O2uWnmKmF0Fbg%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Programmer/Coder/Developer/Data Scientist/Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"San Mateo, CA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates can achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergistic understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and are looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Positions 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernetes, and REST API experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who match client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or C2C candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación y Programación, Ciclo de vida de desarrollo de software (SDLC), Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3983564235/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=6Rlu3RGi4Cpgf21cPEjpAA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Seattle, WA","Acerca del empleo
Data Engineer, EMR / EPIC
Location: Seattle, Washington (Hybrid)
Target Salary: $90-120K Annually, potentially more, DOE

Our Client is expanding, and hiring a Data Engineer to develop solutions to challenges that have continued to afflict our healthcare system. The Data Engineer will bring robust experience and ability to own the API strategy for a cloud-based warehouse environment that configures and loads customer data for modeling and utilization. 

2024 is a critical foundational year for systems and technology for our Client. As a result they have identified the need for a customer facing analytics solution as part of product implementation. This requires an experienced Data Engineer to support their B2B data pipeline strategy that is integral for scaling the first 100 customers and beyond.

About Client
They are a privately held organization, delivering revolutionary solutions to challenges that have long plagued our healthcare system. They design and build products that significantly improve patient outcomes. They have launched the innovative and broadly patented blood collection system, the only device on the market proven to significantly reduce false positive blood cultures - the standard-of-care for diagnosing sepsis. They have achieved outstanding clinical results and proudly count some of the most respected healthcare systems in the country as early customers. They are offering qualified candidates an opportunity to get in early and help build a company dedicated to improving the health and well-being of millions of people around the world.

About the Role
The objective of the Data Engineer is to own the API strategy of they Analytics customers into their Snowflake instance. They will perform the initial setup of data from the hospital, transform data into relevant insights that ultimately power a customer facing dashboard for their customers. In collaboration with customer-facing Clinical Solutions Analysts and Account Managers, they will be the point of contact to maintain current APIs related to hospital data or hospital code changes. They will be the steward of customer data per HIPAA/SOC2 compliance and strive to improve and harden the structure of MMT Snowflake tables.
 This position is part of their Revenue Operations team at this foundational stage of organizational growth. With guidance from the Director, the Data Engineer will collaborate with core team members to achieve customer success and achievement of goals. Ideally, this position is a hybrid and local to the Seattle Office headquarters, working 2-3 days per week onsite with the team.
 Responsibilities and Duties:
Manage cloud-based warehouse environment.
Expertly configure hospital data and load into Snowflake
Model data within Snowflake and utilize other Internal Company systems
Gather, document, communicate and configure to business and user requirements, as well as explore and deliver data solution to these requirements related to Offering.
Understand and navigate through hospital technological landscapes particularly the EPIC ecosystem
Partner with Internal Company subject matter and domain experts to develop and maintain hospital data pipeline that provide the foundation of Product.

Required Skills and Experience
7+ years of data engineering experience
Bachelor’s degree required. Master’s degree preferred
Strong expertise and experience with cloud-based data warehouse (Snowflake)
Strong understanding of the healthcare EMR, specifically EPIC ecosystem, including data identifiers (patient, payer, provider). *Certified in Epic Beaker and/or Epic Bridges, highly desired.
Demonstrated experience in data modeling, ability to enrich data, snapshot, etc.
Mastery of ETL tools: Experience with establishing external access and extract, transform, and load (ETL) strategies with multiple types of data sources, particularly a variety of electronic medical record systems.
Ability to assemble and query complex data from databases, perform in depth analysis and decipher computational data. *Programming and scripting skills, Java Script
Strong project leadership skills. Ability to prioritize and manage multiple projects/initiatives in a timely and cost-effective manner
Strong business acumen relative to project business case development and market analysis, and department budget management
Excellent interpersonal, written and verbal communication skills
Capability to work in a fasted paced environment and ability to develop entrepreneurship, agility, and open innovation mindset
Extensive experience with using project management and/or collaboration tools
Ability to travel frequently between sites as needed","Herramientas ETL y Ingeniería de datos, Asistencia sanitaria, Epic Systems, JavaScript, Lenguaje de consulta (query), Modelado de datos, Multitarea, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3958835034/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=zKaN7W1IEJ9FZdl6QxfOQA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Brentwood, TN","Acerca del empleo
Data Engineer
Remote | Product and Development | Full-Time
At Ncontracts we help financial institutions make better decisions. This is achieved by creating tools that help them understand their risks and compliance concerns and provide pathways for them to mitigate them. We also have teams of industry leading experts to walk them through the process.
This is an individual contributor role, working closely with other data team members to complete the building out of ETL pipelines and Operational Data Store (ODS). Your day-to-day tasks will include extracting data from Ncontracts applications, transforming it according to business logic, and efficiently loading it into our Operational Data Store (ODS). You will work with members from the cross-functional team to help them with the consumption of the data. You will be expected to use your expertise to optimize the process for performance and accuracy. Your commitment to continuous improvement will drive enhancements in our data architecture and ensure that our data system can scale.
A successful Data Engineer at Ncontracts will:
Contribute to building out ETL pipeline to include data from all Ncontracts products
Write performant and maintainable code
Ensure the quality of ETL process
Communicate well with other members of the data team and across the functional area
Advocate for best practices and continuous improvement
Participate in architecting and building a scalable data system
Mentor team members on their area of expertise
It is expected you will have the following skills and experience:
3+ Years of experience as Data Engineer designing and implementing data warehouse and data lake solutions on Microsoft Azure.
Experience with Azure Data Factory is required
Azure Data Lake is a plus
Exposure to Microsoft Fabric is a plus
Very strong background in MS SQL Server is required
Expertise in data architecture using different database types and data formats
Expertise in building data pipelines to clean, enrich, and transform data
Experience with Python is a plus and highly desirable
Expertise in database design and tuning techniques
Strong understanding of ETL process and tooling
Experience with version control systems (Azure DevOps, Git)
Familiar with CI/CD concept
It is helpful for you to have at least some of the following:
Experience working in horizontally scaling systems
Exposure to Azure SQL/warehouse/data lake products such as Azure SQL, Synapse, Databricks, ADLS
Exposure to Microsoft Fabric is a plus
Experience with other cloud data stacks (Google, AWS) is a plus
Familiarity with message/event driven architecture patterns and distributed systems architecture
Familiarity with systems integration
An automation mindset
A Data Engineer at Ncontracts is expected to exhibit the following behaviors:
Intentional mentorship:&#8239;Ncontracts is dedicated to teaching and growing talent and expects everyone to help those less experienced.
Honesty: Whether reviewing someone’s code, participating in retrospectives, or working with your team on what direction to take a project we expect openness and honesty. Honesty creates trust, and we believe that all great teams are built on trust.
Low Ego: Have confidence in your skills and experience but be willing to alter your opinions and ideas when another, better one comes along. Have strong opinions, but loosely held.
Deep Curiosity:&#8239;You will be expected to research new and exciting technologies, perfect the use of existing technologies, and discover new libraries and tools that can affect change across the organization.
Motivation:&#8239;You are a natural self-starter, and you enjoy solving problems. You can solve the problem with minimal instruction and figuring out what should be done.
WE OFFER
A fun, fast-paced work environment
Responsible PTO Plan that meets or exceeds state and local medical and family leave laws
11 paid holidays
Community and social events to keep you connected and engaged
Mental Health Benefits
Medical, Dental and Vision insurance
Company-paid Group Life Insurance, Short- and Long-Term Disability
Flexible Spending Account & Health Savings Account
Aflac Benefits – Critical Illness, Cancer Protection, & Hospital Choice
Pet Insurance
401 (k) with company match with eligibility on Day 1 of employment
2 Paid Volunteer Time Off Days
And much more!
Compensation Information
Pursuant to state and local law disclosure requirements, the pay range for this role, with final offer amount dependent on education, skills, experience and location is $130,000 to $150,000 per year. This position may be eligible for an annual discretionary incentive award. The incentive award amount is dependent upon company performance and your personal performance and is not guaranteed.
AAP/EEO Statement
Ncontracts provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.
This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
Other Duties
Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.","Almacenamiento de datos, Arquitectura de datos, Azure Data Factory, Extraer, transformar y cargar (ETL), Ingeniería de datos y Microsoft SQL Server, Arquitectura técnica, Comunicación, Control de versiones y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984380596/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=%2BVwzXOx6KBMBdETNmaj8xA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 3 días,Estados Unidos,"Acerca del empleo
About Inclusion Cloud:

Inclusion Cloud is a leading provider of digital transformation services with a focus on integrating cutting-edge technologies to drive operational efficiency and enhance business performance. We specialize in delivering tailored solutions across various industries, leveraging our expertise in IT Service Management (ITSM), IT Operations Management (ITOM), and HR Service Delivery (HRSD) to empower organizations to achieve their goals.

Job Description:

We are seeking a talented and motivated Data Engineer to join our AWS Partner Intelligence team. This role involves working directly with Software Engineering, Business Intelligence, Data Science, and Product teams to continuously improve our data infrastructure, design, tools, and pipelines. Your work will directly influence and drive organizational insights, customer-facing features, and machine learning models. This is a fully remote position.

Key Responsibilities:

Architecture Design: Implement next-generation data pipelines and BI solutions.
AWS Resource Management: Manage AWS resources including EC2, RDS, Redshift, Kinesis, EMR, Lambda, etc.
Data Architecture: Build and deliver high-quality data architecture and pipelines to support business analysts, data scientists, and customer reporting needs.
Data Integration: Interface with other technology teams to extract, transform, and load data from a wide variety of data sources.
Process Improvement: Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.

A Day in the Life:

Collaborate with Software Engineers, Product Managers, Data Scientists, and Business Intelligence Engineers to design, plan, and deliver on high-priority data initiatives serving internal stakeholders and AWS customers.
Build automated, fault-tolerant, and scalable data solutions leveraging state-of-the-art technologies including but not limited to Spark, EMR, Python, Redshift, Glue, and S3.
Continuously evaluate and improve our strategy, architecture, tooling, and codebase to maximize performance, scalability, and availability.

Qualifications:

Basic Qualifications:
1+ years of data engineering experience.
Experience with data modeling, warehousing, and building ETL pipelines.
Experience with one or more query languages (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala).
Experience with one or more scripting languages (e.g., Python, KornShell).
Preferred Qualifications:
Experience with big data technologies such as Hadoop, Hive, Spark, EMR.
Experience with any ETL tool like Informatica, ODI, SSIS, BODI, Datastage, etc.

Why Join Us:

Innovative Environment: Work in a dynamic and innovative environment where your ideas are valued and encouraged.
Professional Growth: Access to abundant training and certification opportunities to enhance your skills and advance your career.
Impactful Work: Contribute to projects that directly impact and improve the way organizations operate and deliver services.
Inclusive Culture: Be part of a diverse and inclusive team that fosters collaboration and creativity.

How to Apply:

If you are ready to take on exciting challenges and make a difference in the world of digital transformation, we want to hear from you! Apply now by submitting your resume and a cover letter outlining your relevant experience and qualifications.

Inclusion Cloud is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.","Almacenamiento de datos, Análisis de datos, Apache Spark, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos y Python, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3969278291/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=EVkWBjUU%2FOkjq0%2BZ57cwAQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"118,9 US$K/año - 205,6 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 2 semanas,"Seattle, WA","Acerca del empleo
Description

The AWS Network Finance and Data Transfer Data Engineering team provides foundational and centralized data platform for AWS Finance (supporting AWS Networking, AWS CloudFront, AWS Direct Connect) to identify financial insights for better understanding of our customers and costs. Our teams take on some of the hardest scalability, performance, and distributed computing challenges. We process big data and provide tools for customers to interactively understand the copious amounts of data we store.

We are looking for experienced, self-driven Data Engineer. In this role, you will be building complex data engineering and business intelligence applications using AWS big data stack. You should have deep expertise and passion in working with large data sets, data visualization, building complex data processes, performance tuning, bringing data from disparate data stores and programmatically identifying patterns. You should have excellent business acumen and communication skills to be able to work with business owners to develop and define key business questions and requirements. You will provide guidance and support for other engineers with industry best practices and direction. Amazon Web Services (AWS) has culture of data-driven decision-making, and demands timely, accurate, and actionable business insights.

Key job responsibilities

 Design, implement, and support data warehouse/ data lake infrastructure using AWS bigdata stack, Python, Redshift, QuickSight, Glue/lake formation, EMR/Spark, Athena etc.
 Develop and manage ETLs to source data from various financial, AWS networking and operational systems and create unified data model for analytics and reporting.
 Creation and support of real-time data pipelines built on AWS technologies including EMR, Glue, Redshift/Spectrum and Athena.
 Collaborate with other Engineering teams, Product/Finance Managers/Analysts to implement advanced analytics algorithms that exploit our rich datasets for financial model development, statistical analysis, prediction, etc.
 Continual research of the latest big data and visualization technologies to provide new capabilities and increase efficiency.
 Use business intelligence and visualization software (e.g., QuickSight) to develop dashboards those are used by senior leadership.
 Empower technical and non-technical, internal customers to drive their own analytics and reporting (self-serve reporting) and support ad-hoc reporting when needed.
 Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
 Manage numerous requests concurrently and strategically, prioritizing when necessary
 Partner/collaborate across teams/roles to deliver results.
 Mentor other engineers, influence positively team culture, and help grow the team.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines
 Experience with SQL
 Bachelor's degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent
 Experience of designing and developing data engineering systems from grounds up and maintaining/supporting existing systems.
 Experience in dealing with large and complex data sets and performance tuning
 Proficiency in one of the scripting languages - Python, Ruby, or similar
 Experience operating large data warehouses or data lakes
 Experience in designing data models that supports structures and unstructured data
 Experience in gathering requirements and formulating business metrics for reporting
 Ability to deal with ambiguities and competing priorities.

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)
 Master's degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent
 Prior experience in programming using Python

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com Services LLC

Job ID: A2692660","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Datos no estructurados, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3982206985/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=4g1BEmm3ZlCbvTGYVBkX4g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/ Fully Remote/ USC or GC Candidate only,"60 US$/h - 80 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,Estados Unidos,"Acerca del empleo
Job Title: Data Engineer
Location: Fully Remote
Duration: CTH

Description
Enterprise Req Skills:
Python, Aws, columnar database
Top Skills Details
1. Data engineer who has worked with ML experts and their requirements to optimize columnar databases using python as a back end technology (use clickhouse for columnar database but open to other technologies)
 2. Need to have worked within a cloud environment, our customer is AWS so that is the preference but open to other technologies
 3. This person should not only be comfortable optimizing databases but modeling them as well because optimization comes after modeling
What we’re looking for…
  *** Client*** is seeking an AI Database Optimization Engineer who will help shape the system and solution design of *** Client*** most advanced GenAI offerings. Ideal candidates will be passionate and have had proven expertise in database technologies especially columnar data modeling, query optimization and OLAP. You would be responsible to make sure the various system databases are well-structured to support downstream model training.
 You are ever curious and hungry to learn, with a bias toward action and delivering value to the customer quickly. We are building out a leading-edge, AI-first monitoring analytics platform for *** Client*** customers and prospects. The capabilities of this platform will eclipse what other observability vendors are bringing. This is the product you\'ll be working on.
   What you’ll be doing…
 • As the lead AI database optimization engineer, you\'ll resonate with and embody our core values –
 • Empathy: We empathize with our customers to help them solve core problems. Our empathy also extends to our team as we enable people to experiment, grow and do their best work.
 • Ownership: Joining at an early stage of this product\'s design and implementation, you will be expected to own large parts of the product and work through high ambiguity without explicit direction. You will also be expected to bring forth feedback and ideas to help shape the product.
 • Communication: Strong communication is required to work through ambiguity and showcase clarity of thought when interacting with customers, prospects, and the team.
 • Ambition: We are here to solve hard problems and want to bring on board people who seek out challenges and want to excel. Have something to prove about yourself? Willing to put in the necessary effort to overcome a lack of particular education or experience? You’ll make your mark here.
 • You will be working key leaders of our Analytics group including CTO of Skylar AI to develop core offering and help build out the team.
  Qualities you possess…
 • Enterprise experience working with highly scalable and performant databases like Clickhouse: Modeling and creating OLAP cubes for exploration. Mastery in query optimization and database system design.
 • Governance and guard-rail monitoring of production databases: Having participated in large scale data product deployment, you will help setup best practices around maintaining robust schema and entity modeling.
 • Understanding of ML Training Data and User Feedback: Guiding the data scientists to collect the right data for fine-tuning, relevance ranking optimization, and benchmarking.
 • BS/MS/PhD in Computer Science, similar technical field of study, or equivalent practical experience.
 • 5+ experience with software engineering developing and deploying production-level code for highly scalable distributed systems.
 • Experience with following tech stack
 o Frontend: React UI, Swagger APIs
 o Backend: Python, ClickHouse, OpenTelemetry
 o Cloud: Mostly AWS, but on-prem is deployed via HELM, as a cloud-native app
 o ML: HuggingFace Python libs, PostgresML, Pytorch, OpenAI","Amazon Web Services (AWS), Aprendizaje automático y Python, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3985051931/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=pxC1ilNei2r3pILHp9KTmQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Digital Healthcare,"170 US$K/año - 200 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,Estados Unidos,"Acerca del empleo
Discover International are partnered with a digital health leader that are revolutionizing cardiometabolic care.

They've built a digital health platform that provides AI-powered digital coaching across pre-diabetes, weight and obesity management. 

They're focused on transforming healthcare. 

This is a critical hire for our client. You will design, implement and maintain highly scalable big data pipelines. You'll work closely with Engineering & Product teams. 

Required Skills & Experience
  Experience in data engineering (additional education beyond the minimum requirement considered as commensurate experience).
Ability to work independently and collaboratively as part of a high-power team
Experience modeling, recommendations/personalization, working with big data using tools such as Apache Spark, Databricks, and Azure cloud computing strongly preferred
Python and SQL programming experience is required
Experience with tools such as Microsoft BI, R, Tableau preferred
Excellent analytical and problem-solving skills
Excellent verbal and written communication skills

Responsibilities
  Develop automated pipelines, streaming and batched, for a variety of data types and datasets.
Design and implement the data architectures to support the data pipelines.
Develop and maintain ETL processes and data pipelines using languages and tools like Databricks, SQL, Python, Apache Airflow, Apache Spark or other data integration platforms, as well as Powershell scripting.
Document data engineering processes, workflows, and data lineage to facilitate collaboration and knowledge sharing.
Work closely with data scientists, analysts, and other external stakeholders to understand their data requirements and ensure data availability
Build and maintain custom data pipelines to support ad-hoc data requests from internal and external stakeholders
Ingest and integrate data from various sources into Databricks, which may include databases, streaming data, data lakes, and external APIs.

To apply please contact Sam Shinner at Discover International","Analítica de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Comunicación, Comunicación escrita y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3968013235/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=%2F1i1%2Fk7d1SnPZLGEjMgGuw%3D%3D&trk=flagship3_search_srp_jobs,IT Data Engineer,"96,4 US$K/año - 160,7 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
GCI's IT Data Engineer IV will build and maintain the supporting framework for enterprise activities such as acquiring, transforming, and delivering data into a format that can be easily analyzed. Develop, maintain, and test infrastructures for analytics across the company. Leverage existing data infrastructure to fulfil all data-related requests.

Minimum Qualifications: 
Required: *A combination of relevant work experience and/or education sufficient to perform the duties of the job may substitute to meet the total years required on a year-for-year basis
High School diploma or equivalent.
Bachelor’s degree in Computer Science, Software/Computer Engineering, or relevant field. *
Minimum of eight (8) years’ experience in corporate data analytics, Real-time analytics and visualization tools, Data collection and/or data pipelines or related background. *
Preferred:
Experience in large database systems such as SQL Server, Oracle, and MySQL.
Other telecom industry or job specific certifications.

EEO: GCI is an equal opportunity employer. Qualified applicants are considered for employment without regard to race, color, religion, national origin, age, sex, sexual orientation, gender identity, marital status, mental or physical disability, veteran status, or any other status or classification protected under applicable state or federal law.","Analítica de datos, Canalizaciones de datos, MySQL y SQL, Administración de sistemas, Bases de datos, Ciencias de la computación, Recogida de datos, Sistemas de bases de datos y Soporte técnico",Solicitar
https://www.linkedin.com/jobs/view/3899572560/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=yh8Fq%2BSjrYCVeVq3WwF%2FGg%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 3 meses,"Houston, TX","Acerca del empleo
Responsibilities
•
Develop, maintain, and optimize data pipelines to extract, transform, and load large datasets from diverse sources into our data ecosystem
•
Design and implement efficient and scalable data models that align with business requirements, ensuring data integrity and performance
•
Collaborate with cross-functional teams to understand data needs and deliver solutions that meet those requirements
•
Work closely with data scientists, analysts, and software engineers to ensure seamless integration of data solutions into larger systems
•
Identify and resolve data quality issues, ensuring accuracy, reliability, and consistency of the data infrastructure
•
Continuously monitor and improve data pipelines and processes, identifying opportunities for automation and optimization


Qualifications
•
Bachelor's or Master's degree in Computer Science, Engineering, or a related field
•
5+years of hands-on experience as a Data Engineer, working on complex data projects and implementing data modeling solutions
•
Solid understanding of SQL and expertise in working with relational databases (e.g., PostgreSQL, MySQL)
•
In-depth knowledge of data modeling techniques and experience with data modeling tools
•
Working knowledge on Data warehousing
•
Familiarity with cloud-based data platforms and services (e.g., Snowflake, AWS, Google Cloud, Azure)
•
Experience with version control systems (e.g., Git) and agile software development methodologies
•
Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders

•
Primary Skillset: Data Engineering
Aptitudes y experiencia deseables
DATA ENGINEER","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Comunicación, Herramientas de modelización, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3983193777/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=nTTMBYnMlE%2Bn3behsSPNiQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Job Code: 1077),"145,6 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Detroit, MI","Acerca del empleo
Responsible for collaborating with business users, stakeholders, and subject matter experts to understand the functional and non-functional requirement gathering and for providing technical solutions; develop pipeline; responsible for notebooks to ingest data from technical content management Postgre SQL tool to Azure Data Lake Gen2 using data Azure Data Factory and Databricks; deliver several projects that include implementation, support, migration, and enhancement projects in the analytical space using SAP BW 7.3, SAP BW4HANA, SAP Data Services, Microsoft Azure Data Factory, Databricks, Azure Synapse Analytics, and Power BI; responsible for developing complex data model interfaces to push large amounts of data from SAP BW/S4HANA to the Azure cloud platform (Data Lake) and develop decision-making reports using Microsoft Power BI; responsible for designing and developing SAP BW/BW4HANA new data flows and enhance existing business data models as per the business requirements; perform, design, review, develop, test, and validate various business data models and writes several ABAP programming languages (Function Modules, Customer Existence, Start, Field, and End Routines) to transform the data and improve the performance of data loads in ETL jobs; responsible to build ADF pipelines and notebooks to ingest the data from SAP open hub tables into an Azure Data Lake; build ADF pipelines to ingest the data from various sources like SFTP, Oracle databases, MSSQL servers, and files, transforming DataStage ETL jobs to ADF pipelines; verify processes and activities are in place during the different stages of project including end user training; be responsible for technical analysis of specification and customer requirements/issues; present solutions to the client SMEs and gather their feedback; contribute to root cause analysis and impact analysis activities; review the issues faced by users and business specifications, analyzing the impacts, and provide possible solution; lead innovation through best practices/learnings from past and provide continuous improvement on all aspects of project delivery and solution development; responsible for identifying the process gaps and provide solutions to the business; communicate with the client about the work status and discuss the technical aspects of the software functionality; review the issues faced by users and business specifications, analyzing the impacts, and provide possible solutions; and lead innovation through best practices/learnings from past and provide continuous improvement on all aspects of project delivery and solution development. Location: Detroit, MI and various unanticipated locations throughout the US; Salary: $145,600 per year; Education: Bachelor’s Degree in Computer Engineering, Computer Science, Electrical Engineering, Electronic Engineering, or in a related field of study (will accept equivalent foreign degree); Experience: Five (5) years in the position above, as an Azure Data Engineer, as an SAP BI Developer, as a Software Engineer, as a Technical Lead, or in a related occupation; Other Requirements: Experience must include one (1) year’s use of all the following: SAP BI, SAP Native HANA, S4HANA, SLT, BODS, Power BI, Azure Cloud Platform, Azure Data Factory, Databricks, BOBJ, PySpark, and SQL. Will also accept any suitable combination of education, training, and/or experience.

Job ID: JOB-236674

Publish Date: 24 Jul 2024

Tagged as: Data Engineer (Job Code: 1077)","Extraer, transformar y cargar (ETL), PySpark y SAP BusinessObjects, ABAP, BI de SAP, BODS, DataStage, Modelo de datos, Protocolo seguro de transferenciade archivos y SAP BW",Solicitar
https://www.linkedin.com/jobs/view/3964347691/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=Y90rquMk%2F3wQKhUFj9xdvQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer with GCP,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,Estados Unidos,"Acerca del empleo
Jd

Data Engineer with GCP exp – GCP is required

ETL

BigQuery

MySQL

PostreSQL

Hadoop, Spark or Kafka

Kotlin & Java

UI integration

React.js

Data Warehouse","Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery, Google Cloud y Ingeniería de datos, Bases de datos, Java, Kotlin y React.js",Solicitud sencilla
https://www.linkedin.com/jobs/view/3787906022/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=DXvsSfZrfnPTLETlYOWCNQ%3D%3D&trk=flagship3_search_srp_jobs,Data Processing Engineer - Junior,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 7 meses,"Columbia, MD","Acerca del empleo
Belay Technologies has been voted Baltimore Business Journal's (BBJ) Best Places to Work 2019, runner up in 2020 and a finalist in 2021!

Belay Technologies is seeking a Data Processing Engineer - Junior Level to join our intel team. The ideal candidate shall convert customer requirements, including informal requirements, into total systems solutions that fit within the technical, schedule, and cost constraints of the program. Collaborate with program management and other technical staff to plan feature releases. Conduct and participate in design reviews, demonstrations, and user feedback discussions. Develop cyber analytic tools using Python, Linux command line tools, and scalable databases (Elastic, PostgreSQL).

Candidates should have the following qualifications: 

TS/SCI Clearance
Minimum Education:
BS in Computer Science, Software Engineering, or Equivalent
Minimum Experience:
BS + 0 yrs
Candidates are required to have the following skills:

DPE Core Skills:
Experience developing in a Linux environment (RHEL, CentOS, Ubuntu)
Knowledge of networking concepts include the OSI model
Project management with Jira and Confluence
Hands-on experience with Python, Virtualization (VMWare, Kubernetes), and Version Control Tools (SVN, Gitlab),
Network analytic skills including use of Wireshark and command line tools such as tshark and tcpdump
Demonstrated ability to participate in cross functional planning, coordination, and task execution situations involving the full spectrum of system engineering and integration activities
Good verbal and written communication skills
Candidates are desired to have the following skills:

DPE Desired Skills:
Hands on experience with YAML, Elasticsearch, SQL, MySQL, or PostgresSQL
Deployment experience with Ansible and/or Docker
Experience with dataflow pipelines (Jenkins, Apache NiFi, etc.)
Django experience a plus
Perks and Benefits:

8 weeks paid leave - 4 weeks of personal leave, 3 Yay! days, take off on your birthday,11 paid holidays and optional leave up to 6 days through Belay's volunteer program
10% matching in 401(k) contributions vested on day one
$5,000 annual training/tuition
Student Loan Repayment Program
100% company-funded HSA
Rich medical coverage (100% coinsurance)
Dental coverage including orthodontia
Up to $420,000 in life insurance, premiums 100% company funded
Amazon Prime, gym reimbursement, monthly lunches, games and prizes
Pet adoption program, generous referral bonus program, fun events, and more!

Belay Technologies is a certified Service-Disabled Veteran-Owned Small Business located in Columbia, Maryland (Baltimore/Washington area). Belay Technologies specializes in systems automation and full stack development. Belay Technologies provides leading technology and engineering solutions to the DoD, as well as state-of-the-art commercial products. We hire software engineers, web designers, test engineers, systems engineers, systems administrators, database engineers and other tech services. We are an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, national origin, gender, sexual orientation, gender identity, age, physical or mental disability, genetic factors, military/veteran status or other characteristics protected by law.

Powered by JazzHR

LayxxIbsMA","Herramientas de control de versiones, Bases de datos, Ciencias de la computación, Comunicación, Control de versiones, Demostración de productos, Ejecución de tareas, GitLab, Red Hat Enterprise Linux (RHEL) y YAML",Solicitud sencilla
https://www.linkedin.com/jobs/view/3971728797/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=M8mrpStv8OTDjPCgAkUj%2FA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer I (55810),"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Vancouver, WA","Acerca del empleo
Job Details

Job Location

Vancouver - Vancouver, WA

Position Type

Full Time

Salary Range

$75,000.00 - $85,000.00 Salary/year

Job Category

Engineering

Description

At Core Health & Fitness, our purpose is to live and share our passion for fitness. We bring innovative health and fitness solutions to the global market with brands like StairMaster, Schwinn, Nautilus, Star Trac, Throwdown, Wexer, and we’re still growing. We press into the future of fitness to ensure the creation of quality products and programming that meet the needs of an ever-evolving industry.

At Core we are committed to building an energetic, diverse, and inclusive workspace. We value our differences and see community strength in diversity and representation. We’re always on the lookout for innovators, dreamers and doers who are passionate about fitness and wellbeing. We explore all opportunities to improve ourselves, our business partners, and our communities. If you’re looking for a fulfilling career in helping people find the best version of themselves, you’ve come to the right place.

We are looking for a Data Engineer I to join our growing organization!

Qualifications

General Position Summary 

As a Data Engineer at Core Health & Fitness, you will play a pivotal role in designing, developing, and maintaining our data infrastructure. This position will work closely with our IT department, Business Intelligence Analysts, Systems Software Engineers, and other stakeholders to ensure the efficient collection, storage, and accessibility of data. Your expertise will enable us to make informed decisions and drive our business forward.

Essential Functions / Major Responsibilities

 Data Pipeline Development: Design, build, and maintain scalable and robust data pipelines to ingest, process, and transform data from various sources (IoT, internal business systems, etc.). 
 Data Warehousing: Develop and optimize data warehouses and data lakes to ensure efficient storage and retrieval of large datasets. 
 ETL Processes: Create and manage ETL (Extract, Transform, Load) processes to ensure data is accurate, reliable, and up-to-date. 
 Collaboration: Work closely with Business Intelligence Analysts, IT, Systems Software Engineers, and other stakeholders to understand data requirements and deliver solutions that meet their needs. 

Specific Job Requirements

 Database Management: Administer and optimize databases, ensuring high performance, security, and availability. 
 Data Quality: Implement data quality checks and validation procedures to maintain the integrity of the data. 
 Automation: Automate repetitive tasks to improve efficiency and reduce manual effort. 
 Documentation: Maintain comprehensive documentation of data processes, architectures, and workflows. 
 Analytics & Reporting: enable data analytics and reporting by providing well-structured, consistent data to either be analyzed within data warehouse or within an external repository system (CRM, ERP, etc.). 

Education And Experience Requirements 

 Education: Bachelor's or master’s degree in computer science, Information Technology, Engineering, or a related field – or equivalent 3+ years of experience in data engineering, data warehousing, or a related field. 
 Technical Skills:
 Proficiency in SQL and experience with relational databases (e.g., MySQL, PostgreSQL, Oracle). 
 Experience with ETL processes which aggregate IoT data from various external sources into a common data lake format. 
 Experience with big data technologies. 
 Proficiency in programming languages such as Python or Java. 
 Familiarity with ETL tools (e.g., Apache NiFi, Talend, Informatica). 
 Experience with cloud platforms (e.g., Azure, AWS) and their data services (e.g., Redshift, Databricks, BigQuery). 
 Experience with data modeling and schema design. 
 Familiarity with data visualization tools (e.g., Power BI, Tableau) 
Job Specific Competencies 

 Problem-Solving – Strong analytical and problem-solving skills with the ability to troubleshoot complex data issues. 
 Communication – Excellent communication skills with the ability to collaborate effectively with other stakeholders. 
 Creativity/Innovation - Generates new ideas, challenges the status quo, takes risks, supports change, encourages innovation, solves problems creatively. 
 Initiative - Tackles problems and takes independent action, seeks out new responsibilities, acts on opportunities, generates new ideas, practices self-development. 
 Productivity - Manages a fair workload, volunteers for additional work, prioritizes tasks, develops good work procedures, manages time well, handles information flow. 
 Quality - Is attentive to detail and accuracy, is committed to excellence, looks for improvements continuously, monitors quality levels, finds root cause of quality problems, owns/acts on quality problems. 

Working Conditions 

The physical and environmental demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Work may require:

 Bending, stooping, reaching, twisting, lifting, pushing, pulling and moving items - The employee is occasionally required to stand, walk, sit, and reach with hands and arms; and stoop, kneel, crouch, or crawl. Requires the ability to move around and maneuver products when necessary. Occasionally lifts and carries items weighing up to 15 pounds 
 Walking and Standing - Requires moving around. 
 Requires corrected vision and hearing to normal range 
 Requires working under stressful conditions or working irregular hours 

Work Environment

Work environment characteristics described here are representative of those an employee encounters while performing essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions: work performed in an office environment. Involves frequent interaction with internal and external customers.

Position Status 

Level: Staff

FLSA: Exempt

Supervises: N/A

This job description reflects management’s assignment of essential functions. It is only a summary of the typical functions of the job, not an exhaustive or comprehensive list of all possible job responsibilities, tasks, and duties. The responsibilities, tasks, and duties of the employee might differ from those outlined in the above job description and, other duties as assigned, might be part of the role. It does not restrict the tasks that may be assigned nor is it considered a contract of employment overriding at-will employment.

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

Core Health& Fitness is an equal opportunity employer that does not discriminate on the basis of race, color, national origin, sex, disability, age, religion, sexual orientation, gender identity, gender expression, creed, disabled veteran status, marital status, or Vietnam-era veteran status. If you are a person with a disability and you need assistance in applying for a position with Core Health & Fitness, please contact our Human Resources department at hr@corehandf.com and direct assistance will be provided.","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Lenguajes de programación, Calidad de datos, Ciencias de la computación, Internet de las cosas, Lagos de datos, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3983759343/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=puGPfsskRvuyk8Kw9W036g%3D%3D&trk=flagship3_search_srp_jobs,BI Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Denver, CO","Acerca del empleo
Job Description

Location: Linthicum Heights, MD or Cicero, IL (local applicants will be considered)

This is a hybrid position (2 days in the office)

Salary: $125k - $135k + 20% bonus

Type: Full-Time, Direct Hire

Seeking a self-motivated, energetic, detail oriented and a team player to join the Enterprise Data & Analytics Platforms team. As a Data Solutions Architect, you will lead our full-stack Business Intelligence solution to enable our data architecture strategy. This includes data modeling, data storage, performance metrics, visualization tools, metadata, data quality and data governance objectives. This role will drive our modernization efforts to ensure our data flows, processing, and reporting are accurate and insightful. The Data Solution Architect will mentor team members, work closely with senior leadership, product management, architects, and developers to design, build, and deploy data warehouse solutions and reporting tools that meet the growing analytical needs of our organization.

Responsibilities

 Perform, Design & Develop Enterprise Solutions
 Lead company's entire Business Intelligence solution that enables our data architecture strategy, including data design, modeling, storage, performance metrics, visualization tools, metadata, data quality, and data governance objectives.
 Drive modernization efforts to ensure our data flows, processing, and reporting are accurate, insightful, and dependable.
 Apply new ways to help make our data platform more scalable, resilient, and reliable.
 Collaborate with internal teams to implement new designs and solutions.
 Drive Data Modernization & Improve Governance.
 Partner with cross-functional senior leadership, product management, architects, and developers to design, build, and deploy data warehouse solutions and reporting tools that meet the growing analytical needs of our organization.
 Change how we think, act, and utilize our data by performing exploratory and quantitative analytics, data mining, and discovery.
 Ensure that the BI and Analytics team adheres to data standards, data governance policies and practices, and add/make recommendations regarding these standards as the need arises.
 Develop and enforce data engineering, security, data quality standards through automation.
 Optimize Data Models & Improve Reporting Platforms
 Identify opportunities to improve team efficiencies and apply best practices.
 Create improved sales and customer service strategies using appropriate predictive modeling.
 Monitor cloud cost/TCO and continuously improve cloud spend efficiency.
 Improve Insights to Address Critical Business Questions
 Collaborate and incorporate new trends and developments in current and future solutions as well as self-service solutions.
 Change how we think, act, and utilize our data by performing exploratory and quantitative analytics, data mining, and discovery.
 Create improved sales and customer service strategies using appropriate predictive modeling.
 Work closely with data analysts and business stakeholders to make this data easily accessible and understandable.
 Mentor and Collaborate
 Provided mentorship and guidance to the data engineering team, fostering a culture of technical excellence and innovation.
 Improve data quality by implementing re-usable data quality frameworks and build the process and tools to create and maintain Machine Learning pipelines
 Maintains Professional & Technical Knowledge
 Other duties, as assigned by the jobholder's supervisor, may also be required.

Qualifications

 Bachelor's degree in compute

To apply please email your resume to jdimond@ledgent.com","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Modelos predictivos, Calidad de datos, Estándares de datos, Modelado de datos y SQL Server Analysis Services (SSAS)",Solicitar
https://www.linkedin.com/jobs/view/3982968969/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=2WyvTQiV4VPRhi4YzMZgnQ%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist/Machine Learning Engineer,"75 US$/h - 85 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.",hace 5 días,Estados Unidos,"Acerca del empleo
Job Description:
We are seeking a highly skilled and motivated Data Scientist/Machine Learning Engineer
to join our dynamic team. 

Education: 
Masters(preferred)/Bachelors in Computer Science, Mathematics, Statistics, Engineering, In formation Technology or related

Core Skills:
Python along with popular and widely used Python libraries
SQL
ML models like Random forest, KNN, Time series forecasting
Neural Network Architectures, Transformers, ANN, BERT, GPT models, Open Source foundational models(e.g; LLama2/3).
mlFlow
Cloud(Azure is valued over AWS/GCP) based ML managed services(e.g; Azure OpenAI, Azure ML).
Vector databases, LM/LLM/GEN-AI based tools, libraries and frameworks(e.g; LangChain, Agentic, Semantic Kernel).
Performance evaluation of ML/DL/LLM systems, drift handling
Experience with PySpark/Databricks is valued, not mandatory.
Experience with Elasticsearch and integrations with AI components, is valued.
Experience of taking LLM based systems or Neural Network based systems or mixture of experts based systems to production, is highly valued.","Analítica de datos, Análisis de datos estadísticos, Análisis predictivo, Aprendizaje profundo, Ciencia de datos, Ingeniería de datos , Procesamiento de lenguaje natural, PySpark, SQL y Visualización de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984546643/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=62yw5E8brH9aqfGcDkat5w%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer with Java ,Hadoop ,Spark","Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,"California, Estados Unidos","Acerca del empleo
Location: Sunnyvale,CA

Duraiton: 1 yr

Designs, develops, and implements Hadoop eco-system based applications to support business requirements. Follows approved life cycle methodologies, creates design documents, and performs program coding and testing. Resolves technical issues through debugging, research, and investigation. Experience/Skills Required:1. Bachelor�s degree in Computer Science, Information Technology, or related field and 5 years experience in computer programming, software development or related2. 3+ years of solid Java and 2+ years experience in design, implementation, and support of solutions big data solution in Hadoop using Hive, Spark, Drill, Impala, HBase3. Hands on experience with Unix, Teradata and other relational databases. Experience with @Scale a plus.4. Strong communication and problem-solving skills
Aptitudes y experiencia deseables
JAVA","Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos y Python, Apache Impala, Ciencias de la computación, Documentos de diseño y Programación informática",Solicitar
https://www.linkedin.com/jobs/view/3978622923/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=0b4SYp10CfmAxdwIYV4u1g%3D%3D&trk=flagship3_search_srp_jobs,Data Mining Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Boston, MA","Acerca del empleo
Job Type: Regular

Time Type: Full time

Work Shift: Day (United States of America)

FLSA Status: Exempt

When you join the growing BILH team, you're not just taking a job, you’re making a difference in people’s lives.

Job Summary: The Data Scientist will be an MEng, PhD, or someone with similar educational or work experience who has both conceptual/theoretical and practical/hands-on expertise in data science, artificial intelligence, machine learning (includuing deep learning), and related areas and is highly motivated to contribute to scientific and engineering advances in a fast paced, high caliber, and entrepreneurial acadmic setting.

Job Description:

Essential Responsibilities: 

Devise, test, and implement algorithms for high-throughput data. 
Contribute to the generation of standard protocols and intellectual property. 
Design and implement computer algorithms, potentially including supervised and unsupervised machine learning methods, deep learning (autoencoders, transformers, geometric deep learning, dynamical systems, model decomposition), etc. 

Required Qualifications:

Master's degree in AI, Machine Learning, Data Science, Math, Eng, Phys or related field required. Doctoral degree in AI, Machine Learning, Data Science, Engineering, Physics, Math or related field preferred. 
0-1 years related work experience required. 
Expertise with Python libraries like Tensorflow/Theano/Keras, Numpy, Scipy, Pandas, Matplotlib, and Seaborn. 
Fluency in Unix/Linux environments, Python and ideally other standard bioinformatics tools (e.g. R, Perl, C, bash/csh/zsh, CUDA, OpenGL), ideally including hands-on experience with parallel processing. 
Demonstrated expertise in computational analysis of large data sets, ideally in imaging. 
Advanced technical computer skills as required for technical support specific to functional area and related systems. 

Preferred Qualifications:

Hands-on expertise with statistical descriptions of complex systems (e.g. energy, entropy, moments, etc.) and their theoretical underpinnings. 
Prior experience with/training in medical imaging. 
Experience building web applications/portals (e.g. Shiny Server or Python analogs). 

Competencies:

Decision Making: Ability to make decisions that are guided by general instructions and practices requiring some interpretation. May make recommendations for solving problems of moderate complexity and importance. 
Problem Solving: Ability to address problems that are varied, requiring analysis or interpretation of the situation using direct observation, knowledge and skills based on general precedents. 
Independence of Action: Ability to set goals and determines how to accomplish defined results with some guidelines. Manager/Director provides broad guidance and overall direction. 
Written Communications: Ability to summarize and communicate in English moderately complex information in varied written formats to internal and external customers. 
Oral Communications: Ability to comprehend and converse in English to communicate effectively with medical center staff, patients, families and external customers. 
Knowledge: Ability to demonstrate in-depth knowledge of concepts, practices and policies with the ability to use them in complex varied situations. 
Team Work: Ability to act as a team leader for small projects or work groups, creating a collaborative and respectful team environment and improving workflows. Results may impact the operations of one or more departments. 
Customer Service: Ability to provide a high level of customer service to patients, visitors, staff and external customers in a professional, service-oriented, respectful manner using skills in active listening and problem solving. Ability to remain calm in stressful situations. 

Physical Nature of the Job:

Sedentary work: Exerting up to 10 pounds of force occasionally in carrying, lifting, pushing, pulling objects. Sitting most of the time, with walking and standing required only occasionally

As a health care organization, we have a responsibility to do everything in our power to care for and protect our patients, our colleagues and our communities. Beth Israel Lahey Health requires that all staff be vaccinated against influenza (flu) and COVID-19 as a condition of employment. Learn more about this requirement.

More than 35,000 people working together. Nurses, doctors, technicians, therapists, researchers, teachers and more, making a difference in patients' lives. Your skill and compassion can make us even stronger.

Equal Opportunity Employer/Veterans/Disabled","Ciencia de datos y Reconocimiento de patrones, Computational Analysis, Comunicación, Comunicación oral, Conocimientos informáticos, Inglés, Observación, Situaciones estresantes y Soporte técnico",Solicitar
https://www.linkedin.com/jobs/view/3970467382/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=xQDDPfUsKiwHOweF1HAEew%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer I, Professional Services, Google Cloud","Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 semanas,"Austin, TX","Acerca del empleo
The application window will be open until at least July 18, 2024. This opportunity will remain online based on business needs which may be before or after the specified date.

Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Austin, TX, USA; Atlanta, GA, USA; Boulder, CO, USA; Chicago, IL, USA.Minimum qualifications:

Bachelor's degree in Computer Science, Mathematics, a related field, or equivalent practical experience.
3 years of experience with data processing software (e.g., Hadoop, Spark, Pig, Hive) and algorithms (e.g., MapReduce, Flume).
3 years of experience in Google Cloud.
Experience managing client-facing projects, troubleshooting technical issues, and working with Engineering and Sales Services teams.
Experience programming in Python and SQL.

Preferred qualifications:

Experience in technical consulting.
Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT, and reporting/analytic tools and environments.
Experience working with Big Data, information retrieval, data mining, or machine learning.
Experience in building multi-tier high availability applications with modern web technologies (e.g., NoSQL, MongoDB, SparkML, TensorFlow).
Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments.

About The Job

The Google Cloud Consulting Professional Services team guides customers through the moments that matter most in their cloud journey to help businesses thrive. We help customers transform and evolve their business through the use of Google’s global network, web-scale data centers, and software infrastructure. As part of an innovative team in this rapidly growing business, you will help shape the future of businesses of all sizes and use technology to connect with customers, employees, and partners.

As a Data Engineer, you will guide customers on how to ingest, store, process, analyze, explore, and visualize data on Google Cloud Platform. You will lead data migrations and transformations, partner with clients to architect scalable data processing systems, build efficient data pipelines, and resolve platform challenges.

In this role, you will collaborate with Google's strategic cloud customers and our team to successfully implement Google Cloud products.

Google Cloud accelerates every organization’s ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google’s cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.

The US base salary range for this full-time position is $118,000-$174,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.

Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.

Responsibilities

 Act as a trusted technical advisor to customers and solve complex Big Data challenges. 
 Create and deliver best practice recommendations, tutorials, blog articles, sample code, and technical presentations, tailoring approach and messaging to varied levels of business and technical stakeholders. 
Analyze on-premises and cloud database environments and consult on the optimal design for performance and deployment on Google Cloud Platform.
Travel regularly up to 30% of the time, in-region for meetings, technical reviews, and onsite delivery activities.
Communicate effectively via video conferencing for meetings, technical reviews, and onsite delivery activities.


Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .","Ciencia de datos, Computación en la nube, Extraer, transformar y cargar (ETL) y Google Cloud, Ciencias de la computación, Comunicación, Evaluaciones técnicas, Presentaciones técnicas, Resolución de incidencias y Tutoriales",Solicitar
https://www.linkedin.com/jobs/view/3889963217/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=N5fG%2BcOdhi0kmyaA0IxvLw%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Analyst/Engineer/Scientist,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 meses,"Plano, TX","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this market also we have been able to help our candidates get multiple job offers and $100k + salaries.

 please check the below links to see the success outcomes of our candidates  our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage see us exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, Google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry?

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 https://www.youtube.com/watch?v=OFoqPTNORew 

 https://www.youtube.com/watch?v=-HkNN1ag6Zk 

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://youtu.be/bJJl27D8bh0 

 We are looking for the right matching candidates for our clients 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Programmer 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java , javascript, C++, or software programming 
 Spring boot, Microservices, Docker, Jenkins, and REST API experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning Positions 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, and data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow 

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3981870362/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=jjLbXOD8W%2BXpbJDIX6xtmA%3D%3D&trk=flagship3_search_srp_jobs,"Digital, Data Engineer","Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 4 días,Washington DC-Baltimore y alrededores,"Acerca del empleo
Madison Energy Infrastructure (“MEI”) believes in the power of clean energy infrastructure and has quickly emerged as a preeminent developer, investor, asset owner, and operator of distributed generation. The Digital Data Engineer will actively support the Digital Team and will assist with development and implementation of data security policies and classifications according to best practices. This position presents significant growth potential within our dynamic organization, offering exposure to Distributed Generations (DG) solar and Battery Energy Storage Systems (BESS) nationwide.

What You’ll Be Doing:
Designing, building, maintaining, and optimizing infrastructure for data collection, storage, management, transformation, and delivery to MEI teams.
Identifying and implementing internal process improvements to ensure greater scalability, optimize data delivery, and automate manual processes.
Building required infrastructure for optimally extracting, transforming, and loading raw data from a variety of sources using AWS and SQL technologies.
Building pipelines to collect, store, transform, and deliver raw data in usable formats for visualization, analysis, business intelligence, and reporting.
Building analytical tools that leverage data pipelines to provide actionable insight into key business performance metrics including financial planning and analysis as well as operational efficiency.
Working with the Digital Director to develop, implement, and improve internal data security policies and classifications.
Working with MEI teams to support data infrastructure needs while assisting with data related technical issues.

What We Are Looking For:
Demonstrates a steady, even pace to ensure precision and high-quality work outcomes.
Possess a technical and analytical focus, working within established systems, standards, and procedures, with job-related communication based on knowledge and expertise.
Engages in decision-making within a well-defined job scope, adhering to established policies and procedures, with managerial support.
Emphasizes job-related knowledge and expertise, maintaining helpful and supportive communication with both management and peers in a structured work environment.
Demonstrates leadership focused on consistent, accurate, and high-quality work output, utilizing a supportive and non-threatening leadership style. Delegates tasks to others appropriately, providing training, coaching, and on-the-job experience.

Location: Vienna, VA or Charlottesville, VA 

Compensation Range: $80,000 - $130,000
Actual salary offered may vary depending on job-related factors including, but not limited to, knowledge, skills, experience, and location. 

Madison Energy Infrastructure is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran, or disability status.","Amazon Web Services (AWS), Canalizaciones de datos, Capacidad de análisis y Gestión de datos, Análisis de requisitos, Infraestructura de datos, Mejora de procesos continua, Métricas de negocio, Recogida de datos y System Performance",Solicitar
https://www.linkedin.com/jobs/view/3967741800/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=GLvPPH68D8cLXakV8MOSPA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer I ICP,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Scottsdale, AZ","Acerca del empleo
Overview Looking to be part of something more meaningful? At HonorHealth, you’ll be part of a team, creating a multi-dimensional care experience for our patients. You’ll have opportunities to make a difference. From our Ambassador Movement to our robust training and development programs, you can select where and how you want to make an impact. HonorHealth offers a diverse benefits portfolio for our full-time and part-time team members designed to help you and your family live your best lives. Visit honorhealth.com/benefits to learn more. Join us. Let’s go beyond expectations and transform healthcare together. HonorHealth is one of Arizona’s largest nonprofit healthcare systems, serving a population of five million people in the greater Phoenix metropolitan area. The comprehensive network encompasses six acute-care hospitals, an extensive medical group with primary, specialty and urgent care services, a cancer care network, outpatient surgery centers, clinical research, medical education, a foundation, an accountable care organization, community services and more. With nearly 14,000 team members, 3,700 affiliated providers and hundreds of volunteers dedicated to providing high quality care, HonorHealth strives to go beyond the expectations of a traditional healthcare system to improve the health and well-being of communities across Arizona. Learn more at HonorHealth.com. Responsibilities Job Summary The Application Systems Data Engineer I develops, codes, implements, and supports ETL processes and Data Warehouse activities. Data Engineer I works closely with the business and translates the requirements into technical specifications to deliver a robust scheduled job to ingest and disseminate the information. Data Engineer I monitors, troubleshoots and supports the production data pipeline jobs that transfer data to and from the databases. Code, test, and implement integration software to ETL solution. Develop efficient mappings for data extraction/transformation/loading (ETL) from different sources to a target data warehouse. Schedule and integrate application/software logs to monitoring application. Implement stored procedures and effectively query the RDMS (MySQL, Oracle, SQL Server) database. Debug and troubleshoot errors and provide required support to the production jobs. Produce in-depth documentation of all development, policies and procedures. Participate in development planning sessions, weekly team status meetings and create/update appropriate project management documentation as required. Available for occasional off-hours emergencies. Maintain current knowledge in new and existing software to ensure productivity, capability, and return on investment. Performs other duties as assigned. Qualifications Education Bachelor's Degree Computer Science or related field Required Experience 1 to 2 years experience as an implementor of ETL software (like Talend or Informatica) Required 1 year or less of Data Warehouse Experience Required 1 year or less of Database development (SQL, PL/SQL, MySQL, Oracle) Required","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos y Visualización de datos, Ciencias de la computación, Ingeniería informática, Lenguaje de consulta (query) y Procedimientos de almacenado",Solicitar
https://www.linkedin.com/jobs/view/3984548442/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=AilYZGWmKFNITO203f9Krg%3D%3D&trk=flagship3_search_srp_jobs,Airflow Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,Estados Unidos,"Acerca del empleo
This is remote role 

 Fulltime or C2C 

 Around 125k/annum as a fulltime salary or 75/hr on C2C 

 Airflow Data Engineer : 

The Airflow Data Engineer is responsible for designing, implementing, deploying, and supporting various data management technologies and architectures. In partnership with business leaders, key stakeholders and cross-functional project teams, the Data Engineer will be an active contributor in a collaborative team structure and will have the opportunity to accelerate the delivery of and improve the quality of products providing increased operational excellence, a greater client experience and other strategic objectives.

 Experience with data pipeline and workflow management tools: Airflow. 
 Experience with developing guidelines for Airflow clusters and DAG's/Task's etc. 
 Experience with Performance tuning of the DAG and task implementation 
 Develop DAG - data pipeline to on-board and change management of datasets 
 Experience installing Apache Airflow, configuring, and monitoring Airflow cluster 
 Understanding of airflow rest services and integration of airflow platform eco-system 
 Working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with databases. 
 Knowledge about big data eco systems like Cloudera/Hortonworks components hadoop,spark,Hive etc.. 
 Experience in building and optimizing - big data- data pipelines, architectures, and data sets. 
 Build processes supporting data transformation, data structures, metadata, dependency and workload management. 
 Orchestrating the Airflow/workflow in hybrid cloud platform AWS/Azure/GCP setup and administration is a plus. 
 Proficient in modern programming languages (Python,Java) with experience and open-source technologies. 
 Possess professional software engineering practices and best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations. 
 Knowledge in DEVOPS models with experience in containerization platform via building the docker,assembly,deployment,automation etc.. and best practices for managing the Applications. 
 Experience in managing a shared services is a plus and knowledge about Infrastructure management capacity assessment,forecast,planning etc.. 
 Ability to maintain clean and secure data environments 
 Fast learner and a team player 
 Apache Airflow Fundamentals 

Agile/Scrum principles","Airflow, Big data, Canalizaciones de datos y Ingeniería de datos, Ajuste de rendimiento, Apache Airflow, Bases de datos, Datasets, Establecer prioridades del trabajo y Gestión de flujos de trabajo",Solicitar
https://www.linkedin.com/jobs/view/3981427897/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=CCUgDVLdMSPUMkt3cNwIvg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II (50290),Presencial Intermedio,hace 6 días,"Mobile, AL","Acerca del empleo
The Hiller Companies, LLC has an immediate opening for Data Engineer II. If you are a dedicated professional with a passion for fire protection and safety, we invite you to apply today.

The Hiller Companies is a leading provider of fire protection and safety solutions, dedicated to safeguarding lives and property all around the world. Headquartered in Mobile, Alabama, Hiller extends its reach globally as well as in domestic markets with offices across the United States. With over 100 years in the industry, our mission is to ensure the highest standards of safety and innovation in comprehensive fire protection services, including installation, inspection, maintenance, and repair of critical re and life safety equipment. Our team remains laser-focused on one goal – making the world a safer place.

Job Summary: This role requires an individual adept in data processing, ETL development, and database management. You will be responsible for architecting and refining sophisticated data pipelines, ensuring the utmost data integrity, and enhancing our data storage and retrieval systems to support complex analytics and reporting needs. Your expertise will be instrumental in driving the company's operational efficiency and deepening our analytical capabilities, requiring effective collaboration with diverse teams to fulfill intricate data requirements and contribute strategically to our overarching data infrastructure goals.

Key Responsibilities

Ownership of building and optimizing data pipelines using .NET frameworks, Apache Spark, or similar technologies to handle large volumes of data efficiently.
Extend and develop API integrations using ASP.NET Web API or similar frameworks to ensure seamless data exchange between systems, with a focus on robust error handling and authentication mechanisms.
Work closely with analytics and business teams to enhance data models, translating business requirements into data model enhancements and iterating based on feedback.
Lead efforts to implement robust processes for monitoring data quality, ensuring accuracy and availability of data through automated validation checks and alerts.
Writing comprehensive tests and maintaining documentation to ensure the reliability and scalability of data pipelines and integrations.
Utilize advanced SQL querying capabilities to analyze complex datasets, investigating data anomalies and providing insights and recommendations based on findings.
Collaborate effectively with cross-functional teams, providing mentorship and guidance to junior team members, and leading initiatives to improve team processes and collaboration.
Design and implement advanced data models using .NET technologies such as Entity Framework Core, and develop complex ETL jobs using frameworks like Azure Data Factory or custom .NET applications.
Architect data integration solutions using .NET technologies like Azure Logic Apps or custom .NET middleware, and define data quality frameworks and standards for consistency and accuracy.
Assess and recommend tools for data governance with .NET compatibility, conducting proof-of-concept evaluations and collaborating with stakeholders to select appropriate tools and technologies.
Provide strategic input into the organization's data platform roadmap, identifying opportunities for innovation and improvement in data infrastructure and aligning data initiatives with business objectives.
Other relevant duties as assigned.

What We Are Looking For

Bachelor's degree in Computer Science, Data Science, Engineering, or a related field is preferred.
6+ years of experience in data engineering or related area, with a proven track record in building and managing data pipelines, data warehousing, and cloud-based data solutions is preferred.
Expertise in ETL tools like Talend, dbt, and data warehousing technologies such as Redshift.
Advanced knowledge of SQL and database management practices in PostgreSQL and SQL Server.
Proficiency in Python for data scripting and automation tasks or equivalent scripting language.
Deep understanding of data modeling, warehousing concepts, and big data technologies.
Exposure to C# and .NET preferred.
Demonstrated experience with data visualization and business intelligence tools like Tableau preferred.
Familiarity with machine learning concepts and data science practices preferred.
Knowledge of DevOps methodologies and tools for data pipeline automation and monitoring preferred.
Experience leading projects.
Strong analytical, problem-solving, and communication skills preferred, including the ability to effectively communicate complex projects to non-technical stakeholders.

Hiller is a drug-free workplace, an equal opportunity employer and ADA compliant.

We are proud to operate according to our Core Values: Passion to Perform, Trust to Act, Act Responsibly, and Make it Fun.

Most employee benefits start from the first day of employment, including:

Competitive compensation package, including pay advancement opportunities for industry certifications and continuing education.
Comprehensive benefits package, including health, dental & vision insurance, retirement plans, company paid & voluntary life insurance, company paid short term disability, voluntary long term disability, critical illness & accident insurance and paid time off.
Company-provided training, tools, and equipment, including $150 annual boot allowance for employees required to wear safety boots in their jobs.
Career advancement potential within a growing company.

Join us in our mission to provide comprehensive re protection solutions and peace of mind to our customers, and together, let’s make the world a safer place.","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, ASP.NET Web API, Amazon Redshift, Datasets, Modelado de datos, Modelo de datos y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3964341760/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=QgGWGdblpLArV4MMDtE8FQ%3D%3D&trk=flagship3_search_srp_jobs,Urgent Hiring || Data Engineer/Python || Plano Tx (Need local candidate only || Any Visa Except H1 || Only W2.,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 semanas,"Plano, TX","Acerca del empleo
Title – Data Engineer/Python

Location – Plano Tx

Contract/Duration – 5 months contract

Visa – Any Visa Except H1

Mode of Interview – Phone/Skype

Job Description -

The contractors will be working on publishing data to streams using a SDK, defining and creating schema for database tables and verifying that data is populated in the database.

The candidate needs to be have hands-on experience with Python, understanding of distributed data processing and familiarity with SQL and AWS.","Almacenamiento de datos, Base de datos relacional, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python, SQL y Streams, SDKs",Solicitud sencilla
https://www.linkedin.com/jobs/view/3977203395/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=TcZfbaSmzyQ1xxRB1FdEDw%3D%3D&trk=flagship3_search_srp_jobs,Remote - Data Scientist/Analyst/Engineer(ENTRY LEVEL),"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Henderson, NV","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3944125265/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=0dJcwd%2BHDDo9JUjNN55cNA%3D%3D&trackingId=esPy82duHmoFPYJ1qdcFcQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 semana,"Dallas, TX","Acerca del empleo
Build and maintain ETL processes in SSIS and Azure Data Factory
Engineer scalable, reliable and performant systems to manage data
Develop, implement and optimize stored procedures and functions
Research and analyze data issues and provide automated solutions
Implement new technologies to enhance the optimization of current practices
Provide valuable suggestions regarding new ideas and technologies

Aptitudes y experiencia deseables
SQL","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Procedimientos de almacenado",Solicitar
https://www.linkedin.com/jobs/view/3983564564/?eBP=BUDGET_EXHAUSTED_JOB&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=UG10esJVjr08KDKnMaFidw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Nueva York, NY","Acerca del empleo
DESCRIPTION

At quip, we design and deliver delightful products and services that keep your mouth healthy.

We value data as a critical asset that informs decision-making and drives ROI. As we transition from our legacy system to more specialized platforms, the importance of a central data hub and a dedicated team to manage, transform, and analyze this data has never been greater.

We are seeking a skilled Data Engineer to join our team and ensure the quality, consistency, and reliability of our data infrastructure. This role will involve both new data engineering projects and ongoing maintenance to support our growing needs. You will work closely with various teams, providing the necessary guidance to prioritize work, maintain high-quality standards, and drive changes within our infrastructure.

This role will report into the Director of Engineering and work closely with Data Analysts to prioritize data infrastructure needs.

Base Salary: 160-180k

This range represents anticipated base salary range for this position, in addition to competitive benefits and equity (as applicable). We carefully assess and consider several factors with each candidate, including location, role-related knowledge, experience and skills. Compensation may vary based on these considerations

RESPONSIBILITIES

Collaborate with business partners to understand and prioritize data engineering tasks.
Ensure high-quality standards and adherence to best practices in data management.
Interact with and enact changes within the data infrastructure.
Write SQL or Python code to build, update, and enhance data transformations and pipelines.
Review pull requests and debug complex data transformations.
Maintain and enhance existing data pipelines and transformations.
Work with BI platforms such as Looker, Tableau, Power BI, Domo, and Mode.
Utilize data orchestration tools like Apache Airflow, Cloud Composer, AWS Data Pipelines, Celigo, and Astronomer.io.
Manage data warehouses like BigQuery, Snowflake, Redshift, and Azure.
Use ETL tools such as Fivetran, Stitch, and Airbyte or develop custom data pipelines.
Communicate effectively with technical and non-technical stakeholders to ensure alignment and understanding of data engineering tasks and outcomes.
Take ownership of projects at both a technical and organizational level, ensuring they are completed on time and meet business objectives.
Work closely with Data Analysts to prioritize data infrastructure needs and support their analytical requirements.

REQUIREMENTS

3-5 years of experience as a Data Engineer in a similar-sized company.
Expertise in SQL and dbt.
High proficiency in Python.
Experience with BI platforms (Looker, Tableau, Power BI, Domo, Mode, etc.).
Advanced skills in data orchestration software (Apache Airflow, Cloud Composer, etc.).
Experience with enterprise data warehouses (BigQuery, Snowflake, Redshift, Azure, etc.).
Familiarity with ETL tools (Fivetran, Stitch, Airbyte) and custom data pipeline development.
Proven ability to collaborate with business stakeholders to understand and prioritize work.
Problem-Solving Skills: Strong analytical and troubleshooting skills to resolve complex data issues.
Attention to Detail: Meticulous attention to detail to ensure data accuracy and integrity.
Communication Skills: Excellent verbal and written communication skills to effectively collaborate with technical and non-technical stakeholders.
Adaptability: Ability to adapt to changing priorities and new technologies in a fast-paced environment.
Project Management: Experience managing projects and coordinating with multiple teams to deliver data solutions on time.

Nice to Haves

NetSuite experience is a plus.
Experience in omni-channel consumer products is a plus.
Relevant certifications such as Google Cloud Professional Data Engineer, AWS Certified Big Data – Specialty, Microsoft Certified: Azure Data Engineer Associate, or similar are a plus.

Benefits

Hybrid working environment, with an office located in the heart of DUMBO with breathtaking views of Manhattan Bridge
Year round early ""summer fridays""
WiFi enabled rooftop
Competitive medical benefit package (with an option to opt into a premium internal dental program)
Commuter benefits for parking & transit
Competitive paid parental leave policy for qualifying employees
Open vacation policy
Dog-friendly office space
401k Tax Benefits
Competitive compensation package
Bonus pool eligibility
Fully stocked kitchen with snacks (make sure to use your quip after snacking!)
Your very own quip with employee discount packages
The opportunity to help us make oral care more simple, effective and accessible!

About quip

quip is a modern oral health company launched in 2015 that provides thoughtfully designed personal oral care products and professional dental care services through a digital platform that makes oral care more simple, accessible, and enjoyable. The current personal care offerings include a wide selection of American Dental Association accepted (ADA seal) adult and kid electric toothbrushes, smart brushes, refillable floss pick and string, refillable mouthwash and gum, all kept fresh with a refill delivery service. quip's professional platform, which is behind quip Aligners and quipcare, is part of the company's future vision to connect personal care, oral health monitoring and professional care in one digital oral care companion app that helps access and manage all your oral care needs and guide and incentivize good oral health habits.

quip is committed to promoting equality, inclusion, and diversity beyond your brushing routine. We believe our company is better equipped to care for every mouth when we listen to fresh perspectives from every voice, and in doing so, we build an equal-opportunity team of the brightest minds (and mouths)—regardless of race, gender, age, religion, sexual orientation, identity, or any other trait that makes you you. By celebrating and supporting our differences, we will thrive in our mission to improve oral health for all.","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, Amazon Redshift, Comunicación, Comunicación escrita, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3971536572/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=Rj1TmOUKG%2FWZjgTz7gfonQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Baltimore, MD","Acerca del empleo
We are seeking a Data Engineer to join our growing Professional Services organization, who will be responsible for converting customer data onto our enterprise software platform. This position involves extensive data analysis, development of solutions via ETL to migrate data to our software and establishing new best practices for high complexity conversions.

This is a remote position, with less than 10% travel.

Responsibilities:

Collaborate with Business Analysts and other Professional Services teams to understand data requirements, provide data analyses to assist with the configuration of our software, and accurately map data to meet customer needs.
Participate in or lead working sessions with customers to identify data requirements, understand and map their data, and consult on best practices for data conversion.
Perform analysis to understand the customer's unique and often complex source data
Execute data profiling and quality testing with attention to detail.
Utilize SQL Server (T-SQL and SSIS), Visual Studio and UniData (structured database query language) to design, develop and execute complex ETL to convert data from the customer's legacy system to our software platform.
Support internal and customer data validation testing.
Own data conversion activities from acquisition through post go-live for assigned projects.
Manage multiple conversion projects simultaneously, with the ability to effectively prioritize workload to meet project schedule deadlines.
Act as a subject matter expert on our data domains and conversion processes.
Advocate for continuous improvement, including development of reusable components and standardized processes for faster data migration and increased accuracy.

Requirements:

3+ years of SQL Server and ETL development experience (preferably SSIS)
Experience converting data to software applications, or similar data migration/data warehouse projects involving significant data analysis and subject area understanding
Strong attention to detail and analytical skills
Understanding of data dictionaries, mapping documents, and relational database concepts
Excellent organizational skills and time management to handle multiple projects at a time
Experience with ETL tools such as Informatica and/or Talend, a plus
Bachelor's degree in Computer Science or similar technical discipline
Must be legally authorized to work in the US without requiring sponsorship now or in the future for employment visa status

Basys is an Equal Opportunity Employer.

Covid Message: We value the safety and well-being of our basys community which includes our staff, customers, business partners and beyond. Which is why our team has chosen to require staff to be fully vaccinated for all in-person business activities, whether that be on-site at our corporate office or traveling for business. Candidates should consider this requirement before applying.","Analítica de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL) y Visualización de datos, Bases de datos, Ciencias de la computación, Conversión de datos, Establecer prioridades del trabajo, Perfiles de datos y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3934825856/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=3U8MgeYgS7ru47S6ZCwNJA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,Estados Unidos,"Acerca del empleo
Title:: Data Engineer

Working Model: Remote.

Pay range: $58.77-$62.82

Looking for W2 resources

Job Description

We are seeking a skilled Data Engineer to join our Data Science team under the Consumer organization.

The Data Engineer will work with data from all parts of the company like netops, field ops, call centers, sales, and finance.

The ideal candidate will build automated data pipelines and data wrangle Big Data.

Responsibilities

 Design, build, maintain, debug, and improve data pipelines
 Assemble large, complex data sets
 Create business intelligence dashboards
 Build, improve and maintain data infrastructure documentation and data dictionaries
 Support the Data Science team by providing clean, reliable, and timely data for building predictive models for churn, customer lifetime value, upgrade propensity, and customer satisfaction

Required Qualifications

 2+ years of experience as a Data Engineer in a similar role
 Advanced working knowledge of SQL (e.g., Microsoft SQL Server, PostgreSQL, MySQL, etc.)
 Experience building automated ETLs (e.g., Alteryx, Apache Airflow/Beam/NiFi, dbt, Dataiku, AWS Glue, etc.)
 Experience building BI dashboards (e.g., PowerBI, Tableau, Sisense, etc.)
 Experience working in data warehouses & data lakes (e.g., Teradata, DataBricks, Snowflake, etc.)
 Experience working in cloud platforms (e.g., AWS, Azure, GCP)
 Familiar with Python and Jupyter notebooks
 Experience wrangling with large, messy, and undocumented datasets
 Excellent communication and teamwork abilities

Preferred Qualifications

The ideal candidate would be familiar with this specific tech stack below. The more of this specific tech stack the candidate already knows, the better.

 Microsoft SQL Server
 Alteryx
 PowerBI
 DataBricks
 AWS (especially S3, Lambda, IAM)

Experience supporting a Data Science team.

Familiarity with data involving telecoms, mobile providers, ISPs, or cable companies.","Almacenamiento de datos, Analítica de datos, Big data, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación y Datasets",Solicitud sencilla
https://www.linkedin.com/jobs/view/3971711419/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=OEF8cRAWAFbDlYvtLWiGRQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 2 semanas,Estados Unidos,"Acerca del empleo
POSITION SUMMARY
Our Data Engineer will play a key role in the maintenance and optimization of data storage, ingestion, and transformation solutions in an Azure/Fabric environment, working closely with the data warehouse architect, data integrity team, and developers as well as the business to support the data lifecycle.
The successful candidate will have knowledge and experience regarding best practices in data engineering in Azure, be passionate about data, be a great teammate, a self-starter, and be flexible and motivated when it comes to working on multiple projects at once. 
RESPONSIBILITIES
Maintain, optimize and audit three Azure subscriptions (dev, test, prd) in a dynamic and evolving data ecosystem, including cost management, access control (IAM), and resource deployment.
Maintain and optimize Power BI/Fabric service, as it integrates with Azure resources. 
In collaboration with the Data Warehouse Architect, design, build, maintain, and audit complex ETL jobs to ingest and process disparate data sources and form a high integrity, high quality, clean data asset. (Oracle/Salesforce/Hubspot/+)
Deploy, optimize, and maintain storage solutions based on business needs and emerging source data in an Azure environment. 
Support the Data Warehouse Architect and Data Integrity team in understanding the impact of changes on Integration/ETL/Data Warehouse/Analytics area and document any cross application/functional impact that should be addressed.
Assists with capturing and documenting system flow and other pertinent technical information about data, database design, and systems.
Be collaborative with BI team members, data producers and data consumers throughout the enterprise.
Keep knowledge and skills current with the latest relevant services, features and best practices.
QUALIFICATIONS
Bachelor’s degree required plus 3+ years of professional experience with Azure ADLS, Azure Synapse Analytics /Azure Data Factory, Azure dedicated SQL pools, Azure SQL Server, and Power BI Service/Fabric.
Mastery of concepts around parameterized orchestration pipelines for ingestion and transformation
Strong API skills are required.
Experience with Azure AI Studio and Azure Machine Learning preferred.
Experience with Microsoft Purview preferred.","Microsoft Power BI y SQL, Microsoft Azure",Solicitud sencilla
https://www.linkedin.com/jobs/view/3971892324/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=QikCk6d7qmxPxlz1gGOn5w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Data Engineer II,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 semanas,"Bellevue, WA","Acerca del empleo
Job Description: Client is looking for a Data Engineer to join the Infrastructure Automation team. Client has over 70 million customers, and developers all over the world rely on our storage, compute, and virtualized services. Our success depends on our world-class network and hardware infrastructure; we’re handling massive scale and rapid integration of emergent technologies. Our goal is to become “The Infrastructure Platform” for the world. The Infrastructure Automation team is responsible for delivering the software that powers our infrastructure.

Responsibilities

 As a Data Engineer you will be working in one of the world's largest and most complex data transformation environments in AWS.
 You will be developing and improving data tables (through API, SNS topics, S3) that give our customers timely, flexible and structured access to their data.
 You will be responsible for designing and implementing metrics within a platform using third-party and in-house development tools, modeling metadata, building reports and dashboards.
 You will work with business customers in understanding the business requirements and implementing solutions to support analytical and reporting needs.
 Able to build scalable solutions along with adherence to privacy/security standards

Required Skills & Experience

 7-10 years of related experience.
 5+ years experience with AWS services (Lambda, Gateway, SNS, Firehose, etc..)
 Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS
 Good knowledge of SQL and other reporting solutions like Quicksight or Tableau
 Should have experience developing complex data models and implementation across large scale systems.
 A good candidate has strong analytical skills and enjoys working with large complex data sets.
 A good candidate can partner with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies.

Preferred

10 years AWS experience

Prior e-commerce experience","Ingeniería de datos , Lenguajes de programación, Python y Scala, Java, Metadatos, Modelo de datos, Necesidades empresariales, Panel de control y Secuencia de comandos",Solicitar
https://www.linkedin.com/jobs/view/3963157106/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=2bLj6sScHY1sgxRD%2FpKdJQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (with IICS and Python ML Experience),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,Estados Unidos,"Acerca del empleo
We are seeking a highly skilled and motivated Data Engineer to join our team. The ideal candidate will have a strong background in data engineering, with specific experience using Informatica Intelligent Cloud Services (IICS) and developing machine learning models using Python. This role involves designing, building, and managing our data infrastructure and pipelines, ensuring the efficient handling of data from various sources, and supporting our analytics and machine learning initiatives.

Data Pipeline Development and Management:

Design, construct, install, test, and maintain highly scalable data management systems
Develop and manage data pipelines using Informatica Intelligent Cloud Services (IICS), ensuring seamless data integration and transformation across various platforms and systems
Implement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes


Machine Learning Model Development:

Utilize Python to develop and implement machine learning models, integrating them into data pipelines to automate and improve business processes
Collaborate with data scientists and analysts to understand business requirements and translate them into technical specifications for model development
Perform data preprocessing, cleaning, and wrangling to prepare data for use in analytics and machine learning applications


Data Architecture and Strategy:

Contribute to the design and implementation of an effective data architecture that supports the organization's goals
Work closely with IT and business teams to identify and execute on opportunities to leverage data for business insights and decisions
Stay abreast of industry trends and advancements in data engineering and machine learning technologies, and recommend ways to improve our capabilities


Collaboration and Communication:

Collaborate with data analysts, scientists, and business teams to understand data needs and deliver solutions that meet these needs
Clearly communicate complex technical concepts to non-technical stakeholders to facilitate informed decision-making
Document data engineering processes, systems, and their outcomes for future reference and compliance


Requirements

Bachelor's or Master's degree in Computer Science, Engineering, or a related field
Proven experience as a Data Engineer, with specific experience in Informatica Intelligent Cloud Services (IICS) and Python for machine learning
Strong expertise in SQL and experience with relational databases, as well as familiarity with NoSQL databases
Solid understanding of data structures, algorithms, and system design
Experience with cloud services (AWS, Azure, Google Cloud) is highly desirable
Excellent problem-solving skills and the ability to work independently or as part of a team
Strong communication skills, with the ability to explain technical concepts to non-technical stakeholders","Base de datos relacional, Ingeniería de datos y NoSQL, Bases de datos, Calidad de datos, Ciencias de la computación, Desarrollo de modelos, Especificaciones técnicas, Necesidades empresariales y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982056203/?eBP=BUDGET_EXHAUSTED_JOB&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=NqKB%2BBMvkusYPcjB5QKYDA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"100 US$K/año - 120 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Virginia, Estados Unidos","Acerca del empleo
Data Engineer 
Remote 
Direct Hire Full Time 
Unable to provide work sponsorship 
Salary: $100k-$120k 

Data Engineer
Strong programming experience with Python plus at least one other Object Oriented programming languages: Ex. Java, C#, C++, Kotlin, Swift, Ruby, etc – cares about the OO principles in programming
Advanced SQL – should be able to write advanced queries, refactor old queries, and optimize 
In Depth knowledge of at least one ETL tool: Ex. NiFi, Kafka, Talend, Informatica PowerCenter, SSIS (Microsoft SQL Server Integrations Services), DataStage, Alteryx, AWS Glue, Oracle Data Integrator (ODI), Pentaho Data Integration (PDI), Data Factory
Experience with one enterprise scheduling tool: Ex. Control-M (BMC), IBM Workload Scheduler, Tidal Workload Automation, Automic Automation (CA Technologies), Redwood RunMyJobs, ActiveBatch, VisualCron, Stonebranch Universal Automation Center, JAMS Scheduler, OpCon (SMA Technologies)
Works independently and has led projects in the past","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , PowerCenter, SQL y SQL Server Integration Services (SSIS), DataStage, Oracle Data Integrator y Pentaho",Solicitud sencilla
https://www.linkedin.com/jobs/view/3969212074/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=VJm3gajeX3A9dAFXFbsP7A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 semanas,"Plymouth, MN","Acerca del empleo
Do you have Data Engineering experience, and are you seeking a new contract in Plymouth? Horizontal Talent is helping a healthcare company find a Data Engineer to join their collaborative team, and the role comes with an attractive pay rate.

As a Data Engineer, you will be responsible for the development of complex data sources and pipelines into the company's data platform (i.e. Snowflake) along with other data applications, such as Azure, Terraform, etc., automation and innovation. You will also create and maintain data pipelines using Azure and Snowflake as primary tools.

During your first few weeks in this Data Engineer role, you can expect to begin work on some of the following:

Create SQL stored procs and functions to perform complex transformations
Understand data requirements and design optimal pipelines to fulfill the use cases
Create logical & physical data models to ensure data integrity is maintained
Design and build best-in-class processes to clean and standardize data.


To apply for this Data Engineer role, your soft skills, expertise, and experience should include:

Excellent communication skills – verbal and written
Excellent knowledge of SQL
Azure Services such as Blobs, Functions, Azure Data Factory, Service Principal, Containers, Key Vault, etc
Knowledge of Snowflake - Architecture, Features, Best Practices
Data warehousing principles, architecture, and its implementation in large environments


You'll receive an excellent pay rate in return for your knowledge, expertise, and flexibility.

To apply for this contract Data Engineer job in Plymouth, please reach out to Horizontal Talent today. We'd love to help you get your next role and enable you to fulfill your professional ambitions.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Integridad de la información, Modelo de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3952959951/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=H7WcV3U6MoCJuZflMWaj0Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Contractor),"74 US$K/año - 130 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Nueva York, NY","Acerca del empleo
About GiveDirectly

GiveDirectly (GD) provides cash grants directly to people living in extreme poverty. Since launching in 2011, GD has raised over $1B, delivered cash to more than 1.5 million recipients, launched operations in 15 countries, and continues to expand its reach across the Global South. GD has also grown the research base supporting unconditional cash with 20 randomized control trials from its programs, generating rigorous evidence across countries and contexts. As a result, GD has been celebrated as one of the most innovative non-profit companies by Fast Company, while the growing cash transfer movement (and GD’s leading role within it) has been featured in the New York Times Magazine, This American Life, Foreign Affairs, and The Economist.

Across our global offices, our culture is candid, analytical, non-hierarchical, and agile. We work alongside 750+ individuals who come from 21 different countries and speak 69 different languages. Team members at GiveDirectly attest that diversity, equity, and inclusion are not just buzzwords, but a fundamental part of our culture and values. We actively seek to recruit individuals from the communities we serve, and use DEI as a lens in our hiring practices, programs, and initiatives. Our goal is to maintain a workplace where everyone can bring their authentic selves to work, and feel valued and respected for who they are. We strive to be inclusive of all cultures and experiences while upholding our values globally. In the spirit of our ""Know Yourself and Grow"" value, we recognize there is always room to improve our team's working experience. But day to day, we aim to ""Create Positive Energy"" - we take care of one another, have fun, aim to maximize flexibility and accessibility in roles, and pursue professional development opportunities to stay challenged & engaged in our work.

We are proud to be an equal opportunity employer, and we do not discriminate on the basis of race, color, religion, gender, sexual orientation, national origin, age, disability, veteran status, or any other status protected by law.

Location: This role is fully remote but must overlap with an East Africa timezone by at least 4 hours. We are unable to sponsor or take over sponsorship of employment Visas in the U.S. or U.K. at this time.

About This Role

The mission of the Central Data team is to empower GiveDirectly with a rigorous, data-driven culture to maximize our efficiency, effectiveness, and scale of delivering dollars to recipients. Building datasets that help us fully understand our donors, improve fundraising interventions, and ultimately raise more money is key to this mission. However, we currently lack many necessary datasets because we do not have pipelines to extract data from key fundraising platforms or combine data across platforms.

We have been awarded a $100,000 grant to fund a contractor role that will work full-time (40 hours / week) within Central Data for around one year to address these needs. You will work closely with the Senior Data Architect who oversees our infrastructure (AWS, Databricks, Tableau) and data pipeline development; the Fundraising Data Manager who will turn these datasets into dashboards and insights; and fundraising stakeholders who will use these insights to raise more money for recipients.

Reports to: Graham Tyler (Director of Data)

Level: Manager

Travel Requirement: There are no travel requirements for this role.

What You’ll Do

Success in this role is determined by meeting these key objectives: 

[20%] Ingest all relevant data sources into our data lakehouse (AWS, Databricks). 
We have existing pipelines, metrics, and dashboards leveraging data from our donor CRM and email provider. You will build new pipelines and jobs to ingest data from our website analytics platform and donor ticketing system.
 

[40%] Build unified datasets to fully understand donors and fundraising interventions. 
Once all data sources are in Databricks, you will work with stakeholders to define metrics, facts, and dimensions necessary for new dashboards, analysis, ML models, and experimentation that will drive fundraising strategy. Then you will build pipelines to clean, transform, and combine data from all platforms into these actionable datasets. 
 

[30%] Reduce data quality incidents with automated data quality tests and monitoring. 
Implement data quality tests and alerting for key donation, donor, and donor engagement variables.
Monitor job and pipeline performance. 
Proactively identify and implement improvements to our existing pipelines. 
 

[10%] Make it easy to maintain your pipelines and tools. 
Create comprehensive, easy to understand documentation to ensure effective knowledge transfer of your work. 
Build within our existing configurable pipeline framework and identify ways to improve this process. 
Leave our systems and processes better than you found them.
What You’ll Bring

Exceptional alignment with GiveDirectly Values and active demonstration of our core competencies: emotional intelligence, problem solving, project management, follow-through, and fostering inclusivity. We welcome and strongly encourage applications from candidates who have personal or professional experience in the low-income and/or historically marginalized communities that we serve.
Language Requirement: English
Language Preferences: No additional language preferences
Critical thinking and analytical approach necessary to develop technical solutions that scale and are resilient to changes over time
Entrepreneurial mindset and stakeholder management skills required to identify, design, and execute technical solutions that solve important, ambiguous organizational problems
Python, SQL, and spark expertise, along with core competencies required to ship high quality data pipelines and data tools fast
Extensive experience with Databricks preferred; experience with Tableau is a plus
Intellectual humility, curiosity, and a commitment to being part of an exceptional team

Compensation

At GiveDirectly, we strive to pay our employees generously and equitably. We use an accredited third party salary aggregator to ensure that staff’s total compensation package (base compensation + bonus) falls within the 75th percentile of similar roles, at similar organizations. We also have a no negotiation policy to ensure we are paying staff equitably across roles.

This role will be compensated at a rate equivalent to $130,000 per year base salary in the United States.
This role will be compensated at a rate equivalent to $74,000 per year base salary in Kenya.

This role is fully remote, so if you are not based in the US or Kenya, we will share an estimated pay rate for the country you are based in during the hiring process. P.S. We have been awarded a $100,000 grant to fund this role, which will be paid at a rate equivalent to your location until the entire grant has been used. So, regardless of location, this role will earn a total of $100,000 over the course of your time working with us. The duration of the role will depend on your location and associated pay rate.

Why work at GiveDirectly?

Role

At GiveDirectly, we work to ensure that you have everything you need to excel in your role and on your team, including:

A positive and supportive team
A demonstrated commitment to helping all staff develop and grow
Flexible work location

Read more about our ongoing diversity, equity, and inclusion efforts here and about our decision to move our central support teams to remote first here.

GiveDirectly is an Equal Opportunity Employer that values the strength diversity brings to the workplace. All qualified applicants are considered for employment without regard to the person’s race, color, religion, national origin, sex, sexual orientation, age, marital status, veteran status, disability, or any other characteristic protected by applicable law.

US applicants only: We invite you to ""Know Your Rights"" as an applicant.

About The Hiring Process

Format: The hiring process follows the same general outline for all open roles:

First interview (30 mins)

Take home skills assignment (~2 hours)

Second interview (1 hour)*

Third interview (1 hour)*

Final interview (1 hour)

Reference checks (30 mins each)

For some roles, second & third interviews are combined into a panel interview. If there are adjustments or variations on this process, those changes will be communicated during the first interview.

Venue: We conduct interviews over Google Meet with camera on (unless communicated otherwise).

Accessibility: Closed captioning is available during all Google Meet interviews, and interviewers will also post interview questions in the chat box throughout the call. If you need assistance accessing either of these features, please let your interviewer know at the start of your interview!

We’re committed to running an inclusive and accessible application process for all of our open roles. If there are questions or concerns you have about the accessibility of our hiring process, we warmly invite you to reach out to careers@givedirectly.org. Please include the word ""Accessibility"" in the email title.

**GD is committed to observing all local, national and international laws that protect children, vulnerable adults, and basic human rights of all. GD is committed to a policy of “zero tolerance for sexual exploitation, abuse, and harassment (SEAH)” and expects anyone who works for GD to uphold the protection and safeguarding of our recipients as a priority.**



Want to put your best foot forward on your GiveDirectly application? Take a look at our Candidate Application Prep Guide!","Apache Spark, Canalizaciones de datos, Ingeniería de datos y SQL, Calidad de datos, Closed Captioning, Datasets, Gestión de partes interesadas, Intercambio de conocimientos y Sistema de seguimiento de incidentes",Solicitar
https://www.linkedin.com/jobs/view/3948506957/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=h0lrQ1d7TGMEuKijWIW7fw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 días,"Mánchester, NH","Acerca del empleo
Brief Description

As the Data Engineer you will help design, develop, and maintain our data pipelines, databases, and analytical systems. This role collaborates with cross-functional teams to gather requirements, implement data solutions, and ensure the reliability and efficiency of our data infrastructure. Primarily, this role will partner with senior data engineers to design and implement scalable data pipelines for ingesting, transforming, and storing large volumes of structured and unstructured data.

Moore is a data-driven constituent experience management (CXM) company achieving accelerated growth for clients through integrated supporter experiences across all platforms, channels and devices. We are an innovation-led company that is the largest marketing, data and fundraising company in North America serving the purpose-driven industry with clients across education, association, political and commercial sectors.

Check out www.WeAreMoore.com for more information.

Your impact:
Build and maintain data warehouses, data pipelines, and other storage solutions to support business intelligence, reporting, and analytics needs.
Design, implement, and maintain scalable data pipelines for extracting, transforming, and loading (ETL) data from various sources into our data warehouse.
Work with data scientists and analysts to understand data requirements and implement solutions for data modeling, analysis, and visualization.
Monitor data pipelines and systems performance, troubleshoot issues, and implement optimizations to improve reliability, scalability, and efficiency.
Support leadership in identifying, designing, and implementing internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.
Stay up to date with emerging technologies and industry trends in data engineering and contribute ideas for continuous improvement of data infrastructure and processes.

Your profile:
2-5 years of experience in data engineering roles.
Bachelor's degree in computer science, information systems, engineering, mathematics, or a related field.
Experience Azure Data Factory for data integration, transformation, and validation processes.
Proficient in T-SQL experience with Snowflake is a plus.
Relevant certifications (optional but preferred) like Microsoft Azure Data Engineer Associate.
Programming skills in Python, JavaScript, or .NET is a plus.
Familiarity with distributed computing frameworks (e.g., Hadoop, Spark).
Solid understanding of ETL processes and tools like SSIS.
Understanding of data modeling, schema design, and data architecture.
Basic understanding of cloud platforms such as AWS, Azure, or Google Cloud.
Proven experience in building and maintaining data pipelines.

Joining Moore, you will:
Do meaningful, life-changing work every day by supporting the nation’s most beloved nonprofits and service organizations.
Join the largest marketing and fundraising company in North America serving the nonprofit industry where we prioritize innovation and professional growth.
Collaborate with industry subject matter experts among over 5,000 employees across the enterprise.
Earn a competitive salary and have access to comprehensive health, wellness, and retirement benefits.
Enjoy paid holidays and generous paid time off, ensuring you have the time and space to recharge and pursue your other passions.

Moore’s commitment to candidates: 
Moore is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Moore is also committed to compliance with all fair employment practices regarding citizenship and immigration status.","Amazon Web Services (AWS), Arquitectura de datos, Canalizaciones de datos, Ingeniería de datos y Python, Microsoft Azure, Modelado de datos, Schema, Snowflake cloud y System Performance",Solicitar
https://www.linkedin.com/jobs/view/3940376412/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=6IIjKTBagE25BPIVibBpSw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 1 semana,"Redmond, WA","Acerca del empleo
The Azure Core Organization is responsible for creating the foundation of Microsoft’s Cloud Platform for utility computing. This platform is one of the lowest levels of the services software/hardware stack and includes an efficient, virtualized computational substrate, a fully automated service management system, and a comprehensive set of highly scalable storage services.

The Azure Compute Capacity and Efficiency (AC2E) team is the team in Azure Core tasked with managing all aspects of Compute capacity and efficiency management across the fleet. Capacity Management needs to ensure that on the one hand, there is sufficient capacity across all regions, allocation domains, and hardware infrastructure to meet all customer demand, while on the other hand ensuring that capacity is provisioned efficiently thereby avoiding overspending and COGS/CAPEX impact. At the scale of Azure’s business, managing this trade-off across the entire Azure Compute fleet is an enormously complex and challenging task, where improvements can make the difference between customer allocation failures on the one hand, and gargantuan savings on the other.

As a Data Engineer II in the team, you will work closely with our software engineers, program managers, and data scientists across different teams within Azure Core. You will also collaborate with a variety of internal partner teams across Azure and Microsoft. You will build reliable, secured, highly scalable, performant data pipelines to enhance the Azure Compute allocation, deliver capacity management and efficiency improvements. The value of your work will be reflected in improvements to the Azure platform, Azure service capacity fulfillment rate, customer satisfaction, and various efficiency metrics, including COGS reduction. You will have opportunities for mentorship, accelerate your career growth, and work on truly high-business impact areas.

Microsoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond.

Responsibilities

With guidance, you’ll implement basic code to extra raw data, validate its quality, and ensure the correct data is ingested across multiple areas of work. You’ll also apply standard modification techniques to transform raw data into forms compatible with downstream data sources.
You will design and maintain assigned data tools that are used to transform, manage, and access data. You’ll also write code to validate the storage and availability of data platforms so that they’re more resilient.
You will follow existing documentation to implement performance monitoring protocols across a data pipeline, building basic visualizations and aggregations to monitor pipeline health. You’ll also implement solutions and improvements that minimize points of failure across a product feature.
You will follow data modeling and handling procedures to maintain compliance with all applicable laws and policies across your assigned workstreams. You’ll also govern data accessibility within your assigned pipelines and models.
Embody our Culture  & Values

Qualifications

Required Qualifications: 

Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering , or related field AND 2+ years experience in business analytics, data science, software development, data modeling or data engineering work
OR Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering or related field AND 1+ year(s) experience in business analytics, data science, software development, or data engineering work
OR equivalent experience.

Other Requirements

Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.

Additional / Preferred Qualifications

Bachelor's Degree in Computer Science , Math, Software Engineering, Computer Engineering , or related field AND 5+ years experience in business analytics, data science, software development, data modeling or data engineering work
OR Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering , or related field AND 3+ years of business analytics, data science, software development, data modeling or data engineering work experience
OR equivalent experience.
Experience in business analytics, data science, software development, data modeling OR data engineering work.
2+ years of experience with cloud-scale applications and live services.

Data Engineering IC3 - The typical base pay range for this role across the U.S. is USD $94,300 - $182,600 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $120,900 - $198,600 per year.

Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here:

https://careers.microsoft.com/us/en/us-corporate-pay

Microsoft will accept applications for the role until Aug 31, 2024.

#Azurecorejobs

Microsoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations.","Analítica, Analítica de datos, Analítica empresarial, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Ingeniería informática, Matemáticas y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3956544199/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=4FpmozCn3Bvxlb%2FJQhsqzg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - TrainingPeaks,"80,3 US$K/año - 133,9 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,"Louisville, CO","Acerca del empleo
Description

Are you ready to work on a product impacting millions of people? At TrainingPeaks our user base of athletes and coaches is growing rapidly. To meet their demands TrainingPeaks needs innovators, collaborators, and excellent engineers like you. Together we’re building the world’s best training platform. Join TrainingPeaks today.

You may know us as TrainingPeaks, MakeMusic, TrainHeroic and Alfred Music. All these brands are under the Peaksware umbrella. TrainingPeaks develops software for coaches and athletes to track, analyze and plan endurance training. TrainHeroic develops software solutions for the strength and conditioning needs of coaches and athletes. MakeMusic develops software to transform how music is composed, taught, learned and performed. Alfred Music creates and publishes educational music to help teachers, students, professionals and hobbyists experience the joy of making music.

We would love to have you join our ever-growing team! All applicants will receive equal consideration for employment regardless of gender, race, national origin, age, sexual orientation, gender identity, physical disability, religion, or length of time spent unemployed.

General Summary

As a Data Engineer, you will be responsible for creating and maintaining data solutions that help meet business needs. You will work closely with data analysts and engineers to improve the quality, reliability and coverage of our data. You must enjoy challenging problems, be proactive, and engage as a collaborative team player.

You are a continuous learner with a hunger for knowledge. You are eager to stay on top of the best data solutions in the industry and think about how they might up-level our data team. You approach challenges as opportunities to improve. You value team members’ input from all levels and you actively seek ways to support your colleagues.

You will sit directly with the data team and report to the Senior Manager of Data Analytics and Insights.

Core Functions

Model, create and maintain end-to-end data pipelines to ensure optimal data flow. 
Independently identify and resolve issues with existing data pipelines, contributing to the continuous improvement of data reliability. 
Work closely with data analysts to comprehend end-user requirements and collaborate with software engineers to design code that aligns with the database structure.
Mentor and train other team members on data engineering and industry best practices.
Stay current with emerging technologies and tools in the data engineering space. 
Collaborate with the data team to assess their potential impact on our data platform and provide strategic recommendations.

The work characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. 

Requirements

Required Qualifications: 

2+ years of experience in a Data Engineer role or in a related field
Strong expertise in SQL and experience with data modeling and database design conventions.
Experience with software engineering, preferably with Python, C# or Java.
Experience working with cloud-based tools and data warehouses, preferably Fivetran, Snowflake and AWS.

Desired Qualifications

Experience working collaboratively in a team environment, utilizing version control for effective collaboration.
Experience working with APIs and effectively leveraging their results.
Familiarity with data pipeline schedulers (e.g., Airflow, Dagster) to enhance visibility and facilitate the debugging of data pipelines.
Exposure to data analytics or data science concepts and best practices.

Degrees are not required, and we value all forms of continued education including traditional four-year degrees, post-graduate degrees, associate degrees, bootcamps, online training, professional certifications, self-teaching, and more.

Don’t meet every single requirement? Don’t worry. We still want to hear from you and encourage you to apply.

Benefits

Compensation

Peaksware/TrainingPeaks is committed to fair and equitable compensation practices. The salary range for this role is $80,317 - $133,861. Final compensation for this role will be determined by various factors such as a candidate’s relevant work experience, skills, and certifications.

This role is eligible for variable compensation, including bonus.

Benefits And Perks

Health

100% company-paid Medical for employees with buy-up options
Dental
Vision
Health Savings Account
Flexible Spending Account
Dependent Care Flexible Spending Account
Paid Parental Leave
Teladoc
Employee Assistance Program (EAP)
Additional coverage options such as accident and critical illness insurance and hospital indemnity

Disability and Life

Company-paid Short Term Disability
Company-paid Long Term Disability
Company-paid Basic Life Insurance and AD&D
Employee-paid Supplemental Life Insurance for Employee, Spouse, and/or Child

Additional

401(K)
401(K) Matching
Pet Insurance
9 paid holidays annually and unlimited Flexible Time Off (FTO)
Free TrainingPeaks, TrainHeroic, MakeMusic accounts, and Alfred Music product
Access to the Performance and Recovery Center (PARC), our on-site fitness facility
Employee only access to on-site locker rooms and showers
Employee only access to secure, indoor bike storage
Access to our onsite Music Studio
An assortment of “grab’n go” fruit and snacks as well as on tap cold brew, kombucha, and beer.
Beautiful onsite cafe that includes indoor and outdoor seating and lounge areas.
Access to e-bikes available exclusively to Peaksware employees
Significant investment in resources for employee growth and development
Corporate discounts on select gym memberships and top brand gear
Flexible work schedule in a culture of trust

Please contact careers@peaksware.com if you require a reasonable accommodation to review our website or to apply online.

Work Environment

This job operates in a professional office environment that is well-lighted, heated, and/or air-conditioned with adequate ventilation and a noise level that is usually moderate. This role routinely uses standard office equipment such as computers, phones, photocopiers and filing cabinets.

All employees must comply with all safety policies, practices and procedures. Report all unsafe activities to your manager and/or Human Resources.

Physical Demands

While performing the duties of this job, the employee is regularly required to sit and move about the facility; use hands to handle, or feel; talk by expressing ideas by means of the spoken word; and hear by perceiving the nature of sounds. The employee is occasionally required to stand, walk, and reach with hands and arms. The employee must occasionally lift and/or move up to 10 pounds. Specific vision abilities required by this job include close vision, distance vision, color vision, peripheral vision, depth perception, and ability to adjust focus.

To view the Peaksware Privacy Policy, click here. By submitting an application, you acknowledge and agree to the Peaksware Privacy Policy.","Analítica de datos, Arquitectura de datos, Ciencia de datos y Ingeniería de datos, Diseño de bases de datos, Flujo de datos, Modelado de datos, Requisitos del usuario, Seating y Spoken word",Solicitar
https://www.linkedin.com/jobs/view/3959916462/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=X8FEX9vA0UNXeylyMMr0jw%3D%3D&trk=flagship3_search_srp_jobs,Fisheries Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Woods Hole, MA","Acerca del empleo
Job Title: Fisheries Data Engineer

 Location: Remote/Woods Hole, MA area

 Clearance Required: Public Trust Eligible

Description

IBSS is seeking a Fisheries Data Engineer who has experience managing large datasets to work with a team of federal NOAA scientists and IT specialists. This Engineer will build a system to collect, validate, and produce data products from multiple data sources to support Ecosystem Based Fisheries Management (EBFM) and Climate Ecosystem Fisheries Initiative (CEFI) projects. This position expected to last 1 year.

NOAA seeks to develop transparent, reproducible, efficient, and automated data workflows for multiple biogeochemical models, remote sensing, and fisheries data streams for application in ecosystem reporting and research projects.

 Key Responsibilities:

 Build, test, and maintain transparent, reproducible, efficient, and automated (where practicable) data pipelines from multiple data sources, including ""big data” sources
 Develop algorithms to transform raw data into useful, actionable data products
 Develop workflows to upload/download data products to/from an ERDDAP server
 Produce documentation for the new data pipelines, including metadata, tutorials, and standard operating procedure documents
 Create tools to acquire and publish metadata to meet Public Access to Research Results (PARR) mandates
 Use project management tools to track progress and provide routine updates
 Adhere to NOAA IT security policies and standard operating procedures and follow industry best practices
 Collaborate with NOAA data specialist colleagues to align activities with standard NOAA communities of practice
 Assist with assimilation of new data streams into ecosystem based management and integrated ecosystem assessment products produced at the Northeast Fisheries Science Center

 Required Skills /Education/ Certifications & Qualifications:

 A degree or extensive experience in data science, data analytics, computer science
 Extensive experience with cronjobs and GitHub automation processes
 Experience in database design and management, big data tools, various data types (e.g. netcdf, flat, nccsv), data extraction, automated scripting, data security, cloud computing and storage
 Knowledge of Open Science principles
 Documentation skills and experience developing standard operating procedures
 The ability to work independently and collaboratively with an interdisciplinary team

 Desired Skills:

 Experience managing oceanographic dataset including biogeochemical model output and/or remote sensing datasets
 Familiarity with machine learning and data serving systems such as ERDDAP, THREDDS or OPeNDAP
 Project management experience
 Experience with SQL and coding in R and/or Python. 

About IBSS Corp.

Since 1992, IBSS, a woman-owned small business, has provided transformational consulting services to the Federal defense, civilian, and commercial sectors. Our services include cybersecurity and enterprise information technology, environmental science and engineering (including oceans, coasts, climate, and weather), and professional management services.

Our approach is to serve our employees by investing in their growth and development. As a result, our employees bring greater capabilities and provide an exceptional level of service to our clients. In addition to creating career development opportunities for our employees, IBSS is passionate about giving back to the community and serving the environment. We strive to leave something better behind for the next generation.

We measure our success by the positive impact we have on our employees, clients, partners, and the communities we serve. Our tagline, Powered by Excellence, is a recognition of the employees that make up IBSS and ensures we deliver results with quality, applying industry best practices and certifications.

IBSS offers a competitive benefits package including medical, dental, vision and prescription drug coverage with company-paid deductible, paid time off, federal holidays, matching 401K plan, tuition/professional development reimbursement, and Flex-Spending (FSA)/Dependent Care Account (DCA) options.

IBSS is an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, disability, age, sexual orientation, gender identity, national origin, veteran status, or genetic information. Click  https://www.eeoc.gov/poster to see that the EEO is the law.

If you require reasonable accommodation in completing this application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please direct your inquiries to the HR Department, Francesca Urrutia at (703) 826-4302, or email at HR@ibsscorp.com","Analítica, Analítica de datos, Arquitectura de datos, Ciencia de datos y Ingeniería de datos, Ciencias de la computación, Datasets, Documentación, Extracción de datos y Productos de datos",Solicitar
https://www.linkedin.com/jobs/view/3984298580/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=uASmG4xmnBa0ANPUIFxwvg%3D%3D&trk=flagship3_search_srp_jobs,BI Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Redmond, WA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
We are urgently hiring a  BI Data Engineer at Redmond, WA If you are interested, please share your updated resume on kaustubh.kudale@nityo.com Role BI Data Engineer Location Redmond, WA Visa prefer: GC, USC Job Description: Required Qualifications for BI Profile BE/B.Tech in Computer Science or related field 5+ years of enterprise software design and development experience Proficiency in SQL, with experience in writing and optimizing complex queries Professional, practical programming using Spark, SQL, Scala/Python Strong technical expertise in Azure, Big Data, Apache Nifi, Hadoop, HDInsights, ADF, ADW etc. Experience working with reporting and visualization tools like Power BI, Tableau, or similar. Hands-on experience with scripting languages like Powershell/Bash for automation and data manipulation tasks. Extensive knowledge and experience in data warehousing, Streaming data processing (ETL), e-metrics/measurement, business intelligence, information retrieval, parallel and distributed computation Experience in implementation of Cloud Computing concepts and platforms Experience in analyzing very large real world datasets and hands-on approach in data analytics Experience with Test Driven Development, Continuous Integration, Continuous Deployment, Telemetry etc. Great design and problem-solving skills, with a strong bias for quality and engineering excellence Excellent verbal and written communications skills Excellent problem-solving and debugging skills with a solid understanding of testing practices Proven sense of high accountability and self-drive to take on and see through big challenges Experience working in a global delivery model Experience with SCRUM, Devops or similar Agile development/implementation methodologies Familiarity with automation tools Visual Studio etc Thanks & Regards, Kaustubh Kudale Nityo Infotech Corp. Suite 1285, 666 Plainsboro Road Plainsboro, NJ, 08536 Email: kaustubh.kudale@nityo.com","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Ciencias de la computación, Comunicación escrita, Manipulación de datos, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3972707364/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=SJzVRNXdfJzqFC0aNkt%2Fcw%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer I, PLEX-SIA","Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 2 semanas,"Nashville, TN","Acerca del empleo
Description

Amazon is looking for a Data Engineer (DE) to join the North America Supply Chain (NASC)'s Science, Intelligence, & Analytics (NASC-SIA) team.

The successful candidate will be a self-starter, comfortable with ambiguity, and able to create and maintain automated processes efficiently. They will set the vision, define the roadmap, and work alongside stakeholders across organizations to deliver results. They know and love working with various AWS Cloud and Data Technologies, understand the principles and techniques of distributed computing, and are enthusiastic about working with data in diverse forms, types, volumes, velocities, etc. This role will have an immediate and lasting impact on both internal and external customers, and provides a unique opportunity to impact Amazon's Supply Chain business at scale for those willing to roll up their sleeves and dive deep to achieve results.

Key job responsibilities

 Implement & maintain an AWS Cloud Infrastructure to securely store, process, and deliver NASC planning data.
 Build and support a modern data architecture that integrates a wide range of available AWS data technologies.
 Ensure the accessibility, trustworthiness, usability, and security of NASC planning data.

About The Team

As part of North America Supply Chain Science Intelligence Analytics team (NASC-SIA), the NASC-SIA Engineering team is responsible for building and maintaining the critical data infrastructure, data management platform, and various planning tools that support NASC's planning and operational needs.

Basic Qualifications

 1+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines
 Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala)
 Experience with one or more scripting language (e.g., Python, KornShell)
 Experience as a data engineer or related specialty (e.g., software engineer, business intelligence engineer, data scientist) with a track record of manipulating, processing, and extracting value from large datasets

Preferred Qualifications

 Experience with big data technologies such as: Hadoop, Hive, Spark, EMR
 Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc.
 Exposure to Supply Chain business operations, particularly the labor planning processes.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com Services LLC

Job ID: A2695372","Apache Spark, Arquitectura de datos, Big data, Extraer, transformar y cargar (ETL), Hadoop, Hive, Ingeniería de datos y PL/SQL, Lenguaje de definición de datos (DDL) y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3972221555/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=EcgTmEqNuyqG8j9UPIBfrw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Hoboken, NJ","Acerca del empleo
Job Description:
Role: Data Engineer 
Hybrid (Twice in Office)- Hoboken, NJ
Permanent Role / Fulltime. 


Needs:
PYTHON
SPARK
PySpark

Nice to Haves:
Tableau 
GCP


Job Description:

Team is responsible for all the Anomaly Detection. They build algorithms and Pricing Models to show where anomalies are occurring. They also do platform develop for specific Platforms that do the same thing. Candidate will be responsible for designing, developing, testing, and deploying their code. A lot of the work also revolves around pipeline development and job development. 


Regards, 
Purnima Pobbathy
purnima@themesoft.com 
Themesoft INC.","Apache Spark, Google Cloud , Ingeniería de datos , PySpark, Python y Tableau, Detección de anomalías",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3971250050/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=OvxcoR1nay5Tv7oTCcc64g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Nueva Jersey, Estados Unidos","Acerca del empleo
What Working at Hexaware offers:
Hexaware is a dynamic and innovative IT organization committed to delivering cutting-edge solutions to our clients worldwide. We pride ourselves on fostering a collaborative and inclusive work environment where every team member is valued and empowered to succeed.
Hexaware provides access to a vast array of tools that enhance, revolutionize, and advance professional profile. We complete the circle with excellent growth opportunities, chances to collaborate with highly visible customers, chances to work alongside bright brains, and the perfect work-life balance.
With an ever-expanding portfolio of capabilities, we delve deep into and identify the source of our motivation. Although technology is at the core of our solutions, it is still the people and their passion that fuel Hexaware’s commitment towards creating smiles.
“At Hexaware we encourage to challenge oneself to achieve full potential and propel growth. We trust and empower to disrupt the status quo and innovate for a better future. We encourage an open and inspiring culture that fosters learning and brings talented, passionate, and caring people together.”
We are always interested in, and want to support, the professional and personal you. We offer a wide array of programs to help expand skills and supercharge careers. We help discover passion—the driving force that makes one smile and innovate, create, and make a difference every day.
The Hexaware Advantage: Your Workplace Benefits
Excellent Health benefits with low-cost employee premium.
Wide range of voluntary benefits such as Legal, Identity theft and Critical Care Coverage
Unlimited training and upskilling opportunities through Udemy and Hexavarsity.

Role: Data Engineer with Databricks
Hybrid
FTE
Sal: $130k

Responsible for designing, developing, and maintaining data processing pipelines using Databricks platform. Working closely with data engineers and data scientists to implement data solutions that meet business requirements, to help client with their cloud migration journey.
It includes the following responsibilities:
Designing and developing data pipelines using Databricks platform.
Writing efficient and optimized code in languages such as Python, Scala, or SQL.
Collaborating with data engineers to ensure data quality and integrity.
Implementing data transformations and aggregations to support analytics and reporting.
Working with data scientists to deploy machine learning models on Databricks.
Troubleshooting and resolving issues related to data pipelines and Databricks environment.
Optimizing performance and scalability of Databricks jobs.
Documenting technical specifications and maintaining code repositories.
Keeping up-to-date with the latest Databricks features and best practices.
Participating in code reviews and providing feedback to improve code quality.
He/she should have a strong understanding of distributed computing concepts and experience with cloud platforms such as Azure. They should also possess good problem-solving skills and be able to work in a collaborative team environment.",Python y Scala,Solicitud sencilla
https://www.linkedin.com/jobs/view/3805258746/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=QeC%2F%2FjYCRg%2FT7Nh5WNBv5g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 7 meses,"Provo, UT","Acerca del empleo
Aristotle, a leading player in the computer software industry, is currently seeking a talented and driven Data Engineer to join our dynamic team. At Aristotle, we have a deep-rooted belief in the importance of the democratic process, which serves as the foundation of everything we do. We are committed to advancing democracy around the world through innovative software solutions that empower organizations and individuals alike.

As a Data Engineer, you'll be an integral part of our mission to revolutionize the way data is utilized. You'll have the opportunity to work and learn in a collaborative environment where your opinions truly matter. We welcome passionate individuals who are dedicated to advancing the democratic process, regardless of their political affiliation. Join us at Aristotle and love what you do while contributing to a greater cause.

Aristotle's Integrity division is a leading provider of identity and age verification services across numerous vertical markets. Our age/identity verification solutions are used by companies to comply with various regulatory requirements such as AML, KYC and Age Verification.

Please visit https://integrity.aristotle.com for more information about this division.

Responsibilities

 Data Load and Transformation: Develop data load processes for efficient storage and retrieval of data from databases and other file systems. Create data conversion and transformation processes and utilities for handling large datasets
 Solution Development: Utilize .NET/SSIS/SQL Server technologies to design and implement solutions that align with data consumer requirements and adhere to business rules
 Web-Based Reporting: Build web-based reporting systems to monitor system performance, transaction metrics, and error rates
 Data Transformation Rules: Collaborate in defining and documenting data transformation rules to ensure data integrity and accuracy
 ETL Design and Performance: Focus on the design, development, and performance tuning of ETL (Extract, Transform, Load) processes to optimize data processing efficiency
 Collaborative Development: Work closely with other developers to provide data services to both existing and new applications. This includes modifying production data, creating and optimizing stored procedures, functions, views, and more
 System Performance Enhancement: Develop and analyze strategies to enhance system performance, ensuring efficient data processing and retrieval
 Documentation and Testing: Prepare comprehensive documentation and test procedures to ensure the reliability and quality of developed solutions
 Industry Standard Practices: Develop software using industry-standard programming techniques to maintain code quality and consistency
 Unit Testing and Debugging: Perform unit testing and debugging of application components to identify and resolve issues promptly


Requirements

Bachelor's or Associate's degree in Computer Science or a related field
Minimum of 2 years of hands-on experience with ETL (Extract, Transform, Load) processes
Proficiency in T-SQL programming and working with Microsoft SQL Server 2005 and 2008. A deep understanding of the SQL Server Query Processing Engine is required
Knowledge and experience in designing, developing, debugging, and deploying SQL Server stored procedures, T-SQL scripts, DTS (Data Transformation Services), and SSIS (SQL Server Integration Services) packages
Ability to manage multiple priorities, adhere to project plans, and consistently meet project deliverables
Proficiency in Microsoft SQL as well as Microsoft SQL Server Reporting Services. Familiarity with MS Office applications, including Word and Excel
Demonstrated ability to quickly learn and adapt to new technologies as needed


Desired Requirements

Proficiency in using Microsoft Visio for creating sequence diagrams, component diagrams, and other UML (Unified Modeling Language) diagrams
Proficiency in data modeling using tools such as MS Visio or Erwin data modeler
Familiarity with identity verification for fraud, marketing, and risk mitigation solutions within the industry
Knowledge of internet technologies, including XML, DHTML, CSS, and JavaScript
Familiarity with ASP.NET 2.0, C#, and Traditional ASP (Active Server Pages)


This role is located in Provo, Utah. If you live within commuting distance of Provo, Utah or are willing to relocate, please include this in your cover letter. 

Benefits

All positions are Full-Time, with competitive compensation, medical benefits, paid vacation, 401k plan and stock options. Casual dress code and a non-corporate atmosphere make this a fun place to work and learn in a team environment. Please visit our website at www.aristotle.com.","Extraer, transformar y cargar (ETL), Microsoft SQL Server y SQL Server Integration Services (SSIS), Diagramas de secuencia, Erwin, HTML dinámico, Lenguaje de consulta (query), Modelado de datos, Procedimientos de almacenado y SQL Server Reporting Services (SSRS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979652565/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=%2BgT7S9tJ6jWhknEMDoVaSw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - GA4 / Tableau (Contractor),"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Santa Clara, CA","Acerca del empleo
Come join VisitorsCoverage, one of Silicon Valley's most successful InsurTech companies, certified as a Great Place to Work®!

We are looking for a Data Engineer with GA4 / Tableau experience to join our Marketing team. If you are experienced with reporting website analytics and building data pipelines, we'd love to have you at VisitorsCoverage. This contract role will offer the right candidate first-hand experience in a booming industry. A successful candidate will be responsible for streamlining reporting and data aggregation with GA4 and other 3rd party marketing tools. You'll lead and assist in building visualizations, dashboards, running SQL queries, creating and managing tags in Google Tag Manager.

What We Do:

VisitorsCoverage is an Insurtech company, located in the heart of Silicon Valley, revolutionizing the way travelers search, compare, purchase, and manage their travel insurance. Imagine a place where buying travel insurance is as easy as ordering an item from your favorite online retailer. You know exactly what the benefits are and what each word on the coverage document means, and you are able to zip through the checkout process. We are obsessed with simplifying Travel Insurance! We wake up everyday thinking of new ways to meet the same expectations that users have from their online retailers and delivery or streaming services. We are a team of people who counter the thought that insurance is boring and love the challenge of delighting our users at every step of their decision-making process.

If this sounds like the perfect role and workplace for you, we encourage you to apply for this position! VisitorsCoverage is on a mission to hire only the best, and we are committed to providing exceptional employee experiences with meaningful work and true work/life balance.

Location

This is a hybrid position. Candidate must be able to commute to the Santa Clara office 3-4 days a week.

Weekly duration: 

20-40 hours a week

Compensation

This is a contract position. Rate is $64-$80 per hour, based on experience.

Requirements

Responsibilities:

Aggregate traffic and revenue data accurately in GA4 from the following sources: Google Ads, Microsoft Ads, Meta, Reddit and additional online paid/organic sources. 
Fix any data discrepancies due to Google Tag Manager, GA4 setup or Tableau integrations
Audit Google Tag Manager's triggers and tags for accuracy and it's data feed to other platforms
Help report data with dashboards, using Tableau or Looker Studio
Work with the team to QA Google tags and data streams for accuracy
Work with Engineering and Marketing teams to align on tracking KPIs


Experience & Qualifications:

3 years of experience in tracking using Google Tag Manager and GA4 (Must have actively used GA4 within the last 2 years.)
3 years of experience integrating data from key marketing platforms, including Meta and Google Ads, to guarantee accurate data flow into Google Analytics 4 (GA4), and Tableau
1 year experience working with Tableau (Or similar online reporting/CRM platform)
Experience with ‘Attribution Modeling'
1 year experience with BigQuery
1+ Year of experience building Data Extraction pipelines in Python
1+ Year of experience in SQL
Significant experience in management of Google Analytics (GA4) architecture and delivery of web analytics insights and reporting from Google Analytics (required) - Especially tracking ‘key events' and ‘revenue'
Significant understanding of attribution models and how this functions for tracking website sales
Proven history of working on technical site integrations or projects
Ability to work independently and collaboratively as part of a team
Strong attention to detail and ability to meet deadlines


Preferred Qualifications:

Experience with Looker Studio, Funnel IO
Experience with Google Sheets
Experience with A/B testing and experimentation
Excellent communication and presentation skills


What do we need in the next 60-90 days:

Get data to a clean and accurate state in a singular location
Fix discrepancies between various marketing tools' data for revenue and sales tracking across below platforms 
GA4
Meta
Google Ads
Reddit
Bing
Clean up Google Tag Manager data/metadata
Export aggregated data to Google Sheets","Google BigQuery y Integración de datos, Análisis web, Attribution Modeling, Flujo de datos, Google Ads, Google Tag Manager, Hojas de cálculo de Google, Looker (software) y Pruebas A/B",Solicitud sencilla
https://www.linkedin.com/jobs/view/3973807406/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=OpP1J9xyl5eve5NYvhdHnw%3D%3D&trk=flagship3_search_srp_jobs,Data Visualization Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,"Milwaukee, WI","Acerca del empleo
We have an opening for a Data Visualization Engineer with our direct client in Milwaukee, WI, requiring work in the downtown Milwaukee office 3 days per week. In this position, you will work with cross-functional teams while leveraging a set of diverse technologies and an automation first approach to understand key metrics, engineer collection using Python and other scripting capabilities, and visualize metrics to tell a narrative in Grafana. This role requires a DevOps mindset with experience in Development/Scripting as well as Grafana Dashboarding. Must have a proven track record of self-motivated accomplishment. This is an exciting opportunity to build upon technical experience with a strong security mindset.

In this role, the candidate can expect to:
Engineer collectors using Python through API calls to primary data source systems
Engineer Grafana Dashboards as code and deploy through CICD pipelines with peer review processes
Participate in a multi-org group identifying, deploying, testing, and reviewing data and dashboards to accomplish the team narrative
Stay apprised of current and proposed security changes impacting regulatory, privacy and security industry best practices

The ideal candidate is:
Passionate about security
Proficient with development and scripting languages, Python experience required.
Proficient developing dashboards in Grafana and defining dashboard visualizations as code.
A team player and enjoy collaborating with cross-functional teams
A great communicator (written and verbal) with an ability to articulate complex topics in a clear and concise manner
Employs a flexible and constructive approach when solving problems
Continuously looking for opportunities to improve our processes and capabilities
Experienced working with application and engineering teams
Comfortable peer-reviewing code and speaking openly to provide remediation guidance
A self-directed individual contributor
Bachelor’s degree, Associate's degree or equivalent experience with an emphasis in Cybersecurity, Computer Science, Computer Engineering, Software Engineering, MIS or related field
2-3+ years' experience in DevOps, AWS, Kubernetes, and cybersecurity.
Knowledgeable about secure architecture, engineering, and design principles
Experience conducting security tests (CSPM IAC - Infra as Code misconfiguration scanning, static and dynamic code analysis, software composition analysis, or penetration tests)
Knowledge of common application and cloud security tools, such as Burp, Zap, Checkmarx, InsightCloudSec (DivvyCloud), PrismaCloud, InsightAppsec, InsightCloudsec, Jfrog Xray
Experience with CICD pipelines to automate application and infrastructure code deployments
Experience with workload orchestration platforms such as Kubernetes
Experience with Infrastructure as code concepts and tooling including Terraform or Cloudformation
Relevant certifications from GIAC, ISC(2) and other recognized cybersecurity industry organizations
 If able to work directly on our payroll without sponsorship in Milwaukee, WI, apply by emailing resume to pete.neja@codeworks-inc.com.
 At Codeworks, we're committed to diversity, equity, and inclusion in our workforce and beyond. We believe in equal opportunities and value the unique perspectives that every individual brings to our team. Join us in creating an inclusive, innovative, and collaborative workplace where your talents can thrive.","Amazon Web Services (AWS), DevOps y Python, Grafana y Herramientas de seguridad",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982078233/?eBP=BUDGET_EXHAUSTED_JOB&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=2XmrRDq3JAHKcf%2FMdOHfkg%3D%3D&trk=flagship3_search_srp_jobs,Azure Data Engineer - Remote | WFH,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Summit, NJ","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
We are seeking a talented Senior Azure Data Engineer to join our team. In this role, you'll have the opportunity to work with cutting-edge technologies and contribute to exciting projects. Your expertise will play a key role in shaping our data solutions and ensuring the success of our data engineering initiatives.

What You'll Do

Technical Expertise: Leverage your experience with Apache Spark, including Spark SQL, Spark Streaming, and PySpark to drive data solutions. Utilize Apache Airflow for effective workflow orchestration. Apply your hands-on experience with cloud platforms such as AWS, Azure, or Google Cloud to build robust data architectures.

Programming Skills: Demonstrate your proficiency in programming languages like Python, Scala, or Java. Use your solid understanding of SQL to work with relational databases and drive data insights.

Data Engineering: Apply your experience in data modeling, data warehousing, and data integration to create efficient data pipelines. Explore and implement Big Data technologies and frameworks. Bonus points if you have experience with containerization technologies like Docker and orchestration tools like Kubernetes. Understanding DevOps practices and CI/CD pipelines will also be valuable.

Problem-Solving: Bring your strong problem-solving skills and attention to detail to tackle complex challenges. Thrive in a fast-paced, agile environment and contribute to continuous improvement.

What We're Looking For

Proven experience with Apache Spark and Spark-related technologies.

Strong proficiency with Apache Airflow and cloud platforms (AWS, Azure, Google Cloud).

Proficiency in Python, Scala, or Java, and solid SQL skills.

Experience with data modeling, warehousing, and integration.

Familiarity with Big Data technologies, containerization, and DevOps practices.

A proactive attitude and the ability to work well in a dynamic environment.

If you're passionate about data engineering and eager to make an impact, we'd love to hear from you!

Employment Type: Full-Time","Apache Spark, Base de datos relacional, Ingeniería de datos , Lenguajes de programación, Python, SQL y Scala, Bases de datos, Java y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3955437742/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=FSTCP%2Bu%2FEaiR8geqRGfqZw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Analyst - Remote,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"San Francisco, CA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos y Lenguajes de programación, Ciencias de la computación, Comunicación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3977205308/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=6nboOMPWlnzHrOokRXJEjg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Sunnyvale, CA","Acerca del empleo
Fluency in Advanced SQL (complex joins, stored procedures, subqueries, window functions, performance optimization, etc.), Snowflake, Python.

 Provide analytical reporting and analytics to the operations team and external partners.
 Ability to operate in a fast paced, rapidly changing environment.
 Ability to rapidly learn and adapt to business changes.
 Excellent communication, project management, and presentation skills
 Create and maintain reports, create and manage data models, leverage data across complex hierarchies using multiple data sources.
 Experience in building ETL pipelines.
 Leverage process improvement techniques to drive improvements in data quality.
 Perform testing to support system implementations and upgrades

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Aptitudes para hacer presentaciones, Bases de datos, Calidad de datos, Comunicación, Modelo de datos y Presentaciones",Solicitar
https://www.linkedin.com/jobs/view/3982976268/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=m%2BJDKc7vJTUp9FwymUut3g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 6 días,"Birmingham, AL","Acerca del empleo
Job Title: Data Engineer - Advanced Metering Infrastructure (AMI) Analytics

Location: Birmingham, AL 35203

Duration: 1 year

Job Summary

The Data Engineer at APC Power Delivery's AMI Analytics team plays a pivotal role in advancing data-driven decision-making and operational efficiency across business units. Working closely with data scientists and within a cloud environment, this role requires expertise in data engineering, cloud technologies, and collaborative problem-solving.

Key Responsibilities

Data Pipeline Development:

Assist TO in designing, building, and maintaining scalable data pipelines to ingest, transform, and store large volumes of AMI data from various sources.
Collaborate with data scientists to ensure seamless integration of data pipelines with analytical models and AI processes.

Data Modeling and Architecture:

Develop and implement data models and architectures optimized for AMI data analytics, ensuring efficiency, scalability, and data integrity.
Implement best practices for data storage, partitioning, and indexing to optimize performance and facilitate analysis.

Cloud Environment Management:

Work within cloud environments - Azure to deploy, manage, and optimize data engineering solutions.
Collaborate with cloud architects and administrators to ensure security, compliance, and cost-effectiveness of cloud infrastructure.

Visualization and Reporting:

Develop and maintain Power BI dashboards and reports to visualize AMI data insights and facilitate data-driven decision-making.
Create custom UI/UX applications
Ensure the accuracy, reliability, and usability of visualizations to meet business requirements.

Data Cataloging and Documentation:

Catalog and document AMI data sources, datasets, and metadata to facilitate data discovery, lineage, and governance.
Implement data cataloging best practices to ensure the availability and accessibility of AMI data assets.

Collaboration and Support:

Collaborate with cross-functional teams to understand data requirements and support analytical initiatives.
Provide technical support and troubleshooting for data-related issues, ensuring the reliability and availability of data infrastructure.

Education/Experience

Job Requirements:

Bachelor’s degree in Computer Science, Information Technology, or related field.
3+ years of experience in data engineering or related roles, preferably in a cloud environment.

Knowledge, Skills & Abilities:

Proficiency in programming languages such as Python, SQL, and Scala.
Proficiency with Power BI for data visualization and reporting.
Experience with cloud-based data platforms (e.g., Azure Databricks, AWS EMR).
Strong understanding of data modeling, ETL processes, and data warehousing concepts.
Familiarity with big data technologies

Behavioral Attributes:

Innovative and adaptable, with a passion for continuous learning and improvement.
Strong communication and collaboration skills, with the ability to work effectively across teams.
Results-oriented and committed to delivering high-quality solutions that meet business needs.
Ethical conduct and commitment to safety in all aspects of work.

We are looking for a motivated and skilled Data Engineer to join our dynamic AMI Analytics team. If you are passionate about leveraging data to drive business insights and innovation, and have expertise in Power BI visualization and data cataloging, we encourage you to apply.

Artech is an Equal Opportunity Employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. We are committed to fostering a diverse and inclusive workplace where all employees feel valued and respected.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Ciencias de la computación, Datasets, Modelado de datos y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3818359656/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=SCTYeJjwIlo40ltnOgFvTA%3D%3D&trackingId=X8gdx0EMtLhS6WidlTBuxQ%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Scientist/Analyst/Engineer - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Dallas, TX","Acerca del empleo
The Job Market is Challenging due to more than 150,000 Tech Layoffs in 2022 and in 2023 more than 240,000 layoffs so almost 3,90,00 tech employees have been laid off since 2022 and its still going on . The effect of this has led hundreds of thousands of laid off Tech employees competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications, are getting ghosted after interviews. In such a scenario the Job seekers need  to differentiate themselves by ensuring to obtain exceptional skills and technologies to be hired by clients as its an employer's market presently and they have a lot of hiring choices.

For more than 12+ years Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs as Clients now post covid can also hire remote workers which increases even more competition for jobseekers.

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT Industry

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.

 please check the below links to see success outcomes of our candidates and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/ 

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

Oracle CloudWorld Event (OCW) Las Vegas 2023/ 2022 | SynergisticIT - YouTube

 https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrn 

 https://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Q 

 https://www.youtube.com/watch?v=NVBU9RYZ6UI 

 https://www.youtube.com/watch?v=EmO7NrWHkLM 

 https://www.youtube.com/watch?v=NVBU9RYZ6UI 

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://www.youtube.com/watch?v=Yy74yvjatVg 

For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ 

 We are looking for the right matching candidates for our clients 

 Please apply via the job posting 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Programmer 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java , javascript , C++ or software programming 
 Spring boot, Microservices, Docker, Jenkins and REST API's experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning Positions 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow 

 If you get emails from our skill enhancement team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements. 

 No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciencias de la computación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3984970807/?eBP=BUDGET_EXHAUSTED_JOB&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=ksKLt9EnfvgzmVfJ2TOO9Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"149,2 US$K/año - 248,6 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Ejecutivo",hace 2 días,Estados Unidos,"Acerca del empleo
Inclusively is partnering with a pharmaceutical and biotechnology corporation to hire a Staff Analytics Data Engineer- Commercial Analytics Engineering. **Please note: this role is NOT an internal position with Inclusively but with the partner company.**

ABOUT INCLUSIVELY:
Inclusively is a digital tech platform that connects candidates with disabilities, who may benefit from workplace accommodations, to inclusive employers. This includes all disabilities under the ADA, including mental health conditions (e.g. anxiety, depression, PTSD), chronic illnesses (e.g. diabetes, Long COVID), and neurodivergence (e.g. autism, ADHD). Applicants with one or more of these conditions are encouraged to apply; Inclusively does not require applicants to disclose their specific disability.

We are looking for an individual contributor Director of Analytics Engineering that will serve as an experienced and dynamic Staff Analytics Data Engineer within our Agile Analytics Engineering team. Tasked with the critical mission of crafting data products that drive our Marketing priorities, this role leads end to end build of data products from deployment to production/release management while upholding best-in-class processes and deliverables.

The team charter includes supporting Data Science and Insight & Strategy partner organizations across the entire data journey from Descriptive to Predictive with key responsibilities for data transformation, creation of consumption-ready data products to be used for visualization across the Commercial business.

The ideal candidate for this role will possess a unique combination of technical expertise, strategic vision, and collaborative skills. They will have a proven track record of influencing the technical direction of engineering teams to build data products and solutions that drive business value. With a deep understanding of data engineering principles and technologies, along with a self-motivated problem-solving approach, they will lead software/data engineering projects, and mentoring teams.

You are skilled in a broad set of ETL/Big Data processing techniques and will lead the architecture, design and implementation of scalable, extensible, and highly available data ETL pipelines on large volume data sets, which will enable actionable insights and ML workloads across our Commercial business.

Data Pipeline Development & Optimization
Lead and own end-to-end strategic data projects
Architect, design and build scalable data pipelines that will churn out reliable and high-quality analytics data products.
Solve complex data challenges to meet the needs of a highly dynamic business.
Lead design reviews and contribute to development and design standards.
Refine development processes and workflows.
Diagnose and resolve complex performance bottlenecks in data processing pipelines and code to enable improved data availability SLAs.
Translate business requirements in partnership with product management into data and engineering specifications.
Analyze technical debt, legacy architectures, and work with other leads on re-platforming efforts.

Collaboration, Partnership, and Mentoring 
Collaborate with senior leadership, internal and external business partners to align on objectives and line-level deliverables.
Architect, lead and build test automation tools and frameworks to test pipelines.
Partner with our Digital Engineering team and identify server APIs that need to be refactored or customized for data analytics and reporting and align the server events for execution in already established data pipelines.
In collaboration with senior leadership, partner with the Platform/Digital and DevOps team to specify platform/tool enhancements, observability & monitoring, security/audit, and alerting on various data pipelines and jobs as needed for operation the commercial analytics engineering organization.
Mentor junior engineers on best practices in design, development, and automation. 

Domain Knowledge and Standards
Ramp up quickly in understanding the data sources required for data products and the purpose of commercial analytics in a pharma setting.
Prioritize excellence in data engineering by following F.A.I.R. principals and adheres to engineering and documentation standards set in the organization.

Agile Practices and Deployment
Lead deployments to production and ensure the team is following agile best practices in the team working agreement.
Ensure the team adheres to agile principle. 

Qualifications
Bachelor’s degree (Master’s or PhD a plus) in computer science, engineering, or related field. Ph.D. Master’s degree preferred in engineering, economics, statistics, computer science, or related quantitative field
10+ years of backend software or data engineering experience with at least 4 years’ experience as lead data engineer of a development scrum team.
Recent Healthcare Life Sciences (pharma preferred) and ecosystem professional industry experience is preferred, commercial/marketing experience is a plus.

Technical Skillset
Expert-level understanding of Agile, SDCL, CI/CD, DevOps, GitOps, and ProdOps is required.
Expert-level expertise in SQL, Python, Scala or similar; other object-oriented programming experience a plus
Expert-level experience in data architecture, data types/formats, optimizing code and ETL jobs, data architecture, data modeling, data contracts – a key requirement for successful collaboration with data architects and visualization developers.
Expert-level professional hands-on experience in extensible data pipeline development and orchestration with tools like Apache Airflow, Step Functions, etc.
Expert-level professional hands-on experience with data warehousing/RDBMS, ETL scripting tools and IDEs, data pipeline development, and management using orchestration/monitoring tools.
Expert-level knowledge and experience in implementing software engineering security best practices and contributing to design/engineering for SecOps.
Expert-level experience with master data management strategies, rules-based engines, and tools for multi-stage data correlation on large data sets.
Must have experience managing deployment and leading the release of code to production.
Expert-level understanding of data visualization tools and requirements for business customers.
Expert-level experience working in agile setting or being an agile best-practice mentor.
Expert-level experience working in Jira, confluence, and contributing to engineering team documentation/playbooks.
Proven experience influencing and partnering with product management, TPMs in translating business requirements into data and engineering specifications.

Competencies
Team Mentorship: Influencing multiple data engineering teams, mentoring managers, and setting team objectives.
Strategic Vision: Ability to shape and steer the data strategy at an organizational level.
Influencing without Authority: Leading multiple teams, fostering leadership in others, setting departmental goals, and ensuring alignment with the company's objectives.
Innovation Management: Spearheading innovation in data practices, technologies, and methodologies.
Stakeholder Management: Engaging with senior stakeholders, negotiating with external partners, and influencing the strategic direction of the business.
Budgeting and Forecasting: Managing large budgets, resource allocation, and financial forecasting for data projects.
Risk Management: Identifying and mitigating risks at an organizational level.

Candidate demonstrates a breadth of diverse leadership experiences and capabilities including: the ability to influence and collaborate with peers, develop and coach others, oversee and guide the work of other colleagues to achieve meaningful outcomes and create business impact.

Relocation support available

Work Location Assignment: Flexible

The annual base salary for this position ranges from $149,200.00 to $248,600.00. In addition, this position is eligible for participation in our Global Performance Plan with a bonus target of 20.0% of the base salary and eligibility to participate in our share based long term incentive program. We offer comprehensive and generous benefits and programs to help our colleagues lead healthy lives and to support each of life’s moments. Benefits offered include a 401(k) plan with company Matching Contributions and an additional company Retirement Savings Contribution, paid vacation, holiday and personal days, paid caregiver/parental and medical leave, and health benefits to include medical, prescription drug, dental and vision coverage. Our compensation structures and benefit packages are aligned based on the location of hire. The United States salary range provided does not apply to Tampa, FL or any location outside of the United States.

Relocation assistance may be available based on business needs and/or eligibility.","Desarrollo web back end, DevOps, Ingeniería de datos , Integración continua y entrega continua (CI/CD) y Metodologías ágiles, Ciclo de vida de desarrollo de software (SDLC), GitOps, Ingeniería y Scrum",Solicitar
https://www.linkedin.com/jobs/view/3944750345/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=AGt6NpXAkl17zjGXIRfwgA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Media jornada
Coincide con tus preferencias de empleo. El tipo de empleo es Media jornada.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
Job Title: Data Engineer

Location: Multiple Location (Onsite)

Required Qualifications / Skills

10 years of strong SQL skills; SQL Server and PostgreSQL is preferred. experience in any other RDBMS is plus..
Proficiency in the Python programming language
Ability to prepare, analyze, and effectively communicate the findings of data analysis to customers, RTC leadership, and outside stakeholders.
Strong written/verbal communication skills.
Must be able to obtain and maintain a Secret Clearance 
Python and spark knowledge is plus.
 Knowledge about Database engineering and Data warehousing Concepts.
Experience with Agile based project methodology.
 Ability to identify risks/issues for the project and manage them accordingly.

Responsibilities

Write secure and high-quality code and maintains algorithms that run synchronously with appropriate systems.
Develop Extract, Transform, Load (ETL) pipelines for data
Work directly in support of Army modernization priorities including working directly on and around helicopters, missiles, and sensors.","Almacenamiento de datos, Analítica de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Lenguajes de programación y Python, Bases de datos, Comunicación, Comunicación oral y SC Clearance",Solicitud sencilla
https://www.linkedin.com/jobs/view/3981618442/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=YQE75aIn3AlZ3Rh5cEKutw%3D%3D&trk=flagship3_search_srp_jobs,Freelance Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,"Alexandría, VA","Acerca del empleo
The Motley Fool is looking for a highly skilled Freelance Data Engineer to join our team on an independent contract basis, 40 hours per week for 12 months. This is a mid to senior level position and requires 4-5+ years of relevant experience.

Note: Though this role is 100% remote, candidates must reside in the United States for consideration. 

Who are we?

We are The Motley Fool, a purpose-driven financial information and services firm with nearly 30 years of experience focused on making the world smarter, happier, and richer. But what does that even mean?! It means we're helping Fools (always with a capital ""F"") demystify the world of finance, beat the stock market, and achieve personal wealth and happiness through our products and services.

The Motley Fool is firmly committed to diversity, inclusion, and equity. We are a motley group of overachievers that have built a culture of trust founded on Foolishness, fun, and a commitment to making the world smarter, happier and richer. However you identify or whatever winding road has led you to us, please don't hesitate to apply if the description above leaves you thinking, ""Hey! I could do that!""

What does this team do?

The Data Engineering team at The Motley Fool creates data pipelines to wrangle data from around the Fool. We collaborate with everyone - from third party vendors to stakeholders to build easily consumable data structures for reporting and business insights. While working closely with our business analysts and machine learning specialists, we serve the data needs of all The Motley Fool Teams!

What would you do in this role?

As a Freelance Data Engineer, you will be responsible for expanding and optimizing data, the data pipeline architecture, the data flow, and collection for cross-functional teams. You are an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. You will help to guide and support our software developers, database architects, data analysts, and data scientists on business initiatives while ensuring optimal data delivery architecture is consistent. Whether it's working on a solo project or with the team, you are self-directed and comfortable supporting the data needs of multiple teams, systems, and products.

But What Would You Actually Do in this role?

Leverage data assets to meet mission needs, ensuring consistent data quality, establishing data standards and governance
Work in an agile, collaborative environment, partnering with client stakeholders, to develop and improve mission-based solutions
Monitor cloud-based systems and components for availability, performance, reliability, security and efficiency
Create and configure appropriate cloud resources to meet the needs of the end users.
Strong, proven problem-solving skills and a proven ability to apply critical/analytical thinking to deliver sustainable and creative solutions to complex requirements.
As needed, document topology, processes, and solution architecture.
Assist with the training and enablement of data consumers.
Share your passion for staying on top of tech trends, experimenting with and learning new technologies

Required Experience:

Enterprise-level data modeling experience; proficiency in SQL, including multi-table joins, window functions, indexing strategies.
Experience developing using Python in context of data ingestion via REST APIs, manipulation with native data types, and database connection
Experience with AWS Services, including Lambda functions, EC2/ECS instances, S3, SQS, DynamoDB Tables, MWAA; familiarity with IAM Roles and Policies.
Experience with development and deployment of data pipelines using Apache Airflow; proficiency in base and third-party operators for complex DAGs.
Experience with Snowflake setting up storage integrations, external stages, data shares, snowpipes, RBAC; setting up tasks using Snowpark API.
Ability to work independently, and deliver results and drive projects with minimal supervision
Strong ability to communicate blockers and issues to management for escalation and timely resolution
Strong team player, with desire to learn new skills and broaden experience
Experience working with complex data sets

Nice to Have:

Experience with DevOps
Experience with event tracking configuration in Google GA4 and analysis using BigQuery
Experience with data migration project refactoring and optimizing complex SQL logic
Experience working with Financial data
Experience investing and/or using The Motley Fool's service offerings 

Compensation: 

Below is our target compensation range. While we are budget conscious, we're also eager to find the right person for this role, so if your target is outside of this range, please don't hesitate to apply and we'd be happy to have a conversation.

Hourly Pay Range

$85—$100 USD

By applying on this site, you acknowledge that The Motley Fool will be collecting the personal data you provide for our recruiting purposes. Please see our Applicant Privacy Notice for additional information about how we process, transfer, and store your data, including where that data is stored, and about any additional privacy rights you may have based on your jurisdiction.","Airflow y Google BigQuery, Calidad de datos, Estándares de datos, Indización, Modelado de datos, Resolución creativa de problemas, Resolución de problemas, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3943508655/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=UMBKw%2FZO9nZtAZt4B%2FsGgw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"110 US$K/año - 135 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Nueva York, NY","Acerca del empleo
ABOUT THE ROLE: 

We are seeking a seasoned full-time Data Engineer II to take on pivotal responsibilities in defining, deploying, and enhancing our mission-critical data infrastructure. Leveraging your expertise in the AWS ecosystem and ETL, you will play a significant role in processing extensive healthcare datasets. This will empower Rightway to yield powerful insights aimed at improving the healthcare experiences of our growing member base that's already over 1 million strong. While familiarity with medical data is beneficial, it's not a requisite for this role.

WHAT YOU'LL DO:

Craft robust, efficient Python and SQL code for production.
Utilize AWS and other services such as Lambda and Glue to handle data operations.
Build and maintain large-scale data pipelines using Apache Airflow.
Oversee the maintenance of Extract, Load, and Transform (ELT) processes using DBT.
Design optimized database schemas in PostgreSQL to ensure data integrity and high performance.

WHO YOU ARE: 

A seasoned data engineer with 2-4 years of industry experience.
Skilled in working with AWS systems in a production environment.
Proficient in writing SQL and Python code in a production environment.
Experienced in scaling data infrastructure to handle high volumes of data.

EXTRA CREDIT:

Experience working with medical and pharmacy claims data.
Knowledge in deploying applications using serverless architecture.
Hands-on experience managing AWS services such as Lambda and Glue.
Familiarity with infrastructure-as-code services such as Terraform or CloudFormation.

BASE SALARY: $110,000 - $135,000

CYBERSECURITY AWARENESS NOTICE

In response to ongoing and industry-wide fraudulent recruitment activities (i.e., job scams), Rightway wants to inform potential candidates that we will only contact them from the @rightwayhealthcare.com email domain. We will never ask for bank details or deposits of any kind as a condition of employment. If you have any questions about a suspicious interaction with Rightway, please feel free to reach out to us at hr@rightwayhealthcare.com.

ABOUT RIGHTWAY:

Rightway is on a mission to harmonize healthcare for everyone, everywhere. Our products guide patients to the best care and medications by inserting clinicians and pharmacists into a patient's care journey through a modern, mobile app. Rightway is a front door to healthcare, giving patients the tools they need along with on-demand access to Rightway health guides, human experts that answer their questions and manage the frustrating parts of healthcare for them.

Since its founding in 2017, Rightway has raised over $130mm from investors including Khosla Ventures, Thrive Capital, and Tiger Global at a valuation of $1 billion. We're headquartered in New York City, with a satellite office in Denver. Our clients rely on us to transform the healthcare experience, improve outcomes for their teams, and decrease their healthcare costs.

HOW WE LIVE OUR VALUES TO OUR TEAMMATES:

We're seeking those with passion for healthcare and relentless devotion to our goal. We need team members who will:

We are human first

Our humanity binds us together. We bring the same empathetic approach to every individual we engage with, whether it be our members, our clients, or each other. We are all worthy of respect and understanding and we engage in our interactions with care and intention. We honor our stories. We listen to—and hear—each other, we celebrate our differences and similarities, we are present for each other, and we strive for mutual understanding.

We redefine what is possible 

We always look beyond the obstacles in front of us to imagine new solutions. We approach our work with inspiration from other industries, other leaders, and other challenges. We use ingenuity and resourcefulness when faced with tough problems.

We debate then commit 

We believe that a spirit of open discourse is part of a healthy culture. We understand and appreciate different perspectives and we challenge our assumptions. When working toward a decision or a new solution, we actively listen to one another, approach it with a ""yes, and"" mentality, and assume positive intent. Once a decision is made, we align and champion it as one team.

We cultivate grit 

Changing healthcare doesn't happen overnight. We reflect and learn from challenges and approach the future with a determination to strive for better. In the face of daunting situations, we value persistence. We embrace failure as a stepping stone to future success. On this journey, we seek to act with guts, resilience, initiative, and tenacity.

We seek to delight

Healthcare is complicated and personal. We work tirelessly to meet the goals of our clients while also delivering the best experience to our members. We recognize that no matter the role or team, we each play a crucial part in our members' care and take that responsibility seriously. When faced with an obstacle, we are kind, respectful, and solution-oriented in our approach. We hold ourselves accountable to our clients and our members' success.

Rightway is a healthcare company looking to improve healthcare outcomes for everyone, everywhere. With that in mind, we have to consider what is good for the health of our team, the company, and the communities we operate in. As such, Rightway has determined a mandatory COVID-19 vaccination policy for all employees, in combination with other safety precautions, is the best way forward.

Rightway is PROUDLY an Equal Opportunity Employer that believes in strength in the diversity of thought processes, beliefs, background and education and fosters an inclusive culture where differences are celebrated to drive the best business decisions possible. We do not discriminate on any basis covered by appropriate law. All employment is decided on the consideration of merit, qualifications, need and performance.","Airflow, Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3917094338/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=l77uNIBJ6IMYtMo4qVBGCw%3D%3D&trk=flagship3_search_srp_jobs,Geospatial Data Engineer (Remote),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,"Ridge, NY","Acerca del empleo
Stefanini Group is looking for a Geospatial Data Engineer (Remote) for a globally recognized company! For interested applicants, click the apply button or you may reach out Micah Andres at (248) 386-7399/Micah.Andres@Stefanini.com for faster processing. Thank you!

Position Overview: We are seeking an experienced Senior Telecom Engineer with a strong background in field networking and data analytics. The ideal candidate will possess expertise in telecom field networking transformation, data analytics, data science, and geospatial predictive analysis. If you are passionate about leveraging technology to drive impactful solutions and thrive in a fast-paced, collaborative environment, we encourage you to apply.

Key Responsibilities

Design, implement, and optimize telecom field networking strategies to aid the transformation teams work while enhancing network performance and reliability.
Utilize data analytics and data science techniques to extract insights from data and drive informed decision-making.
Conduct geospatial predictive analysis to anticipate network demand and optimize resource allocation.
Lead/Support telecom field networking transformation initiatives, driving innovation and efficiency across the organization.
Collaborate with cross-functional teams to develop and implement strategies for network optimization and enhancement.
Provide technical expertise and support for telecom field networking projects, troubleshooting complex issues as needed.
Develop and maintain SQL queries and scripts to extract, manipulate, and analyze data.
Bachelor's degree in Electrical Engineering, Telecommunications, Computer Science, or related field. Master's degree preferred.
Proven experience in telecom field networking, with a focus on network design, implementation, and optimization.
Strong background in data analytics and data science, with proficiency in tools such as Python, R, or MATLAB.
Experience conducting geospatial predictive analysis and familiarity with GIS software (e.g., ArcGIS, QGIS).
Demonstrated expertise in telecom field networking transformation initiatives, driving innovation and efficiency.
Proficiency in SQL for data manipulation and analysis.
Excellent communication skills, with the ability to effectively collaborate with cross-functional teams and stakeholders.
Strong problem-solving skills and ability to troubleshoot complex technical issues.
Proven track record of delivering results in a fast-paced, deadline-driven environment.","Analítica de datos, Análisis predictivo, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Comunicación, Manipulación de datos, Optimización y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3964336675/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=Rw4cBHO0iQ51RYXBdKCwcg%3D%3D&trk=flagship3_search_srp_jobs,Data/Analytics Engineer:,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 semanas,Estados Unidos,"Acerca del empleo
100% Remote. Client is in CA

Need valid LinkedIn

Candidates need to take a video screen with prime vendor prior to submittal

Must be independent

what this person will be doing: pulling data from affinity CRM (this is a must have)

Responsibilities

Data/Analytics Engineer: Leverage your ETL experience on Google Cloud and Snowflake, with a focus on extracting data from Affinity CRM (must-have) and integrating notifications back to Slack.

Data Infrastructure Design: Design, build, and maintain the data infrastructure for optimal data extraction, transformation, and loading from various data sources using SQL, NoSQL, and big data technologies.

Data Collection Systems: Develop and implement robust data collection systems that seamlessly integrate diverse data sources, including proprietary and third-party data.

End-to-End Data Processes: Create an end-to-end process to collect, process, and visualize user engagement data, contributing to data-driven decision-making across the firm.

Collaboration: Work closely with the Head of Data & Analytics to ensure the data stack and infrastructure align with the firm’s operational needs and goals.

Minimum Qualifications

Education: Bachelor’s or Master’s degree in Computer Science, Database Management, or a related field.

Experience: At least 3 years of experience as a Data Engineer, with expertise in data warehousing, data monitoring, and ETL pipeline construction and maintenance.

Technical Skills: Deep experience with data pipeline and workflow management tools, such as Airflow. Solid knowledge and hands-on experience in database design, setup, and maintenance. Proficiency in Python and SQL.

Communication Skills: Excellent communication skills, capable of effectively articulating complex data concepts to a non-technical audience.

Bonus Points

Experience with venture capital data operations or working with financial data.

Familiarity with data visualization tools like Looker, Tableau, or PowerBI","Airflow, Almacenamiento de datos, Arquitectura de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y SQL, Construcción de gasoductos, Gestión de flujos de trabajo, Modelado de datos y Supervisión de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3973210294/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=4dQxy9wedzaHLwlm9A0%2FlQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"85 US$K/año - 115 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
A career that makes a difference!

RKD Group, a direct response marketing and advertising agency, is seeking a Data Engineer to achieve breakthrough results that make a positive impact in this world. By joining our team, you become an integral part of that mission. With our headquarters located in Dallas, Texas, RKD Group is a remote-first company that has helped hundreds of nonprofits raise billions of dollars to fulfill their missions. RKD Group's multichannel approach leverages technology, advanced data science, and award-winning strategic and creative leadership to accelerate net revenue growth, build long-term donor relationships, and drive online and offline engagements and donations. With a growing team of professionals, RKD Group creates breakthroughs never thought possible.

The Data Engineer owns the planning, development, management, and ongoing support of data pipelines and customer data warehouses. All data pipelines begin by extracting unstructured data from data lakes, transforming that data according to detailed specifications, and loading the data into data warehouses used to drive marketing operations for our customers.

The important work done by Data Engineers is instrumental in everything our company does to serve clients – from data science to campaign execution and reporting. To succeed in this position, you should have strong analytical skills and the ability to combine data from different sources. You should also have deep expertise in SQL and programmatic scripting. You need to be capable of working independently and collaboratively, be detail oriented with excellent writing and oral communication skills.

RESPONSIBILITIES: 
Develop data pipelines using extensive SQL and Knime
Implement robust error monitoring and reporting features within new and existing data pipelines
Troubleshoot data pipelines, data accuracy, and performance issues when they arise
Support other Data Engineers through peer review, knowledge sharing, and pair engineering
Implement and enhance automated data accuracy testing and reporting capabilities throughout customer data warehouses
Identify opportunities to improve and make more efficient all aspects of the company’s data engineering practices
Review data governance documentation and ensure data pipelines and customer data warehouses comply with business rules
Properly estimate the complexity of highly technical work and the required engineering duration

PREFERRED SKILLS AND QUALIFICATIONS:
Bachelor of Science degree in computer science, computer engineering, or information management preferred; qualified experience may substitute
3+ years of experience managing enterprise databases or data warehouses is preferred
3+ years of deep experience in SQL preferred
Marketing data experience is preferred
Mentoring and/or teaching other engineers is preferred
Proficiency with the following technologies:
SSIS and SMSS
SQL including both PostgreSQL & MSSQL
Microsoft SQL Server
IBM Db2
Knime
Snowflake
PHP, Python, Bash, and other scripting languages
JIRA or another technical project management platform

LOCATION: Remote.

Pay Range expected for the position: $85,000 - $115,000 annually.
(The position level and compensation for this role will be determined based on the market location, experience, job skills, and qualifications of the candidate)
RKD Group offers highly competitive compensation commensurate with experience, and a full range of benefits. 
RKD Group is an Equal Employment Opportunity employer. 
No sponsorships are available for this position.

Qualified candidates, please apply to this position using the link below.
https://rkdgroup.com/about-us/careers/","Microsoft SQL Server, SQL y SQL Server Integration Services (SSIS), DB2, KNIME, PostgreSQL y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3979168654/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=YAHWjtZ12DO9ti0e3QPe5w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 1 semana,"Seattle, WA","Acerca del empleo
Description

Data Engineer

Come build the future as a Data Engineer at Amazon, where you will be inspired working along best-in-class inventors and innovators! You will have the opportunity to create meaningful experiences that deliver on the ever-evolving needs of our customers, and your work will impact millions of people around the world.

As an Amazon Data Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. You will find that there is an unlimited number of opportunities within Amazon, where developing your career across a wide range of teams is highly supported. We are committed to making your work experience as enjoyable as the experiences you’ll be creating for our customers.

Apply now and you will be eligible for Amazon Data Engineer positions that are based on your preferred location, team, and more. We’re hiring across Amazon Stores in the United States and Canada.

Teams with available positions include, but are not limited to:

 Consumer Technology: Build new generation features and products for amazon.com, constantly improving the Customer and Seller experience for billions around the globe. Whether building site wide features such as reviews and recommendations, category specific software for the likes of Pharmacy, Electronics, Digital Software and Video Games or seller infrastructure, there are a variety of complex problems to tackle using a range of technologies in the design of your technical solutions.
 Operations Technology: Shape the future of transportation planning and execution on a global scale, that impacts hundreds of fulfillment centers, thousands of Amazonians, and millions of customers across the world. Your technology will support thousands of operators worldwide to design, build and run the best-in-class Amazon transportation network. We are building intelligent software to make transportation more reliable, faster, and less costly, providing a better and less expensive experience for our customers.
 Human Resources Technology: Create a seamless experience for millions of Amazonians and/or candidates. Whether supporting technologies for onboarding, time and attendance, compensation, amazon.jobs, or recruiting, you’ll deliver robust feature sets, elegant designs, intuitive user interfaces and systems that make it easy for Amazonians to excel at performing critical business functions.

About Us

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance be-tween your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign opportunities based on what will help each team member develop into a better-rounded contributor.

Inclusive Team Culture

Here at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Key job responsibilities

 Design, implement, and support a platform providing secured access to large datasets.
 Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
 Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
 Tune application and query performance using profiling tools and SQL.
 Analyze and solve problems at their root, stepping back to understand the broader context.
 Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
 Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
 Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
 Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com LLC - A03

Job ID: A2703397","AWS Lambda, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Datasets, Modelado de datos y Object Storage",Solicitar
https://www.linkedin.com/jobs/view/3959678549/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=AYB%2B3gGAPRBwRISV5LdIlQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Nueva York, NY","Acerca del empleo
Altana provides the world’s only dynamic, intelligent map of the global supply chain - the Altana Atlas - using AI and machine learning models to connect with and learn from massive sets of public and private data. Through the Atlas, companies and governments can understand the distant origins of products well beyond their own direct suppliers; discover trading relationships and national security risks deep in their networks; measure labor and environmental impacts; identify related risks and opportunities; ensure effective compliance and enforcement with trade requirements; and collaborate to manage all of it.

We have built a fundamental understanding of how the world’s economy works, and the implications for global resiliency, sustainability and opportunity are enormous. Backed by leading investors and used by the world’s most important organizations (Maersk, US Customs and Border Protection, Boston Scientific, and more), Altana’s mission is to power a new era of globalization organized around trusted supply chain networks.

This is a lofty mission, and our success depends on building a diverse, global team and creating an environment in which they can thrive. We operate in accordance with our values: we focus on value creation, not capture; we foster diversity and embrace difference; we embrace reality; we get things done; we amaze our clients. When you join Altana, you’ll be joining a vibrant, collaborative team working together to solve complex problems with the potential for global societal impact.

Company Overview

To solve climate change, wealth inequality, supply chain stability, and national security we must change how our global supply chains work. Altana is a Trusted Commerce Platform built on a shared source of truth for the global supply chain. The purpose of Altana is to enable resilient, sustainable, secure, and inclusive global commerce: Globalization 2.0.

We have built a layer of shared intelligence across the world’s supply chain information: a living map of trillions of dollars of B2B commercial activity, covering 400M companies connected by billions of shipments. This knowledge graph powers Altana’s Trusted Commerce Platform - the Altana Atlas.

Our product suite enables our customers to gain unprecedented visibility, benefit from shared artificial intelligence across a federated network of data, and interact across the network through a shared source of truth. We help our customers to build and manage trusted global supply chains.

The Implementation/Data Operations team is looking for talented Data Engineers who are passionate about data and all things related to data products and visibility. We are searching for individuals who are excited to build scalable solutions for our ever growing data needs, and are motivated by delivering functional data products for our internal teams and end users. You’ll be a major contributor to data acquisition and ingest processes, including dataset evaluation, ETL pipelines, and continuous data quality improvements. Your work will enable teams across the company to utilize newly incoming data, while partnering with software engineering and machine learning teams to implement scalable processes and define handover points.

Responsibilities

Contribute to the development, deployment, and maintenance of pipelines to extract, transform and load incoming data from a variety of 3rd party sources 
Analyze and evaluate new and existing data sources and provide recommendations regarding data quality and utility
Communicate with teams across the organization to ensure the success of data ingest deliverables, identify pain points, and envision improvements to existing processes
Collaborate with the overall engineering organization and contribute to the existing codebase
Help maintain ETL pipeline architecture and systems documentation
Follow engineering best practices

About You

3+ years of experience as a Sr Data Analyst, Data Engineer or a similar role, or education equivalent
Programming skills are required, preferably in Python
Experience with SQL and relational databases
Experience with data analysis, data warehousing, and knowledge of ETL pipelines
Excellent analytical skills, deadline-focused, detail-oriented, well organized, and self-motivated
Experience with code versioning tools (e.g. git)
Working knowledge of cloud services like AWS, Azure, or GCP
You are committed to collaboration, knowledge-sharing, and expanding your engineering and analytical skill set to fit the needs of the team
You have strong written and verbal communication skills

Nice To Have, But Not Required

Experience in a big data processing framework such as Spark
Experience with different data lake technologies (AWS Data Lake, Azure Data Studio,...)

This role can be based in New York or London (UK) with hybrid work flexibility and candidates must be within commuting distance from our Hub offices.

Why it’s great to work at Altana

We love to collaborate, and we win as a team!
We are committed to engineering excellence
We value personal and professional development
We learn from diverse backgrounds and perspectives
We impact the world, from enabling developing countries to identifying drug traffickers

At Altana, we believe that a diverse workforce enables greater creativity, performance, and adaptability. We’re proud to be an equal opportunity employer and welcome you to join us as you are. Our employment opportunities and decisions are based on business needs and individual qualifications, without regard to race, color, religious creed, national origin, ancestry, age, physical or mental disability, medical condition, marital status, sexual orientation, gender identity or expression, genetic information, family care or medical leave status, military or veteran status, or any other characteristic protected by the laws or regulations in the areas in which we operate. We prohibit discrimination and harassment of any type, in any situation.","Almacenamiento de datos, Analítica de datos, Análisis de datos, Apache Spark, Big data, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación y Comunicación oral",Solicitud sencilla
https://www.linkedin.com/jobs/view/3976469818/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=nKD6uKcwedbdVyvroz2eeQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer with Google Looker,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Basking Ridge NJ, Dallas TX or Remote

Who We Are:

Excelacom has a global presence in the consulting and technology solutions industry with a focus on Communications and Media providers. We collaborate with our clients to solve their complex business, technology and operational challenges through a combination of consulting expertise and telecom-focused solutions and products. Excelacom provides extensive experience in product development, IT consulting, telecom-focused software and managed services, as well as strategic management and thought leadership to steer our clients towards a successful business and technology transformation.

At Excelacom, we recognize that our most valuable asset is our people. We strive to create an environment where our employees can contribute their unique perspectives and grow individually and collectively. Our diverse and inclusive culture empowers our people to embrace their ideas and transform them into innovative solutions.

Who You Are:

At the Consultant-level, you will be expected to independently contribute ideas, recommendations, and solutions toward solving business challenges, construct key components of deliverables/presentations, and contribute to the development of Intellectual Capital (IC) and Marketing Collateral. You will serve as a team leader for individual work streams and acquire a comprehensive understanding of the firm’s expertise, product set, and capabilities. Most importantly, you will embrace our firm’s brand and core values.

Role and Responsibilities:

You may work on various projects throughout your career with Excelacom. The roles and responsibilities noted below pertain to the specific role listed above.

Decompose key business problems to identify value areas and structure and implement complex technology solutions for the client.
Create innovative and differentiated offerings, staying relevant and in sync with market demand and technology evolution.
Deliver advisory services to Senior Management and above in support of technology-enabled business changes.
Manage and Deliver technology projects including application development, integration, data migration, testing, etc. for IT organizations.
Meet with client staff on a daily basis to understand key issues with how they operate today and identify opportunities for improvement in their operations and technology landscape.
Leveraging Excelacom’s domestic and international assets and expertise to bring new ideas to clients.
Supporting the development of improved capabilities in clients across business processes, people and technology.
Developing business cases and implementation roadmaps to support investment decisions.
Providing project management and change management support on large- or small-scale transformation projects.
Identify and qualify additional business opportunities
Prepare solutions and proposals and present to clients
 Design, develop, and maintain robust and scalable web applications. 
 Embed visualizations from various BI platforms, including Google Looker, into our web applications. 
 Collaborate with cross-functional teams to define, design, and ship new features. 
 Optimize application performance and troubleshoot issues as they arise. 
 Implement and maintain integrations with third-party APIs and services. 
 Ensure the technical feasibility of UI/UX designs. 
 Write clean, maintainable, and efficient code. 
 Participate in code reviews to maintain code quality and share knowledge with the team. 

Required Qualifications:

Bachelor’s degree, preferably in Computer Science or related field
Prior consulting experience –in an agile environment – with a leading strategy firm, consulting practice, or consulting technology firm
Proficiency in Microsoft Office (Word, Excel, PowerPoint, Visio)
Highly collaborative, team-oriented with strong stakeholder management skills
Ability to lead project working groups and possesses strong facilitation and data gathering skills
Strong verbal and written communication skills with the ability to interact with technical and non-technical personnel
 Proficiency in front-end technologies (HTML, CSS, JavaScript, React, Angular, or Vue.js). 
 Strong back-end development skills (Node.js, Python, Ruby on Rails, or Java). 
 Experience with database management (SQL, NoSQL). 
 Expertise in embedding BI visualizations, specifically with Google Looker. 
 Well versed with other BI platforms (Tableau, Adobe Analytics, Power BI, etc.) 
 Understanding of RESTful APIs and web services. 
 Knowledge of version control systems (Git). 

Preferred Qualifications:

Prior experience in Telecommunications, Media, or Cable is highly preferred
Past success in generating new business with existing clients
Good time management skills
 Experience with cloud platforms (Google Cloud, Azure, AWS to lesser extent). 
 Familiarity with CI/CD pipelines and DevOps practices. 
 Experience with data visualization libraries (D3.js, Chart.js). 
 Knowledge of security best practices for web applications. 

Compensation and Benefits:

Our competitive salaries are just one component of Excelacom’s total compensation package for our regular full-time employees. Other rewards and benefits include: health, vision, and dental insurance, ancillary benefits, Life and AD&D insurance, 401k, and generous paid time off policies.

EEO Statement:

Excelacom, Inc. is an equal opportunity employer. Any decision affecting employment, compensation, promotion, or transfer will be based solely on personal qualifications and merit, regardless of sex, race, color, religion, gender identity, sexual orientation, marital status, national origin, disability, age, results of genetic testing, service in the military, pregnancy, childbirth or other related medical conditions or any other factor protected under applicable law.","Analítica de datos, Ciencia de datos y D3.js, Administración de bases de datos, Angular, Chart.js, Desarrollo front end, HTML, React.js y Vue.js",Solicitar
https://www.linkedin.com/jobs/view/3983288191/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=ajaWehzWRaqJ8%2F9f6kHg8Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"105 US$K/año - 125 US$K/año Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.",hace 5 días,"Denver, CO","Acerca del empleo
Description

 Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouses and data lakes.
 Implement efficient and scalable data ingestion mechanisms, ensuring data quality, integrity, and consistency, while optimizing performance and resource utilization.
 Collaborate with key stakeholders to ensure the availability, quality, and accuracy of data required for analysis and modeling.
 Participate in cross-functional projects, working closely with teams such as investment management, product management, client solutions, and compliance to understand their analytical needs and provide data-driven insights.
 Build and optimize data models, schemas, and data warehousing solutions to support advanced analytics and reporting needs.
 Implement data governance policies, data security measures, and access controls to ensure data privacy and compliance with industry regulations.
 Automate data processes and workflows using scripting languages and orchestration tools.
 Proactively monitor data pipelines, identify and resolve issues.
 Contribute to the development of data best practices, tools, and processes within the organization.
 Stay up to date with the latest data engineering technologies, tools, and best practices, and contribute to the continuous improvement of our data infrastructure.

Requirements

ETL Develpoment
SQL
Snowflake
PowerBI/Tableau

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Calidad de datos, Modelo de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3977070237/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=%2BJhE1ZG8pAKtVASTrqahbA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 semana,"Austin, TX","Acerca del empleo
Nomi Health was founded in 2019 as a direct healthcare company with a simple yet bold mission: rebuild the healthcare system so it is accessible and affordable for everyone. We are rebuilding the healthcare system by cutting costs, confusion, and complexity through direct contracts and payment with providers, deep data dives, and convenient patient care.

We are looking for a talented Data Engineer to join our team in Austin, Texas. You will be responsible for the design, implementation, maintenance of our data pipelines to integrate with our platform ensuring data is accessible, clean, and optimized. This is hybrid working onsite 3 days a week (Tuesday, Wednesday, Thursday).

How you will make an impact

Design, implement, and maintain efficient data pipelines (ETLs) in order to integrate data from a variety of sources into Nomi’s platform
Support automated processes for complex data analysis, management, and visualizations that incorporate data governance
Make decisions on complex data issues regarding technical approach for project components and reporting requirements
Design and implement data model changes that align with Nomi’s standards
Develop and execute testing strategies to ensure high quality data
Work with different teams within Nomi to reach end goals


What we are looking for

Hybrid working onsite 3 days a week (Tuesday, Wednesday, Thursday) in our Austin office
2+ years in data engineering or related field
Skilled with NoSQL and SQL (Snowflake)
AWS (preferred but not required) resources such as lambdas, docDB, EC2
Data first approach
Experience with Python, Git, and Jira
Experience with sensitive data is a bonus


Nomi’s journey is just starting in delivering disruptive healthcare solutions, in partnership with like-minded employers, public sector organizations, advisors (brokers/consultants), and payers/TPAs. We are dedicated to our mission to remove healthcare hurdles and rebuild healthcare the way it should have always been: for everyone.

The system must change, and we’re the ones to do it. Join us on the journey.

Benefits/Perks

 Medical, dental, and vision
 401(k) with company match
 Open PTO
 Continuous learning
 Family leave


Learn more about us

 LinkedIn
 X
 Facebook
 Glassdoor","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Bases de datos, Requisitos de información y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3982971553/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=qiFejA2FaHieb2mdsEOWsA%3D%3D&trk=flagship3_search_srp_jobs,Big Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,Cincinnati y alrededores,"Acerca del empleo
Job Summary: 
Responsible for building outstanding software solutions to drive the success of a business. Build various aspects of the company's infrastructure to power innumerable conversations at scale. 

Primary Responsibilities: 
Maintain the company platform uptime, performance, stability, and scalability 
Design, guide, mentor and challenge system architecture and design with others 
Develop and maintain a public API 
Develop best possible, most robust, and extensible solutions from feature requests 
Work with big data technology (Kafka, Hadoop, Spark, etc) 
Work with Data Scientists to develop rich value-added features 
Work with DBA to create ETL and Data Warehouse system 
Work with Operations to automate solutions and increase service reliability 
Closely monitor all platform related production systems 
Building tools and processes to support analytics, monitoring, machine-learning and data-warehousing platforms. 
Define and implement various strategies covering everything from subnets to backups to fog networking/computing configuration and deployments. 
Provision, configure, maintain, backup, and monitor onsite and cloud based server resources.
Define and implement deployment strategies for client-facing and internal tool systems.
Continual improvement and fine-tuning of various alerting and monitoring systems.
Qualifications: 
Experience in software/systems development. 
Strong software development background, experience building software systems. 
Working knowledge of at least one of the following languages: PHP, Ruby, Python, JavaScript, Elixir, Go or comparable. 
Strong background in Linux administration. 
Strong experience with cloud providers such as AWS, Digital Ocean, Google Cloud, etc. 
Strong understanding of IT security best practices. 
Experience with automation/configuration software (puppet, ansible), and/or orchestration software (docker swarm, kubernetes, etc). 
Understanding of computer networks. 
Experience with administration of at production scale.","Docker, Desarrollo de sistemas, Linux, Ruby, Sistemas de software y Soluciones de software",Solicitud sencilla
https://www.linkedin.com/jobs/view/3964478155/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=aDA%2BvBQyTQFea84hqgV8hg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer with Healthcare - Contract - W2 Only - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",hace 3 semanas,"Bloomfield, CT","Acerca del empleo
Data Engineer with Healthcare

This posting is for a W-2 employee only. Staffing company candidates and independent contractors will not be considered. All work under this opportunity will be performed in the United States.

Position Details

Type: REMOTE POSITION as a contractor. If converted will be Hybrid in Bloomfield, MN; Bloomington, MN; St. Louis, MO; Morris Plains, NJ.

Responsibilities

Coordinate and perform data intake tasks across all data feed types, including large/complex clients, ensuring high-quality and on-time delivery.
Implement multiple data feeds and complex projects simultaneously.
Support new data feed types to expand programs.
Utilize technical skills, data knowledge, and creativity to solve complex problems.
Maintain comprehensive knowledge of all product offerings and business processes.
Work closely and effectively with customers, vendors, IT, stakeholders, and other internal business partners to meet all data feed and project objectives.
Collaborate with internal business leads to understand the needs of downstream data users and how the Inbound Data team can drive quality outcomes.
Provide business insight to assist in developing IT solutions that maximize the Inbound Data team’s efficiency and improve the quality of externally sourced data.
Serve as lead on projects where a data SME is necessary, representing the goals and functions of ingesting quality data.
Lead efforts to build new capabilities and functionality to accommodate new business initiatives and enhance platform effectiveness and efficiency.
Create and execute test plans to check for data quality across multiple systems.
Maintain accurate and comprehensive documentation (data dictionaries, gap analysis, vendor documentation, project plans, etc.).
Build and document new processes and follow existing processes.

Qualifications

Minimum of 5 years of experience in the healthcare industry (Required).
Demonstrated experience in data exchange, particularly healthcare and clinical data (Required).
Expert organizational skills with the ability to be process-oriented and work on multiple implementations and projects simultaneously (Required).
Expert analytical skills and the ability to use creativity, logic, and various tools and technologies (Advanced Excel, Power BI, SQL, Tableau, Python, etc.) to identify and analyze data issues (Required).
Ability to adapt quickly to change and willingness to learn new tools, processes, and technologies (Required).
Excellent written and verbal communication skills (Required).
Experience working independently with some direction (Required).
Forward-thinking mindset with the ability to proactively anticipate and investigate potential future issues (Required).
Ability to work effectively with internal and external partners to investigate and resolve any failures or issues that may arise (Required).
Demonstrated project management and coordination experience (Highly Preferred).
Previous experience collaborating with teams across multiple organizations and matrix environments (Highly Preferred).
Experience working in an agile environment and utilizing tools like Jira (Highly Preferred).
Experience with industry-standard data formats (FHIR, HL7, X12) (Helpful).
Customer service, help desk, or technical support experience (Helpful).

Education

Bachelor’s degree in Business, MIS, CIS, or equivalent work experience (Required).

For more information please contact Tom Mazzulla at tmazzulla@itechsolutions.com

Since 1995, iTech Solutions Inc., has been providing IT Consulting and Direct Hire Services to the Insurance, Financial, Communications, Manufacturing and Government sectors with local offices in Connecticut, Minnesota, Colorado, Massachusetts, Tennessee, North Carolina, and New Jersey / Pennsylvania area.

Our recruiting strategy is simple, if you want to find qualified IT professionals then use IT professionals to find them. So at iTech Solutions, our personnel are all career IT professionals with a wide range of IT experience. We can honestly say our staff understands the technologies, the complexities of finding and selecting the appropriate personnel and the pressures of running successful IT projects. 

Employer will not sponsor applicants for any employment visas, at hiring or in the future, including but not limited to H-1B visas. Corp-to-Corp or subcontract personnel will not be considered for this position.
Aptitudes y experiencia deseables
DATA, INTAKE, DATA FEED, EXCEL, POWER BI, TABLEAU, SQL, PYTHON, FHIR, HL7, X12, HEALTHCARE, HEALTHCARE COMPANIES, CIGNA","CICS, Comunicación, Comunicación oral, Datos clínicos, Formación en comunicación, Fuentes de datos, Intercambio de datos, Microsoft Exchange, Sector sanitario y Sistemas de gestión de información (MIS)",Solicitar
https://www.linkedin.com/jobs/view/3948468546/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=3hQC6JwjEaDZJ29Qq3lFuA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer 2,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 3 semanas,"Boise, ID","Acerca del empleo
Our vision is to transform how the world uses information to enrich life for all. 

Micron Technology is a world leader in innovating memory and storage solutions that accelerate the transformation of information into intelligence, inspiring the world to learn, communicate and advance faster than ever.

Function as a key member of a multi-functional team responsible for developing and growing Micron’s methods and systems for extracting new insight for our expanding data streams. Collaborate with data scientists, engineers, technicians and data mining teams to design and implement systems to extract data from Micron’s business systems, transforming it into an actionable format, and as needed, create dynamic presentation layers for use by high-level engineers and managers throughout the company. Create new solutions, as well as, support, configure, and improve existing solutions. Build and maintain data/solution pipelines. Work in a technical team through development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics using machine learning and artificial intelligence. Design and optimize data structures in data management systems (Hadoop, Snowflake, and Cloud platforms) to enable Artificial Intelligence/ Machine Learning (AI/ML) solutions. Build custom software components and analytics applications. May telecommute from home part-time.

Employer will accept a Bachelor's degree in Industrial Engineering, Computer Science, or Business Analytics or related field.

As a world leader in the semiconductor industry, Micron is dedicated to your personal wellbeing and professional growth. Micron benefits are designed to help you stay well, provide peace of mind and help you prepare for the future. We offer a choice of medical, dental and vision plans in all locations enabling team members to select the plans that best meet their family healthcare needs and budget. Micron also provides benefit programs that help protect your income if you are unable to work due to illness or injury, and paid family leave. Additionally, Micron benefits include a robust paid time-off program and paid holidays. For additional information regarding the Benefit programs available, please see the Benefits Guide posted on micron.com/careers/benefits.

Micron is proud to be an equal opportunity workplace and is an affirmative action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, age, national origin, citizenship status, disability, protected veteran status, gender identity or any other factor protected by applicable federal, state, or local laws.

To learn about your right to work click here.

To learn more about Micron, please visit micron.com/careers

For US Sites Only: To request assistance with the application process and/or for reasonable accommodations, please contact Micron’s People Organization at hrsupport_na@micron.com or 1-800-336-8918 (select option #3)

Micron Prohibits the use of child labor and complies with all applicable laws, rules, regulations, and other international and industry labor standards.

Micron does not charge candidates any recruitment fees or unlawfully collect any other payment from candidates as consideration for their employment with Micron.","Almacenamiento de datos, Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Ciencias de la computación y Comunicación",Solicitar
https://www.linkedin.com/jobs/view/3882242954/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=H0Ursfke6lQ%2FopmjXXPuNQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Skillbridge Intern) - 18377,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Prácticas",Publicado de nuevo hace 2 semanas,"Augusta, GA","Acerca del empleo
Enlighten, honored as a Top Workplace from the Baltimore Sun, is a leader in big data solution development and deployment, with expertise in cloud-based services, software and systems engineering, cyber capabilities, and data science. Enlighten provides continued innovation and proactivity in meeting our customers’ greatest challenges.

We recognize that the most effective environment for your projects doesn’t always look the same. Our hybrid work approach ensures that you can make lasting relationships with your team and collaborate in-person to get the job done—while having the flexibility to be working from home when needed to achieve focused results.

Job Description

This is an UNPAID internship through the DoD SkillBridge Program for transitioning active-duty US military personnel. DoD SkillBridge Internships are available to help transitioning active-duty military personnel gain real-world experience in the work force sometime during their final 180 days of active-duty service. The intern will actively train on meaningful projects and work closely with a mentor and with senior company leadership. Enlighten Internship programs are focused on placing transitioning military into internships that require KSAs, Education & Military Training similar to their current or previous military jobs; positions that could easily transition over to a full-time regular and permanent job with Enlighten.

Essential Job Responsibilities

The SkillBridge intern will train as a Data Engineer with Enlighten, reporting to a designated Enlighten Supervisor, with the goal of learning Enlighten’s approach to design, build, and optimize Big Data Platform (BDP) systems for data collection, storage, access, and analytics at scale. The intern will be assigned special projects as needed.

Desired End State (3-4 month target) 

At the end of four months, the intern will be able to understand basic data engineer concepts. Data engineers, as part of enterprise data analytics teams, manage, optimize, oversee, and monitor data retrieval, storage, and distribution throughout the BDP enclaves. 
The intern will develop a good understanding of all critical tasks to successfully conduct data engineer objectives:
Data acquisition 
Develop data set processes (Extract, transform, load (ETL)) 
Use programming language and tools (Java, NiFi, maven, etc) 
Identify ways to improve data reliability, efficiency, and quality 
Prepare data for predictive and prescriptive modeling 
Use data to discover tasks that can be automated 
Enlighten will benefit from the military background of the SkillBridge intern and considers the SkillBridge internship an overall positive experience. 
Assumptions/Restrictions

If candidate is selected and approved, SkillBridge Intern can travel in conjunction with this internship; Enlighten will fund all travel costs. 
SkillBridge Intern will be available during core hours for critical meetings and training. 

Training Plan

Phase 1 Basics

Week 1: In processing, Introductions/office familiarization 
Week 2: Intro to the Big Data Platform (BDP) technologies. Intro to BDP analytics and applications 
Week 3: Intro to Army and Air Force Cyber data ETL process 

Phase 2 Parser Overview

Week 4: Data Acquisition, Taxonomy/Schema. Review existing parsers and deployment flows; understand existing transforms available for use in parsers.
Weeks 5-6: BDP technologies - Kafka and Nifi workflows; Establish a development environment for BDP parser development

Phase 3 Parser Development, Deployment, and Testing

Weeks 7-8: Modify Kafka and Nifi workflows; modify existing parser schema and transforms to alter ingest pipeline.
Week 9: Understand how to access logs to identify errors; understand common parser development errors; troubleshoot errors.
Weeks 10-end: Implement data parsing best practices, building, deploying, and troubleshooting workflows.

Additional Goals

Project management

Minimum Qualifications

Active-Duty Military ONLY within final 180 days of active-duty service. 
Minimum of 5 years relevant data engineering (including experience with Python or Java) experience with Bachelors in related field; 3 years relevant experience with Masters in related field; or High School Diploma or equivalent and 9 years relevant experience.
Additional experience dependent on Program of Instruction.

Physical Requirements

Physical Requirements will vary and is dependent on the Program of Instruction.

We have many more additional great benefits/perks that you can find on our website at www.eitccorp.com [eitccorp.com].

Enlighten, an HII Company, is an Equal Opportunity/Veterans and Disabled Employer. U.S. citizenship may be required for certain positions.","Analítica de datos, Apache Kafka, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, Adquisición de datos, Datos empresariales, Java, Parseo y Phase III",Solicitar
https://www.linkedin.com/jobs/view/3982221890/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=wRVJq9jxgSVZMBZZKQenKQ%3D%3D&trk=flagship3_search_srp_jobs,Azure Data Intelligence Engineer II,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 3 días,Estados Unidos,"Acerca del empleo
JOB SUMMARY

Consumer Direct Care Network is seeking a dynamic, experienced, and engaging leader to fill the position of Azure Data Intelligence Engineer II in our Missoula, MT location. The Data Intelligence Engineer II will be responsible for data integration and the consolidation of data from different systems. This position requires strong experience in Azure Data Factory and Azure Databricks. This position will also support other IT systems and software.
In the Consumer Direct Care Network our employees are committed, dependable, professional, and accountable. We have high standards for how our employees conduct themselves at work and in the communities we support. Our company values of Respect, Integrity, Service, and Excellence guide us in all we do. Come join our team and be a part of a rewarding career helping others!


JOB DUTIES

Supporting and maintaining the company's data warehouse systems, which store and organize the company's data for reporting and analysis purposes.
Designing and coding ETL (extract, transform, load) data solutions related to the Data Warehouse. This involves working with large amounts of data, identifying patterns and trends, and transforming data to make it usable for analysis.
Utilizing CDCN (critical data control network) platforms such as 1EDI, Solomon, U2, Workday, Docuware, EVV, etc. to ensure that data is accurate, timely, and secure.
Providing support for the Helpdesk, which involves responding to requests and resolving issues related to data systems and tools.
Troubleshooting escalated issues during business hours, or during afterhours on-call rotation, to ensure that critical data systems are always operational.
Assisting in analyzing solutions for processing efficiencies, which involves identifying opportunities to optimize data processes and systems to reduce costs and improve efficiency.
Effectively communicating within the team and company, which involves collaborating with other engineers, business analysts, and stakeholders to ensure that data systems meet business needs.
Performing other duties as assigned, which may include working on special projects or taking on additional responsibilities as needed.
In this role, you'll need to have a strong understanding of data systems, databases, and data integration technologies, as well as experience with ETL development and support. You'll also need to be highly analytical, detail-oriented, and able to work effectively under pressure. Strong communication skills and the ability to work collaboratively with others are also essential.


QUALIFICATIONS

Required experience for this position:
Up to two years of experience working with data platforms and data warehouses, including experience in designing, developing, and maintaining data systems.
Up to two years of experience with data structure and file processing knowledge, including familiarity with ANSI X12/JSON/XML and other data exchange formats
Proven abilities in ETL (extract, transform, load) processes, SQL queries/scripting, and familiarity with cloud platforms such as Azure.
Experience working with application/software and file processing knowledge, including familiarity with Synapse, Data Factory, Data Bricks, Azure Dev Ops, SSMS, and other related tools.
Excellent communication skills, including the ability to communicate technical information effectively to non-technical stakeholders.
Strong attention to detail and the ability to work accurately and efficiently in a fast-paced environment.
Ability to work independently and collaboratively as part of a team, and to take ownership of tasks and projects.
Strong organizational and project management skills, including the ability to manage multiple projects simultaneously and prioritize tasks effectively.
Functional knowledge of key CDCN platforms mentioned above, such as 1EDI, Solomon, U2, Workday, Docuware, EVV, etc.
A degree in a related field is preferred, but relevant experience in place of a degree will also be considered


The incumbent typically works in an office environment and uses a computer, telephone and other office equipment as needed to perform duties. The noise level in the work environment is typical of that of an office. Incumbent may encounter frequent interruptions throughout the workday. The employee is regularly required to sit, talk, or hear; frequently required to use repetitive hand motion, handle or feel, and to stand, walk, reach, bend or lift up to 20 pounds. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Integración de datos y JSON, ANSI X12, Bases de datos, Comunicación, Resolución de cuestiones, Sistemas de datos y X12",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982791827/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=YDvPk5YXuyk9tOZ1K45Uzw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Sonata Software North America, is seeking the following. Apply via Dice today!

Minimum Qualifications

Essential to perform duties without training

BS/BTech (or higher) in Computer Science, Engineering or a related field.

2+ years experience as a python engineer building data pipelines.

2+ years of experience working with SQL or other database querying language on large

multi-table data sets

Strong self-starter able to work independently and complete tasks in timely manner

Preferred KSA s

Nice to have knowledge, skills, and/or abilities

Experience working with health-tech systems, like Electronic Health Records, Clinical data, etc.

Domain Specific Experience

Data Infrastructure:

Experience in designing, building and optimizing data pipelines and ETL processes

Proficiency in working with large datasets and knowledge of data storage technologies

Experience working with data ingestion systems and optimizing performance for

handling large-scale data processing and analysis

In-depth knowledge of database systems

Familiarity with database replication, sharding and other techniques for scalability and

high availability of databases

Experience in performance monitoring and optimization of data systems and

infrastructure

Experience with containerization and orchestration technologies such as Docker and

Kubernetes

Experience building continuous integration and continuous deployment(CI/CD) pipelines

Experience with security and systems that handle sensitive data

Data Engineer","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Datasets, Optimización y Sistemas de bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3916793519/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=PYMxZwA%2B0E5DOvWgMseMjQ%3D%3D&trk=flagship3_search_srp_jobs,Data Warehouse Engineer (Remote Eligible),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,"Illinois, Estados Unidos","Acerca del empleo
Position Summary

The Data Warehouse Engineer is a key technical role responsible for the design, development, maintenance, and optimization of data warehouse processes. This includes creating and maintaining ETL (Extract, Transform, Load) processes, defining database architecture, and assisting in cloud migration efforts. The Data Warehouse Engineer collaborates with cross-functional teams to deliver data solutions that support Business Intelligence and analytics, ensuring data is accurate, timely, and easily accessible

Duties & Responsibilities

Design, develop, and maintain data warehouse ETL processes to ensure accurate and timely data availability for business intelligence and analytics. 
Define and implement efficient database architecture, including data modeling, indexing, partitioning, and performance tuning. 
Collaborate with cross-functional teams to gather requirements, define data integration and transformation strategies, and deliver data solutions. 
Assist in migrating existing on-premise data infrastructure to cloud platforms (e.g., AWS, Azure, or Google Cloud Platform). 
Monitor and troubleshoot data pipeline performance, ensuring data quality, and addressing any data-related issues. 
Stay up-to-date with the latest data engineering best practices, tools, and technologies to continuously improve and optimize data warehouse processes. 
Develop and maintain technical documentation, including data flow diagrams, data dictionaries, and system specifications. 

Education & Experience

Knowledge Of

Proficiency in SQL and experience with various database management systems (e.g., MySQL, PostgreSQL, Oracle, SQL Server). 
Solid understanding of data warehouse concepts, including star schema, snowflake schema, and dimensional modeling. 
Experience with ETL tools such as Microsoft SQL Server Integration Services (SSIS). 
Knowledge of cloud platforms and experience in cloud migration, specifically with AWS, Azure, or Google Cloud Platform. 
Strong programming skills in languages such as Python, Java, or Scala. 
Knowledge of data visualization tools such as IBM Cognos, Tableau, or Power BI. 

Ability To

Excellent problem-solving skills, attention to detail, and ability to work effectively both independently and in a team environment. 
Strong communication skills, with the ability to explain complex technical concepts to non-technical stakeholders
Ability to lead and influence discussions across a variety of stakeholders to achieve results and proactively address sources of conflict
Self-starter with great communication skills (verbal and written)

Education And Training

Bachelor's degree in Computer Science, Engineering, Information Systems, or a related field. A Master's degree is preferred. 
Minimum two years of experience in the technology industry, knowledge of the Financial Industry is a plus. 
3+ years of experience in data engineering, ETL development, and database management. 
Certifications in data engineering, database management, or cloud platforms is a plus. 
Requires knowledge of Microsoft Office. 

Busey values a diverse and inclusive workplace and strives to recruit, develop and retain individuals with exceptional talent. A team with diverse talent, working together, is essential to Busey’s commitment of delivering service excellence. Busey is an Equal Opportunity Employer including Disability/Vets. Visit Busey.com/Careers to learn more about Busey’s Equal Opportunity Employment.","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos y SQL Server Integration Services (SSIS), Administración de bases de datos, Bases de datos, Modelos dimensionales, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3951221864/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=ELlvenpShtTOblwwqnr2EA%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Scientist/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"San Francisco, CA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3979448391/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=yS1SdzqsYp9XDq0T4fPJSg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II - AWS/Databricks/Python/ETL,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Hartford, WI","Acerca del empleo
Who Are We?

Taking care of our customers, our communities and each other. That's the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.

Job Category

Technology

Compensation Overview

The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.

Salary Range

$121,000.00 - $199,600.00

Target Openings

1

What Is the Opportunity?

Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate stories found in data by leveraging a variety of data programming techniques. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights. What Will You Do?

Build and operationalize complex data solutions, correct problems, apply transformations, and recommending data cleansing/quality solutions.
Design complex data solutions
Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.
Incorporate core data management competencies including data governance, data security and data quality.
Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.
Perform data and system analysis, assessment and resolution for complex defects and incidents and correct as appropriate.
Test data movement, transformation code, and data components.
Perform other duties as assigned. 


What Will Our Ideal Candidate Have?

Bachelor's Degree in STEM related field or equivalent
Eight years of related experience
Highly proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and a full understanding of modern software engineering practices.
The ability to deliver work at a steady, predictable pace to achieve commitments, deliver complete solutions but release them in small batches, and identify and negotiate important tradeoffs.
Demonstrated track record of domain expertise including understanding technical concepts necessary and industry trends, and possess in-depth knowledge of immediate systems worked on and some knowledge of adjacent systems.
Strong problem solver who ensures systems are built with longevity and creates innovate ways to resolve issues.
Strong written and verbal communication skills with the ability to work collaborate well with team members and business partners.
Ability to lead team members and help create a safe environment for others to learn and grow as engineers. and a proven track record of self-motivation in identifying opportunities and tracking team efforts.


What is a Must Have?

Bachelor's degree or equivalent training with data tools, techniques, and manipulation.
Four years of data engineering or equivalent experience.


What Is in It for You?

Health Insurance: Employees and their eligible family members - including spouses, domestic partners, and children - are eligible for coverage from the first day of employment.
Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.
Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.
Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs.
Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.


Employment Practices

Travelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.

In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions.

If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.

Travelers reserves the right to fill this position at a level above or below the level included in this posting.

To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.","Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Análisis de sistemas, Calidad de datos, Comunicación, Datos de prueba, Edición de imágenes, Limpieza de datos y Prácticas recomendadas en ingeniería de software",Solicitar
https://www.linkedin.com/jobs/view/3958941248/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=65nD6yITcfaLIfnaZFqfuQ%3D%3D&trk=flagship3_search_srp_jobs,ETL Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 semana,"Seattle, WA","Acerca del empleo
Design and develop ETL jobs and data pipeline.

 Design, Develop and support enterprise data warehouse solutions leveraging Teradata and Teradata tools and utilities.
 Extracting data from various data sources, transforming, and loading it into target systems.
 Ensuring data quality, integrity, and consistency across different systems.
 Developing and maintaining data warehouses, data lakes and data mart architecture.
 Optimizing ETL processes for performance, scalability and reliability.
 SQL proficiency (intermediate skill level minimum)
 Ability to manage Teradata/SQL Server/ Oracle database migration including code migration, data loads and server retirements.
 Troubleshooting and resolving data integration issues
 Solid skills with writing/translating SQL queries and SQL data validation.
 Experience in migration SSIS jobs to Teradata pipeline/jobs.
 Performance tuning of SQL queries.

Aptitudes y experiencia deseables
TeraData","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Calidad de datos, Lagos de datos, Teradata y Validación de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3964834953/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=Lsvm5BE4Kl1W8oFwXtvChg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 6 días,"Midland, TX","Acerca del empleo
Permian Resources (NYSE: PR) is currently seeking a Data Engineer in Midland, TX. Reporting to the Sr. Director of Data & Analytics, the Data Engineer will be an integrated member of the Data & Analytics Team responsible for data engineering within Databricks for all the business teams at Permian Resources

General Responsibilities

Responsible for designing, building, and optimizing data pipelines from source systems into an analytics ready environment
Convert raw data into usable formats for data consumers
Monitor and support the development and production environments within Databricks to ensure system availability and quality
Continuously develop, test, and manage the data pipelines
Build data pipelines that transform, clean, and aggregate data from the disparate systems
Develop data models and pipelines for reporting, dashboards, and machine learning
Collaborate with multiple business SMEs, data scientists and IT members to provide reliable and clean data
Work on multiple projects in a fast-paced environment 


Qualifications

Proven experience within the Databricks Data Engineering platform or similar
BS/BA in Business Analytics, Computer Science, Statistics, or relevant field
Proficient knowledge of SQL, Python, R or other similar languages
Must be self-motivated and capable of independently integrating within multi-disciplinary teams
Must have strong verbal and written communication skills and be able to communicate effectively with all levels of the organization","Almacenamiento de datos, Analítica de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Comunicación, Comunicación escrita, Equipos multidisciplinarios y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3934093668/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=cunwoedE%2FiCg9UgCFEhWRA%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Analyst/Engineer - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"Austin, TX","Acerca del empleo
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this market also our candidates get multiple job offers and $100k + salaries.

 please check the below links to see success outcomes of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 https://www.youtube.com/watch?v=OFoqPTNORew

 https://www.youtube.com/watch?v=-HkNN1ag6Zk

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

 https://youtu.be/bJJl27D8bh0

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3984656751/?eBP=BUDGET_EXHAUSTED_JOB&refId=MfSM1HCvvgxAKC3gEIYiSQ%3D%3D&trackingId=EfbwyxFMLyRa3XYcJcxscg%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Analyst/Engineer - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Filadelfia, PA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this Job market also, our candidates can achieve multiple job offers and $100k + salaries.

please check the below links to see the success outcomes and salaries of our candidates .

We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)(phone number removed)di

All Positions are open for all visas and US citizens

We at Synergistic understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and are looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernetes, and REST API experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who match client requirements.

No phone calls, please. Shortlisted candidates would be reached out. No third-party or agency candidates or C2C candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3981370056/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=n7kIqQ9xCX6%2FxjRwkwdGrQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"65 US$/h - 75 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,Estados Unidos,"Acerca del empleo
As a Contract Ads Data Engineer you will:
Design, build, and ship monitoring and alerting infrastructure to ensure reliable functioning of our ads-related data systems
Work closely with Ads Engineers to build pipelines that feed data into and ingest data from the systems they develop
Work with data scientists and product owners to co-design and implement data models and metrics and see that data through to effective analytics reporting
Produce and champion high code quality and development standards
Collaborate with a geographically distributed, multidisciplinary team

Professional Profile:
Excellent written and verbal communication skills in English
Advanced coding skills in SQL (BigQuery) and Python
A history of building and operating high volume, low latency data pipelines
7+ years experience as a software engineer or related role, with at least 3 of those years specifically working as a data engineer
Capacity to grasp complexity quickly and to focus on key issues with minimal direction or support documentation
You lead with empathy. You value teamwork and teammates. You are invested in knowledge sharing and learning from others. You contribute positively and meaningfully to cultivate an inclusive and equitable team culture.

Desirable Skills (not all required but a strong candidate will have at least some of the attributed from this list):
Experience with Google Cloud Platform (GCP), Airflow, and Looker
Direct experience working with ads-related data
Familiarity with structured approaches to data modeling and metric design
Experience working with data streaming technologies such as Apache’s Spark Streaming and/or Beam
Experience as a Data Scientist and/or Data Analyst","Airflow, Apache Spark, Google BigQuery, Google Cloud , Ingeniería de datos , Python y SQL, Beam, Looker (software) y Publicidad",Solicitud sencilla
https://www.linkedin.com/jobs/view/3968180487/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=at2cIeo5%2BLa2ham30PJ%2FpA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 días,Estados Unidos,"Acerca del empleo
About Niche

Niche is the leader in school search. Our mission is to make researching and enrolling in schools easy, transparent, and free. With in-depth profiles on every school and college in America, 140 million reviews and ratings, and powerful search tools, we help millions of people find the right school for them. We also help thousands of schools recruit more best-fit students, by highlighting what makes them great and making it easier to visit and apply.

Niche is all about finding where you belong, and that mission inspires how we operate every day. We want Niche to be a place where people truly enjoy working and can thrive professionally.

About The Role

We are looking for a skilled Data Engineer to join the Data Engineering team. You’ll build and support data pipelines that can handle the volume and complexity of data while ensuring scale, data accuracy, availability, observability, security, and optimum performance. You’ll be developing and maintaining data warehouse tables, views, and models, for consumption by analysts and downstream applications. This is an exciting opportunity to join our team as we’re building the next generation of our data platform, and engineering capabilities. You’ll be reporting to the Head of Data Engineering.

What You Will Do

During the First Month:

Learn about Niche by meeting with various team members to learn more about our company through our Onboarding meetings
Build relationships with data engineering team members, understand the day to day operating model, and stakeholders that we interact with on a daily basis
Start to learn about our data platform infrastructure, data pipelines, source systems, and inter-dependencies
Start participating in standups, planning, and retrospective meetings

Within 3 Months:

Participate in periodic data engineering activities, e.g. monthly insights reporting, profile data updates, etc
Start delivering on assigned data engineering tasks to support our day to day, and roadmap
Start troubleshooting production issues, and participating in on-call activities
Identify areas for improving data engineering processes, and share with the team

Within 6 Months:

Contribute consistently towards building our data platform, which includes data pipelines, and data warehouse layers
Start to independently own workstreams whether it is periodic data engineering activities, or work items in support of our roadmap
Deepen your understanding, and build subject matter expertise of our data & ecosystem

Within 12 Months:

Your contributions have led to us making significant progress in implementing the data platform strategy, and key data initiatives to support the company's growth
You’ve established yourself as a key team member with subject matter expertise within data engineering

What We Are Looking For

Bachelor’s degree in Computer Science, Data Science, Information Systems, or a related field.
3-5 years of experience in data engineering.
Demonstrated experience of building, and supporting large scale data pipelines - streaming and batch processing.
Software engineering mindset, leading with the principles of source control, infrastructure as code, testing, modularity, automation, CI/CD, and observability.
Proficiency in Python, SQL, Snowflake, Postgres, DBT, Airflow, Docker, Kubernetes, Kafka.
Experience of working with Google Analytics, Marketing, Ad & Social media platform, CRM/Salesforce, and JSON data; Government datasets, and geo-spatial data will be a plus.
Knowledge and understanding of the modern data platform, and its key components - ingestion, transformation, curation, quality, governance, and delivery.
Knowledge of data modeling techniques (3NF, Dimensional, Vault).
Self-starter, analytical problem solver, highly attentive to detail, effective communicator, and obsessed with good documentation.

Are you excited about the position and its responsibilities, but hesitant because your experience doesn't align 100% with the posted requirements? We believe you are more than a resume, so go for it! You won’t want to miss the opportunity to play a part in helping students find where they belong.

Compensation

Our national target base salary range is $95,000-$119,000 plus participation in our Annual Bonus and Stock Option Program. Base compensation will be commensurate with experience and skills.

At Niche, our Total Rewards Philosophy is centered around creating a workplace environment that attracts, motivates, and retains top talent by providing a comprehensive and competitive rewards package. This philosophy is built on the principles of performance-based compensation, best-in-class benefits and work-life balance, and employee well-being.

Why Niche?

We are a fully flexible workforce empowering our employees to choose to work remotely, in our Pittsburgh office or whatever combination suits you
Full time, salaried position with competitive compensation in a fast-growing company
Best-in-class 100% paid employee health plan, including vision and dental and supplemental coverage
Flexible Paid Time Off Policy
Stipend that allows you to build your work from home office in a style and function that suits your personal preferences
Parental leave for all employees (12 weeks fully paid) in addition to short term disability for birthing parents
Meaningful 401(k) with employer match
Your ideas and work will make an immediate impact on our company and millions of users
You will join a team that cares about you, our mission, our work - and celebrates our wins together!

Niche will only employ those who are legally authorized to work in the United States without sponsorship now or in the future for this opening.

We are currently hiring in states where we currently have employees: AZ, CA, CT, FL, GA, IL, IN, KY, LA, ME, MD, MA, MI, MO, NE, NH, NJ, NY, NC, OH, OK, OR, PA, SC, TN, TX, VA, WA, DC, WV.

Candidates only. No recruiters or agencies, please. Sorry, we do not offer relocation assistance.

Niche is an equal opportunity employer committed to fostering an inclusive, innovative environment with the best employees. Therefore, we provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, or any other protected status in accordance with applicable law.

All interviews are being held remotely. If there are preparations we can make to help ensure you have a comfortable and positive interview experience, please let us know.","Airflow, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Comunicación, Modelado de datos, Procesamiento por lotes, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3981837814/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=5OzrxxaLluYY6PAKztvBOg%3D%3D&trk=flagship3_search_srp_jobs,Data analytics Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Eden Prairie, MN","Acerca del empleo
Job Description & Skill Requirement:

1. Automotive Engineer with knowledge of vehicle failures & failure chain reactions.
2. Domain knowledge about the RV products and modes of failures.
3. Knowledge of FMVSS safety standards for RVs.
4. Possess beginner to intermediate level understanding of Data Analytics tools and methods like Databricks, Jupiter notebook, Machine learning algorithms, Machine learning process.
4. Possess beginner to intermediate level understanding of Business Intelligence tools and data engineering terms like data normalization, data integration, SQL, Power BI, Power Automate.

Qualification:
BE Mechanical","Almacenamiento de datos, Analítica de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3953815766/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=3X6Xt2GOm96ATRCPlRUC6Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,Estados Unidos,"Acerca del empleo
Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.

We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:

Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
Defining streaming event data feeds required for real-time analytics and reporting
Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance

As a Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product.

Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!

Responsibilities

Build our next generation data warehouse
Build our event stream platform
Translate user requirements for reporting and analysis into actionable deliverables
Enhance automation, operation, and expansion of real-time and batch data environment
Manage numerous projects in an ever-changing work environment
Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
Build processes for topnotch security, performance, reliability, and accuracy
Provide mentorship and collaborate with fellow team members

Qualifications:

Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
3+ years of experience building data pipelines
3+ years of experience building data frameworks for unit testing, data lineage tracking, and automation
Fluency in Scala is required
Working knowledge of Apache Spark
Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)

Nice-to-Haves:

Experience with Machine Learning
Familiarity with Looker a plus
Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)

Please note: This position can be remote/telecommute. Notice for candidates located in the following states: CA, CO, NJ, NY, WA: The base salary range for this position is between $110,000 - $130,000 (salary is dependent on location, experience, knowledge, and skills based on the responsibilities outlined in the job description).

PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners.

Diversity, Equity And Inclusion Program/Affirmative Action Plan

We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.

Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.

As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.

The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers.

Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy.

PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.","Almacenamiento de datos, Apache Spark, Ciencia de datos, Ingeniería de datos y Scala, Bases de datos, Ciencias de la computación, Fuentes de datos, Modelado de datos y Requisitos del usuario",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982285435/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=E%2BRxNjNiyVfyEQIyEJDdWA%3D%3D&trk=flagship3_search_srp_jobs,Staff Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,Estados Unidos,"Acerca del empleo
Introduction to Demandbase: 
Demandbase helps B2B companies hit their revenue goals using fewer resources. How? By using the power of AI to identify and engage the accounts and buying groups most likely to purchase. Our account-based technology unites sales and marketing teams around insights that you can understand and facilitates quick actions across systems and channels to deliver big wins. It’s flexible, scalable ABM built for you.


As a company, we prioritize both the advancement of careers and the development of world-class technology. We invest heavily in people, our culture, and the communities around us. We have offices strategically located in San Francisco and New York in the US, and Hyderabad, in India and we embrace a hybrid work model in these regions. Outside of these areas we offer a remote work option and boast a significant presence in Austin, TX, Atlanta, GA, and London, UK. Continuously lauded as a great place to work, we are Great Place to Work Certified, and have earned distinctions such as ""Fortune's Best Workplaces in the Bay Area,""Best Workplaces in Technology,"" ""Best Workplaces for Millennials,"" and ""Best Workplaces for Parents""

!
We're committed to attracting, developing, retaining, and promoting a diverse workforce. By ensuring that every Demandbase employee is able to bring a diversity of talents to work, we're increasingly capable of achieving our mission to transform the way B2B companies go to market. We encourage people from historically underrepresented backgrounds and all walks of life to apply. Come grow with us at Demandbas

e!
About the Ro
le:In this role, you will improve the core pipelines that power our Identification product as well as design new processes that enable our data science team to test and deploy new ML/AI models. The product delivered by this team is integrated into the core product stack and is a critical component of Demandbase’s account intelligence platform. If you are an experienced engineer that would like to advance your data science background and have a passion for data, we have the position for y

ou!
The compensation range for this role is: $200,000 - $240

,000
What you'll be d
oing:Lead initiatives to build, expand, and improve real-world entity identification dat
asetsCoordinate with downstream stakeholders with dependencies on identification dat
asetsDesign and build new pipelines to increase identification coverage and detect e
rrorsCollaborate with a skilled data science team to enable new ML/AI model develo
pmentProvide insights into optimizing existing pipelines for performance and cost-effic
iencyWriting descriptive plans for new feature implementation (UML diagrams and technical design docum
ents)What we're looking
 for:Bachelor’s degree in computer science, engineering, mathematics, or related 
fieldProgressive experience in all of the following a
reas:Object oriented programming in Scala, Java, or P
ythonProductionizing and deploying Spark pipe
linesComple
x SQLApache Airflow or a similar orchestration
 toolStrong SDLC principles (CI/CD, unit testing, Git process, 
etc.)General understanding of AWS services (IAM, EC2
, S3)An interest in data sc
iencePreferred experi
ence:Ideal candidates will have a background in ad-tech industry j
argonExperience modeling and working with graph-based dat

asets
Ben
efits:Our benefits include options for up to 100% paid Medical and Vision premiums for employees, a flexible PTO policy, no internal meetings Fridays, as well as access to Modern Health and other mental wellness resources. Additionally, we offer eight paid holidays and two additional week-long breaks when all Demandbase employees in the US take time off simultaneously (the week of July 4th and the week of Thanksgiving). We also provide 401(k), short-term/long-term disability, life insurance, and other great ben

efits.
Our Commitment to Diversity, Equity, and Inclusion at Dema
ndbase:At Demandbase, we believe in creating a workplace culture that values and celebrates diversity in all its forms. We recognize that everyone brings unique experiences, perspectives, and identities to the table, and we are committed to building a community where everyone feels valued, respected, and supported. Discrimination of any kind is not tolerated, and we strive to ensure that every individual has an equal opportunity to succeed and grow, regardless of their gender identity, sexual orientation, disability, race, ethnicity, background, marital status, genetic information, education level, veteran status, national origin, or any other protected status. We do not automatically disqualify applicants with criminal records and will consider each applicant on a case-by-case

 basis.
We recognize that not all candidates will have every skill or qualification listed in this job description. If you feel you have the level of experience to be successful in the role, we encourage you t

o apply!
We acknowledge that true diversity and inclusion requires ongoing effort, and we are committed to doing the work required to make our workplace a safe and equitable space for all. Join us in building a community where we can learn from each other, celebrate our differences, and work 

together.
Personal information that you submit will be used by Demandbase for recruiting and other business purposes. Our Privacy Policy explains how we collect and use personal in

formation.","Airflow, Amazon Web Services (AWS), Apache Spark, SQL y Scala",Solicitar
https://www.linkedin.com/jobs/view/3976635654/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=6W4w6g%2FqJpEW8AV25UGsHQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"155 US$K/año - 180 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Nueva York, NY","Acerca del empleo
Who we are at Osmo:

Osmo is a digital olfaction company, on a mission to give computers a sense of smell to improve the health and wellbeing of human life. Why? Our sense of smell both enriches and saves lives, and has a deep and direct connection to our emotions and memory. Vision and hearing have been digitized, but not scent. It's time we solve this. We're bringing an unprecedented combination of software, data, hardware and capital to the historic challenge of giving computers a sense of smell.

We believe in the power of automation and carefully applied AI/ML to solve problems that are beyond what the unaided human mind can tackle on its own. In the first phase of our development, we are using our map of odor along with cutting-edge generative AI to create the next generation of aroma molecules (e.g., the ingredients in the fragrances we wear and the products we use). Beyond fragrance machine olfaction has application across many industries, including manufacturing, military, medicine, and more. Working for Osmo means solving problems using digital olfaction for these customers. We're a hybrid company with a laboratory and center-of-mass in New York, NY and offices in Somerville, MA.

Your Role

Osmo is hiring a Data Engineer, to work on the Data Platform that underpins Osmo's scientific and technical initiatives to digitalize smell.

What you will be doing:

Organizing and instrumenting a world-class data platform to support data collection, development of ML models, and chemical discovery over billions of compounds
Designing schemas and systems that can scale with our data collection needs.
Implement processes to monitor data quality, ensuring collected data is accurate and available for key business processes that depend on it
Writing unit/integration tests and contributing to our engineering wiki

What we need to see:

3+ years of overall technology experience that includes at least 2+ years of hands-on software development, data engineering, and systems architecture
Proven track record of building systems that effectively manage data throughout its entire lifecycle
Strong ability to gather requirements and work across multi-discipline teams
Interested in learning about molecular chemistry and the biology of olfaction.
Willingness to work in our office in New York, NY office 2-5 days a week.
US work authorization

Ways to stand out from the crowd:

Prior Experience working with Google Cloud Platform tools like Dataflow, BigQuery, and Bigtable
Past experience working with Chemistry datasets and library such as rdkit
Basic familiarity with Chemoinfomatics

What we offer:

Equity in Osmo
401k match
Medical, Dental & Vision Insurance
WFH Stipend
Flexible time off and working hours

Expected Salary: $155,000 - $180,000/year



If this role inspires you we'd encourage you to apply. We are committed to recruiting, developing, and retaining an incredible team optimized for a diversity of thought, background, and approaches. All employment decisions and responsibilities are determined based on current ability and your ability to grow, without regard to race, color, gender identity, sex, sexual orientation, religion, age, marital status, physical, mental, or sensory disability, or any other characteristic protected by applicable law.

Recruitment & Staffing Agencies: Osmo does not accept unsolicited resumes from any source other than candidates. The submission of unsolicited resumes by recruitment or staffing agencies to Osmo or its employees is strictly prohibited unless contacted directly by the Osmo Talent Acquisition team. Any resume submitted by an agency in the absence of a signed agreement will automatically become the property of Osmo and Osmo will not owe any referral or other fees with respect thereto.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery y Ingeniería de datos, Arquitectura de sistemas, Bases de datos, Calidad de datos, Datasets y Desarrollo de software",Solicitar
https://www.linkedin.com/jobs/view/3885905325/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=mpOmawkjE%2Bb4qpOFf6bDKQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 3 meses,"Kirkland, WA","Acerca del empleo
Company Description
Castle Design Build specializes in delivering top-tier design and build services. Our commitment to innovation and quality has positioned us as a leader in the industry. 

We seek a Data Engineer/Web Scraping Specialist to join our team. This role involves strategically planning, sourcing, extracting, and analyzing data from various online sources to support decision-making and improve business operations accross a wide range of applications

Responsibilities:
Design, develop, and maintain web crawlers and data extraction pipelines.
Process and analyze structured and unstructured data to generate actionable insights.
Work closely with the development team to integrate data collection systems into our operational framework.

Requirements:
Speed and efficiency accross all digital platforms
Comfortable working with new systems and thinking oustide the box
Proficiency in data engineering, web scraping tools, and methodologies.
Experience with programming languages such as Python, JavaScript, or similar for data processing.
Ability to work with databases and data processing frameworks (e.g., SQL, Pandas).
Strong analytical skills and attention to detail.","Almacenamiento de datos, Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Atención al detalle, Bases de datos, Datos no estructurados, Procesamiento de datos y Web scraping",Solicitud sencilla
https://www.linkedin.com/jobs/view/3978337170/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=UR3ac5iCNObfh833kI%2Ffgg%3D%3D&trk=flagship3_search_srp_jobs,Data Governance Analyst/Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Miami, FL","Acerca del empleo
About Us:As a SR Data Governance Analyst/Engineer at Kenility, you’ll join a tight-knit family of creative developers, engineers, and designers, who strive to develop and deliver the highest quality products into the market. Technical Requirements:

Bachelor's degree in Computer Science, Engineering, or related field.
7-10 years of experience in data governance, data management, or related fields.
Deep knowledge of data architecture principles, data modeling, and database management systems.
Proven experience in successfully implementing enterprise-level data governance programs.
Strong understanding of data privacy regulations and compliance requirements.
Experience using advanced data governance and analytics tools.
Relevant certifications (e.g., CDMP, DGCP, CRISC) are a plus.

Soft Skills:

Responsibility
Proactivity
Flexibility
Great communication skills","Analítica, Arquitectura de datos, Gestión de datos y Gobierno de datos, Bases de datos, Ciencias de la computación, Comunicación, Modelado de datos, Normas de privacidad y Sistema de gestión de bases de datos (SGBD)",Solicitar
https://www.linkedin.com/jobs/view/3965105859/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=sD4IxDAJJpTBRDaRYNPWMw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 3 semanas,"Huston, ID","Acerca del empleo
The data engineer will be responsible for designing, building, and maintaining enterprise data warehouse, ensuring data quality, and supporting data lake and surrounding technology ecosystem, including MDM, ELT , Data Security and Data Catalog.
Domain: Financial Services - Banking
Key responsibilities and expected “output”

• Design, build, and maintain data architectures, pipelines, and ETL processes to ensure efficient and reliable data ingestion, processing, and storage.
• Develop and maintain data models, schema, and metadata to enable effective data analysis.
• Collaborate with data analysts, data scientists, and other stakeholders to understand data requirements and ensure data quality.
• Implement data validation and cleansing processes to ensure data accuracy and consistency as required.
• Monitor and troubleshoot data pipelines to ensure that they are running smoothly and efficiently.
• Keep up to date with the latest data engineering trends and technologies and apply them as appropriate
“Required” tech stack and other related details:
• 10+ years relevant experience
• Strong experience in data modeling, data integration, and data management
• Experience with distributed computing frameworks such as Hadoop and Spark
• Experience with cloud data technologies e.g. GCP, Dataproc, Big query. Airflow , Snowflake , Data Factory
• Strong programming skills in languages such as Python, Java, or Scala
• Strong experience with SQL and database management systems
“Good to have” tech stack and other related details:

• Experience on Git, Azure DevOps

Any other information considered “critical” / “Useful"": (any detail that can be a “game changer” / will make our candidate most preferred”).

• EXPERTISE IN DIMENSIONAL MODELING AT LARGE ENTERPSIES.
• Write complex queries and Stored Procedures
• Build Data pipelines and framework for a scalable solution to support the dynamic cloud environment.","SQL, Enterprise Databases",Solicitud sencilla
https://www.linkedin.com/jobs/view/3975230057/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=zO3dG0YuoR34yvc0kb6FdQ%3D%3D&trk=flagship3_search_srp_jobs,Data Visualization Engineer,"Presencial Media jornada
Coincide con tus preferencias de empleo. El tipo de empleo es Media jornada.
Intermedio",hace 1 semana,"San Antonio, TX","Acerca del empleo
Education: Bachelor'sClearance Required: Public Trust (Full Clearance)

Role Title CDM Data Visualization Engineer RI Start Date for assignment 08/12/2024 End Date for assignment 09/30/2025 # of Resources Needed 1 Hours per Week 40 Job Description This work is Sold Unsold

Specialization : NA

Technical Skills : Skill Years/Level of Experience Microsoft Power BI P4 - Expert Data Visualization P4 - Expert Delivery Excellence P4 - Expert Big Data Architecture P3 - Advanced Data Analytics P4 - ExpertP1 - Beginner (0-2 yrs experience)P2 - Intermediate (3-5 yrs experience)P3 - Advanced (7-10 yrs experienceP4 - Expert (10+ yrs experience)

Role Description :""Lead the effort to gather, analyze and model client data (customers, financials, operational, organizational, access channel), key performance indicators, and/or market data (competitors, products, suppliers), using a broad set of analytical tools and techniques to develop quantitative and qualitative business insights and improve decision-making.

Requirements gathering, Design, development, and implementation of Power BI Reports ad-hoc and long-term reports for a big data application.o Lead and manage architecture solution, support, and development of ad-hoc and long-term Power BI Reports

dditional Preferred Skills:o zure Databricks- Intermediateo Spark- Intermediate

 Microsoft Power BI - Expert2. Data Visualization - Expert3. Delivery Excellence - Expert4. Big Data Architecture - Advanced5. Data Analytics - Expert""","Analítica, Analítica de datos, Arquitectura de datos, Big data, Ciencia de datos, Microsoft Power BI, Minería de datos, Visualización y Visualización de datos, Modelado de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3911002833/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=BOTxkkUsm4qLnp9p7DxESA%3D%3D&trk=flagship3_search_srp_jobs,Data Visualization Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"Boston, MA","Acerca del empleo
Job Title: Data Visualization Engineer

Duration: 2+ Months (11 Weeks)

Location: Fully Remote- ET Hours

 No Sponsorship at this time! 

Job Summary

We are seeking a highly skilled and creative Data Visualization Engineer to join our dynamic team. The ideal candidate will be responsible for designing, developing, and implementing innovative data visualization solutions that effectively communicate complex data insights to stakeholders. As a Data Visualization Engineer, you will collaborate closely with data scientists, analysts, and business stakeholders to understand requirements and translate them into visually appealing and intuitive dashboards, reports, and interactive visualizations.

Responsibilities

 Work closely with cross-functional teams to gather requirements and understand data visualization needs. 
 Design and develop interactive dashboards, reports, and visualizations using tools such as Tableau, Power BI, or similar platforms. 
 Utilize programming languages like Python, R, or JavaScript to create custom visualizations and enhance existing ones. 
 Implement best practices for data visualization and ensure consistency, accuracy, and usability across all visualizations. 
 Optimize performance of visualizations for large datasets and ensure scalability. 
 Stay updated on industry trends and advancements in data visualization techniques and tools. 
 Conduct user testing and gather feedback to continuously improve visualizations and user experience. 
 Provide training and support to end users on navigating and interpreting visualizations effectively. 

Qualifications

 Bachelor's degree in Computer Science, Information Technology, Data Science, or related field; Master's degree preferred. 
 Proven experience (X years) in data visualization, dashboard development, or related roles. 
 Proficiency in data visualization tools such as Tableau, Power BI, or similar platforms. 
 Strong programming skills in languages like Python, R, JavaScript, or similar for data manipulation and visualization. 
 Solid understanding of data modeling, data structures, and database concepts. 
 Experience working with SQL for data extraction, manipulation, and analysis. 
 Excellent communication and collaboration skills with the ability to work effectively in a team environment. 
 Strong attention to detail and ability to prioritize tasks in a fast-paced environment. 
 Experience with version control systems (e.g., Git) and agile development methodologies is a plus. 

Preferred Qualifications

 Certification in data visualization or related field. 
 Experience with advanced analytics and statistical methods. 
 Familiarity with cloud platforms such as AWS, Azure, or Google Cloud. 
 Experience with data preprocessing and cleaning techniques. 
 Knowledge of web development frameworks (e.g., Flask, Django) for building interactive web applications

Aptitudes y experiencia deseables
DATA VISUALIZATION, POWERBI, DASHBOARD, TABLEAU, SQL","Analítica, Analítica de datos, Ciencia de datos, Minería de datos, Python y Visualización de datos, Dashboard Building, Datasets, Manipulación de datos y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3985576995/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=Ixz%2BDqx57Yw0UiyR2tOj7A%3D%3D&trk=flagship3_search_srp_jobs,Associate Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 días,"Worcester, MA","Acerca del empleo
For more than 170 years, The Hanover has been committed to delivering on our promises and being there when it matters the most. We live our values every day, demonstrating we CARE through our values, ESG initiatives and IDE journey.

Our Corporate Actuarial Department is seeking an Associate Data Engineer to join our growing team. This is a full time, hybrid, exempt role. 

As a Data Engineer, you will play a crucial role in designing, implementing, and maintaining our data infrastructure on the Azure Cloud platform. Your primary responsibilities will include developing robust ETL (Extract, Transform, Load) processes, optimizing data workflows, and ensuring the seamless integration of various data sources. Proficiency in SQL, Python is essential, and Spark is desirable, as you will be responsible for crafting efficient data pipelines and leveraging these technologies to extract actionable insights from our vast datasets.

You will collaborate closely with cross-functional teams to understand business requirements, translating them into effective data solutions. Experience with Azure Cloud services, such as Azure Data Factory and Azure Databricks, is a desirable, as you will be responsible for deploying and maintaining data pipelines in a cloud environment.

IN THIS ROLE, YOU WILL:

Design, develop, and maintain ETL processes using SQL, Python, and Spark to ensure efficient and reliable data integration.
Collaborate with data scientists, analysts, and other stakeholders to understand business requirements and implement data solutions that meet those needs.
Implement and optimize data warehousing solutions on Azure Cloud, utilizing services such as Azure SQL Data Warehouse and Azure Synapse Analytics.
Monitor and troubleshoot data pipelines to ensure data accuracy, completeness, and timely delivery.
Stay up-to-date with industry trends and emerging technologies to continuously enhance and improve data engineering processes. 


WHAT YOU NEED TO APPLY:

2 years related experience and a bachelor's degree. 
Degrees and/or related experience in Applied Data Science, Data Science, Information and Data Science, Information Management Analytics, Data Mining, Predictive Analytics are most sought-after. 
Degrees in Statistics, Mathematics, Computer Science, Information Systems are also preferred. 


CAREER DEVELOPMENT:

It's not just a job, it's a career, and we are here to support you every step of the way. We want you to be successful and fulfilled. Through on-the-job experiences, personalized coaching and our robust learning and development programs, we encourage you - at every level - to grow and develop.

BENEFITS:

We offer comprehensive benefits to help you be healthy, build financial security, and balance work and home life. At The Hanover, you'll enjoy what you do and have the support you need to succeed.

Benefits include:

Medical, dental, vision, life, and disability insurance 
401K with a company match 
Tuition reimbursement 
PTO 
Company paid holidays 
Flexible work arrangements 
Cultural Awareness Day in support of IDE 
On-site medical/wellness center (Worcester only) 
Click here for the full list of Benefits


EEO statement:

The Hanover values diversity in the workplace and among our customers. The company provides equal opportunity for employment and promotion to all qualified employees and applicants on the basis of experience, training, education, and ability to do the available work without regard to race, religion, color, age, sex/gender, sexual orientation, national origin, gender identity, disability, marital status, veteran status, genetic information, ancestry or any other status protected by law.

Furthermore, The Hanover Insurance Group is committed to providing an equal opportunity workplace that is free of discrimination and harassment based on national origin, race, color, religion, gender, ancestry, age, sexual orientation, gender identity, disability, marital status, veteran status, genetic information or any other status protected by law.""

As an equal opportunity employer, Hanover does not discriminate against qualified individuals with disabilities. Individuals with disabilities who wish to request a reasonable accommodation to participate in the job application or interview process, or to perform essential job functions, should contact us at: HRServices@hanover.com and include the link of the job posting in which you are interested.

Privacy Policy:

To view our privacy policy and online privacy statement, click here.

Applicants who are California residents: To see the types of information we may collect from applicants and employees and how we use it, please click here

Other Details

 Pay Type Salary


Apply Now","Almacenamiento de datos, Analítica, Analítica de datos, Análisis predictivo, Apache Spark, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3984132996/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=fBGWuaAELzSBKy5%2FaXgNLA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Irving, TX","Acerca del empleo
Job Description

 Data Engineer 

 Toronto, Canada (Local) 

 Long Term 

 Key: Python and Rust, PostgreSQL, MySQL, Apache Spark, DuckDb, Clickhouse, Hadoop, Snowflake, AWS Redshift, Google BigQuery 

Responsibilities

 Build and optimize our data pipelines and architectures for ingesting, processing, and storing large volumes of structured and unstructured data from Github.
 Develop and maintain our robust ETL (Extract, Transform, Load) processes to ensure data integrity and consistency.
 Collaborate with cross-functional teams to understand data requirements and translate them into efficient data models and schemas.
 Maintaining our data warehousing solutions, utilizing both traditional and emerging database technologies (e.g., Postgres & Clickhouse).
 Develop and maintain data processing workflows using programming languages such as Python and Rust.
 Continuously monitor and optimize data systems for performance, scalability, and reliability.
 Contribute to the development of data quality assurance processes and tools.
 Requirements
 4
years of experience as a Data Engineer or in a similar role, working with large-scale data pipelines and architectures.
 Bachelor s degree in Computer Science, or a related field
 Strong proficiency in Python and Rust.
 Extensive experience with relational databases (e.g., PostgreSQL, MySQL)
 Familiarity with emerging and big data technologies and frameworks (e.g., Apache Spark, DuckDb, Clickhouse, Hadoop).
 Experience with data warehousing solutions (e.g., Snowflake, AWS Redshift, Google BigQuery).
 Knowledge of data modeling techniques and best practices.
 Strong problem-solving and analytical skills.
 Excellent communication and collaboration abilities.
 Familiarity with Docker and Kubernetes.
 We are even more interested in chatting if you...
 Knowledge of cryptocurrency, blockchain technology, and the crypto ecosystem (e.g., DeFi, NFTs, L1s, L2s).
 Have experience in web scraping (puppeteer, selenium).
 Have experience with React and front-end development.
 Hands-on experience with cryptocurrency transactions and interacting with blockchain networks.","Ingeniería de datos, Aseguramiento de la calidad de los datos, Calidad de datos, Comunicación, Desarrollo front end, Modelado de datos, Modelo de datos, React.js, Resolución de problemas y Rust (lenguaje de programación)",Solicitar
https://www.linkedin.com/jobs/view/3983712251/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=R9UBuGJE3AXcNpTxpnILpw%3D%3D&trk=flagship3_search_srp_jobs,Data Analytics Engineer I - Performance Excellence,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 4 días,"Irving, TX","Acerca del empleo
Description

Summary:

The Data Analytics Engineer I is responsible for developing and maintaining data sets and visualizations. This role uses programming skills to build, customize, and manage integration tools, databases, and analytical systems.

The Data Analytics Engineer I assists in designing and implementing solutions to integrate, process, and analyze large data sets, requiring knowledge of data and systems design, development, and analytics.

Responsibilities

Meets expectations of the applicable OneCHRISTUS Competencies: Leader of Self, Leader of Others, or Leader of Leaders.
Assist in analyzing large data sets to identify trends and insights.
Collaborate with other team members and engineers to design and implement data processing and integration solutions.
Recommend data handling and processing improvements based on the latest technological advancements.
Participate in projects that involve sorting, analyzing, processing, and integrating large amounts of data.
Develop visualizations using various technologies to represent data findings effectively.
Collaborate by developing software solutions in languages like Java, Python, C, HTML, and SQL.
Participate in testing software solutions to ensure they meet functional requirements.
Develop and maintain code bases across multiple programming languages, ensuring that programming tasks are completed effectively and efficiently.
Responsible for documenting all development and data processing phases for future reference.
Continuously learn and apply best practices in data analytics and software development.

Requirements

Bachelor’s degree in Computer Science, Engineering, Math, or a related field is preferred.
0 - 1 years of analytics solutions or Healthcare IT experience preferred.
Microsoft SQL Server environment experience is preferred.
Certifications in Hadoop or Java are preferred.

Work Type

Full Time

EEO is the law - click below for more information: 

https://www.eeoc.gov/sites/default/files/2023-06/22-088_EEOC_KnowYourRights6.12ScreenRdr.pdf

We endeavor to make this site accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact us at (844) 257-6925.","Analítica de datos, Ciencia de datos y Ingeniería de datos, Bases de datos, Documentación, HTML, Procesamiento de datos, Pruebas de software, Requisitos funcionales y Soluciones de software",Solicitar
https://www.linkedin.com/jobs/view/3977204531/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=c4pbiHf3DbCFC%2BoDUBzgOg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Quincy, MA","Acerca del empleo
Role: Data Engineer

 Location: Quincy, MA

 Role type: Contract

Job Description

 Senior/Lead developer position who has extensive experience with development and designing of bigdata platform with Hadoop, Scala and Python experience.
 The candidate should have extensive experience in the bigdata developer with data engineering/big data developer.
 The candidate should have good experience in Scala & Python programming language for data bricks development
 Experience on Databricks
 Candidate should have very good experience in the Dataware housing (ETL Concepts, Types of SCD)
 Candidate should have very good experience in the data lake and delta tables concepts.
 Candidate should have experience in databases like SQL server/Oracle
 Willing to work out of Quincy, MA

 Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Aptitudes y experiencia deseables
BIGDATA, HADOOP, SCALA, PYTHON, DATABRICKS, ETL, DATAWARE, SQL, ORACLE","Analítica, Big data, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python, SQL y Scala, Azure Databricks, Bases de datos y Lagos de datos",Solicitar
https://www.linkedin.com/jobs/view/3982961774/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=mc9JGoCyBMzt0eKVpe3NcQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,"Birmingham, AL","Acerca del empleo
Location: Birmingham, AL
** Only open to US Citizens and Green Card holders

Our enterprise client is seeking a Data Engineer to join their dynamic team. The selected candidate will be instrumental in enhancing data-driven decision-making and operational efficiency across various business units. This role involves close collaboration with data scientists and operating within a cloud environment, requiring proficiency in data engineering, cloud technologies, and team-based problem-solving. If you are enthusiastic about using data to drive business insights and innovation, and have expertise in Power BI visualization and data cataloging, this opportunity is for you!

What you’ll be doing: 
Data Modeling and Architecture:

Develop and implement data models and architectures optimized for data analytics, ensuring efficiency, scalability, and data integrity.
Implement best practices for data storage, partitioning, and indexing to optimize performance and facilitate analysis.

Data Pipeline Development:
Assist in designing, building, and maintaining scalable data pipelines to ingest, transform, and store large volumes of data from various sources.
Collaborate with data scientists to ensure seamless integration of data pipelines with analytical models and AI processes.

Data Cataloging and Documentation:
Catalog and document data sources, datasets, and metadata to facilitate data discovery, lineage, and governance.
Implement data cataloging best practices to ensure the availability and accessibility of data assets.

Cloud Environment Management:
Work within cloud environments - Azure to deploy, manage, and optimize data engineering solutions.
Collaborate with cloud architects and administrators to ensure security, compliance, and cost-effectiveness of cloud infrastructure.

Visualization and Reporting:
Develop and maintain Power BI dashboards and reports to visualize data insights and facilitate data-driven decision-making.
Create custom UI/UX applications
Ensure the accuracy, reliability, and usability of visualizations to meet business requirements.

Collaboration and Support:
Collaborate with cross-functional teams to understand data requirements and support analytical initiatives.
Provide technical support and troubleshooting for data-related issues, ensuring the reliability and availability of data infrastructure.

What you need to have: 
Bachelor’s degree in Computer Science, Information Technology, or related field.
3+ years of experience in data engineering or related roles, preferably in a cloud environment.
Proficiency in programming languages such as Python, SQL, and Scala.
Proficiency with Power BI for data visualization and reporting.
Experience with cloud-based data platforms (e.g., Azure Databricks, AWS EMR).
Strong understanding of data modeling, ETL processes, and data warehousing concepts.
Familiarity with big data technologies
Innovative and adaptable, with a passion for continuous learning and improvement.
Strong communication and collaboration skills, with the ability to work effectively across teams.
Results-oriented and committed to delivering high-quality solutions that meet business needs.
Ethical conduct and commitment to safety in all aspects of work.","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Microsoft Power BI, Python, SQL y Scala, Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3980914333/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=Il%2BNPfHwCi0XC0%2F6XJ290Q%3D%3D&trk=flagship3_search_srp_jobs,Azure Databricks - Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Raleigh, NC","Acerca del empleo
Job Description

Infosys is seeking an Azure Databricks - Data Engineer. In this role, you will provide technical guidance to teams; anchor Proof of Concept developments and support opportunity identification and pursuit processes and evangelize Infosys brand. You will collaborate with some of the best talent in the industry to create and implement innovative high-quality solutions. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.

Required Qualifications

 Candidate must be located within commuting distance of Richardson (TX) or Raleigh, NC or be willing to relocate to the area 
 Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education. 
 At least 4 years of experience in Information Technology. 
 At least 3 years of hands-on experience in using Azure Data Factory (ADF) to orchestrate data ingestion from various sources into Azure Synapse and Azure Data Lake Storage (ADLS). 
 At least 2 years of hands-on experience with Azure, databricks and Hadoop distributed frameworks while handling large amount of big data using Spark and Hadoop Ecosystems. 
 At least 2 years of experience designing and implementing data pipelines using Azure Databricks for data cleaning, transformation, and loading into Azure Synapse Analytics 
 Hands on experience in end-to-end implementation of data warehouse and data marts 
 Experience in writing SQL queries to analysis Type-2 dimension data 
 Understanding of implementing Medallion Architecture using Azure Services 
 U.S. citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time. 

Preferred Qualifications

 Experience in software engineering and data engineering roles, with a focus on Azure and Databricks. 
 Strong understanding of databases and big data software technologies, specifically Azure and Databricks. 
 Exposure to Unity Catalogue 
 Experience with Agile methodologies, particularly Scrum. 
 Experience in coordinating with offshore data engineering teams 
 Experience with scripting languages like Python, Java, etc. 
 Strong problem-solving skills related to data, data structures, and algorithms. 

About Us

Infosys is a global leader in next-generation digital services and consulting. We enable clients in 50 countries to navigate their digital transformation. With over three decades of experience in leading the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver outstanding levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.

Infosys is an equal opportunity employer and all qualified applicants will receive consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, spouse of protected veteran, or disability.

To learn more about Infosys and see our perspectives in action please visit us at www.Infosys.com","Almacenamiento de datos, Apache Spark, Azure Data Factory, Azure Data Lake, Data Marts, Hadoop y Ingeniería de datos, Azure Databricks, Limpieza de datos y Microsoft Azure",Solicitar
https://www.linkedin.com/jobs/view/3833866561/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=GuXT0mHfiMBY1Mal6bIgOQ%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Programmer/Coder/Developer/Data Scientist/Analyst/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Atlanta, GA","Acerca del empleo
SYNERGISTICIT is aware that the Job Market is Challenging due to almost 300,000 Tech Layoffs within the past year due to which The Job market is flooded with thousands of laid off Techies who are competing with existing Jobseekers. For entry level Job seekers to get client interviews and jobs they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.

Since 2010 we have helped Jobseekers differentiate themselves by providing the clients with candidates who have the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers. All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this economy no client wants or has the resources to take an entry level person and spend resources on upgrading their skills and on top of that pay the jobseeker. That's the specific reason there are so many techies both experience and freshers who are unemployed.

Clients have now the option to hire remote workers from anywhere so for a Jobseeker its important to introspect and see how they can become better and have the skills and technologies to meet client requirements. We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped thousands of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few. We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients. Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates looking to make their careers in IT Industry We welcome candidates with all visas and citizens to apply. We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then you should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

Candidates who Lack Experience or are freshers with No actual on Job experience with projects with clients Have had a break in careers Lack Technical Competency or skills being demanded by clients Different visa candidates (Like OPT/H4EAD/L2EAD )who want to get employed and settle down in the USA please check the below links to see success outcomes of our candidates https://www.synergisticit.com/candidate-outcomes/

We are also silver sponsors at Oracle Cloudworld , Las vegas from sept 18-21st— please visit us

https://www.oracle.com/cloudworld/sponsor-listing/#synergistic-it

Watch the below videos of us participating at Industry events with the Top companies in Technology at Oracle Cloud world /Oracle Java one (Las vegas) and at Gartner Data Analytics Summit (Florida)

Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube https://www.youtube.com/watch?v=OAFOhcGy9Z8

https://www.youtube.com/watch?v=EmO7NrWHkLM https://www.youtube.com/watch?v=NVBU9RYZ6UI

https://www.youtube.com/watch?v=Yy74yvjatVg SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube

If you have relevant skills and industry experience, please apply

For preparing for interviews please visit

https://www.synergisticit.com/interview-questions/

REQUIRED SKILLS For Java /Software Programmers

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Project work on the skills
 Knowledge of Core Java , javascript , C++ or software programming
 Spring boot, Microservices, Docker, Jenkins and REST API's experience
 Excellent written and verbal communication skills

For data Science/Machine learning

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Project work on the technologies needed
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
 Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis

Please understand skills and relevant experience on real world projects are required by clients for selection even if its Junior or entry level position the additional skills and Project work with hands on experience building projects at client site are the only way a candidate can be picked by clients. If not having the skills or hands on project work at client site then candidates can optionally opt for skill enhancement to gain the required skills and project work. No third party candidates or c2c candidates

please only apply to the posting

No phone calls please. Shortlisted candidates would be reached out.","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Programación, Ciencias de la computación, Comunicación, Cracking, Desarrollo de software y Lista de preseleccionados",Solicitar
https://www.linkedin.com/jobs/view/3980915029/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=yQXygwoncSjh8bbmf%2BmTMw%3D%3D&trk=flagship3_search_srp_jobs,Azure and Snowflake Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Richardson, TX","Acerca del empleo
Job Description

Infosys is seeking an Azure and Snowflake Data Engineer. In this role, you will enable digital transformation for our clients in a global delivery model, research on technologies independently, recommend appropriate solutions and contribute to technology-specific best practices and standards. You will be responsible to interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.

Required Qualifications

 Candidate must be located within commuting distance of Richardson, TX or be willing to relocate to the area. This position may require travel in the US 
 Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education. 
 U.S. citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time. 
 At least 4 years of Information Technology experience 
 At least 2 years of Azure experience 
 At least 2 years of Snowflake experience 
 Strong knowledge and hands-on experience in SQL, Unix shell scripting, and Python 

Preferred Qualifications

 Experience and desire to work in a global delivery environment 
 Experience in Banking/Financial domain 
 Sound knowledge of software engineering design patterns and practices 
 Strong knowledge Data structures, Data Engineering concepts, Algorithms, Collections, Multi-threading and memory management and concurrency 
 Experience in large scale cloud data migrations using technologies such as Snowflake, Python, Spark, Databricks 
 Good understanding of Agile software development frameworks 
 Strong communication and Analytical skills 
 Good to have experience AWS Databricks 
 Ability to work in teams in a diverse, multi-stakeholder environment comprising of Business and Technology teams 

The job entails sitting as well as working at a computer for extended periods of time. Should be able to communicate by telephone, email or face to face. Travel may be required as per the job requirements.

About Us

Infosys is a global leader in next-generation digital services and consulting. We enable clients in more than 50 countries to navigate their digital transformation. With over four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.

Infosys provides equal employment opportunities to applicants and employees without regard to race; color; sex; gender identity; sexual orientation; religious practices and observances; national origin; pregnancy, childbirth, or related medical conditions; status as a protected veteran or spouse/family member of a protected veteran; or disability.","Apache Spark, Ingeniería de datos , Python y SQL, Azure Databricks, Comunicación, Guiones shell, Microsoft Azure, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3980763725/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=er%2FY51G25gGVdSu8bHyugA%3D%3D&trk=flagship3_search_srp_jobs,Maximo Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,Estados Unidos,"Acerca del empleo
We are seeking a Data Engineer with Maximo experience for a contract position with our client. This position is fully remote from the US and requires working EST hours.

Job Description
Accountable for developing and delivering technological responses to targeted business outcomes. 
Analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise. Understand and follow reusable standards, design patterns, guidelines, and configurations to deliver valuable data and information across the enterprise, including direct collaboration, where needed.
Working knowledge of databases and SQL Experience with software development methodologies and SDLC Candidate possess a problem-solving attitude and can work independently
Must be very organized, able to balance multiple priorities, and self-motivated
Complex Project delivery via multiple teams with inter-dependency of work products.
Leadership of multiple team projects.
Experience with multiple vendor teams with differing project management approaches and resource allocation.
Direct experience working with Product Managers, PMO, and Stakeholders to provide project status and to translate leadership directives into actionable tasks.
Leadership of database centric conversion projects.
Leadership or design experience in Application System conversions/migrations.
Experience with system integrations as well as bulk data movement design patterns.
Depth of knowledge in areas enabling the ability to mentor and assist developers.
System Architecture design and analysis.
Data Architecture design and analysis.
Experience with Enterprise Asset Management systems.
Relevant functional experience (Manufacturing, Logistics, Retail – Grocer)
Collaborate with team members to improve the company’s engineering tools, systems and procedures, and data security.
Deliver quality customer service and resolve end-user issues in a timely manner
Draft architectural diagrams, interface specifications and other design documents
Participate in the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy
Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio
Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses
Troubleshoot production issues and coordinate with the development team to streamline code deployment.
 Key Responsibilities:
Data Analysis, Systems Analysis, Business Process Analysis
Data “Wrangling” – Data validation techniques, data cleansing methodology
Tools:
Advanced Excel
SQL
Relevant Scripting languages
Communication:
Meeting Leadership (Peer and Leadership)
Written (Status, Designs, etc.)
Project Management Methodology:
Agile – Kanban, Scrum","Analítica de datos y SQL, Análisis de procesos empresariales, Asignación de recursos, Bases de datos, Conversión de datos, Metodologías de desarrollo de software, Resolución de problemas, Sistemas integrados y Validación de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3968544903/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=4VrDNoxj1rl%2FdJjpkez4FQ%3D%3D&trk=flagship3_search_srp_jobs,Remote Data Scientist/Engineer - Entry Level,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Houston, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this Job market also, our candidates can achieve multiple job offers and $100k + salaries.

please check the below links to see the success outcomes and salaries of our candidates.

We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

(url removed)(phone number removed)di

All Positions are open for all visas and US citizens

We at Synergistic understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like Apple, google, Paypal, Western Union, Client, visa, Walmart labs etc to name a few.

Currently, We are looking for entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and are looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernetes, and REST API experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who match client requirements.

No phone calls, please. Shortlisted candidates would be reached out. No third-party or agency candidates or C2C candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3833864752/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=krI%2FGtS9%2F%2FeulQHJYXQoQw%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Programmer/Coder/Developer/Data Scientist/Analyst/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"San José, CA","Acerca del empleo
At SynergisticIT, we aim to bring aboard IT professionals to help them build a rewarding career in cutting-edge technologies. Being in the industry for more than 10 years, we provide a splendid range of lucrative opportunities to sustain a position in our top tech clients like Google, Apple, Cognizant, Client, PayPal, to name a few.

Our seasoned team firmly believes that the new tech talent can scale any business if given the right opportunity. We value your integrity, hard work, and commitment to make a difference in the technical sphere. For this reason, we focus on providing end-to-end career assistance and enhancing your already existing IT skills and knowledge.

Currently, we are looking for qualified entry-level Data Scientists who can apply Data Science principles to design, test, implement, and develop data-based solutions, including reporting, auditing, and preparing large databases for statistical analysis.

Qualifications

 Minimum Background and Qualifications Requirement 

 Bachelor's degree or Master's degree in Computer Engineering, Computer Science, Mathematics, Electrical Engineering, Information Systems, or IT 
 Must have Mathematics or Statistics background 

Required

 Technical and Soft Skills Required 

 Experience in Python programming and understanding of the software development life cycle. 
 Knowledge of Linear Algebra, Statistics, and Mathematics concepts. 
 Excellent written and verbal communication skills. 
 Highly motivated, self-learner, team player, and technically inquisitive. 
 Strong work ethics and creative problem-solving abilities. 

 Preferred skills 

 Deep Learning 
 Data visualization 
 NLP 
 Scala 
 Django 

 Roles and Responsibilities 

 Collaborate with dynamic teams of engineers, developers, and scientists who research and integrate algorithms to develop an application, software, and computer system solutions to address complex data problems. 
 Assess project requirements and develop data analysis algorithms. 
 Engage developers to share their opinions, knowledge, and recommendations to meet the deliverables. 
 Contribute to technical solutions and implement software analyses to unlock the secrets held by big data sets. 
 Integrate components like web-based UI, commercial indexing products, and access control mechanisms to create operational information and knowledge discovery systems. 

Benefits

 Competitive salary 
 Flexible work schedule & part-time off 
 E-verified 
 No relocation 
 H1B filing 
 On job technical support 
 Skill Enhancement 
 Opportunity to work with Fortune 500 Companies 

 Who Should Apply? 

Recent IT graduates looking to build a solid career in the tech industry. If you're lured by the endless possibilities presented by AI, Machine Learning, IoT, and Data Science, this job opportunity can be the right career path for you.

 Candidate's Outcome  : Best Programmers in USA | Best Coding Bootcamp - SynergisticIT

 No third-party candidates or c2c candidates 

 If you are interested, please apply to the posting. 

 No phone calls please, Shortlisted candidates would be reached out.","Analítica de datos, Análisis de datos, Programación y Python, Ciencias de la computación, Competencias transversales, Comunicación, Desarrollo de software, Matemáticas y Resolución creativa de problemas",Solicitar
https://www.linkedin.com/jobs/view/3981806746/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=aPGzKwKqqFX3NybGKHnSUA%3D%3D&trk=flagship3_search_srp_jobs,"Data Links & Comm Engineer SME, DoD Contractor","Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 4 días,"Andover, MA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Description

Job Description

MAINSAIL Group is seeking candidates for the role of Data Links & Communication Engineer SME, located at Hanscom AFB, in Bedford, MA.

Desired Qualifications

5 Years experience in software design and software requirements development
Proficiency in C++ an Java Programming languages and development frameworks
Proficiency in MBSE modeling, languages, and tools (DODAF, SysML, UML, Cameo)
Networking and embedded mission systems experience
Familiarity with government acquisition planning, tailoring and documentation (Acquisition Strategy Plan, Statement of Work, System Engineering Plan, User Agreements, Product Roadmap). 

Additional Qualifications

Provide knowledge and experience in the development, integration, testing, and deployment of Air Force systems and their associated data links, to include definition of data link interoperability, interfaces, and verification and test assessment requirements. This includes interfacing with the Federal, Department of Defense (DoD), United States Air Force (USAF), United States Army, and Joint communities to ensure the data link communications capability complies with the applicable Federal Aviation Administration, Defense Information Systems Agency (DISA), National Security Agency (NSA), Service Acquisition Executive, Joint Mission Planning System (JMPS), Damage Assessment and Casualty Report business and technical rules. 
Provide resources with knowledge and experience in Developmental and Operational test and Evaluation at the sub-system and system level. This includes interface with Air Combat Command, Air Force Command and Control Integration Center, Joint Interoperability Test Command, Space and Naval Warfare Systems Command, and all levels of the organizations that develop and manage the C3 and encryption key distribution infrastructure for controlling a networked system, providing technical advice on C4 issues, including pre-launch conditioning and planning, aircraft-weapon pre- and post-release communications, and real-time targeting. 
The Contractor shall advise and assist the program Chief Engineer, Integrated Product Team (IPT) Leads, Program Managers (PMs), and joint combined test team customers on performance, schedule, and cost issues with respect to planning, communications, and control. 
The contractor will perform other duties as assigned. 

The candidate may be called upon for knowledge and experience in the following specific areas:

Combat Net Radio (CNR)
Test and Demonstration: includes engineering support for interoperability testing weapon configurations with Threshold and Objective launch platforms/controllers and ground Joint Terminal Attack Controllers over Ultra-High Frequency (UHF) CNR and Link 16 communications. 
Joint Concept of Employment (CONEMP)
Cryptologic Systems
Link 16 Networking
Interface Control
Digitally Aided Close Air Support (DACAS)
United States Message Text Format (USMTF)
Joint Interoperability of Tactical Command and Control Systems (JINTACCS)

Qualifications/Technical Experience Requirements:

Must be a US citizen
Active Top Secret Security Clearance

Company Description

MAINSAIL Group is located in Boston, MA and Washington, DC. We leverage our extensive operational expertise and technical knowledge to provide best-in-class solutions to our customers. Our specialized services support offices across the Defense Department and Department of Veterans Affairs delivering Program Management, Software Development, Systems Engineering, C4ISR Systems Acquisition, Strategic Planning, Agile methodologies and more.

MAINSAIL Group is an equal opportunity employer and does not discriminate in hiring or employment on the basis of any legally protected characteristic including, but not limited to, race, color, religion, national origin, marital status, gender, sexual orientation, ancestry, age, medical condition, military veteran status or on the basis of physical handicap which, with reasonable accommodation, render the application to satisfactorily perform the job available.

MAINSAIL Group is located in Boston, MA and Washington, DC. We leverage our extensive operational expertise and technical knowledge to provide best-in-class solutions to our customers. Our specialized services support offices across the Defense Department and Department of Veterans Affairs delivering Program Management, Software Development, Systems Engineering, C4ISR Systems Acquisition, Strategic Planning, Agile methodologies and more. MAINSAIL Group is an equal opportunity employer and does not discriminate in hiring or employment on the basis of any legally protected characteristic including, but not limited to, race, color, religion, national origin, marital status, gender, sexual orientation, ancestry, age, medical condition, military veteran status or on the basis of physical handicap which, with reasonable accommodation, render the application to satisfactorily perform the job available.","Ingeniería de datos, Comunicación, Diseño de software, Ingeniería de sistemas basada en modelos (MBSE), Java, Link 16, Operational Test & Evaluation, Requisitos de software, SysML y UHF",Solicitar
https://www.linkedin.com/jobs/view/3974798078/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=3yuEccXbPQXRx%2F%2FGstV%2Bdg%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, Analytics","209,7 US$K/año - 235,4 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 5 días,"Nueva York, NY","Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Meta Platforms, Inc. (f/k/a Facebook, Inc.), is seeking the following. Apply via Dice today!

Meta Platforms, Inc. (f/k/a Facebook, Inc.) has the following position in New York, NY: 

Data Engineer, Analytics: Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making . (ref. code REQ-2406-139559: $209,720/year - $235,400/year).

Individual pay is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base salary only, and do not include bonus or equity or sales incentives, if applicable. In addition to base salary, Meta offers benefits. Learn more about benefits at Meta at this link: https://www.metacareers.com/facebook-life/benefits. For full information & to apply online, visit us at the following website http://www.metacareers.com/jobs & search using the ref code(s) above.","Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Solicitar
https://www.linkedin.com/jobs/view/3833865584/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sB7YETbILs2Gfdx3Ivmgyg%3D%3D&trackingId=E0pqYNuOZC7bjaaBRCQZhQ%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Programmer/Coder/Developer/Data Scientist/Analyst/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Dallas, TX","Acerca del empleo
SYNERGISTICIT is aware that the Job Market is Challenging due to almost 300,000 Tech Layoffs within the past year due to which The Job market is flooded with thousands of laid off Techies who are competing with existing Jobseekers. For entry level Job seekers to get client interviews and jobs they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.

Since 2010 we have helped Jobseekers differentiate themselves by providing the clients with candidates who have the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers. All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this economy no client wants or has the resources to take an entry level person and spend resources on upgrading their skills and on top of that pay the jobseeker. That's the specific reason there are so many techies both experience and freshers who are unemployed.

Clients have now the option to hire remote workers from anywhere so for a Jobseeker its important to introspect and see how they can become better and have the skills and technologies to meet client requirements. We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped thousands of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few. We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients. Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates looking to make their careers in IT Industry We welcome candidates with all visas and citizens to apply. We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then you should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

Candidates who Lack Experience or are freshers with No actual on Job experience with projects with clients Have had a break in careers Lack Technical Competency or skills being demanded by clients Different visa candidates (Like OPT/H4EAD/L2EAD )who want to get employed and settle down in the USA please check the below links to see success outcomes of our candidates https://www.synergisticit.com/candidate-outcomes/

We are also silver sponsors at Oracle Cloudworld , Las vegas from sept 18-21st please visit us

https://www.oracle.com/cloudworld/sponsor-listing/#synergistic-it

Watch the below videos of us participating at Industry events with the Top companies in Technology at Oracle Cloud world /Oracle Java one (Las vegas) and at Gartner Data Analytics Summit (Florida)

Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube https://www.youtube.com/watch?v=OAFOhcGy9Z8

https://www.youtube.com/watch?v=EmO7NrWHkLM https://www.youtube.com/watch?v=NVBU9RYZ6UI

https://www.youtube.com/watch?v=Yy74yvjatVg SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube

If you have relevant skills and industry experience, please apply

For preparing for interviews please visit

https://www.synergisticit.com/interview-questions/

REQUIRED SKILLS For Java /Software Programmers

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Project work on the skills
Knowledge of Core Java , javascript , C+
or software programming
Spring boot, Microservices, Docker, Jenkins and REST API's experience
Excellent written and verbal communication skills

For data Science/Machine learning

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Project work on the technologies needed
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis

Please understand skills and relevant experience on real world projects are required by clients for selection even if its Junior or entry level position the additional skills and Project work with hands on experience building projects at client site are the only way a candidate can be picked by clients. If not having the skills or hands on project work at client site then candidates can optionally opt for skill enhancement to gain the required skills and project work. No third party candidates or c2c candidates

please only apply to the posting

No phone calls please. Shortlisted candidates would be reached out.","Analítica, Analítica de datos, Lenguajes de programación y Programación, Ciencias de la computación, Comunicación, Cracking, Desarrollo de software, Lista de preseleccionados y Transferencia de Estado Representacional (REST)",Solicitar
https://www.linkedin.com/jobs/view/3962211332/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=ywd9VoqqEwC%2B646cwtJbsw%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Omaha, NE","Acerca del empleo
Barbaricum is a rapidly growing government contractor providing leading-edge support to federal customers, with a particular focus on Defense and National Security mission sets. We leverage more than 15 years of support to stakeholders across the federal government, with established and growing capabilities across Intelligence, Analytics, Engineering, Mission Support, and Communications disciplines. Founded in 2008, our mission is to transform the way our customers approach constantly changing and complex problem sets by bringing to bear the latest in technology and the highest caliber of talent. 

Headquartered in Washington, DC's historic Dupont Circle neighborhood, Barbaricum also has a corporate presence in Tampa, FL, Bedford, IN, and Dayton, OH, with team members across the United States and around the world. As a leader in our space, we partner with firms in the private sector, academic institutions, and industry associations with a goal of continually building our expertise and capabilities for the benefit of our employees and the customers we support. Through all of this, we have built a vibrant corporate culture diverse in expertise and perspectives with a focus on collaboration and innovation. Our teams are at the frontier of the Nation's most complex and rewarding challenges. Join us. 

Barbaricum is seeking a Data Engineer to provide support an emerging capability for the USSTRATCOM J2 at Offutt Air Force Base near Omaha, Nebraska.

This individual will work to migrate existing ad hoc data flows to JWICS AWS available to the enterprise. Initially, the Data Engineer will use Python to automate data gathering and data cleaning efforts. Using these foundational efforts, the Data Engineer will then develop, implement, and operate a data management system for the intelligence enterprise.

Due to security requirements, this position is primarily required to be performed on-site. However, subject to project and customer requirements, team members may be provided flexibility for limited remote support. 

Responsibilities

Design, implement, and operate data management systems for intelligence needs
Use Python to automate data workflows
Design algorithms databases, and pipelines to access, and optimize data retrieval, storage, use, integration and management by different data regimes and digital systems
Work with data users to determine, create, and populate optimal data architectures, structures, and systems; and plan, design, and optimize data throughput and query performance
Participate in the selection of backend database technologies (e.g. SQL, NoSQL, etc.), its configuration and utilization, and the optimization of the full data pipeline infrastructure to support the actual content, volume, ETL, and periodicity of data to support the intended kinds of queries and analysis to match expected responsiveness
Assist and advise the Government with developing, constructing, and maintaining data architectures
Research, study, and present technical information, in the form of briefings or written papers, on relevant data engineering methodologies and technologies of interest to or as requested by the Government
Align data architecture, acquisition, and processes with intelligence and analytic requirements
Prepare data for predictive and prescriptive modeling deploying analytics programs, machine learning and statistical methods to find hidden patterns, discover tasks and processes which can be automated and make recommendations to streamline data processes and visualizations
Design, implement, and support scalable data infrastructure solutions to integrate with multi heterogeneous data sources, aggregate and retrieve data in a fast and safe mode, curate data that can be used in reporting, analysis, machine learning models and ad-hoc data requests
Utilize Amazon Web Services (AWS) hosted big data technologies to store, format, process, compute, and manipulate data in order to draw conclusions and make predictions

Qualifications

Active DoD Top Secret clearance required 
3 years of demonstrated experience in software engineering
Bachelor’s degree in computer science or a related field. A degree in the physical/hard sciences (e.g., physics, chemistry, biology, astronomy), or other science disciplines (i.e., behavioral, social, and life) may be considered if it includes a concentration of coursework (typically 5 or more courses) in advanced mathematics and/or other relevant experience
3 years of experience working with AWS big data technologies (S3, EC2) and demonstrate experience in distributed data processing, Data Modeling, ETL Development, and/or Data Warehousing
Demonstrated mid-level knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
Experience using analytical concepts and statistical techniques
3+ years of demonstrated experience across Mathematics, Applied Mathematics, Statistics, Applied Statistics, Machine Learning, Data Science, Operations Research, or Computer Science especially around software engineering and/or designing/implementing machine learning, data mining, advanced analytical algorithms, programming, data science, advanced statistical analysis, artificial intelligence

Preferred Experience

ArcGIS expertise
Experience using Python’s NumPy
Familiar with git-based revision control
Familiar with DevSecOps analytics development

Additional Information

For more information about Barbaricum, please visit our website at www.barbaricum.com . We will contact candidates directly to schedule interviews. No phone calls please.","Almacenamiento de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon EC2, Automatización, Ciencias de la computación, Modelado de datos y SC Clearance",Solicitar
https://www.linkedin.com/jobs/view/3917060340/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=8rCuX5vvN%2BaEk2EzTBbRNw%3D%3D&trk=flagship3_search_srp_jobs,Geospatial Data Engineer (Remote),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,"Basking Ridge, NJ","Acerca del empleo
Details:

Job Description

Stefanini Group is looking for a Geospatial Data Engineer (Remote) for a globally recognized company! For interested applicants, click the apply button or you may reach out Micah Andres at (248) 386-7399/Micah.Andres@Stefanini.com for faster processing. Thank you!

Position Overview: We are seeking an experienced Senior Telecom Engineer with a strong background in field networking and data analytics. The ideal candidate will possess expertise in telecom field networking transformation, data analytics, data science, and geospatial predictive analysis. If you are passionate about leveraging technology to drive impactful solutions and thrive in a fast-paced, collaborative environment, we encourage you to apply.

Key Responsibilities:

Design, implement, and optimize telecom field networking strategies to aid the transformation teams work while enhancing network performance and reliability.
Utilize data analytics and data science techniques to extract insights from data and drive informed decision-making.
Conduct geospatial predictive analysis to anticipate network demand and optimize resource allocation.
Lead/Support telecom field networking transformation initiatives, driving innovation and efficiency across the organization.
Collaborate with cross-functional teams to develop and implement strategies for network optimization and enhancement.
Provide technical expertise and support for telecom field networking projects, troubleshooting complex issues as needed.
Develop and maintain SQL queries and scripts to extract, manipulate, and analyze data.

Job Requirements

Details:

Bachelor's degree in Electrical Engineering, Telecommunications, Computer Science, or related field. Master's degree preferred.
Proven experience in telecom field networking, with a focus on network design, implementation, and optimization.
Strong background in data analytics and data science, with proficiency in tools such as Python, R, or MATLAB.
Experience conducting geospatial predictive analysis and familiarity with GIS software (e.g., ArcGIS, QGIS).
Demonstrated expertise in telecom field networking transformation initiatives, driving innovation and efficiency.
Proficiency in SQL for data manipulation and analysis.
Excellent communication skills, with the ability to effectively collaborate with cross-functional teams and stakeholders.
Strong problem-solving skills and ability to troubleshoot complex technical issues.
Proven track record of delivering results in a fast-paced, deadline-driven environment.","Analítica de datos, Análisis predictivo, Ciencia de datos, Ingeniería de datos y SQL, Comunicación, MATLAB, Manipulación de datos, Optimización y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3888429121/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=cbLMdFo1Fdp%2FIQLhh0Zc3Q%3D%3D&trk=flagship3_search_srp_jobs,jr Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,"Raleigh, NC","Acerca del empleo
Position: Jr Data Engineer

Location: Raleigh, NC or Boston, MA

Job Length: Long term

Position Type: C2C/W2

Qualifications

4 years of Experience in Java , Python , Spark
4 years Experience in Snowflake, Data Pipelines , SQL
Stays current with technology trends in order to provide best options for solutions Self-directed and is able to decompose work into problem sets for self and project team.
Equally capable working as part of a team or independently

Responsibilities

Designs, develops, tests, and delivers software solutions using one or more commercial languages as well as, open-source tools. Data processing and analysis using Snowflake.
Data warehouse using Data Pipelines along with data transformation and optimization.
Comfortable working within a culture of accountability and experimentation
Work closely with internal stakeholders to implement solutions and generate reporting to meet business goals.
Demonstrate critical thinking for potential roadblocks; comprehends bigger picture of the business and effectively communicates these issues to greater news digital organization.
Collaborates with reporting teams and business owners to turn data into actionable business insights using self-service analytics and reporting tools.

Skills Required : Snowflake, Data Pipelines , SQL","Apache Spark, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Java, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3979171116/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=ozTt4BeIZOssBkHvt9av1A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 1 semana,"Bellevue, WA","Acerca del empleo
Description

Data Engineer

Come build the future as a Data Engineer at Amazon, where you will be inspired working along best-in-class inventors and innovators! You will have the opportunity to create meaningful experiences that deliver on the ever-evolving needs of our customers, and your work will impact millions of people around the world.

As an Amazon Data Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. You will find that there is an unlimited number of opportunities within Amazon, where developing your career across a wide range of teams is highly supported. We are committed to making your work experience as enjoyable as the experiences you’ll be creating for our customers.

Apply now and you will be eligible for Amazon Data Engineer positions that are based on your preferred location, team, and more. We’re hiring across Amazon Stores in the United States and Canada.

Teams with available positions include, but are not limited to:

 Consumer Technology: Build new generation features and products for amazon.com, constantly improving the Customer and Seller experience for billions around the globe. Whether building site wide features such as reviews and recommendations, category specific software for the likes of Pharmacy, Electronics, Digital Software and Video Games or seller infrastructure, there are a variety of complex problems to tackle using a range of technologies in the design of your technical solutions.
 Operations Technology: Shape the future of transportation planning and execution on a global scale, that impacts hundreds of fulfillment centers, thousands of Amazonians, and millions of customers across the world. Your technology will support thousands of operators worldwide to design, build and run the best-in-class Amazon transportation network. We are building intelligent software to make transportation more reliable, faster, and less costly, providing a better and less expensive experience for our customers.
 Human Resources Technology: Create a seamless experience for millions of Amazonians and/or candidates. Whether supporting technologies for onboarding, time and attendance, compensation, amazon.jobs, or recruiting, you’ll deliver robust feature sets, elegant designs, intuitive user interfaces and systems that make it easy for Amazonians to excel at performing critical business functions.

About Us

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance be-tween your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign opportunities based on what will help each team member develop into a better-rounded contributor.

Inclusive Team Culture

Here at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Key job responsibilities

 Design, implement, and support a platform providing secured access to large datasets.
 Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
 Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
 Tune application and query performance using profiling tools and SQL.
 Analyze and solve problems at their root, stepping back to understand the broader context.
 Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
 Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
 Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
 Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com LLC - A03

Job ID: A2703397","AWS Lambda, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Datasets, Modelado de datos y Object Storage",Solicitar
https://www.linkedin.com/jobs/view/3982531394/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=qS19lGRXDgIpuHXeuBWYzA%3D%3D&trk=flagship3_search_srp_jobs,Business Intelligence Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Las Vegas, NV","Acerca del empleo
About AVB

AVB is a fast-growing, stable company that has a firm mission to do whatever it takes to keep our members profitably in business. Our vision is to deliver best in class growth solutions and service fueled by a passionate mission belief, innovation, and an unwavering commitment to doing what is right for our members, organization, and team members. AVB has an experienced development team, smart strategy team, bright and creative designers, along with the rest of our dream team of enthusiastic, entrepreneurial minded individuals who want to help drive the success of our members!

We offer competitive salaries, full benefits, a flexible remote work environment, an employee reimbursement program to support enriching your life, EAP, employee discounts, a WFH stipend, an annual allowance for AVB apparel, 401k options, a robust sick time policy, and 5 Floating Holidays per year, along with a minimum of 10 days of vacation time that starts accruing from your first day.

In addition, from time to time, our team members will be required to engage in travel to attend AVB’s Annual All-Employee Meeting, as well as other travel obligations which may be required of the position- such as meetings with members, attendance at conventions, and other events, where you will represent AVB!

We are always looking for smart, resourceful, and dynamic people to join the team, so if you are looking for a company that will welcome you, collaborate with you, challenge you, and help develop your skills to the next level- then we are looking for you! We are geared up for growth, are you ready to grow with us?

Home - AVB Marketing

Position Summary/Objective

The Business Intelligence Data Engineer is part of an analytics team focused on building and maintaining systems that collect, manage, and transform data with the goal of driving decision making and supporting expanded data programs, services, process optimization and advanced business intelligence. The incumbent will be the project owner of AVB’s data warehouse. Additionally, this role is responsible for developing processes and strategies for maintaining and assuring data quality from external sources.

Essential Functions And Responsibilities

Reasonable accommodations may be made to enable individuals with disabilities to perform these essential functions.

 Responsible for developing and maintaining data pipelines that support and enable the overall strategy of expanded data programs, services, process optimization and advanced business intelligence
 Leading data discovery sessions with business teams, comprising product owners, data analysts, and cross-team technologists to understand data requirements of analytics projects
 Partner with business domain experts, system analysts, data/application architects, and development teams to ensure data design is aligned with business strategy and direction
 Identify and document methodologies, standards, and architecture guidelines
 Architect and Design Data models
 Utilize SQL to create, transform, and investigate data sources and models
 Work to automate ELT processes including extraction from various sources (API, Flat files, etc..)
 Dive deep, as required, to assist Business Intelligence Engineers through technical hurdles impacting delivery

Required Skills And Abilities

 Thorough understanding of best practices related to data management including database architecture and data warehousing
 Experience in custom ELT design, implementation, and maintenance
 Hands on experience designing and working with relational databases – specifically MSSQL, MySQL, and BigQuery/RedShift
 Familiarity working with/on servers (mostly SQL, Web, and FTP)
 Knowledge of programming languages (e.g. Python and Object Oriented Programming)
 Strong knowledge and experience with CI/CD processes and Agile/Scrum methodology and iterative practices in a service delivery lifecycle
 Experience with internal development and source control tools including git/GitHub, Visual Studio, Agile, and JIRA
 Excellent communication and interpersonal skills with a demonstrated ability to influence a large organization
 Passionate about data solutions, technologies, and frameworks

Education And Experience

 5+ years of technical experience related to data architecture such as data analysis, data modeling, and data integration.
 3+ years of experience with SQL and at least one programming language: Python, JavaScript, PowerShell, and/or other languages
 3+ Experience with BigQuery, RedShift, SQL Server, MySQL
 Bachelor’s degree in Computer Science, Information Systems, or related field

Physical Requirements

 Prolonged period of sitting at a desk and working on a computer.
 Must be able to lift up to 15 pounds at times.","Almacenamiento de datos, Analítica de datos, Google BigQuery y SQL, Amazon Redshift, Bases de datos, Ciencias de la computación, Comunicación, JavaScript y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3980532455/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=7yaHSWxQaR08lk0YQvKwJg%3D%3D&trk=flagship3_search_srp_jobs,BCT Partners - Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Municipio de East Brunswick, NJ","Acerca del empleo
Data Engineer

It is a time of excitement and opportunity at BCT Partners – a great moment to join our team! We are seeking an experienced Data Engineer for our growing Technology line of business. 

 

About BCT 

We solve complex social problems. 

 

BCT is a management consulting firm that tackles complex social problems through data analytics, equity-centered solutions and program management. We believe complex issues are best solved through collaboration. As a result, we employ a multidisciplinary approach that combines domain expertise, research, evaluation, technology, organizational development, and a passion for change. 

 

To learn more about how we live our values of Ubuntu (“I am because we are”) and our mission to harness the power of diversity, insights, and innovation to transform lives, accelerate equity, and create lasting change, go to our website and follow us on social media. 

https://www.bctpartners.com/ 

 

Summary of the Project

The U.S. Department of Health and Human Services (HHS) Administration for Community Living (ACL) aims to enhance the independence, well-being, and health of older adults and people with disabilities. The Enterprise IT Services (EITS) project focuses on providing operational and developmental support for ACL’s enterprise-level cloud-based infrastructure and cybersecurity services. The ACL utilizes a hybrid multi-cloud architecture, primarily leveraging Microsoft Azure government cloud services with a FedRAMP moderate authorization, ensuring a secure and compliant environment for its various programs.

About The Data Engineer

The Data Engineer will be a vital part of the EITS team, responsible for designing, implementing, and managing data infrastructure and pipelines within ACL’s cloud environment. This role involves working with large datasets, ensuring their quality, and making them accessible for analysis and reporting. The Data Engineer will collaborate with data scientists, analysts, and other stakeholders to develop robust data solutions that support the organization’s decision-making processes. 

In addition to building and maintaining data pipelines, the Data Engineer will focus on optimizing data flow, ensuring data security, and supporting data integration efforts. This role requires a strong understanding of data engineering principles, excellent problem-solving skills, and the ability to work in a dynamic, collaborative environment. The Data Engineer will contribute to the success of the EITS project by delivering reliable, scalable, and efficient data solutions.

What you bring to the table

Bachelor’s degree in Computer Science, Information Technology, Data Science, or a related field. 
Minimum of 5 to 7 years of experience in data engineering or a similar role. 
Strong knowledge of data engineering principles and best practices. 
Proficiency in programming languages such as Python, SQL, and Java. 
Experience with cloud platforms such as AWS, Azure, or Google Cloud. 
Familiarity with data storage solutions like data warehouses and data lakes. 
Excellent problem-solving and debugging skills. 
Strong understanding of data security and compliance requirements. 
Ability to work collaboratively in a team environment. 
Professional certifications in data engineering or cloud technologies are preferred. 
Excellent communication and documentation skills. 

BCT offers a competitive total compensation package that, for this position includes base pay with a starting annual salary range of $110,000 - $125,000 (based on qualifications), along with a generous benefits package.  BCT’s benefits include heavily subsidized medical, dental and vision coverage, fully vested 401k plan with company match, company paid life and disability insurance plans, a strong work-life balance/time-off structure. 

 

This is a remote position, located in any of the fifty United States or Washington, DC. BCT Partners works primarily on Eastern Time, though we do have staff in all four time zones.  We support and encourage a strong staff community, leveraging virtual communication tools and collaborative work practices. The African philosophy of Ubuntu (“I am because we are”) is an overarching value that influences our leadership and interactions. 

BCT Partners is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, military and or/ veterans’ status, or any other federal or state legally protected class. BCT Partners will not discriminate against persons because of their disability, including disabled veterans, and will make reasonable accommodations for known physical or mental limitations of qualified employees and applicants with disabilities. If you are interested in applying and require special assistance or accommodation due to a disability, please contact our Human Resources Department.

Powered by JazzHR

7mVVaXcMxL","Canalizaciones de datos, Ciencia de datos, Google Cloud , Ingeniería de datos , Integración de datos y SQL, Ciencias de la computación, Comunicación, Flujo de datos y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3967389849/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=ZBc2tQ%2FjMxiiFZReTq4lRQ%3D%3D&trk=flagship3_search_srp_jobs,Marketing Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 3 días,Estados Unidos,"Acerca del empleo
Who We Are

Samsara (NYSE: IOT) is the pioneer of the Connected Operations™ Cloud, which is a platform that enables organizations that depend on physical operations to harness Internet of Things (IoT) data to develop actionable insights and improve their operations. At Samsara, we are helping improve the safety, efficiency and sustainability of the physical operations that power our global economy. Representing more than 40% of global GDP, these industries are the infrastructure of our planet, including agriculture, construction, field services, transportation, and manufacturing — and we are excited to help digitally transform their operations at scale.

Working at Samsara means you’ll help define the future of physical operations and be on a team that’s shaping an exciting array of product solutions, including Video-Based Safety, Vehicle Telematics, Apps and Driver Workflows, Equipment Monitoring, and Site Visibility. As part of a recently public company, you’ll have the autonomy and support to make an impact as we build for the long term.

About the team:

Data and Analytics is a critical team within Marketing. Our mission is to enable revenue performance by providing marketing and sales teams with the insights, tools, infrastructure and consultation to make data-driven decisions. We are a scrappy and growing team that loves all things data! The team will be composed of data engineers, analytics managers and data scientists. We are passionate about leveraging world-class data and analytics to deliver a great customer experience.

Our team promotes an agile, collaborative, supportive environment where diverse thinking, innovative design, and experimentation are welcomed and encouraged.

This role is open to candidates residing in the US except the San Francisco Bay Area (125 mi. radius from 1 De Haro St, San Francisco) and NYC Metro Area (50 mi. radius from 131 W 55th St, New York).

You should apply if:


You want to impact the industries that run our world: Your efforts will result in real-world impact—helping to keep the lights on, get food into grocery stores, reduce emissions, and most importantly, ensure workers return home safely.
You want to build for scale: With over 2.3 million IoT devices deployed to our global customers, you will work on a range of new and mature technologies driving scalable innovation for customers across industries driving the world's physical operations.
You are a team player: Working on our partners requires a mix of independent effort and collaboration. Motivated by our mission, we’re all racing toward our connected operations vision, and we intend to win—together.
You are a life-long learner: We have ambitious goals. Every Samsarian has a growth mindset as we work with a wide range of technologies, challenges, and customers that push us to learn on the go.


In this role, you will:


Develop and maintain marketing databases, datasets, pipelines and Samsara’s Customer Data Platform (CDP) to enable advanced segmentation, targeting, automation and analytics.
Manage critical data pipelines to enable our growth initiatives and advanced analytics. Manage the SLAs for those data pipelines and constantly improve efficiency and data quality.
Facilitate data integration and transformation requirements for moving data between applications; ensuring interoperability of applications with data mart and CDP environments.
Develop and improve the current data architecture, data quality, monitoring and data availability.
Identify data needs from broad stakeholders, understand requirements for metrics and analysis, and build efficient and scalable data products to enable a data-driven marketing approach.
Write sophisticated yet optimized data transformations in Python/SQL to generate data products consumed by customer systems and Analytics, Marketing Operations, Sales Operations teams.
Champion, role model, and embed Samsara’s cultural principles (Focus on Customer Success, Build for the Long Term, Adopt a Growth Mindset, Be Inclusive, Win as a Team) as we scale globally and across new offices


Minimum requirements for the role:


3+ years of working experience in a data engineering or data engineering adjacent role.
Excellent Python and SQL knowledge with strong hands-on data modeling. 
Experience with data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
Hands-on experience working with modern data technologies stack, such as Databricks, DBT, Google BigQuery, Redshift, RDS, Snowflake or similar solutions. 
Comfort in working with business customers to gather requirements and gain a deep understanding of varied datasets.
Familiarity with customer, marketing and/or web data. 
Experience integrating data from core Sales and Marketing platforms (e.g. Marketing Automation, CRM, and web analytics).
Self-starter, motivated, responsible, innovative and technology-driven individual who performs well both independently and as a team member.
A proactive problem solver and have good communication as well as project management skills to relay your findings and solutions across technical and non-technical audiences.


An ideal candidate also has:


Knowledge of Marketo, Salesforce.com and Google Analytics.
Experience working with CDPs such as Segment, Lattice, Blueshift, Lytics or Adobe Real-time CDP.
Experience with data visualization tools and packages (e.g. Looker, Domo, Tableau, MixPanel).
Familiarity with Marketing Technologies (MarTech stacks).
Data Science, machine learning or predictive analytics experience.


Samsara’s Compensation Philosophy: Samsara’s compensation program is designed to deliver Total Direct Compensation (based on role, level, and geography) that is at or above market. We do this through our base salary + bonus/variable + restricted stock unit awards (RSUs) for eligible roles. For eligible roles, a new hire RSU award may be awarded at the time of hire, and additional RSU refresh grants may be awarded annually.

We pay for performance, and top performers in eligible roles may receive above-market equity refresh awards which allow employees to achieve higher market positioning.

The range of annual base salary for full-time employees for this position is below. Please note that base pay offered may vary depending on factors including your city of residence, job-related knowledge, skills, and experience.

$95,200—$160,000 USD

At Samsara, we welcome everyone regardless of their background. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, gender, gender identity, sexual orientation, protected veteran status, disability, age, and other characteristics protected by law. We depend on the unique approaches of our team members to help us solve complex problems. We are committed to increasing diversity across our team and ensuring that Samsara is a place where people from all backgrounds can make an impact.

Benefits

Full time employees receive a competitive total compensation package along with employee-led remote and flexible working, health benefits, Samsara for Good charity fund, and much, much more. Take a look at our Benefits site to learn more.

Accommodations 

Samsara is an inclusive work environment, and we are committed to ensuring equal opportunity in employment for qualified persons with disabilities. Please email accessibleinterviewing@samsara.com or click here if you require any reasonable accommodations throughout the recruiting process.

Flexible Working 

At Samsara, we embrace a flexible working model that caters to the diverse needs of our teams. Our offices are open for those who prefer to work in-person and we also support remote work where it aligns with our operational requirements. For certain positions, being close to one of our offices or within a specific geographic area is important to facilitate collaboration, access to resources, or alignment with our service regions. In these cases, the job description will clearly indicate any working location requirements. Our goal is to ensure that all members of our team can contribute effectively, whether they are working on-site, in a hybrid model, or fully remotely. All offers of employment are contingent upon an individual’s ability to secure and maintain the legal right to work at the company and in the specified work location, if applicable.

Fraudulent Employment Offers

Samsara is aware of scams involving fake job interviews and offers. Please know we do not charge fees to applicants at any stage of the hiring process. Official communication about your application will only come from emails ending in ‘@samsara.com’ or ‘@us-greenhouse-mail.io’. For more information regarding fraudulent employment offers, please visit our blog post here.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Integración de datos, Amazon Redshift, Datasets, Marketing basado en datos y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3905974173/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=fpLFhk3evUniF%2BJrK2W7Ug%3D%3D&trk=flagship3_search_srp_jobs,Data Science Engineer - remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 4 meses,"Burlington, MA","Acerca del empleo
Our client is an established growing part of the Higher Education technology industry. They are a Global SaaS Company with over 300 campus partners and 2 million students actively using their platform. 
  They help higher education institutions better engage their students, improve the student life experience on campus, and student success. They are committed to improving student success and college graduation rates worldwide by crafting digital experiences that build communities and increase student engagement. They have a diverse and world-class global team poised for their next phase of rapid growth. 
    Understanding students and institutions and their behaviours through data lie at the foundation of their work to improve student success. Every data point in their systems is important to helping them achieve their main goal. They are looking for people with a strong background in data engineering and analytics to help them design, build, scale, and maintain their data pipelines and models. 
    As a Data Science Engineer, you will be working with various internal teams across engineering, product, and business to help solve their data needs. Your work will directly and tangibly impact the success of millions of students across the world. 
    In terms of the role and responsibilities, you will: 
      Identify the data needs of the engineering, product, and business teams, understand their specific requirements for metrics and analysis, then build efficient, scalable, accurate, and complete data pipelines to enable data-informed decisions across the company. 
  Architect data pipelines and models that power internal analytics for teams, as well as customer-facing data visualization product features 
  Be the subject matter expert on data-driven processes, visualization workflows, and tools with which to analyze data 
  Build processes supporting data transformation, data structures, metadata, dependency and workload management. 
  Drive the collection of new data and the evolution of existing data sources, collaborate with the engineering teams to manage the product instrumentation strategies and data structures 
  Help the product and engineering teams understand and generalize statistical models from research efforts and help build data systems that would allow these models to be used directly in product to drive student success. 
  Work with the Product Manager on the squad to ensure productive, fast-moving sprints that deliver the maximum value to their customers. 
  Work with the other senior engineers and architects to ensure that the integration stack is reliable, flexible, and scalable. 
  Help to improve the team processes of the engineering team continuously. 
    You should: 
    Have at least 4 years of experience in a Data Science or Data Engineering role, with a focus on instrumenting data collection, building data pipelines, data modelling and driving insights from complex data 
  Have a strong engineering background and are interested in data 
  Care deeply about the integrity of data, have a good nose for inconsistencies in data, and be able to pinpoint the issue to ensure that the team is not making decisions based on inaccurate or incomplete data 
  Working experience designing data systems 
  Experience with data related tools such as Athena, Apache Airflow or Google Composer 
  Strong analytical skills to pull insights from quantitative and qualitative data sets. 
  Have extensive experience in a scientific computing language - Python 
  Experience and understanding of APIs 
  A successful history of manipulating, processing and extracting value from large disconnected datasets 
  Advanced hands-on SQL knowledge and experience working with relational databases, query authoring 
  Have experience building systems that process data across multiple data stores and technologies, including MySQL, Redis, Elasticsearch 
  The ability to set up and manage a data warehouse is a plus 
  Know the best practices of how different types of data should be visualized in different contexts 
  Have good writing and verbal communication skills 
     This is a remote role based anywhere in North America.

Aptitudes y experiencia deseables
DATA SCIENCE","Airflow, Base de datos relacional, Canalizaciones de datos, Ciencia de datos, Informática científica y Ingeniería de datos, Establecer prioridades del trabajo, Lenguaje de consulta (query), Modelado de datos y Recogida de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3970563324/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=KCMVFry9OxbSRF95lhlEVg%3D%3D&trk=flagship3_search_srp_jobs,Market Data Analytics Engineer,"174,2 US$K/año - 235,8 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,Nueva York y alrededores,"Acerca del empleo
Why join Pave?

At Pave, our vision is simple - Make Compensation Fair.

How are we going to get there? By building the most intelligent compensation platform, powered by the largest real-time compensation dataset on earth.

We partner with our customers to help them build and retain world-class teams through planning, communicating and benchmarking their team’s compensation in real time. And you don’t have to just hear it from us — you can hear it from our customers: Hover, Plaid, Fanduel and more!

We’re growing fast, building an incredible team and product, and having plenty of fun as we do it in our San Francisco and New York City offices.

If operating with intellectual honesty, playing to win, and focusing on upholding a platinum standard sounds exciting - we highly encourage you to reach out. We’d love to partner on our journey to change the world of compensation!

Data Science @ Pave

Data is truly at the heart of what we do here at Pave.

So, it’s no surprise that Pave Analytics Engineers are integral collaborators and strategic owners with company-wide impact, enabling us to build a stellar product experience.

Our Analytics Engineers partner closely with data scientists, software engineers, and product managers to produce innovative new insights using of Pave’s one-of-a-kind real-time compensation dataset.

They’re not only responsible for designing and maintaining robust data pipelines and models but also helping build the infrastructure that powers our product, ultimately providing customers with insights that influence billions in compensation spend.

What we're looking for:

Product Mindset: You’ve been a core contributor in building and maintaining the data infrastructure for a product. You intuitively understand how decisions made within the data pipeline affect the user experience downstream.
Scalability: You design and implement systems that are robust and scalable, ensuring they can efficiently handle future growth and evolving use-cases.
Bias for Action: You’re a catalyst and an accelerator. You’re constantly unblocking yourself and others while making strategic trade-offs.
4+ years of experience in a Data/Analytics Engineering role, ideally in a product-facing capacity.
Exceptional data manipulation and modeling skills with a proficiency in SQL.

What You'll Get Out Of It

Impact: Your work will impact billions in compensation spend by influencing decisions made in offices and boardrooms worldwide.
Ownership: You’ll own major strategic initiatives from end-to-end with the support of your incredible teammates and collaborators.
Visibility: Play a key role in establishing the company as an authority in comp tech through high-profile marketing content and data-driven insights.

A final note — we highly encourage you to apply for this role, even if you don’t feel entirely qualified, or entirely sure. You never know!

Compensation, It's What We Do.

This salary range may include multiple levels. Your level is based on our assessment of your interview performance and experience, which you can always ask the hiring manager about to understand in more detail. Salary is just one component of Pave's total compensation package for employees. Your total rewards package at Pave will include equity, top-notch medical, dental and vision coverage, commuter benefits, catered lunch, an unlimited PTO policy, and many other region-specific benefits.

Pave's salary range for this position

$174,250—$235,750 USD

Our Compensation Philosophy

Pave’s compensation philosophy is to target the 75th percentile of the market for both cash and equity at your job level. This means that the “mid point” of every band at Pave is the 75th percentile of the broader market.

Pave also has a merit-based philosophy when it comes to compensation increases. We run a performance cycle twice per year to evaluate employees’ performance. Higher than average performance ratings result in compensation increases to the upper end of the individual’s compensation range for their role. The result is that high performers at Pave are paid above the 75th percentile of the market at large.

Pave is committed to pay equity. If you get an offer from Pave, it will be based on your level as determined by your interview performance. And nothing else. We explicitly do not negotiate salary and equity to ensure that we aren’t introducing bias that could lead to pay inequities within the team between candidates who have different negotiation tactics.

FAQ’s: 

How big is Pave today? 

We were founded in late 2019, and have grown to 160 employees across San Francisco, New York and the UK.

Where are the Pave offices? 

Our company HQ is in San Francisco's FiDi with a high energy in-person culture. We also have an office in NYC and a hub in England. 🌁 🗽 🇬🇧

What do employee benefits at Pave look like? 

As an employee at Pave, you will have your choice of medical, dental and vision insurance, as well as access to mental health services and other perks to promote your wellbeing. To enhance your personal and professional growth, you will have a monthly L&D stipend. We take our snacking seriously - employees receive catered lunch, dinner and many fun snacks throughout the day.

Who are some of Pave’s customers? 

Pave is working with 5,500+ companies today, including some of the best technology logos out there like Credit Karma, RO, Faire, Dropbox, Airtable, Sweetgreen, Checkr, Hubspot, Snackpass, Attentive and more!

What can I expect in interviewing at Pave? 

At Pave, we value intellectual honesty and transparency, and we bring this to our interview process. Throughout your time interviewing with us, we will be evaluating where you can best make an impact through multiple conversations with your recruiter, hiring manager, peers and cross-functional partners. We also use our interviews to determine leveling, which is finalized at the end of your interview process by your hiring manager. The majority of our roles have 3-5 interview rounds. You can expect to hear back from our team within 7 days of application, as well as within 2 days after each interview round. Once we extend an offer, we hope to hear back from you within 1 week. It is extremely important to us that we find a great mutual fit - we’re excited to get to know you!

More Questions?  Check out our candidate resources page! 

Pave is committed to a diverse and inclusive workforce. We are an equal opportunity employer and do not discriminate on the basis of race, ethnicity, gender, gender identity, sexual orientation, protected veteran status, disability, age, or another legally protected status. For individuals with disabilities who would like to request accommodation, please email recruiting@pave.com. Sponsorship for work visas or other permits may be available for certain positions, subject to Pave's policies and legal requirements.","Almacenamiento de datos, Analítica, Analítica de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos y Manipulación de datos",Solicitar
https://www.linkedin.com/jobs/view/3946956500/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=atMsKLwYp%2BIspvXVxKYqjw%3D%3D&trk=flagship3_search_srp_jobs,Finance Data Engineer,"73,9 US$K/año - 170,3 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 4 días,"San Francisco, CA","Acerca del empleo
Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

The Opportunity 

This is an exciting opportunity to join the Digital Media Finance team as we continue to propel the business through data-driven forecasts and provide influential insights. In this role, you will have the opportunity to help drive Adobe’s pivotal initiatives by data engineering solutions that connect systems to visualization platforms for management reporting. The position will report to the Group Manager, Finance Automation to drive our systems and automation effort. This position will provide an outstanding opportunity to drive timely insights across Digital Media (DMe) Business at Adobe and help drive the Finance Transformational efforts undertaken by the Finance Automation team.

The ideal candidate for this role is a well-rounded top performer with exceptional data engineering, analysis, and visualization skills, capable of driving growth in a fast-paced environment. You should possess a continuous learning mentality to explore and implement innovations in the team's reporting and transformational projects. Strong cross-functional collaboration within Adobe is essential. You will be responsible for automating complex financial models, developing engaging and interactive dashboards, streamlining processes, and enhancing efficiencies in management reporting

What you'll do:

Responsible for the strategy and execution of DMe lifecycle of financial and analytics dashboards and ensuring scalability of systems and processes. 
Manage data transitions and platform migrations between SAP HANA/DataBricks, Tableau/PowerBI, and related ETL processes. 
Develop data pipelines, connections, and infrastructure to better enable forecasting and data science modelling. 
Correct data errors via systematic, logical fixes and provide updates within the data pipelines, connections and infrastructures that are built to support the teams. 
Create documentation and support enablement for the teams who are interested and able to help themselves. 
Accountable for ad-hoc support & participation in critical business analytics and key project support as directed by management. 
Partner closely with IT, Finance Systems, Finance Transformation Office, and other Finance counterparts to continually improve, streamline & enhance planning and reporting processes. 

What you need to succeed:

BS/BA with preferred focuses in areas of Business, Finance, or Information Systems. Master’s in Data Science a plus. 
3+ years of demonstrated experience working with large and complex data structures and develop efficient queries to create calculated fields and data aggregates. 
2+ years of proven experience designing and developing dashboards using PowerBI or Tableau. 
3+ years of relevant experience in data science and analytics in creating/maintaining financial models for a subscription/SaaS business a plus. 
Proficient in data platforms/systems (such as SQL, Databricks), ETL tools (such as Python, SnapLogic), process automation, and standardization. 
Self-starter with high attention to details, excellent interpersonal skills, and ability to take charge, set objectives, and deliver results. 
Strong project management skills with ability to juggle multiple priorities. 
Strong team orientation and a learning mentality. 

Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $73,900 -- $170,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.

Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances.

Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.

Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.

Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, Automatización, Habilidades sociales, Panel de control y SAP HANA",Solicitar
https://www.linkedin.com/jobs/view/3970562391/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=pCTLs1EtcTpOy%2FvPjARHPg%3D%3D&trk=flagship3_search_srp_jobs,Market Data Analytics Engineer,"174,2 US$K/año - 235,8 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,San Francisco y alrededores,"Acerca del empleo
Why join Pave?

At Pave, our vision is simple - Make Compensation Fair.

How are we going to get there? By building the most intelligent compensation platform, powered by the largest real-time compensation dataset on earth.

We partner with our customers to help them build and retain world-class teams through planning, communicating and benchmarking their team’s compensation in real time. And you don’t have to just hear it from us — you can hear it from our customers: Hover, Plaid, Fanduel and more!

We’re growing fast, building an incredible team and product, and having plenty of fun as we do it in our San Francisco and New York City offices.

If operating with intellectual honesty, playing to win, and focusing on upholding a platinum standard sounds exciting - we highly encourage you to reach out. We’d love to partner on our journey to change the world of compensation!

Data Science @ Pave

Data is truly at the heart of what we do here at Pave.

So, it’s no surprise that Pave Analytics Engineers are integral collaborators and strategic owners with company-wide impact, enabling us to build a stellar product experience.

Our Analytics Engineers partner closely with data scientists, software engineers, and product managers to produce innovative new insights using of Pave’s one-of-a-kind real-time compensation dataset.

They’re not only responsible for designing and maintaining robust data pipelines and models but also helping build the infrastructure that powers our product, ultimately providing customers with insights that influence billions in compensation spend.

What we're looking for:

Product Mindset: You’ve been a core contributor in building and maintaining the data infrastructure for a product. You intuitively understand how decisions made within the data pipeline affect the user experience downstream.
Scalability: You design and implement systems that are robust and scalable, ensuring they can efficiently handle future growth and evolving use-cases.
Bias for Action: You’re a catalyst and an accelerator. You’re constantly unblocking yourself and others while making strategic trade-offs.
4+ years of experience in a Data/Analytics Engineering role, ideally in a product-facing capacity.
Exceptional data manipulation and modeling skills with a proficiency in SQL.

What You'll Get Out Of It

Impact: Your work will impact billions in compensation spend by influencing decisions made in offices and boardrooms worldwide.
Ownership: You’ll own major strategic initiatives from end-to-end with the support of your incredible teammates and collaborators.
Visibility: Play a key role in establishing the company as an authority in comp tech through high-profile marketing content and data-driven insights.

A final note — we highly encourage you to apply for this role, even if you don’t feel entirely qualified, or entirely sure. You never know!

Compensation, It's What We Do.

This salary range may include multiple levels. Your level is based on our assessment of your interview performance and experience, which you can always ask the hiring manager about to understand in more detail. Salary is just one component of Pave's total compensation package for employees. Your total rewards package at Pave will include equity, top-notch medical, dental and vision coverage, commuter benefits, catered lunch, an unlimited PTO policy, and many other region-specific benefits.

Pave's salary range for this position

$174,250—$235,750 USD

Our Compensation Philosophy

Pave’s compensation philosophy is to target the 75th percentile of the market for both cash and equity at your job level. This means that the “mid point” of every band at Pave is the 75th percentile of the broader market.

Pave also has a merit-based philosophy when it comes to compensation increases. We run a performance cycle twice per year to evaluate employees’ performance. Higher than average performance ratings result in compensation increases to the upper end of the individual’s compensation range for their role. The result is that high performers at Pave are paid above the 75th percentile of the market at large.

Pave is committed to pay equity. If you get an offer from Pave, it will be based on your level as determined by your interview performance. And nothing else. We explicitly do not negotiate salary and equity to ensure that we aren’t introducing bias that could lead to pay inequities within the team between candidates who have different negotiation tactics.

FAQ’s: 

How big is Pave today? 

We were founded in late 2019, and have grown to 160 employees across San Francisco, New York and the UK.

Where are the Pave offices? 

Our company HQ is in San Francisco's FiDi with a high energy in-person culture. We also have an office in NYC and a hub in England. 🌁 🗽 🇬🇧

What do employee benefits at Pave look like? 

As an employee at Pave, you will have your choice of medical, dental and vision insurance, as well as access to mental health services and other perks to promote your wellbeing. To enhance your personal and professional growth, you will have a monthly L&D stipend. We take our snacking seriously - employees receive catered lunch, dinner and many fun snacks throughout the day.

Who are some of Pave’s customers? 

Pave is working with 5,500+ companies today, including some of the best technology logos out there like Credit Karma, RO, Faire, Dropbox, Airtable, Sweetgreen, Checkr, Hubspot, Snackpass, Attentive and more!

What can I expect in interviewing at Pave? 

At Pave, we value intellectual honesty and transparency, and we bring this to our interview process. Throughout your time interviewing with us, we will be evaluating where you can best make an impact through multiple conversations with your recruiter, hiring manager, peers and cross-functional partners. We also use our interviews to determine leveling, which is finalized at the end of your interview process by your hiring manager. The majority of our roles have 3-5 interview rounds. You can expect to hear back from our team within 7 days of application, as well as within 2 days after each interview round. Once we extend an offer, we hope to hear back from you within 1 week. It is extremely important to us that we find a great mutual fit - we’re excited to get to know you!

More Questions?  Check out our candidate resources page! 

Pave is committed to a diverse and inclusive workforce. We are an equal opportunity employer and do not discriminate on the basis of race, ethnicity, gender, gender identity, sexual orientation, protected veteran status, disability, age, or another legally protected status. For individuals with disabilities who would like to request accommodation, please email recruiting@pave.com. Sponsorship for work visas or other permits may be available for certain positions, subject to Pave's policies and legal requirements.","Almacenamiento de datos, Analítica, Analítica de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos y Manipulación de datos",Solicitar
https://www.linkedin.com/jobs/view/3982681170/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=%2FR3sj5wM1EyqMFDlyKStgw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 6 días,"Colorado, Estados Unidos","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Hello!

I have a new position I'm working! Please review the details below and let me know who you think would be a good fit.

Please send resume, VISA, and confirm the best rate. Whoever does this will be considered first. Thank you!

Please Note: As of July 22, 2021, our team will require that all candidate submissions include a LinkedIn profile. Please do not submit any candidates that do not have a LinkedIn.

KFORCE URGENT REQUIREMENT

Looking For Candidates Regarding The Following

POSITION

 Data Engineer

LOCATION

2x week onsite in Castle Rock, CO (Must be local)

DURATION

 12+ months

INTERVIEW TYPE

Video

VISA RESTRICTIONS

Any

Required Skills 

SQL Server , T-SQL, Azure Data Lake, Azure Data Facotry, MS Master Data Services, SSIS
Power BI report builder
Power Shell, Python
CI/CD pipelines
MS Dynamics 365 and MS Purview","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3964741441/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=IdO8C1g2q63BibxCoAQGZQ%3D%3D&trk=flagship3_search_srp_jobs,GCP Data Engineer,"Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 semanas,"Toronto, OH","Acerca del empleo
Title: GCP Data Engineer 

Location: remote -Canada

Duration: 6+ month contract to extend or convert

Project Details

 They need to build a customer database (data asset) to help them identity: who are our customers, prospects, what RGUs/revenue insights do we have and proper segmentation of the customers - creating a unified version of this data view for the marketing team
 They will leverage this data for marketing campaign team (outreach, scorecards, etc)
 Individual will work with the business, take the business requirements convert them to technical specifications leveraging SQL/Data mapping/Python. Acting as a liaison between technical and business.
 Also trying to migrate their processes from on-prem to GCP
 They will leverage this data for marketing campaign team (outreach, scorecards, etc

Skills

3+ years of Experience
Experience building pipelines from data source to GCP for consumption
Experience working with SQL database
Python experience for Automation
Moving data from on prem to GCP
ETL pipeline development and Process experience
Datamapping

Plus Skills: neither are required

GCP Certification
Previous Telecom Exp

Responsibilities

There will be a ramp up period initially is 1-2 months, Team Lead will guide the resource for the first 1-3-ish months, then they are expected to be self sufficient, working with the business requirements, building pipelines into existing source data to pull into GCP, automation with python (for pulling the data sets), participating in daily stand ups, etc

TSG is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

69516","Base de datos SQL, Extraer, transformar y cargar (ETL), Google Cloud , Ingeniería de datos , Python y SQL, Asignación de datos, Bases de datos, Especificaciones técnicas y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3833869544/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=7YbcWpZ%2BsspvyXxCBID%2F7g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer with Cognos resource,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 meses,Estados Unidos,"Acerca del empleo
Data Engineer

6-12 months

Remote

USC, GC

60-65/hr. C2C

 Looking for a Data Engineer with Cognos resource and must have experience with QuickSight, ETL, Cognos, Insurance experience . 
 Must strong Cognos and Quicksight and ETL","Almacenamiento de datos, Amazon QuickSight, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos y Microsoft Power BI, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3967919094/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=sO4w9DiI%2FB%2FnYE1dn0LD9Q%3D%3D&trk=flagship3_search_srp_jobs,Risk Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Beaverton, OR","Acerca del empleo
Overview

 Join the nation’s leader in second-look finance servicing as  our Risk Data Engineer! 

The Impact You’ll Have At Concora Credit

As a Data Engineer, your primary role will be to build and optimize foundational data marts, which will support reporting and analysis across different areas in the organization. This will include building data pipeline architecture, data flows, data transformations and clear documentation. At Concora Credit, you’ll join our Analytics Engineering team and partner with data owners and users to build, problem-solve and optimize key data structures. We are transitioning our data warehouse and analytics to Azure cloud and databricks analytics platform. Join us in this adventure!

We hire people, not positions. That's because, at Concora Credit, we put people first, including our customers, partners, and Team Members. Concora Credit is guided by a single purpose: to help non-prime customers do more with credit. Today, we have helped millions of customers access credit. Our industry leadership, resilience, and willingness to adapt ensure we can help our partners responsibly say yes to millions more. As a company grounded in entrepreneurship, we're looking to expand our team and are looking for people who foster innovation, strive to make an impact, and want to Do More! We’re an established company with over 20 years of experience, but now we’re taking things to the next level. We're seeking someone who wants to impact the business and play a pivotal role in leading the charge for change.

Responsibilities

 As our Risk Data Engineer you will: 

 Create and manage large, complex data sets that meet business requirements and serve as the foundation behind analytics and management reporting, informing critical decision making across the organization. 
 Create and maintain optimal data pipeline architecture, including optimal extraction, transformation and loading of data, leveraging databricks notebooks, Apache Spark (SQL, Python, Scala) and other Microsoft Azure or open-source tools as required. 
 Maintain high quality documentation about data structures in shared data governance tool for the business to access. 
 Collaborate with data owners, data stewards, analysts, software developers and DBAs. 
 Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. 
 Upon request, perform ad-hoc analysis to address a specific business or data question. 
 Work with stakeholders to interpret, research, and resolve data-related technical issues and support their data infrastructure needs. 

Qualifications

 Requirements: 

 A minimum of 2 years of experience in a Data Engineer role. 
 Experience with the following tools: SQL or relational databases, Python, data pipeline management such as SSIS, Azure Data Factory or Apache Spark for batch and streaming. 
 Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. 
 Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. 
 Strong communication skills – verbal and written. 
 Ability to work in a fast-paced environment with a focus on continuous improvement. 
 Results Focus: Self-starter with ability to work independently and collaboratively in a cross-functional team. 
 Strong organizational skills. 
 Extreme attention to detail, strong analytical, quantitative, problem solving and conceptual skills. 

Preferred Requirement

 Ideally this candidate has experience with the following tools: Apache Spark, databricks, Hadoop or comparable tool. 
 3+ years of experience in a Data Engineer role. 

What’s In It For You

 Medical, Dental and Vision insurance for you and your family 
 Relax and recharge with Paid Time Off (PTO) 
 6 company-observed paid holidays, plus 3 paid floating holidays 
 401k (after 90 days) plus employer match up to 4% 
 Pet Insurance for your furry family members 
 Wellness perks including onsite fitness equipment at both locations, EAP, and access to the Headspace App 
 We invest in your future through Tuition Reimbursement 
 Save on taxes with Flexible Spending Accounts 
 Peace of mind with Life and AD&D Insurance 
 Protect yourself with company paid Long-Term Disability and voluntary Short-Term Disability 

Concora Credit provides equal employment opportunities to all Team Members and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

Concora Credit Is an equal opportunity employer (EEO).

Please see the Concora Credit Privacy Policy for more information on how Concora Credit processes your personal information during the recruitment process and, if applicable, based on your location, how you can exercise your privacy rights. If you have questions about this privacy notice or need to contact us in connection with your personal data, including any requests to exercise your legal rights referred to at the end of this notice, please contact caprivacynotice@concoracredit.com .","Apache Spark, Azure Data Factory, Canalizaciones de datos, Hadoop, Ingeniería de datos y SQL Server Integration Services (SSIS), Bases de datos, Comunicación, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3981309783/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=iobRQ0FJiwFlZL7oAn7XDg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 5 días,"Boxborough, MA","Acerca del empleo
Product Insight is a world-class team of product development professionals focused on helping companies identify solutions to their most complex problems and developing the technical products that enable those solutions at scale.

For us, assembling, growing, and evolving an exceptional team is paramount. We excel at creating innovative solutions to intricate technical challenges across various industries, delivering life-saving therapies, enhancing workplace safety and productivity, transforming operational models, and shaping the future of transportation. Our development team is focused on what products could be, getting from zero to one.

We are seeking a highly skilled Data Engineer to help build our logging and telemetry pipelines for robotics systems. You will work closely with Systems and Software Engineers to build tooling to generate real world insights for root cause analysis, product validation, and reliability studies. You will build software that work on IoT platforms and in the cloud using Python and C++ interfacing to observability stacks. This role has the opportunity to have a big impact on product development, working directly with clients and internal stakeholders on a small cohesive team!

Key Responsibilities:
Develop infrastructure for logging telemetry from robotic systems on edge and in the cloud
Develop processing pipelines for data analysis
Build dashboards and 3D data visualization tools
Participate in root cause analysis, reliability studies, and product validation
Work with a cross disciplinary team to identify software requirements and architecture based on customer needs.
Quickly prototype and determine feasibility of design and technology.
Participate in design and code reviews.
Work with management to develop timelines and work schedules.
Coordinate cross team engineering tasking.
Provide technical guidance to other software engineers.
Excellent communication and interpersonal abilities.
Strong organizational and time-management skills, managing multiple projects simultaneously.
Demonstrates a strong team spirit, working to achieve team goals and maintain high team morale.

Qualifications
Bachelor, master’s or PhD in Computer Science, Electrical Engineering, Computer Engineering, Robotics, Software Engineering, or related discipline.
3 to 5 years’ of demonstrated experience in observability development.
Knowledge of modern observability stacks (ELK, Influx DB, etc.)
Experience deploying production grade software via docker, docker-compose, kubernetes
Experience with C++ and Python
Good written and verbal communication skills with the ability to explain/teach technical concepts to others.
Actively involved in developing an excellent software design process

At Product Insight, we aim to be a place where a diverse mix of talented individuals want to come, stay, and contribute their best work. We take pride in delivering top-tier products to the market, recognizing that achieving this goal necessitates a team reflecting the diversity of our world. We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, or veteran status.","Python, Amazon CloudWatch, C++, Desarrollo de software, Elasticsearch, Gestión del tiempo, InfluxDB, Multiple Projects Simultaneously y Requisitos de software",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979448160/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=nWHrQ9zYld4QWRTqyyrvqA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Analyst,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 semana,"Eugene, OR","Acerca del empleo
Work Schedule

Standard (Mon-Fri)

Environmental Conditions

Office

When you’re part of a team at Thermo Fisher Scientific, you provide important work with real-world impact; and be enthusiastically supported in achieving career goals. Employees are valued and recognized for their performance, all while collaborating with engaged and dynamic teammates. This role supports our Biosciences division, which represents over $4B in sales annually.

We are looking for an experienced data engineer/analyst to support end-to-end data analytics projects for the divisional marketing team. Are you astute at simplifying sophisticated data with outstanding communication skills?

Location:

Hybrid role - 3 days a week in a local Thermo Fisher Scientific office

Primary responsibilities:

Leverage data integrations using systems such as AWS Athena, Redshift, Cognos, SharePoint and Power BI Services among other technologies

Propose data development optimizations and implement to deliver solutions and enhanced reporting capabilities

Provide timely and consistent updates and recommendations on dataflow and data pipeline operational issues and improvements to team and partners

Become a domain authority of data sources used by the Analytics team (Sales Force, Adobe Analytics, Redshift, internal databases, etc.) along with reports built

Consult data source owners to understand and detail data collection and reporting processes

Champion data quality and integrity through proactive sleuthing and oversight

Establish alerts for data connection issues both technical and beyond encouraged thresholds (anomalies)

Influence and maintain strong and balanced team culture through active involvement, inclusion, and leadership!

Proactive self-education and contribution to advance our analytics practices, methods, and strategy

Experience requirements:

 Experience in taking requirements and understanding the needs of the business to tackle problems
 Experience in the design and implementation of the different data processes like profiling, exploration, collection, processing, cleaning, and preparation of data, ETL, ELT, ETLT data processes and data pipelines with Python, SQL, Non-SQ, MS Power BI Services and Power Query
 Experience in semantic data models and schemas like flat, star and snowflake schemas
 Experience in crafting business intelligence reports/dashboards from scratch
 Strong graphics and visualization aesthetics to convey messages simply and impact-fully

Minimum Qualifications and Experience:

 Bachelor’s degree in the sciences or engineering

5+ years experience in a data analytics or engineering role with a solid understanding of technical, business, and operational process

Proven ability with Power BI Desktop and Service (or similar business intelligence and data visualization tool), M, Python and SQL desirable

Advanced MS Excel

Experience with GitHub and agile scrum

At Thermo Fisher Scientific, each one of our 90,000 extraordinary minds have a unique story to tell. Join us and contribute to our singular mission—enabling our customers to make the world healthier, cleaner and safer.

Thermo Fisher Scientific is an EEO/Affirmative Action Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status. We will make reasonable accommodation as required.

Compensation And Benefits

The salary range estimated for this position based in New York is $75,800.00–$120,000.00.

This position may also be eligible to receive a variable annual bonus based on company, team, and/or individual performance results in accordance with company policy. We offer a comprehensive Total Rewards package that our U.S. colleagues and their families can count on, which includes:

A choice of national medical and dental plans, and a national vision plan, including health incentive programs
Employee assistance and family support programs, including commuter benefits and tuition reimbursement
At least 120 hours paid time off (PTO), 10 paid holidays annually, paid parental leave (3 weeks for bonding and 8 weeks for caregiver leave), accident and life insurance, and short- and long-term disability in accordance with company policy
Retirement and savings programs, such as our competitive 401(k) U.S. retirement savings plan
Employees’ Stock Purchase Plan (ESPP) offers eligible colleagues the opportunity to purchase company stock at a discount

For more information on our benefits, please visit: https://jobs.thermofisher.com/global/en/total-rewards","Analítica, Analítica de datos y Extraer, transformar y cargar (ETL), Astute, Calidad de datos, Lenguaje de consulta (query), Modelo de datos, Panel de control, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3945906032/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=TcyYrEg8Bu7mqZQEzufXLA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 5 días,"Atlanta, GA","Acerca del empleo
The ETL Engineer performs design, development and implementation of integration processes for both the Enterprise Data lake, Data Warehouse and Applications
Analyzes requirements and existing resources to create efficient database and integration designs that meet company IT standards.
Works with project and business analyst leads to develop and clarify in-depth technical requirements.
Participates in all phases of the integration development lifecycle, including unit testing, quality assurance(QA) and ongoing support.
Helps with Production support as needed
Excellent communication skills (both verbal and written) 
Proven ability to provide strong problem solving skills.
Must be self-motivated and know when to seek guidance
Must be flexible, be able to change priorities quickly, and handle multiple tasks concurrently
Individual must be a self-starter and capable of working independently as well as part of a team
Capable of learning new tools and technologies

Aptitudes y experiencia deseables
SQL","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amdocs CRM, Comunicación, Requisitos técnicos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3909924225/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=%2BRWK3XaDGWXArmKI3e8bmw%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 meses,"Olympia, WA","Acerca del empleo
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this market also our candidates get multiple job offers and $100k + salaries.

 please check the below links to see success outcomes of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 https://www.youtube.com/watch?v=OFoqPTNORew

 https://www.youtube.com/watch?v=-HkNN1ag6Zk

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

 https://youtu.be/bJJl27D8bh0

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3975013763/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=CD5QAgUDjBLTwFP5wX96xw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 6 días,"Jackson, MI","Acerca del empleo
If you're looking for a career that will provide a challenging work environment and many opportunities to learn, our company is the place for you. Join our team and become part of a company full of exciting opportunities. We help employees achieve their personal best by offering many opportunities that develop and enhance their skills for career advancement.

Duties

Duties: 

As an Azure Data Engineer, your role involves designing, implementing, monitoring, and maintaining data and analytical solutions on Microsoft Azure.

Data Pipeline Design And Implementation

Create and maintain data pipelines that facilitate data movement, transformation, and integration.
Ensure efficient and reliable data flow from source systems to target destinations.

Data Storage Solutions

Design and manage data storage solutions within Azure, including:
Azure Data Lake: Storing large volumes of raw data.
Azure SQL Database: Providing structured data storage.
Azure Blob Storage: Handling unstructured data.

Data Processing And Transformation

Use tools like Databricks and dbt to process and transform data.
Leverage Azure Databricks for running dbt transformations in production

Data Integration

Collaborate with data analysts and scientists to understand data requirements.
Design effective data workflows that enable data-driven decision-making.

Performance Optimization

Optimize SQL queries generated by dbt for better performance.
Ensure consistency between development (using a Databricks SQL warehouse) and production (using Azure Databricks compute) environments

Security And Compliance

Implement secure and compliant data processing pipelines.
Use Azure services and frameworks to produce cleansed and enhanced datasets for analysis

Skills

Microsoft Azure, Azure Databricks, and DBT
Knowledge of data catalog tools (e.g., Microsoft Purview)
Proficiency in data processing languages, including SQL and Python
Excellent problem-solving skills and ability to work in a collaborative environment

Education

Bachelor’s degree in Computer Science, Engineering, or related field.
Experience as an Azure Data Engineer or similar role.

Certifications & Licenses

Proficiency in data processing languages, including SQL and Python

Required

Skills: 

Microsoft Azure
Problem-Solving
Data Pipelines
Python
SQL

Additional

Datasets
Performance Optimization
SQL Queries
Data Integration

Additional Info

At FastTek Global, Our Purpose is Our People and  Our Planet . We come to work each day and are reminded we are helping people find their success stories . Also, Doing the right thing is our mantra . We act responsibly, give back to the communities we serve and have a little fun along the way.

We have been doing this with pride, dedication and plain, old-fashioned hard work for 24 years !

FastTek Global is financially strong, privately held company that is 100% consultant and client focused .

We've differentiated ourselves by being fast, flexible, creative and  honest . Throw out everything you've heard, seen, or felt about every other IT Consulting company. We do unique things and we do them for Fortune 10, Fortune 500, and technology start-up companies.

Benefits

Our benefits are second to none and thanks to our flexible benefit options you can choose the benefits you need or want, options include:

Medical and Dental (FastTek pays majority of the medical program)
Vision
Personal Time Off (PTO) Program
Long Term Disability (100% paid)
Life Insurance (100% paid)
401(k) with immediate vesting and 3% (of salary) dollar-for-dollar match

Plus, we have a lucrative employee referral program and an employee recognition culture.

FastTek Global was named one of the Top Work Places in Michigan by the Detroit Free Press in 2013, 2014, 2015, 2016, 2017, 2018, 2019,  2020, 2021, and 2022!

To view all of our open positions go to: https://www.fasttek.com/fastswitch/findwork

Follow us on Twitter: https://twitter.com/fasttekglobal

Follow us on Instagram: https://www.instagram.com/fasttekglobal

Find us on LinkedIn: https://www.linkedin.com/company/fasttek

You can become a fan of FastTek on Facebook: https://www.facebook.com/fasttekglobal/","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Ciencias de la computación, Datasets, Procesamiento de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3937286321/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=UjoeKfDYDUlx5HU3KIDhhw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Azure Data Platform),"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 semana,"Malaga, WA","Acerca del empleo
As an Azure Data Engineer specialized in Microsoft Azure tools, you will support the implementation of projects focused on collecting, aggregating, storing, reconciling, and making data accessible from disparate sources to enable analysis and decision making.

This role will also play a critical part in the data supply chain, by ensuring stakeholders can access and manipulate data for routine and ad hoc analysis. Additionally, you will support the full lifecycle of data from ingesting through analytics to action.

Come join us

As a member of the Data Engineering team, you’ll have access to the research, knowledge, and tools to create leading-edge solutions across Avanade’s Business Intelligence practice.

The role of Data Engineer is perfect for ambitious technologists passionate about working with the latest Microsoft cloud technology and Microsoft experts. Our clients look to us for innovation, which means you’ll have early access to the newest Microsoft technologies so you can master them and stay ahead of the curve.

Together we do what matters!

Skills And Experiences

Around 2 years of working experience as a Data Engineer (working with T-SQL) 
Knowledge of, at least, 6 months with Databricks and Data Factory 
Experience in T-SQL Data-Modeling 
Nice-to-have knowledge of Power BI solutions.

About You

Some of the best things about working at Avanade 

Opportunity to work for Microsoft’s Global Alliance Partner of the Year (14 years in a row), with exceptional development and training (minimum 80 hours per year for training and paid certifications) 
Real-time access to technical and skilled resources globally 
Dedicated career advisor to encourage your growth 
Engaged and helpful coworkers genuinely interested in you 

Find out more about some of our benefits Employee Benefits at Avanade | Avanade



What You'll Do

Implementation of technical solutions 
Knowledge of data warehouse concepts and T-SQL relational/non-relational databases for data access and Advanced Analytics 
Knowledge of Power BI to enhance better data visualizations to the clients.

Learn more

To learn more about the types of projects our Analytics team works on check out this information: 

thyssenkrupp Materials Services uses data to help strike a delicate operational balance 
What matters to SSE Renewables is innovating for a sustainable future 
Hachette UK enables employees with machine learning-driven contract searches 
Marston Holdings identifies significant cloud savings

Interested in knowing what’s going on inside Avanade? Check out our blogs: 

Avanade Insights – exchange ideas that drive tomorrow’s innovation 
Inside Avanade – explore what life is like working at Avanade

Enjoy your career

Some Of The Best Things About Working At Avanade

Opportunity to work for Microsoft’s Global Alliance Partner of the Year, with exceptional development and training (minimum 80 hours per year for training and paid certifications) 
Real-time access to technical and skilled resources globally 
Dedicated career advisor to encourage your growth 
Engaged and helpful coworkers genuinely interested in you 

Find out more about some of our benefits Employee Benefits at Avanade 

A great place to work

As you bring your skills and abilities to Avanade, you’ll get distinctive experiences, limitless learning, and ambitious growth in return. As we continue to build our diverse and inclusive culture, we become even more innovative and creative, helping us better serve our clients and communities. You’ll join a community of smart, supportive collaborators to lift, mentor, and guide you, and to lean on your expertise. You get a company purpose-built for business-critical, leading-edge technology solutions, committed to improving the way humans work, interact, and live. It’s all here, so take a closer look!

We work hard to provide an inclusive, diverse culture with a deep sense of belonging for all our employees.

Visit our Inclusion & Diversity page. 

 

Create a Future For Our People That Focuses On

Expanding your thinking
Experimenting courageously
Learning and pivoting 

Inspire Greatness In Our People By

Empowering every voice
Encouraging boldness
Celebrating progress 

Accelerate The Impact Of Our People By

Amazing the client
Prioritizing what matters
Acting as one.","Almacenamiento de datos, Analítica, Azure Data Factory, Extraer, transformar y cargar (ETL), Minería de datos, Python y SQL, Acceso a datos, Azure Databricks y Bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3973123532/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=4SHhvTUMxWKMFl2TgIQrrQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (W2 Only),"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 semanas,"Bellevue, WA","Acerca del empleo
Job Title: Data Engineer 
 Location: Bellevue, WA 98004 
 Job Type: W2 Contract 
 Work Type: Hybrid 
   Job Description: 
   This is an Onsite interview and need to take a basic test assessment on Python and SQL before submitting the profile. 
   Primary responsibilities :
   Design, implement, and support an analytical data infrastructure providing ad hoc access to large datasets and computing power 
  Managing AWS resources including EC2, RDS, Redshift, etc 
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies 
Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency 
Collaborate with BIEs to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation 
Collaborate with DS to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learning 
Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers 
AWS technologies such as Redshift, RDS, S3, EMR, Glue, ADP, Hive, Kinesis, SNS/SQS, and NAWS supported streaming services 

   Thanks & ragards 
 Leo

Recruiter

Phone: 972-724-5170 *424

Mail: leo@tekvividinc.com 

Aptitudes y experiencia deseables
Python,SQL,AWS","Almacenamiento de datos, Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Amazon Redshift, Bases de datos y Datasets",Solicitar
https://www.linkedin.com/jobs/view/3982799243/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=xhvSu1VLpXqR6Ssze3QZNA%3D%3D&trk=flagship3_search_srp_jobs,,,,,,,
https://www.linkedin.com/jobs/view/3981416986/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=mEsout4G6qmWXlssNo4vHQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 días,"Marshall Creek, Texas, Estados Unidos","Acerca del empleo
Position Details:
Title: Data Engineer
Location: 1 week onsite in a Month , From Sept 2 weeks onsite , 2 weeks remote ( Westlake, TX )
Long term role with possible extensions
 Location Options - Westlake , TX
 The Expertise and Skills You Bring
B.S. degree in math, statistics, computer science, or equivalent technical field.
6+ years of experience in designing and delivering data lake, data warehouses and reporting platforms.
Strong database knowledge and proven experience with data analysis as well as working familiarity with a variety of databases.
Strong analytic skills related to working with unstructured datasets.
Fluent in SQL, Python, and at least one analytic tool such as OBIEE, Power BI, or Tableau is required.
Solid experience with cloud data warehouse platforms such as Snowflake, Redshift, BigQuery, Synapse, Databricks, etc.
Solid experience with Data warehousing, Extract, Transform & Load development is required.
Data Modeling skills and experience with relational and dimension models is required.
Experience with Cloud technologies such as AWS or Azure.
Experience in DevSecOps standard and tools like Jenkins Core and GitHub.
Experience with API architecture design and development.
Solid experience working in a reciprocal, team-based environment with a constant focus on learning, mentoring, and encouraging others.
Proven understanding of computer science fundamentals, data structures, and algorithms to ensure alignment to data engineering methodologies.
Experience building data pipelines and architectures and optimizing data sets.
Experience of manipulating, processing, and extracting value from multiple large datasets.
Proven understanding of modern data engineering principles.
Effective communication and influencing skills to integrate into an inclusive culture.
Enthusiastic desire to learn new skills and emerging technologies/industry trends quickly.
Experience working with healthcare benefit administration or financial data is a plus.
The Value You Deliver
Collaborating with architecture, security, and risk to build architecturally compliant healthcare analytics platform solutions and implementation.
Partnering with platform squads to define data engineering standards, standard methodologies, and tooling.
Creating data tools for analytics and data scientist teams in building and optimizing healthcare product and service offerings.
Providing engineering thought leadership to advance healthcare benefits and management outcomes and facilitate a cloud-first approach.
Collaborating with other team members and stakeholders to analyze and tackle problems.
Staying ahead of the curve by aligning data engineering strategy, architecture, and security with chapters and COEs.
Dexian is a leading provider of staffing, IT, and workforce solutions with over 12,000 employees and 70 locations worldwide. As one of the largest IT staffing companies and the 2nd largest minority-owned staffing company in the U.S., Dexian was formed in 2023 through the merger of DISYS and Signature Consultants. Combining the best elements of its core companies, Dexian's platform connects talent, technology, and organizations to produce game-changing results that help everyone achieve their ambitions and goals.

Dexian's brands include Dexian DISYS, Dexian Signature Consultants, Dexian Government Solutions, Dexian Talent Development and Dexian IT Solutions. Visit https://dexian.com/ to learn more.

Dexian is an Equal Opportunity Employer that recruits and hires qualified candidates without regard to race, religion, sex, sexual orientation, gender identity, age, national origin, ancestry, citizenship, disability, or veteran status.","Almacenamiento de datos, Analítica, Analítica de datos, Ciencia de datos y Ingeniería de datos, Ciencias de la computación, Datasets, Lagos de datos, Modelado de datos y OBIEE",Solicitud sencilla
https://www.linkedin.com/jobs/view/3818367347/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=sIKK4xtRJHowRlAJmbsKNg%3D%3D&trackingId=npm1Fv%2FKWvqOla%2FEDXAFgA%3D%3D&trk=flagship3_search_srp_jobs,Remote - Data Scientist/Analyst/Engineer(ENTRY LEVEL),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Indianápolis, IN","Acerca del empleo
At SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.

Why Us ? 

SynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.

REQUIRED SKILLS For Java /Software Programmers 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
Highly motivated, self-learner, and technically inquisitive 
Experience in programming language Java and understanding of the software development life cycle 
Project work on the skills 
Knowledge of Core Java , javascript , C++ or software programming 
Spring boot, Microservices, Docker, Jenkins and REST API's experience 
Excellent written and verbal communication skills 

For data Science/Machine learning 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
Project work on the technologies needed 
Highly motivated, self-learner, and technically inquisitive 
Experience in programming language Java and understanding of the software development life cycle 
Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis 

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 

Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube

https://www.youtube.com/watch?v=OAFOhcGy9Z8 

https://www.youtube.com/watch?v=EmO7NrWHkLM 

https://www.youtube.com/watch?v=NVBU9RYZ6UI 

https://www.youtube.com/watch?v=Yy74yvjatVg 

SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube

For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ 

We are looking for the right matching candidates for our clients 

Please apply via the job posting 

REQUIRED SKILLS For Java /Full Stack/Software Programmer 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
Highly motivated, self-learner, and technically inquisitive 
Experience in programming language Java and understanding of the software development life cycle 
Project work on the skills 
Knowledge of Core Java , javascript , C++ or software programming 
Spring boot, Microservices, Docker, Jenkins and REST API's experience 
Excellent written and verbal communication skills 

For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
Project work on the technologies needed 
Highly motivated, self-learner, and technically inquisitive 
Experience in programming language Java and understanding of the software development life cycle 
Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow 

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Análisis predictivo, Ciencia de datos, Programación, Reconocimiento de patrones y Visualización de datos, Ciencias de la computación, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3974929678/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=dCPKIWT5fX1tHiG12itY0w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,Estados Unidos,"Acerca del empleo
Position Title: Data Engineer II

Salary Range: $92,000-$105,800

Department: Information Technology

Reports to: Data Engineering Manager

Location: Remote

Schedule: (Overnight and Weekend)

Formerly the Mental Health Association of New York City (MHA-NYC), Vibrant Emotional Health’s groundbreaking solutions have delivered high quality services and support, when, where and how people need it for over 50 years. Through our state-of-the-art technology-enabled services, community wellness programs, and advocacy and education work, we are building a society in which emotional wellness can be a reality for everyone.

Position Summary

The Data Engineer II is responsible for the development of data pipelines, the orchestration and planning of data transformations and the development and support of data automations. This role interfaces with the Analytics and Research team around the design of data points, measures, and the implementation of models; with Data Governance roles around the alignment of policy and definitions with their implementation; and with Dev Ops, Security and Software Development teams around the broader organization’s use of data infrastructure. This role focuses on providing weekend and overnight support to key stakeholders.

Duties/Responsibilities

Develop patterns for data ingestion using Fivetran, AWS Lambda, and related technologies.
Orchestrate data transformations using DBT, database functions, materialized views and similar patterns.
Write tests, integrity checks, conduct performance monitoring and tuning of data systems.
Write anomaly detection routines to support alerting and monitoring.
Design and develop secure, high performance, API’s to support data requests.
Establish patterns for automation of forecasts, scoring, and support of ML/AI implementations.
Establish patterns for publishing and distributing data sets.
Establish and maintain archiving and retention schedules.
Ensure data systems conform with data governance and data security best practices.
Support automated reporting functions.
Deploy and support reporting platform.

Required Skills/Abilities

Extensive experience building and supporting data infrastructure.
Direct experience working with Snowflake, DBT and Fivetran.
Strong experience with AWS services such as Data Migration Services, RDS, Lambda, API Gateway, S3, etc.
Expert knowledge of SQL / PSQL.
Ability to code in Python and R (additional languages a +)
Strong track record of managing high availability systems.
Initiative to solve complex problems; takes an outside in perspective to identify innovative solutions.
We value candidates who have demonstrated commitment to the goal of working with people to achieve mental and emotional wellbeing with dignity and respect.

Required Qualifications

Bachelors’ or Masters’ Degree in an analytics or engineering focused discipline or equivalent experience and knowledge.
1+ years of data engineering experience developing and maintaining high availability systems.

Excellent comprehensive benefits, including medical, dental, vision, supplemental income insurance, pre-tax transit/parking, pre-tax FSA for medical and dependent care, and 401K available. 4 weeks’ vacation, plum benefits, etc.

Studies have shown that women and people of color are less likely to apply for jobs unless they believe they are able to perform every task in the job description. We are most interested in finding the best candidate for the job, and that candidate may be one who come from a less traditional background. Vibrant will consider any equivalent combination of knowledge, skills, education and experience to meet minimum qualifications. If you are interested in applying, we encourage you to think broadly about your background and skill set for the role.

Vibrant Emotional Health is an equal opportunity employer. Applicants are considered for positions without regard to veteran status, uniformed service member status, race, creed, color, religion, gender, gender identity, sex, sexual orientation, citizenship status, national origin, marital status, age, physical or mental disability, genetic information, caregiver status or any other category protected by applicable federal, state or local laws.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Bases de datos, Materialized Views y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3984137962/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=ZdcWFP%2BFu%2FYJem4OEkcj2w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 4 días,"Westborough, MA","Acerca del empleo
Description

Amazon Robotics develops state-of-the art robotics for Amazon’s Fulfillment Centers, which handle more individual items than any company in the world. We are combining computer vision, mobile robots, advanced end-of-arm tooling and high-degree of freedom movement to solve real-world problems at huge scale. Our team collaborates with a variety of customers across Amazon worldwide to conceive, develop, prototype and deploy a wide range of robotic systems. Within Amazon Robotics, Global Operations - Artificial Intelligence (GO-AI) enables Amazon to accelerate and scale our machine learning transformation via human-in-the-loop support. GO-AI’s Technology & Development Team is a cross-functional team of Data Engineers, Software Developers, Technical Program Managers, and Business Intelligence Engineers who are responsible for the technical strategy and delivery for GO-AI.

Key job responsibilities

 Design, implement, and deploy GO-AI data solutions that are high quality (e.g., secure, testable, maintainable, low-defects, efficient, well documented etc.) and incorporate best practices.
 Focus on operational excellence, constructively identifying problems and proposing solutions, taking on projects that improve GO-AI’s data architecture, making it better and easier to maintain.

A day in the life

Amazon offers a full range of benefits that support you and eligible family members, including domestic partners and their children. Benefits can vary by location, the number of regularly scheduled hours you work, length of employment, and job status such as seasonal or temporary employment. The benefits that generally apply to regular, full-time employees include:

 Medical, Dental, and Vision Coverage
 Maternity and Parental Leave Options
 Paid Time Off (PTO)
 401(k) Plan

If you are not sure that every qualification on the list above describes you exactly, we'd still love to hear from you! At Amazon, we value people with unique backgrounds, experiences, and skillsets. If you’re passionate about this role and want to make an impact on a global scale, please apply!

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines
 Have a solid understanding of data design approaches and how to best use them
 Bachelor's degree in engineering, computer science or relevant field

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, FireHose, Lambda, IAM roles and permissions, Athena, RDS, DynamoDB, and AWS Elastic Search
 Experience with relational and non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)
 Experience in object oriented scripting languages like Python, SQL, typescript, and Java
 Experience writing REST APIs
 Experience working with cross-functional engineering teams developing artificial intelligence, machine learning, and/or robotic solutions

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.


Company - Amazon.com Services LLC - A57

Job ID: A2710370","Arquitectura de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Ciencias de la computación, Modelado de datos, Resolución de problemas y TypeScript",Solicitar
https://www.linkedin.com/jobs/view/3961462286/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=%2BI5yU%2BDrPcOBA8HMpZRxEQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Newark, CA","Acerca del empleo
This inclusive employer is a member of myGwork – the largest global platform for the LGBTQ+ business community.  

Experience Level: Experienced Hire

Categories

Engineering & Technology

Location(s):

Remote - United States, US
Remote - United States, US
Remote - United States, US
7 World Trade Center, 250 Greenwich Street, New York, New York, 10007, US
7575 Gateway Blvd., Suite 300, Newark, California, 94560, US
Remote - United States, US
Remote - United States, US

At Moody's, we unite the brightest minds to turn today's risks into tomorrow's opportunities. We do this by striving to create an inclusive environment where everyone feels welcome to be who they are-with the freedom to exchange ideas, think innovatively, and listen to each other and customers in meaningful ways.

If you are excited about this opportunity but do not meet every single requirement, please apply! You still may be a great fit for this role or other open roles. We are seeking candidates who model our values: invest in every relationship, lead with curiosity, champion diverse perspectives, turn inputs into actions, and uphold trust through integrity.

The Compliance & 3rd-Party Risk unit at Moody's leverages data-driven insights and cutting-edge technologies to help businesses deliver prudent risk management in the KYC, Supply Chain, and Trade Credit space. We are currently seeking a Data Engineer to join our dynamic and motivated Data & Analytics team. The ideal candidate will have a strong foundation in data engineering concepts such as pipelines, orchestration and streaming; as well as a passion for working with cutting-edge technologies to ensure clean, quality, and usable data is democratized to the business.

In this role, you will be highly involved working on creating foundational data models and data products within our data warehouse which supports various business and product reporting needs. In your day-to-day you will be working in a squad consisting of data engineers, analysts, software engineers, and product managers.

The Work

Develop and maintain product-specific data pipelines to ensure the timely and accurate flow of data from various sources to our Databricks platform.
With the guidance of technical peers, learn to work with business and other cross-functional team members to identify, integrate, and distribute data data from sources to destinations.
Work on project squads adhering to project management best practices and participating in scrum ceremonies
Gain an understanding of data usage patterns and requirements within the organization to design and implement appropriate data models in the data warehouse, ensuring compliance with industry regulations.
Utilize Databricks and AWS/GCP services to manage, store, and process data, ensuring optimal performance, security, and scalability while adhering to data protection standards.
Leverage orchestration tools, such as Databricks Workflows or Airflow, to orchestrate and monitor data pipelines, ensuring seamless integration, data quality, and compliance with regulatory requirements.
Implement data transformation and validation processes using dbt and SQL to ensure data accuracy, consistency, and regulatory compliance across the organization.
Continuously monitor, troubleshoot, and optimize data pipelines to ensure data reliability, performance, and adherence to best practices and industry regulations
Stay current with industry trends, emerging technologies, and regulatory changes to ensure the company remains at the forefront of data engineering best practices in the compliance and AML space.

Qualifications

0-2+ years of experience in data, or backend engineering roles
Bachelor's degree in Computer Science, Engineering, or a related field.
Some understanding of data engineering concepts such as orchestration & pipelining, data modeling, streaming and warehousing
Excellent problem-solving skills and the ability to work independently or as part of a team.
Eagerness to learn new technologies and adapt to industry trends and changes

For US-based roles only: the anticipated hiring base salary range for this position is $106,600.00 - $154,600.00, depending on factors such as experience, education, level, skills, and location. This range is based on a full-time position. In addition to base salary, this role is eligible for incentive compensation. Moody's also offers a competitive benefits package, including not but limited to medical, dental, vision, parental leave, paid time off, a 401(k) plan with employee and company contribution opportunities, life, disability, and accident insurance, a discounted employee stock purchase plan, and tuition reimbursement.

Moody's is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody's also provides reasonable accommodation to qualified individuals with disabilities or based on a sincerely held religious belief in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications.

For San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance.

This position may be considered a promotional opportunity, pursuant to the Colorado Equal Pay for Equal Work Act.

Click here to view our full EEO policy statement. Click here for more information on your EEO rights under the law. Click here to view our Pay Transparency Nondiscrimination statement. Click here to view our Notice to New York City Applicants.

Candidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody's Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.","Airflow, Almacenamiento de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Modelado de datos, Modelo de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3930193076/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=4WL8xrlxQDcDP1PqS8clUQ%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer - Life Sciences - Python, SQL, ETL - Professional Services","En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,"Boston, MA","Acerca del empleo
Who We Are

TetraScience is the Scientific Data and AI Cloud company with a mission to radically improve and extend human life. TetraScience combines the world's only open, purpose-built, and collaborative scientific data and AI cloud with deep scientific expertise across the value chain to accelerate and improve scientific outcomes. TetraScience is catalyzing the Scientific AI revolution by designing and industrializing AI-native scientific data sets, which it brings to life in a growing suite of next generation lab data management products, scientific use cases, and AI-based outcomes. For more information, please visit tetrascience.com.

Our core values are designed to guide our behaviors, actions, and decisions such that we operate as one. We are looking to add individuals to our team that demonstrate the following values:

Transparency and Context- We execute on our ambitious mission by starting with radical data transparency and business context. We openly and proactively share all vital data and make it actionable, so our employees and stakeholders can solve any problem presented to them
Trust and Collaboration- We are committed to always communicating openly and honestly at every level of the organization, functionally, cross-functionally, internally, and externally. Empowering our employees will drive positive change across our entire ecosystem
Fearlessness and Resilience- We must be fearless and resilient to fulfill our potential. We proactively run toward challenges of all types, we unblinkingly acknowledge and confront the brutal facts - which all innovative growth companies invariably face - and we embrace uncertainty and take calculated risks
Alignment with Customers- We know that our customers' success is our success. We are honored and humbled by their commitment to us, and we are completely committed to ensuring they achieve their mission to unlock the world's most important scientific innovations
Commitment to Craft- We take our craft seriously and seek to be best-in-class in all we do, regardless of our functional role, seniority, or tenure. We are members of one team that combines intellectual horsepower and curiosity, humility, and empathy to ensure we are always learning and evolving
Equality of Opportunity- We cannot imagine our journey without a workforce which reflects humanity's diversity. We seek out the best of the best who bring with them unique and invaluable perspectives and talents and embody our common values - regardless of gender, ethnicity, race, or age


What You Will Do

Own, prototype, and implement customer solutions
Research and prototype data acquisition strategy for scientific lab instrumentation
Research and prototype file parsers for instrument output files (.xlsx, .pdf, .txt, .raw, .fid, many other vendor binaries)
Design and build data models
Design and build Python data pipelines, unit tests, integration tests, and utility functions
Build visualization, report, and dashboards using Spotfire, Tableau, Jupyter notebook and etc
Work with the customer to test and make sure the solution fulfills their requirements and solves their need
Coordinate project kickoff meetings; manage the customer relationship throughout the project, and conduct formal project closeout meetings
Facilitate internal project post-mortems to identify areas of improvement on the next implementation


Requirements

What You Have Done

>5 years in Python and SQL
Passionate about science and building solutions to make the data more accessible to the end-users
Elasticsearch, science background, or experience with scientific instruments
Experience with tools like Spotfire, Tableau, Jupyter notebook (any of them)
Undergraduate or graduate degree in chemistry, biology, computer science, statistics, public health, etc
Excellent communications skills, attention to details, and the confidence to take control of project delivery
Quickly understand a highly technical product and effectively communicate with product management and engineering
Strong project, account management, and proactive problem-solving skills
High-bandwidth: thrives when managing multiple simultaneous projects
Intellectually curious: Unwavering drive to learn and know more every day
Ability to think creatively on how to solve projects risks without reducing quality
Team player and ability to ""roll up your sleeves"" and do what it takes to make the team successful


Benefits

100% employer-paid benefits for all eligible employees and immediate family members
Unlimited paid time off (PTO)
401K
Flexible working arrangements - Remote work + office as needed
Company paid Life Insurance, LTD/STD

We are not currently providing visa sponsorship for this position


Aptitudes y experiencia deseables
AWS
Biotech
Data Engineering
Data Integration
Data Science
Pharmaceutical
Professional Services
Python
cloud computing","Canalizaciones de datos, Ingeniería de datos , Jupyter y Python, Cierre de proyectos, Control de proyectos, Elasticsearch, Modelo de datos, Resolución de problemas y Spotfire",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979169479/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=iaDZNbUQY8H6IqdVoZ40cg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 1 semana,"Tempe, AZ","Acerca del empleo
Description

Data Engineer

Come build the future as a Data Engineer at Amazon, where you will be inspired working along best-in-class inventors and innovators! You will have the opportunity to create meaningful experiences that deliver on the ever-evolving needs of our customers, and your work will impact millions of people around the world.

As an Amazon Data Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. You will find that there is an unlimited number of opportunities within Amazon, where developing your career across a wide range of teams is highly supported. We are committed to making your work experience as enjoyable as the experiences you’ll be creating for our customers.

Apply now and you will be eligible for Amazon Data Engineer positions that are based on your preferred location, team, and more. We’re hiring across Amazon Stores in the United States and Canada.

Teams with available positions include, but are not limited to:

 Consumer Technology: Build new generation features and products for amazon.com, constantly improving the Customer and Seller experience for billions around the globe. Whether building site wide features such as reviews and recommendations, category specific software for the likes of Pharmacy, Electronics, Digital Software and Video Games or seller infrastructure, there are a variety of complex problems to tackle using a range of technologies in the design of your technical solutions.
 Operations Technology: Shape the future of transportation planning and execution on a global scale, that impacts hundreds of fulfillment centers, thousands of Amazonians, and millions of customers across the world. Your technology will support thousands of operators worldwide to design, build and run the best-in-class Amazon transportation network. We are building intelligent software to make transportation more reliable, faster, and less costly, providing a better and less expensive experience for our customers.
 Human Resources Technology: Create a seamless experience for millions of Amazonians and/or candidates. Whether supporting technologies for onboarding, time and attendance, compensation, amazon.jobs, or recruiting, you’ll deliver robust feature sets, elegant designs, intuitive user interfaces and systems that make it easy for Amazonians to excel at performing critical business functions.

About Us

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance be-tween your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign opportunities based on what will help each team member develop into a better-rounded contributor.

Inclusive Team Culture

Here at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Key job responsibilities

 Design, implement, and support a platform providing secured access to large datasets.
 Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
 Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
 Tune application and query performance using profiling tools and SQL.
 Analyze and solve problems at their root, stepping back to understand the broader context.
 Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
 Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
 Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
 Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com LLC - A03

Job ID: A2703397","AWS Lambda, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Datasets, Modelado de datos y Object Storage",Solicitar
https://www.linkedin.com/jobs/view/3932049861/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=XF%2BnH%2BdweWu0HfkiR9Y7lg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Miami, FL","Acerca del empleo
We're hear.com, a tight-knit crew of tech enthusiasts, creators, and innovators dedicated to shaking things up in hearing care. We're not your average company—we thrive on creativity, collaboration, and a passion for making a real impact. Forget the corporate jargon and stiff office culture; we believe in flexibility, growth, and always having fun while doing what we love.

As a Data Engineer at hear.com you will play a pivotal role in architecting, building, and optimizing our data infrastructure. You will collaborate with a cross-functional team of experts to design and implement robust, scalable, and high-performance data solutions that empower our decision-making and contribute to our mission of providing better hearing care.

What You’ll Do

Design and implement scalable and reliable data pipelines, incorporating streaming data processing with Apache Pulsar to handle real-time data efficiently.
Optimize Data Systems: Work closely with peers, analysts, and product teams to improve data reliability, efficiency, and quality.
Innovate: Stay ahead of the curve by researching and implementing cutting-edge technologies that can enhance our data capabilities and leverage our business.
Data Governance and Strategy: Play a key role in defining and implementing data governance and security policies to ensure data integrity and compliance.

Technology Stack

Languages & Frameworks: Python, SQL
Data Warehousing & Streaming: Snowflake, Pulsar, Kinesis
Workflow Management: Airflow
Databases: PostgreSQL, MongoDB
Containers & Orchestration: Docker
Infrastructure as Code: Terraform
Cloud: AWS

What You’ll Need

Experience: 5+ years of experience in data engineering with a proven track record of building and optimizing data systems.
Technical Expertise: Strong programming skills in Python and SQL. Hands-on experience with our technology stack is highly desirable.
Problem-Solving Skills: Ability to tackle complex data challenges and deliver innovative solutions.
Team Player: Excellent communication skills and the ability to work effectively in a collaborative environment.
Continuous Learner: Passion for learning and adapting to new technologies and methodologies.

Why You’ll Love Working With Us

Global Vision and Growth: Be part of a company with a long-term vision and robust growth trajectory.
Customer Impact: Work with happy and grateful customers every single day.
Creative Environment: An open-minded and international working environment that fosters creativity.
Growth Mindset: Access to courses, conferences, and more to support your continual learning and development in an innovative, fast-moving company.
Autonomy and Responsibility: Enjoy a high degree of autonomy and responsibility from day one.
Unique Culture: Innovative, driven, and family-like work culture.
Hybrid Schedules: Enjoy the flexibility of working 2 days remotely and 3 days in our beautiful office in Coral Gables, Florida or Denver, Colorado.
Excellent Benefits and Compensation: Full medical, dental, vision, open PTO, paid company holidays and sick time, paid parental leave, and matching 401K program. Base salary range $140k-$150k, dependent on experience. 
Great Perks: Weekly lunch, a fully stocked kitchen, and a small, close-knit team that values ownership and collaboration.

At hear.com, we're not just shaping the future of hearing care; we're redefining it. Join us in our mission to help everyone hear well to live well.","Ciencia de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Comunicación, Resolución de problemas y Sistemas de datos",Solicitar
https://www.linkedin.com/jobs/view/3964649530/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=odGl88MrjbgWa7P65z55WQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,"Seattle, WA","Acerca del empleo
Atimi seeks an experienced data engineer to fill a contracting position in Seattle, U.S.A. This position is on-site at the client offices. Atimi works with some of the leading companies in North America, providing them with high-quality software solutions that integrate both mobile and web experience. If you are a creative, self-motivated individual with vast user experience working with complex problems, Atimi is the place for You. We are looking for an established leader in the domain with solid experience in software development principles, data engineering, hands-on knowledge of the latest cloud technologies, business intelligence, and soft skills. You work well with colleagues, partners and clients and have excellent communication skills.

Responsibilities

Working closely with stakeholders to understand their requirements, design and implement the right solution
Work closely with other teams, analyze source systems, define underlying data sources and transformation requirements, design suitable data models and document the design/specifications
Implementing data models and data mining solutions
Designing and implementing an ETL and data cleansing solutions
Build and maintain the infrastructure to answer questions with data, using software engineering best practices, data management fundamentals, data store principles
Delivering fair outcomes for our customers, ensuring conduct maintains a high level of expertise


Requirements

Basic qualifications

Bachelor's degree in Computer Science, Engineering, Math, Finance, Statistics or related discipline from a recognized university
5+ years of relevant experience in a business intelligence role, including data warehousing and business intelligence tools, techniques and technology
3+ years of experience in analytics, business analysis or comparable consumer analytics solutions
Expert knowledge of SQL (DQL and DDL)
Expertise in very large Data Warehousing and Online Analytical Processing 
Expertise in AWS Technology stack (Redshift, RDS, S3, EMR)
Familiarity with Linux scripting (bash, python, javascript)
Solid experience with business intelligence reporting tools (Tableau, OBIEE, Cognos, MicroStrategy, SSAS Cubes)
Hands-on experience with recent advances in distributed computing such as MapReduce, MPP architecture and NoSQL databases
Experience with Hadoop, Hive, Spark, EMR
Expertise in data warehouse design
Expertise in relational database design 
Expertise in data mining algorithms 
Expertise in data cleansing
Experience working directly with clients and other project stakeholders to define and refine requirements
Strong English skills (written and verbal.)


The position is open to anyone, but you must be located in Seattle, WA, U.S.A. Relocation support is not provided.

Please submit your resume and cover letter for review. All applications will be reviewed, but only those who are able to demonstrate the right skills will be contacted for a remote interview.

The successful candidate will be required to work from office at least three days every week.

Benefits

About Atimi Software

Hello, we're Atimi. If you've got a smartphone or a computer, you've seen our work. You may not know our name, but you use our software - whether it's on Apple or Android devices, you're already familiar with what we do. You just don't know it yet. We work with high-profile companies that want to extend their brand reach. Our clients hire us to do the flagship work for major brands. We know what it takes to get noticed: over 60% of our apps have been featured by Apple in TV ads, iTunes advertising, and in-store or in-print ads. We work with Fortune 500 companies who want to be recognized for being innovative and want to ensure a true brand experience at every customer touch-point.

Fundamentally, Atimi believes in compensating people based on the value they provide. All of us are evaluated on the core skills we are able to demonstrate when doing our job. Once you demonstrate new skills, there's no reason that shouldn't be recognized. We want to provide developers with fast-moving, cutting-edge projects where everybody has a voice, and nobody is concerned with ego or internal politics, so all of us are challenged to improve constantly.

About The Interview Process

The interview process for this position involves multiple stages that cover:

Communication and soft-skills skills evaluation, technical skills evaluation, and live practical exercise 
Cultural fit with other team members and final approval by the client","Almacenamiento de datos, Analítica de datos, Hive y MapReduce, Cubes, Limpieza de datos, Modelado de datos, Modelo de datos, OBIEE y SQL Server Analysis Services (SSAS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982339990/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=l2EJL7vjUdncTeKPCbSvEA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer_W2_Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 6 días,Estados Unidos,"Acerca del empleo
Location

REMOTE,

Required Skills

4+ years of enterprise data warehouse development with SQL, Python, Azure Data Factory (ADF), Azure Databricks, Kafka, and ideally Snowflake experience

Additional Skills

Job Description

Looking for a Data Engineer with SQL, Python, ADF, Databricks, Kafka and ideally Snowflake experience to work 100% remote.

Data Engineer to develop CI/D data pipelines and ETL processes to curate and transform pharmacy data from different sources (both on premise and cloud).
Build large scale databases that are robust and secure to be stored in a data lake or data warehouse.
Automate data workflows and processes including ingestion, cleaning, structuring, formatting of data.
Build engineering solutions that support ML/data science projects.
Collaborate with Data Scientists and business partners to deploy machine learning models in production (preferred).
Extensively work on different kinds of datasets including text, voice, images, unstructured, structured.

Requirements

4-6 years of enterprise data warehouse development - preferably on SQL or Snowflake
4-6 years of experience creating, enhancing, and maintaining ETL frameworks using ETL tools.
Hands-on experience with SQL, PL/SQL, Python and/or Shell Scripting is a must.
Experience migrating from RDBMS to Snowflake is a plus.
End-to-end dataflow design and development experience is a plus.
Hands-on experience with Azure Cloud, including Azure Data Factory and Databricks.
Kafka experience to setup/attach to Databricks.

 ","Almacenamiento de datos, Azure Data Factory, Extraer, transformar y cargar (ETL), Herramientas ETL, Lenguajes de programación, PL/SQL y SQL, Azure Databricks, Guiones shell y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3941711036/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=tAZGeWnT%2FqL6Qb5SDMAvig%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Dallas, TX","Acerca del empleo
Overview
We are accepting applications for the Data Engineer positions to work on a product that helps improve the health of our population.
Do you enjoy working with leading-edge technologies?
Be part of a fast-moving, fast-growing, innovative technology team.
Directly improve the daily lives of tens of thousands of our neighbors?
If so, please contact us!

Who We Are
Leap Metrics is headquartered in Richardson, TX in the Dallas/Fort Worth Metroplex. We are a healthcare analytics and care management software company founded to improve health outcomes, lower the cost of care, and improve regulatory compliance. Our software platform is called Sevida which stands for “In the service of human lives”. We are on a mission to leverage technology to serve populations with chronic healthcare needs.
We are a company of experienced technologists; a passionate dedicated group of developers, designers, client advocates, and entrepreneurs. Our team has some of the best enterprise software minds in the technology industry.
To learn more about us, please visit our website at http://leapmetrics.io

Why Work for Leap Metrics?
MEANINGFUL MISSION – Empowering health providers to focus on care
FULFILLMENT – Make an impact by building a meaningful product
LEADING EDGE TECHNOLOGY – Advance your technical skills by working with the latest and most innovative technology.
COMPANY CULTURE: - team-oriented, collaborative, supportive, innovative
REMOTE WORK – Work from anywhere in the US.

Job Description
Work with seasoned technology entrepreneurs
Develop effective ETL/ELT pipelines
Data Modeling and building advanced queries
Analyze, design, develop, and maintain the application
Develop and commit organized and well-structured code
Automate data pipelines
Assist with the management of data infrastructure
Conduct rigorous testing and identify and debug issues
Test, troubleshoot, and debug pipelines
Write technical documentation as required
Product support

Qualifications
Experience with Python programming
Knowledge of data visualization tools like Sigma Computing, PowerBI, Tableau, or equivalent
Experience in Machine Learning is a plus
Strong understanding of development Tools such as Git and Jira
Creating appropriately detailed documentation
Working as an individual contributor and as a member of a high-performing team
Working creatively and analytically in a problem-solving environment
Thinking clearly under pressure to solve problems and deliver solutions
Experience working with teams distributed globally
Excellent communication and teamwork skills
Great attention to detail
MS or BS in computer science, engineering, or equivalent experience in a related field (or experience gained as a self-taught hacker)
Pursuing MS or BS in computer science, engineering, or equivalent experience in a related field (or experience gained as a self-taught hacker)

Nice to have
Exposure to ETL/ELT pipelines
Knowledge of Snowflake or equivalent Big Data technologies
Knowledge of Apache Airflow and DBT
Experience writing and consuming GraphQL APIs
Exposure to Google Cloud

Job Type
Full-time

Location
US only - Dallas/Ft. Worth Metroplex
US only - Hybrid

Benefits
Flexible schedule
Experienced mentors
Professional development assistance

Other
Applicants for employment in the US must have work authorization that does not require sponsorship of a visa for employment authorization in the United States.","Extraer, transformar y cargar (ETL), Inteligencia empresarial, Python y SQL, Apache Airflow, Data Build Tool (DBT), Modelado de datos y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3946959130/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=yfOwAC3qL9p%2FMx9BZqfsKg%3D%3D&trk=flagship3_search_srp_jobs,Finance Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 4 días,"Seattle, WA","Acerca del empleo
Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

The Opportunity 

This is an exciting opportunity to join the Digital Media Finance team as we continue to propel the business through data-driven forecasts and provide influential insights. In this role, you will have the opportunity to help drive Adobe’s pivotal initiatives by data engineering solutions that connect systems to visualization platforms for management reporting. The position will report to the Group Manager, Finance Automation to drive our systems and automation effort. This position will provide an outstanding opportunity to drive timely insights across Digital Media (DMe) Business at Adobe and help drive the Finance Transformational efforts undertaken by the Finance Automation team.

The ideal candidate for this role is a well-rounded top performer with exceptional data engineering, analysis, and visualization skills, capable of driving growth in a fast-paced environment. You should possess a continuous learning mentality to explore and implement innovations in the team's reporting and transformational projects. Strong cross-functional collaboration within Adobe is essential. You will be responsible for automating complex financial models, developing engaging and interactive dashboards, streamlining processes, and enhancing efficiencies in management reporting

What you'll do:

Responsible for the strategy and execution of DMe lifecycle of financial and analytics dashboards and ensuring scalability of systems and processes. 
Manage data transitions and platform migrations between SAP HANA/DataBricks, Tableau/PowerBI, and related ETL processes. 
Develop data pipelines, connections, and infrastructure to better enable forecasting and data science modelling. 
Correct data errors via systematic, logical fixes and provide updates within the data pipelines, connections and infrastructures that are built to support the teams. 
Create documentation and support enablement for the teams who are interested and able to help themselves. 
Accountable for ad-hoc support & participation in critical business analytics and key project support as directed by management. 
Partner closely with IT, Finance Systems, Finance Transformation Office, and other Finance counterparts to continually improve, streamline & enhance planning and reporting processes. 

What you need to succeed:

BS/BA with preferred focuses in areas of Business, Finance, or Information Systems. Master’s in Data Science a plus. 
3+ years of demonstrated experience working with large and complex data structures and develop efficient queries to create calculated fields and data aggregates. 
2+ years of proven experience designing and developing dashboards using PowerBI or Tableau. 
3+ years of relevant experience in data science and analytics in creating/maintaining financial models for a subscription/SaaS business a plus. 
Proficient in data platforms/systems (such as SQL, Databricks), ETL tools (such as Python, SnapLogic), process automation, and standardization. 
Self-starter with high attention to details, excellent interpersonal skills, and ability to take charge, set objectives, and deliver results. 
Strong project management skills with ability to juggle multiple priorities. 
Strong team orientation and a learning mentality. 

Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $73,900 -- $170,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.

Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances.

Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.

Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.

Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, Automatización, Habilidades sociales, Panel de control y SAP HANA",Solicitar
https://www.linkedin.com/jobs/view/3982683008/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=L0M8aWxvudwrjadOp4ydeA%3D%3D&trk=flagship3_search_srp_jobs,BigQuery Consultant/ Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 día,"Dallas, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
POSITION

BigQuery Consultant/ Data Engineer

LOCATION

Hybrid Dallas, TX

DURATION

Long term contract

INTERVIEW TYPE

Video

VISA RESTRICTIONS

None

Required Skills

Data Engineer
Google Stack (Google analytics)
BigQuery
Python
SQL
Senior-Level Expertise: Extensive experience and ability to teach and train team members.
Google Analytics (GA): Experience with GA data stored and analyzed in BigQuery.
BigQuery: 4-5 years of hands-on experience working with BigQuery.
Visualization Tools: Proficiency with PowerBI or other data visualization tools.
SQL and Python: Strong skills in SQL and Python for data processing and analysis.
Google data is from web traffic and should have b2b or b2c (ecommerce or retail background)

Cloud BC Labs Inc is a digital transformation organization aimed at creating seamless solutions for clients to effectively manage their business operations. The company specializes in Business and Management Consulting, AI/ML, Data Analytics & Visualization, Cloud Data Warehouse Migration, Snowflake Implementation, Informatica Implementation & Upgrade, Staffing Services and Data Management Solutions","Analítica, Analítica de datos, Ciencia de datos, Google Analytics, Google BigQuery, SQL, Visualización y Visualización de datos, Análisis web y Web Traffic",Solicitar
https://www.linkedin.com/jobs/view/3973764935/?eBP=BUDGET_EXHAUSTED_JOB&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=jb%2BKBsqFuKNSec3AOKSB2A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Analyst,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 4 días,"Madison, WI","Acerca del empleo
Work Schedule

Standard (Mon-Fri)

Environmental Conditions

Office

When you’re part of a team at Thermo Fisher Scientific, you provide important work with real-world impact; and be enthusiastically supported in achieving career goals. Employees are valued and recognized for their performance, all while collaborating with engaged and dynamic teammates. This role supports our Biosciences division, which represents over $4B in sales annually.

We are looking for an experienced data engineer/analyst to support end-to-end data analytics projects for the divisional marketing team. Are you astute at simplifying sophisticated data with outstanding communication skills?

Location:

Hybrid role - 3 days a week in a local Thermo Fisher Scientific office

Primary responsibilities:

Leverage data integrations using systems such as AWS Athena, Redshift, Cognos, SharePoint and Power BI Services among other technologies

Propose data development optimizations and implement to deliver solutions and enhanced reporting capabilities

Provide timely and consistent updates and recommendations on dataflow and data pipeline operational issues and improvements to team and partners

Become a domain authority of data sources used by the Analytics team (Sales Force, Adobe Analytics, Redshift, internal databases, etc.) along with reports built

Consult data source owners to understand and detail data collection and reporting processes

Champion data quality and integrity through proactive sleuthing and oversight

Establish alerts for data connection issues both technical and beyond encouraged thresholds (anomalies)

Influence and maintain strong and balanced team culture through active involvement, inclusion, and leadership!

Proactive self-education and contribution to advance our analytics practices, methods, and strategy

Experience requirements:

 Experience in taking requirements and understanding the needs of the business to tackle problems
 Experience in the design and implementation of the different data processes like profiling, exploration, collection, processing, cleaning, and preparation of data, ETL, ELT, ETLT data processes and data pipelines with Python, SQL, Non-SQ, MS Power BI Services and Power Query
 Experience in semantic data models and schemas like flat, star and snowflake schemas
 Experience in crafting business intelligence reports/dashboards from scratch
 Strong graphics and visualization aesthetics to convey messages simply and impact-fully

Minimum Qualifications and Experience:

 Bachelor’s degree in the sciences or engineering

5+ years experience in a data analytics or engineering role with a solid understanding of technical, business, and operational process

Proven ability with Power BI Desktop and Service (or similar business intelligence and data visualization tool), M, Python and SQL desirable

Advanced MS Excel

Experience with GitHub and agile scrum

At Thermo Fisher Scientific, each one of our 90,000 extraordinary minds have a unique story to tell. Join us and contribute to our singular mission—enabling our customers to make the world healthier, cleaner and safer.

Thermo Fisher Scientific is an EEO/Affirmative Action Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status. We will make reasonable accommodation as required.

Compensation And Benefits

The salary range estimated for this position based in New York is $75,800.00–$120,000.00.

This position may also be eligible to receive a variable annual bonus based on company, team, and/or individual performance results in accordance with company policy. We offer a comprehensive Total Rewards package that our U.S. colleagues and their families can count on, which includes:

A choice of national medical and dental plans, and a national vision plan, including health incentive programs
Employee assistance and family support programs, including commuter benefits and tuition reimbursement
At least 120 hours paid time off (PTO), 10 paid holidays annually, paid parental leave (3 weeks for bonding and 8 weeks for caregiver leave), accident and life insurance, and short- and long-term disability in accordance with company policy
Retirement and savings programs, such as our competitive 401(k) U.S. retirement savings plan
Employees’ Stock Purchase Plan (ESPP) offers eligible colleagues the opportunity to purchase company stock at a discount

For more information on our benefits, please visit: https://jobs.thermofisher.com/global/en/total-rewards","Analítica, Analítica de datos y Extraer, transformar y cargar (ETL), Astute, Calidad de datos, Lenguaje de consulta (query), Modelo de datos, Panel de control, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3984254484/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=2MYIli7MzcmfAZpINKoj7A%3D%3D&trk=flagship3_search_srp_jobs,Big Data Engineer/Developer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Tampa, FL","Acerca del empleo
Role name: Big Data Engineer/Developer

 Location- Tampa, FL

Responsibilities

Develop and maintain data pipelines using ETL processes

Take responsibility for Apache Hadoop development and implementation

Work closely with data science team to implement data analytics pipelines

Help define data governance policies and support data-versioning processes

Maintain security and data privacy, working closely with data protection officer

Analyze vast number of data stores to uncover insights

 Competencies: Digital : Apache Spark, Digital : PySpar

Required Skills And Qualifications.

Experience with Python, Spark, and Hive.

Understanding of data-warehousing and data-modeling techniques.

Knowledge of industry-wide visualization and analytics tools (ex: Tableau, R)

Strong data engineering skills with Azure cloud platform.

Experience with streaming frameworks such as Kafka.

Knowledge of Core Java, Linux, SQL, and any scripting language.

Good interpersonal skills and positive attitude.

Preferred Skills And Qualifications.

Degree in computer science, mathematics, or engineering.

Expertise in ETL methodology for corporate-wide solution design using DataStage.

 Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","Apache Kafka, Apache Spark, Extraer, transformar y cargar (ETL), Hadoop, Hive y Ingeniería de datos, Comunicación interpersonal, Java, Plataforma Java y Secuencia de comandos",Solicitar
https://www.linkedin.com/jobs/view/3980829783/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=DWgDclORRHWHcSKQZvfQ9g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 semana,"Irving, TX","Acerca del empleo
Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.

Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.

Position Summary

Understands the Enterprise data systems and acquires knowledge on the relevant processes need for project delivery.
Participate in project estimation process and provide inputs to Tech Lead.
Participate in Agile scrum activities/project status meetings on regular basis.
Participate in User story grooming/Design discussion with technical lead.
Analyzes complex Data structure from disparate data sources and design large scale data engineering pipeline.
Uses strong programming skills to build robust data pipelines for ETL (Extract / Transform / Load) processes, designs database systems and develops tools for data processing.
Perform all Data Engineering job activities EDW/ETL project development/testing and deployment activities.
Work closely with the developers on the ETL Jobs/Pipelines development.
Create the Project process/automation by integrating the involved components.
Documents data engineering processes, workflows, and systems for reference and knowledge-sharing purposes.
Implements data quality checks and validation processes to ensure the accuracy, completeness, and consistency of the data.
Be a team player and work with team members for Business solution and implementation.


Required Qualifications

1+ years of Experience in executing Data warehousing ETL projects.
1+ years of Experience with Python
1+ years of Experience with SQL
1+ years of hands-on Experience with bash shell scripts, UNIX utilities & UNIX Commands
1+ years of hands-on Experience with a major cloud platform (GCP, AWS, Azure)


Preferred Qualifications

GCP Experience - BigQuery, Cloud SQL, Python, Cloud composer/Airflow , Cloud Storage & Dataflow/Data Fusion
Hands-on experience building and deploying data transformation and processing solutions using Teradata utilities (BTEQ, TPT, FastLoad & SQL Queries).
GCP - Data Engineer certification strongly preferred.
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.
Strong problem-solving skills and critical thinking ability
Strong collaboration and communication skills within and across teams
Knowledge in Flask, JavaScript, HTML , CSS, Django
Knowledge in BI Tools MicroStrategy, Tableau
Must understand software development methodologies including waterfall and agile.
Health Care/PBM domain experience
Excellent communication and presentation skills.


Education

Bachelors Degree in computer science or engineering or equivalent work experience

Masters Degree preferred

Pay Range

The typical pay range for this role is:

$72,100.00 - $175,100.00

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.

For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits

We anticipate the application window for this opening will close on: 07/27/2024

Qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.","Almacenamiento de datos, Bash, Extraer, transformar y cargar (ETL), Google Cloud y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Comunicación, Resolución de problemas y UNIX Utilities",Solicitar
https://www.linkedin.com/jobs/view/3960518012/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=FfSzAQchhxBzXejTiVNVvA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Analyst,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",Publicado de nuevo hace 1 semana,"Atlanta, GA","Acerca del empleo
Job Description

Epsilon Analytics partners with both internal and external clients, and data providers, leveraging various analytics to drive strategic thought and effective decision making. The Data Analyst is responsible for conducting data analyses using SQL and other tools in support of a variety of analytic solutions.

Roles & Responsibilities

Collaborate with internal/external stakeholders to manage data logistics – including data specifications, transfers, structures, and rules.
Access and extract data from a variety of sources of all sizes (including client marketing databases) via SAS Access, SQL, etc.
Master and perform all steps required to create analysis-ready data sets, including data integration/merging, variable preparation, and quality control (QA/QC)
Develop and execute SQL (or related) programs with detailed direction and supervision.
Provide problem solving and data analysis, derived from programming experience.
Demonstrate proficiency with desktop and UNIX toolsets (SAS, SAS ODS, SQL, MS Office) to create pivot tables and/or report content such as tables, reports, graphs, etc. (some positions require proficiency in digital analytic tools including Google and/or Adobe Analytics and familiarity with digital data, in addition to or in lieu of SAS/SQL)
Document and articulate steps taken in an analysis to project managers.
Answer questions about data sets and analyses
Follow all policies and procedures for programming, project documentation, and system management.
Become familiar with…
all offerings outlined in the Insider’s Guide to ACG
various statistical offerings and methods (CHAID, logistic/multiple regression, cluster analysis, factor analysis)
Epsilon data assets
the SAS macro library
Participate in the design, planning & execution of projects
Effectively manage time and resources in order to deliver on time / correctly on a limited number (1-4) of concurrent projects
Proactively communicate with supervisor regarding workload and the status of assignments
Prepare basic report content (Word, Excel, PowerPoint) in support of deliverables
Perform two tasks related to the role of Sr. Data Analyst during the year
Minimum Qualifications

Bachelor’s degree in a quantitative discipline (e.g., Statistics, Economics, Mathematics, Marketing Analytics) or significant relevant coursework
1-2 years of experience in the marketing analytics field
Demonstrated proficiency in SQL programming; minimum 2 years of experience
Strong analytic thought process and ability to interpret findings
Acute attention to detail (QA/QC)
Working knowledge of MS Office; including PowerPoint, Word, Excel and Outlook
Ability to work on multiple assignments concurrently
Excellent verbal and written communication skills
Highly motivated and collaborative team player with strong interpersonal skills
Effective organization and time management skills

Desirable Qualifications

Advanced degree (Master’s/PhD) in Statistics, Economics or other quantitative discipline
Database marketing experience/knowledge
Automotive industry knowledge
Ability to program in newer and emerging languages such as SAS, R, and Python

Additional Information

About Epsilon

Epsilon is a global advertising and marketing technology company positioned at the center of Publicis Groupe. Epsilon accelerates clients’ ability to harness the power of their first-party data to activate campaigns across channels and devices, with an unparalleled ability to prove outcomes. The company’s industry-leading technology connects advertisers with consumers to drive performance while respecting and protecting consumer privacy. Epsilon’s people-based identity graph allows brands, agencies and publishers to reach real people, not cookies or devices, across the open web. For more information, visit epsilon.com.

When you’re one of us, you get to run with the best. For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC

Our Culture https //www.epsilon.com/us/about-us/our-culture-epsilon
Life at Epsilon https //www.epsilon.com/us/about-us/epic-blog
DE&I https //www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR https //www.epsilon.com/us/about-us/corporate-social-responsibility

Great People Deserve Great Benefits

We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.

Epsilon is an Equal Opportunity Employer. Epsilon’s policy is not to discriminate against any applicant or employee based on actual or perceived race, age, sex or gender (including pregnancy), marital status, national origin, ancestry, citizenship status, mental or physical disability, religion, creed, color, sexual orientation, gender identity or expression (including transgender status), veteran status, genetic information, or any other characteristic protected by applicable federal, state or local law. Epsilon also prohibits harassment of applicants and employees based on any of these protected categories. Epsilon will provide accommodations to applicants needing accommodations to complete the application process.

REF235548W","Analítica de datos, Análisis de marketing, Ingeniería de datos , Integración de datos y SQL, CHAID, Comunicación, Control de calidad, Documentación de proyectos y Habilidades sociales",Solicitar
https://www.linkedin.com/jobs/view/3983056964/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=8J41jljzvcwGPCK3hDJHzg%3D%3D&trk=flagship3_search_srp_jobs,Python Developer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 días,"Jersey City, NJ","Acerca del empleo
Job Description:
Design and build horizontally scalable components within the major platform that the team is developing. At least of 7 years’ experience.
Review and provide code feedback in terms of best practices with keen eye towards performance and stability optimizations.
Become subject matter expert on code deployed on our platform and support to other developers.
Write reusable and extendable code and become a key contributor of the core platform.
Consistently work to make our software simpler.
Challenge yourself and your peers to always improve.
Required Skills:
Expertise in functional and object-oriented programming, specifically in Python.
Experience in databases (relational/document/etc.) including NoSQL databases.
Strong in Algorithms and Data Structures.
Experience building distributed and scalable complex services as well as robust micro services.
Experience in writing unit tests in pytest or unit test.
Working knowledge of CI/CD pipelines and automation.
Strong sense of ownership, urgency, and drive.
Self-motivated with a strong work ethic and a passion for learning and problem solving.","Desarrollo web back end, JSON, MongoDB, Python y XML, Bases de datos, HTML5, Hojas de estilos en cascada (CSS), JavaScript y Programación orientada a objetos (POO)",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3818366649/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=knc1z0WsqI0BPYp4x3JjHA%3D%3D&trk=flagship3_search_srp_jobs,(Remote) Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Anaheim, CA","Acerca del empleo
The Job Market is Challenging due to almost 300,000 Tech Layoffs since October 2022 due to which thousands of laid off Techies are competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications forget about getting client interviews. As the Saying goes ""when the Going gets tough the Tough get going” Candidates who want to make a tech career they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.

Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs. Clients now post covid can also hire remote workers which increases even more competition for jobseekers.

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, PayPal, western union, Client, visa, Walmart labs etc. to name a few.

We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT Industry

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.

please check the below links to see success outcomes of our candidates

https://www.synergisticit.com/candidate-outcomes/

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

Oracle CloudWorld Event (OCW) Las Vegas 2023/ 2022 | SynergisticIT - YouTube

https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrn

https://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Q

https://www.youtube.com/watch?v=OAFOhcGy9Z8 https://www.youtube.com/watch?v=EmO7NrWHkLM https://www.youtube.com/watch?v=NVBU9RYZ6UI https://www.youtube.com/watch?v=Yy74yvjatVg

For preparing for interviews please visit https://www.synergisticit.com/interview-questions/

We are looking for the right matching candidates for our clients

Please apply via the job posting

REQUIRED SKILLS For Java /Full Stack/Software Programmer

Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Project work on the skills
Knowledge of Core Java, JavaScript, C++ or software programming
Spring boot, Microservices, Docker, Jenkins and REST API's experience
Excellent written and verbal communication skills

For data Science/Machine learning Positions

Required Skills

Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Project work on the technologies needed
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, TensorFlow

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos y Lenguajes de programación, Ciencias de la computación, Comunicación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3959825029/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=uwXG5JtP%2Fn3mr17ClHOAjA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 4 días,"Tewksbury, MA","Acerca del empleo
Description

Leidos’ Security Enterprise Solutions (SES) operation is seeking a motivated and talented Data Engineer to join our multidisciplinary, highly energized, and expert data science team. Leidos is a leading innovator in deploying AI solutions, and we are dedicated to pushing the boundaries of technology and data-driven insights. We are on a mission to harness the power of data to drive our business forward and make impactful decisions. Join our dynamic team and be part of shaping the future and make global impact in airport security detection, including the ProVision scanner found in airports around the world.

We are seeking a passionate and detail-oriented Data Engineer to take full ownership of our data collection processes. The ideal candidate will be responsible for ensuring the quality, accuracy, and integrity of the data that our Data Science teams rely on. As a “Data Champion”, you will play a pivotal role in driving our data initiatives and ensuring our data is fit for purpose.

Primary Responsibilities

Design, implement, and manage data collection strategies and methodologies to ensure high-quality data acquisition and data labeling
Collaborate closely with cross-functional teams, including data scientists, algorithm engineers, and image scientists, to understand data requirements and execute data collections on our equipment that meets their need
Ensure the accuracy, completeness, and consistency of collected data files and data labels by performing regular data quality checks and audits
Develop and maintain documentation for data collection processes and standards
Train and support technician team members on best practices for data collection, labeling, and usage
Work with subject matter experts within the team to identify data gaps and recommend solutions to address them
Maintain inventory of simulants, detection targets, and clutter objects, and work hands-on with these materials to execute data collections
Stay current with industry trends and advancements in data collection techniques and tools.

Desired Qualifications

Bachelor’s degree and 2+ years experience, preferably in a technical field. May consider additional years of experience in lieu of a degree.
Must have the ability to obtain a Public Trust clearance (US citizenship required)
Proven experience in data collection, data management, or similar role
Strong understanding of data collection methodologies and best practices
Proficiency with data collection tools and technologies (e.g., SQL, Python, etc.)
Excellent analytical and problem solving skills
Strong attention to detail and commitment to data accuracy and integrity
Ability to work with various teams and stakeholders and take ownership of projects
Ability and willingness to work hands-on with materials and equipment required to execute a data collection plan
Excellent communication and collaboration skills

As a member of the Leidos Security Enterprise Solutions (SES) team, you will be joining a diverse and dedicated group who are excited about producing industry leading screening technology. Our team produces a comprehensive suite of fully automated and integrated products for aviation, shipping ports, border crossings, and critical infrastructure customers. These systems provide threat detection by screening baggage, cargo, and people at checkpoints around the world.

Check out the links below to learn more about Security Enterprise Solutions (SES)

https://careers.leidos.com/pages/security-enterprise-solutions

https://www.leidos.com/markets/aviation/security-detection

Original Posting Date

2024-06-25

While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.

Pay Range

Pay Range $65,000.00 - $117,500.00

The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Atención al detalle, Calidad de datos, Comunicación, Confianza ciudadana, Recogida de datos y Resolución de problemas",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3979924621/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=%2BfAoNjBQRmV%2BjtGRDIgT9g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Python,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Austin, TX","Acerca del empleo
Key Skills: Snowflake, Big data processing, Python, pyspark, ETL.

Job Description

Engineers they need is more for Data Engineering work (90% Data Engg and 10% Client model training). From Data engineering, we need folks with experience in Big data processing, Python, pyspark, ETL, feature engineering etc .

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","Analítica, Big data, Extraer, transformar y cargar (ETL), Ingeniería de datos , Minería de datos, PySpark y Python, Procesamiento de datos, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3980829032/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=UWJqWNBRBYJ8oN1riVuwpA%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer/Scientist - Junior,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Fayetteville, NC","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3984145095/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=ZyzM1VE95TMZpbFcwgYNPQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (IICS & GBQ),"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Atlanta, GA","Acerca del empleo
Job Description

"" Five or more years experience in software engineering.

"" Five or more years experience in large scale RDBMS environments

"" Four or more years experience with Informatica PowerCenter or IICS

"" One or more years experience in Erwin

"" Experience in code automation (e.g. pattern based integration)

"" Experience in advanced SQL and PL/SQL techniques

"" Experience in building re-usable Utility packages

"" Experience with testing the code

"" Focus on continuous improvement

"" Experience in Unix shell and Python scripting

"" Integration design & data modeling skills in Data lake and Data Warehousing environments

"" Experience with Streaming technologies is a plus( STRIIM, Kafka, etc)

"" Experience with other Informatica tools is plus e.g., Metadata manager, Analyst, DVO, Data Quality

"" Exposure to both on-prem and cloud Integration solutions

"" Familiarity with non-relational DB technologies is a plus

"" Experience with automated testing

"" Experience with both batch and real-time patterns for integrations

"" Ability to build and analyze complex integration workflows from heterogeneous data sources

"" Experienced in large Enterprise Data Warehouse & Integration projects.

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","Lenguajes de programación, PL/SQL, PowerCenter, SQL y Sistema de gestión de bases de datos relacionales, Erwin, Guiones shell, Informatica (empresa), Informatica PowerCenter y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3984140731/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=iDkyM1tW8vsDN2IvNlII7A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (IICS & GBQ),"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Atlanta, GA","Acerca del empleo
Data Engineer (IICS & GBQ)

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Aptitudes y experiencia deseables
DATA ENGINEER

(IICS & GBQ)","Almacenamiento de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y MongoDB, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3833862753/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=6wv1ZKvghxaam5p%2FiNaRQQ%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Developer/Coder/Programmer/Data Scientist/Analyst/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Fremont, CA","Acerca del empleo
SYNERGISTICIT wants every Job seeker to be aware that the Job Market is Challenging and to stand out you need to have exceptional skills and technologies which make you stand out from other Job seekers.

If your skills and your project work are similar to others then it's difficult to stand out to the clients. Since 2010 we have helped Jobseekers stand out by ensuring only the best candidates with the requisite skillset go to the clients and get the attention that they need. We just don't focus on getting you a Job we make careers

Position open for all visas and US citizens

We at SynergisticIT understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped thousands of candidates get jobs at technology clients like Apple, Google, Paypal, Western Union, Client, Walmart labs, etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists.

 Who Should Apply:  Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates looking to make their careers in IT Industry

We welcome candidates with all visas and citizens to apply.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

Candidates who are serious about their future in the IT Industry and have set big goals for themselves.

Candidates having difficulty in finding jobs or cracking interviews or who want to improve their skill portfolio. We also offer Skill enhancement programs if the candidates are missing skills or experience which our clients need with great outcomes

Candidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement

Candidates Who Lack Experience

 Have had a break in careers 

 Lack Technical Competency 

 Different visa candidates who want to get employed and settle down in the USA 

Please also check the below links

Best Programmers in USA | Best Coding Bootcamp - SynergisticIT

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://www.synergisticit.com/java-track/ 

 https://www.synergisticit.com/data-science-track/ 

 https://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/ 

 https://www.synergisticit.com/contact-us/ 

If not a match candidates can opt for Skill enhancement

For getting help with interviews please visit

 https://www.synergisticit.com/interview-questions/ 

Required Skills

 REQUIRED SKILLS For java /   Software   Programmers 

 Bachelor's degree or Masters's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Highly motivated, self-learner, and technically inquisitive
 Experience in the programming language Java and understanding of the software development life cycle
 Project work on the skills
 Knowledge of Core Java, Javascript, C++, or software programming
 Spring boot, Microservices, Docker, Jenkins, and REST API experience
 Excellent written and verbal communication skills

 For data Science/Machine learning 

Required Skills

 Bachelor's degree or Masters's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Project work on the technologies needed
 Highly motivated, self-learner, and technically inquisitive
 Experience in the programming language Java and understanding of the software development life cycle
 Knowledge of Statistics, SAS, Python, Computer Vision, and data visualization tools
 Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, Time series analysis 

Please understand skills are required by clients for selection even if it's a Junior or entry-level position the additional skills and Project work are the only way a candidate can be picked by clients. If not having the above candidates can opt for skill enhancement to gain the required skills and project work.

No third-party or C2C candidates

Please apply to the posting","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Comunicación, Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3818367428/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=ubFnXXHM0kaiOrBDItyLrw%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data/ML Engineer (Remote),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Houston, TX","Acerca del empleo
2024 is finally here and we hope the Job market improves however as per a resume builder survey based on response from more than 900 companies 4 out of 10 companies are planning to have layoffs in 2024 or have a hiring freeze.  Almost 390,000 tech employees have been laid off since 2022 and it's still ongoing. The effect of this has led hundreds of thousands of laid off Tech employees competing with existing Jobseekers.

 AI is replacing many normal jobs which were done by people. As per news reports Google is planning to Client off 30,000 employees in its ad sales who will be replaed by AI ad technology.

 Entry level Job seekers struggle to get responses to their applications, are getting ghosted after interviews.  In such a scenario the Job seekers need  to differentiate themselves by ensuring to obtain exceptional skills and technologies so that they can wear multiple roles at a client as clients now would want to expand roles and responsibilities assigned to a particular job to save costs.

Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs as Clients now post covid can also hire remote workers which increases even more competition for jobseekers.

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We also offer Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If a Jobseeker is qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection are totally based on clients discretion not ours.

 If you applied for a job and got emails from our skill enhancement team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

 please check the below links to see success outcomes of our candidates and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrn

 https://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Q

 https://www.youtube.com/watch?v=NVBU9RYZ6UI

 https://www.youtube.com/watch?v=EmO7NrWHkLM

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/

 We are looking for the right matching candidates for our clients

 Please apply via the job posting

REQUIRED SKILLS For Java /Full Stack/Software Programmer

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Project work on the skills
 Knowledge of Core Java , javascript , C+
or software programming
 Spring boot, Microservices, Docker, Jenkins and REST API's experience
 Excellent written and verbal communication skills

 For data Science/Machine learning Positions

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Project work on the technologies needed
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
 Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow

 If you get emails from our skill enhancement team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements. 

 No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Ciencia de datos, Lenguajes de programación, Programación y Reconocimiento de patrones, Ciencias de la computación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3980246022/?eBP=BUDGET_EXHAUSTED_JOB&refId=s7YEFsdVz1ShE%2FaRWacqxA%3D%3D&trackingId=CcF7giM%2BUQ4att476bvTWw%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, Analytics","Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 días,"Los Ángeles, CA","Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Meta Platforms, Inc. (f/k/a Facebook, Inc.), is seeking the following. Apply via Dice today!

Meta Platforms, Inc. (f/k/a Facebook, Inc.) has the following position in Los Angeles; CA: 

Data Engineer, Analytics: Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making. (ref. code REQ-2406-139551: $171455/year - $196900/year).

Individual pay is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base salary only, and do not include bonus or equity or sales incentives, if applicable. In addition to base salary, Meta offers benefits. Learn more about benefits at Meta at this link: https://www.metacareers.com/facebook-life/benefits. For full information & to apply online, visit us at the following website http://www.metacareers.com/jobs & search using the ref code(s) above.","Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Solicitar
https://www.linkedin.com/jobs/view/3976187644/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=vgcgqhMJFUOQ%2FF%2FPWZEM5w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Description
About O’Reilly Media
O’Reilly’s mission is to change the world by sharing the knowledge of innovators. For over 40 years, we’ve inspired companies and individuals to do new things—and do things better—by providing them with the skills and understanding that’s necessary for success.
At the heart of our business is a unique network of experts and innovators who share their knowledge through us. O’Reilly Learning offers exclusive live training, interactive learning, a certification experience, books, videos, and more, making it easier for our customers to develop the expertise they need to get ahead. And our books have been heralded for decades as the definitive place to learn about the technologies that are shaping the future. Everything we do is to help professionals from a variety of fields learn best practices and discover emerging trends that will shape the future of the tech industry.
Our customers are hungry to build the innovations that propel the world forward. And we help you do just that.
Learn more: https://www.oreilly.com/about/
Diversity
At O’Reilly, we believe that true innovation depends on hearing from, and listening to, people with a variety of perspectives. We want our whole organization to recognize, include, and encourage people of all races, ethnicities, genders, ages, abilities, religions, sexual orientations, and professional roles.
Learn more: https://www.oreilly.com/diversity
About The Team
Our data platform team is dedicated to establishing a robust data infrastructure, facilitating easy access to quality, reliable, and timely data for reporting, analytics, and actionable insights. We focus on designing and building a sustainable and scalable data architecture, treating data as a core corporate asset. Our efforts also include process improvement, governance enhancement, and addressing application, functional, and reporting needs. We value teammates who are helpful, respectful, communicate openly, and prioritize the best interests of our users. Operating across various cities and timezones in the US, our team fosters collaboration to deliver work that brings pride and fulfillment.
About The Role
We are looking for a thoughtful and experienced data engineer to help grow a suite of systems and tools written primarily in Python. The ideal candidate will have a deep understanding of modern data engineering concepts and will have shipped or supported code and infrastructure with a user base in the millions and datasets with billions of records. The candidate will be routinely implementing features, fixing bugs, performing maintenance, consulting with product managers, and troubleshooting problems. Changes you make will be accompanied by tests to confirm desired behavior. Code reviews, in the form of pull requests reviewed by peers, are a regular and expected part of the job as well.
Salary Range: $110,000 - $138,000
What You’ll Do
Develop data pipelines or features related to data ingestion, transformation, or storage using Python and relational databases (e.g., PostgreSQL) or cloud-based data warehousing (e.g.,BigQuery)
Collaborate with product managers to define clear requirements, deliverables, and milestones
Team up with other groups within O’Reilly (e.g. data science or machine learning) to leverage experience and consult on data engineering best practices
Review a pull request from a coworker and pair on a tricky problem
Provide a consistent and reliable estimate to assess risk for a project manager
Learn about a new technology or paper and present it to the team
Identify opportunities to improve our pipelines through research and proof-of-concepts
Help QA and troubleshoot a pesky production problem
Participate in agile process and scrum ceremonies
What You’ll Have 
Required: 
3+ years of professional data engineering experience (equivalent education and/or experience may be considered)
2+ year experience of working in an agile environment
Proficiency in building highly scalable ETL and streaming-based data pipelines using Google Cloud Platform services and products
Experience in building data pipelines using Docker
Experience in building data pipelines using tools such as Talend, Fivetran 
Proficiency in large scale data platforms and data processing systems such as S3, GCS, Google BigQuery and Amazon Redshift
Excellent Python and PostgreSQL development and debugging skills
Experience building systems to retrieve and aggregate data from event-driven messaging frameworks (e.g. RabbitMQ and Pub/Sub)
Experience with deployment tools such as Jenkins to build automated CI/CD pipelines
Strong drive to experiment, learn and improve your skills
Respect for the craft—you write self-documenting code with modern techniques
Great written communication skills—we do a lot of work asynchronously in Slack and Google Docs
Empathy for our users—a willingness to spend time understanding their needs and difficulties is central to the team
Desire to be part of a compact, fun, and hard-working team
Preferred:
Experience with Google Cloud Dataflow/Apache Beam
Experience with Django RESTful endpoints
Experience working in a distributed team
Knowledge and experience with machine learning pipelines
Contributions to open source projects
Knack for benchmarking and optimization","Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery y Ingeniería de datos, Amazon Redshift, Comunicación, Comunicación escrita, Elasticsearch y Google Cloud Dataflow",Solicitar
https://www.linkedin.com/jobs/view/3938135085/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=lsAotOhT%2F4j4rEfCyhqLcA%3D%3D&trk=flagship3_search_srp_jobs,AWS Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",Publicado de nuevo hace 1 mes,"Tacoma, WA","Acerca del empleo
AWS Data Engineer
 Start - ASAP
 Duration – 12 months 
Remote role – consultant can live anywhere in the US
 Citizenship does NOT matter but communication does matter and must be excellent. 
  About the Role:
We are seeking an experienced AWS Data Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience in data engineering with a strong focus on AWS technologies. This role involves designing, developing, and maintaining scalable data pipelines and processing systems. The candidate should be adept at managing and optimizing data architectures and be passionate about data-driven solutions. Knowledge of machine learning is a plus.
Key Responsibilities:
Design and implement scalable data pipelines using AWS services such as Glue, Redshift, S3, Lambda, EMR, Athena
Develop and maintain ELT processes to transform and integrate data from various sources.
Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver high-quality data solutions.
Optimize and tune performance of data pipelines and queries.
Ensure data quality and integrity through robust testing and validation processes.
Implement data security and compliance best practices.
Monitor and troubleshoot data pipeline issues and ensure timely resolution.
Stay updated with the latest developments in AWS data engineering technologies and best practices.
Required Skills and Qualifications:
Bachelor’s or Master’s degree in Computer Science, Information Technology, or a related field.
5+ years of experience in data engineering with a focus on AWS technologies.
Expertise in AWS services such as Glue, Redshift, S3, Lambda, EMR, Athena,
Strong programming skills in Python, Pandas, SQL
Experience with database systems such as AWS RDS, Postgres and SAP HANA.
Knowledge of data modeling, ETL processes, and data warehousing concepts.
Familiarity with CI/CD pipelines and version control systems (e.g., Git).
Experience writing infrastructure as code using Terraform.
Familiarity with Glue Notebooks, Sagemaker Notebooks, Textract, Rekognition, Bedrock, and any GenAI/LLM tools
Strong problem-solving skills and attention to detail.
Excellent communication and collaboration skills.
Nice to Have:
AWS Certification (e.g., AWS Certified Data Analytics, AWS Certified Solutions Architect).
Experience with machine learning frameworks and libraries (e.g., TensorFlow, PyTorch, Scikit-learn).
Knowledge of AWS SageMaker and its integration within data pipelines.
Knowledge of big data technologies such as Apache Spark, Hadoop, or Kafka.
Experience with data visualization tools like Tableau, Power BI, or AWS QuickSight.
Familiarity with Azure DevOps and Azure Pipelines.
Familiarity with Data Catalog and Governance tools such as AWS DQ, Collibra, and profiling tools such as AWS Databrew",Amazon Web Services (AWS),Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3963955968/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=f8ZkqeNrio%2FiWom9GZ%2BXZQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
- Client Flex Full Time - salary range -

Job Title: Data Engineer : with Python and SQL Experience.

Full Time

Visa Status- Apart H1B, anything is fine.

Location Remote: Flexible until the pandemic

The end Client is Facebook

1st Round: 1 HourHands-on-technical Coding Interview

2nd Round Discussion for 30 Mins.

Responsibilities:-

Utilize strong SQL & Python expertise to engineer sound data pipelines and conduct routine and ad hoc analysis to assess the performance of legacy products and the saliency of new features.

Build reporting dashboards and visualizations to design, create and track campaign/program KPIs

Perform analyses on large data sets to understand drivers of marketing engagement and provide recommendations on campaign and product optimization

Project manage end-to-end process of analytics tooling feature development, including request intake, requirements evaluation, cross-functional team alignment, feature execution, QA testing, and stakeholder

communications

Interface and consult with marketers, analysts, and cross-functional partners to

understand their reporting and data needs, serving as the point of contact for requests, inquiries, and action items

Interface with other data engineering, product, and data science teams to implement client needs and initiatives.

TLDR: Strong SQL and Python skills are essential to be successful at this role.

So if you see people who have worked as Data Engineer - Ask them to rate themselves on Python and SQL.

Above is for your reference

If the rating is 3 or above, then we are good. I will send you the document which has the detail on how the rating should be done.
Aptitudes y experiencia deseables
DATA ENGINEER","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Bases de datos, Comunicación y Optimización",Solicitar
https://www.linkedin.com/jobs/view/3973308735/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=EEtkZEPN1Rqw%2BGWIlmDTEg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Miami-Fort Lauderdale y alrededores,"Acerca del empleo
We are seeking a talented Data Engineer for an exciting opportunity with an Enterprise level company in sunny South Florida. The ideal candidate will possess some, or all, of the qualifications below.
 Essential Duties and Responsibilities:
  Create and maintain technical design documentation. 
Conduct requirements gathering, data mapping and designing.
Create, build, and maintain complex data pipelines from disparate sources that meet functional / non-functional business requirements.
Create, maintain, and refine existing ETL/ELT processes, employing a variety of data integration and data preparation tools.
Develop event-based, real-time, and micro-batch data pipelines. 
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing pipelines for greater scalability, etc.
Work with stakeholders including Product, Data and Business teams to assist with data-related technical issues and support their data needs.
Create datasets for: (1) operational reports, key performance indicators/metrics, or other insights into current organizational activities, (2) analytics and data science to provide the ability to uncover the answers to major questions that help organizations make objective decisions and/or gain a competitive edge.
Write, debug, and implement complex queries involving multiple tables or databases across platform(s)
 Qualifications, Knowledge and Skills:
  3+ years of experience with Python, SQL, and Command Line Interfaces.
3+ years of experience with streaming technologies (Kafka, Pubsub, Kinesis) and log-based architectures and experience writing batch and stream processing jobs (i.e. Apache Beam, Google Cloud DataFlow, Apache Spark, Apache Storm).
3+ years of experience working and creating datasets for a data warehouse.
Clear understanding of data modeling patterns.
3+ years of experience with ETL/ELT development tools (Azure Data Factory (ADF) preferred).
3+ years of experience with code version control using tools, such as Git, and experience with Agile tools (Jira or Azure DevOps preferred).
3+ years of cloud experience (Azure preferred).
Experienced in using best practices in designing, building and managing data pipelines that require data transformations as well as metadata and workload management.
Experienced in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional and new data integration technologies (such as ETL, ELT, data replication, change data captures, message-oriented data movement, API design, stream data integration and data virtualization)
Experienced with implementing data quality frameworks.
Expert level knowledge with programming languages including Python, SQL, CLI.
Expert level knowledge with relational SQL databases such as Oracle and SQL Server.

Huge PLUSES
Experience with ERP systems 
Experience with NoSQL databases
Knowledge of the hospitality industry","Apache Kafka, Apache Spark, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Python y SQL, Amazon Kinesis, Asignación de datos, Datasets, ERP de Infor y JIRA",Solicitud sencilla
https://www.linkedin.com/jobs/view/3961906747/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=Elth6RpsaDaxUh9ekDZkGQ%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist / Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",Publicado de nuevo hace 6 días,"Deerfield, IL","Acerca del empleo
Job Title: Data Scientist / Data Engineer

Location: Remote within the US (Work hours are CST)

Duration & Type: Initial 2-Month Contract with likely extensions up to 2 years

Compensation: Competitive hourly rate, Access to Healthcare, Dental, and Vision Insurance Plan of Choice, and 401K

Chamberlain Advisors is currently seeking a Data Scientist / Data Engineer for our direct client in the healthcare and retail pharmacy industry. This is an initial 2-month contract with likely extensions up to 2 years. The ideal candidate will have experience in Azure data factory, Azure ADLS, Azure SQL, and Data modeling. Click apply now and join the Chamberlain experience.

What You Will Be Accountable For

Work with stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions. 
Leverage Azure stack such as Data Factory to bring new data into the cloud data lake and transform it for consumption. 
Mine and analyze data from company databases to drive optimization and improvement of product development, marketing techniques and business strategies. 
Assess the effectiveness and accuracy of new data sources and data gathering techniques. 
Develop custom data models and algorithms to apply to data sets. 
Use predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes. 
Develop company A/B testing framework and test model quality. 
Coordinate with different functional teams to implement models and monitor outcomes. 
Develop processes and tools to monitor and analyze model performance and data accuracy. 

What Qualifications You Need

Bachelor’s degree required 
3+ years of experience as a Data Sceintist / Data Engineer 
Expereince with Azure data factory 
Experience with Azure ADLS 
Experience with Azure SQL 
Experience with Data modeling 
Experience with Technical documentation creation 
Large scale data experience with tables with over a billion of records 
Ability to troubleshoot and work independantly (with support) 
Strong Communication 
Preferred/Nice to have - Azure Data Engineer certification(s) 

Why Join Right Now?

Our fortune 20 client is seeking innovative and intelligent individuals to join their team. Here is your opportunity to join one of the largest healthcare and retail pharmacy companies in the U.S, with more than 10 million customers, over 8,000 retail stores, and a presence in multiple countries. Our client is constantly creating groundbreaking ways to meet customer needs, improve their health, and be a force for good in the world. This is your chance to work in a truly supportive environment and be a part of a progressive organization dedicated to the well-being of their customers, team members, and communities.

As a leader in the healthcare and retail pharmacy industry, our client is a major contractor to the United States federal government. As such, the client is subject to Executive Order 14042 - Executive Order on Ensuring Adequate COVID Safety Protocols for Federal Contractors. This Order requires that all on-site employees, contractors and visitors be vaccinated against COVID-19, except in limited circumstances where an individual is legally entitled to an accommodation. If you are not vaccinated and do not anticipate becoming vaccinated before the start date of this assignment, please consider applying for another open position.”

Why Work with Chamberlain?

Chamberlain Advisors is a veteran-owned business that provides human capital solutions across a wide range of industries and engagement types. Chamberlain candidates benefit from our unique hiring and interviewing process which has been designed to increase the likelihood that they will be successful in their job searches. This is achieved through our 5-step recruitment process, ensuring a top-of-the-line candidate experience. Find out what makes us different; apply to Chamberlain today.

Equal Employment Opportunity

Chamberlain Advisors provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, Chamberlain Advisors complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.

Chamberlain Advisors expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of Chamberlain Advisors' employees to perform their job duties may result in discipline up to and including discharge.","Análisis de datos, Azure Data Factory, Ciencia de datos, Modelos predictivos, Procesamiento de lenguaje natural y SQL, Comunicación, Documentación técnica, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3956090057/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=tM4%2BL7%2Bu4Eq%2FktrQxjz4cg%3D%3D&trk=flagship3_search_srp_jobs,Software/Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Chicago, IL","Acerca del empleo
Scalesology is a growing consulting company that helps businesses scale their data and processes with the right technology. Weare lookingfor passionate, innovative, and mindfulindividuals tobuild our Software Engineering team. This is a remote position, which will require adequate bandwidth to conduct video calls with the Scalesology team. You must reside in Illinois, Indiana, or Wisconsin for this position.

At Scalesology, we pride ourselves on providing a positive atmosphere of integrity and respect for our employees and clients. Our Core Values are a daily part of everything we do.

Commit to the Cause: We are committed to being responsive, passionate, intentional and diligent in everything we do. 
Treat other like you want to be treated: We treat our clients and our colleagues with mindfulness, humility, respect, and integrity
Focus on the Leading Edge: We embrace continuous learning in order to implement innovative technologies for our clients. We are not satisfied with the status quo. 
Celebrate the Journey: We believe in working hard, recognizing our employees’ efforts along the way, and having fun as we do it. 

If you have a strong development background, appreciate working on a diversity of projects, and solving challenging issues across different industries, this is the job for you!

Responsibilities

Foster and engineer software solutions from conception to deployment. 
Create CI/CD pipelines from data sources to data warehouse/data lakes
Evaluate customer requirements and determine existing product reach, potential restrictions, and future development feasibility. 
Work independently while testing all software in a fluid environment, facilitating verification and quality assurance throughout
Maintain direct communication with the customer to understand needs throughout the software development lifecycle. 
Work with the rest of the engineering team to execute timely and cost-effective solutions. 
Provide on-going support and maintenance to the customer as needed
Attend daily stand-ups and connect with the team using Slack, Jira, and Microsoft Teams 
Stay up-to-date on technology trends and developer sentiments

Requirements

3+ years of experience in Software Development, specifically using PHP, Laravel, and LiveWire
Ability to communicate and assist clients as needed
Proficiency in one or more frontend frameworks (e.g. React, LiveWire)
Proficiency in one or more backend languages/frameworks (Node.js, PHP, or Python)
HTML and CSS3
Knowledge of REST APIs, JSON, XML
Understanding of Microservices architecture
Source code management with Git 
Understanding of Accessibility / WCAG
Working knowledge of databases and SQL

Bonus skills

Bachelor's Degree in Computer Science or a similar field
Jira
Single sign-on (SSO) integration
Familiarity with cloud architecture in AWS, GCP, or Azure
Unit Testing and/or Test-Driven Development
Experience building applications using a Lambda or Azure function

Powered by JazzHR

tqiduo7S24","Bases de datos, Ciencias de la computación, Comunicación, Desarrollo de software, Doctrine, Hojas de estilos en cascada (CSS), Laravel, Livewire, PHP y Pautas de accesibilidad para el contenido web",Solicitud sencilla
https://www.linkedin.com/jobs/view/3963685451/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=RefhG9NrdS2IVjo%2Ff9PVjw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Azure),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 semanas,"Saint Paul, MN","Acerca del empleo
This is a direct hire position for one of our clients. This position is a fully remote role. Candidates must be able to work in the US without sponsorship.**

We are looking for a highly motivated Data Engineer with expertise in Azure Data Factory and PowerBI to support our data integration, transformation, and visualization efforts. The ideal candidate will have experience with ETL processes and data warehousing, with additional skills in SSIS being a plus.

Key Responsibilities:

Design, develop, and maintain robust data pipelines using Azure Data Factory.
Implement and manage ETL processes to ensure efficient data extraction, transformation, and loading.
Create and maintain data visualizations and reports using PowerBI.
Collaborate with data analysts, data scientists, and business stakeholders to understand data requirements and deliver solutions that meet their needs.
Optimize and troubleshoot data workflows to ensure high performance and reliability.
Develop and maintain data warehousing solutions to support analytical and reporting needs.
Ensure data quality and integrity across all data processes.
Document data processes, configurations, and procedures to facilitate knowledge sharing and maintain compliance.

Required Qualifications:

Proven experience as a Data Engineer or in a similar role.
Strong proficiency in Azure Data Factory and PowerBI.
Experience with ETL processes and data integration.
Solid understanding of data warehousing concepts and best practices.
Proficiency in SQL for data querying and manipulation.
Excellent problem-solving skills and attention to detail.
Strong communication and teamwork abilities.

Nice to Have:

Experience with SSIS (SQL Server Integration Services).
Knowledge of other ETL tools and technologies.
Familiarity with big data technologies and frameworks.
Experience with cloud platforms such as Azure, AWS, or Google Cloud.
Certifications in Azure Data Engineering or related areas.

Our Vetting Process

At Emergent Software, we work hard to find the software engineers who are the right fit for our clients. Here are the steps of our vetting process for this position:

Application (5 minutes)
Online Assessment & Short Algorithm Challenge (40-60 minutes)
Initial Phone Interview (30-45 minutes)
3 Interviews with the Client
Job Offer!

 #EmergentStaffing","Almacenamiento de datos, Azure Data Factory, Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos , Integración de datos, SQL y SQL Server Integration Services (SSIS), Edición de imágenes y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3985513273/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=JwqMEvjY0qJaX9JjCVqJPA%3D%3D&trk=flagship3_search_srp_jobs,Machine Data Process Engineer-Remote,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Director",hace 3 días,"Tarrytown, NY","Acerca del empleo
Join us in pioneering breakthroughs in healthcare. For everyone. Everywhere. Sustainably.

Our inspiring and caring environment forms a global community that celebrates diversity and individuality. We encourage you to step beyond your comfort zone, offering resources and flexibility to foster your professional and personal growth, all while valuing your unique contributions.

Apply now for the position of Machine Data Process Engineer and you will become the Data Governance representation on projects to implement and coordinate digitization efforts for Machine Data Management.

Your role:

Collaborate with multiple business functions and product business units subject matter experts across the enterprise to develop, streamline / improve existing and emerging systems and processes.
upport EU Data Act activities to prepare for the data sharing, design, and information obligations.
business function subject matter experts with data governance focused analytical thinking for improved planning, monitoring, evaluation, and reporting. 
processes and procedures with subject matter experts where knowledge gaps exist.

Your expertise:

BS / BA with a concentration in Computer Science/ Analytics or related degree required. 
Minimum 3+ years’ experience using business governance techniques, with deep professional know-how and experience in process engineering. 
Experience with setting up a data catalog, preferably Microsoft Purview
Deep knowledge of machine data including familiarity with test results and other assay data is mandatory.
Familiar with Data Privacy principles and Knowledge of the EU Data Act

To find out more about the specific business, have a look at https://www.siemens-healthineers.com

Who we are:

We are a team of more than 71,000 highly dedicated Healthineers in more than 70 countries. As a leader in medical technology, we constantly push the boundaries to create better outcomes and experiences for patients, no matter where they live or what health issues they are facing. Our portfolio is crucial for clinical decision-making and treatment pathways.

How we work:

When you join Siemens Healthineers, you become one in a global team of scientists, clinicians, developers, researchers, professionals, and skilled specialists, who believe in each individual’s potential to contribute with diverse ideas. We are from different backgrounds, cultures, religions, political and/or sexual orientations, and work together, to fight the world’s most threatening diseases and enable access to care, united by one purpose: to pioneer breakthroughs in healthcare. For everyone. Everywhere. Sustainably. Check our Careers Site at Careers - Siemens Healthineers USA (siemens-healthineers.com)

The pay range for this position is $83,000 - $109,000 U.S. dollars annually; however, base pay offered may vary depending on job-related knowledge, skills, and experience, as well as where the work will be performed. The annual incentive target is (6%) of base pay. Siemens Healthineers offers a variety of health and wellness benefits including paid time off and holiday pay. Details regarding our benefits can be found here: https://benefitsatshs.com/index.html

This information is provided per the required state Equal Pay Act. Base pay information is based on market location. Applicants should apply via Siemens Healthineers external or internal careers site.

Beware of Job Scams

Please beware of potentially fraudulent job postings or suspicious recruiting activity by persons that are currently posing as Siemens Healthineers recruiters/employees. These scammers may attempt to collect your confidential personal or financial information. If you are concerned that an offer of employment with Siemens Healthineers might be a scam or that the recruiter is not legitimate, please verify by searching for the posting on the Siemens Healthineers Career Site.

“Successful candidate must be able to work with controlled technology in accordance with US export control law.” “It is Siemens Healthineers’ policy to comply fully and completely with all United States export control laws and regulations, including those implemented by the Department of Commerce through the Export Administration Regulations (EAR), by the Department of State through the International Traffic in Arms Regulations (ITAR), and by the Treasury Department through the Office of Foreign Assets Control (OFAC) sanctions regulations.”

As an equal opportunity employer, we welcome applications from individuals with disabilities.

We care about your data privacy and take compliance with GDPR as well as other data protection legislation seriously. For this reason, we ask you not to send us your CV or resume by email. Please create a profile within our talent community and subscribe to personalized job alert that will keep you posted about new opportunities.

To all recruitment agencies:

Siemens Healthineers' recruitment is internally managed, with external support permitted only when a qualified supplier has established a formal contract with us. Unsolicited candidate submissions and referrals, absent a current supplier contract, do not establish consent and are ineligible for fees. We delete and destroy unsolicited information, thus, would recommend you refrain from any such practices. Your adherence to our policies is appreciated.

Equal Employment Opportunity Statement

Siemens is an Equal Opportunity and Affirmative Action Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to their race, color, creed, religion, national origin, citizenship status, ancestry, sex, age, physical or mental disability unrelated to ability, marital status, family responsibilities, pregnancy, genetic information, sexual orientation, gender expression, gender identity, transgender, sex stereotyping, order of protection status, protected veteran or military status, or an unfavorable discharge from military service, and other categories protected by federal, state or local law.

Reasonable Accommodations

If you require a reasonable accommodation in completing a job application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please fill out the accommodations form by clicking on this link Accommodation for disability form If you’re unable to complete the form, you can reach out to our AskHR team for support at 1-866-743-6367. Please note our AskHR representatives do not have visibility of application or interview status.

EEO is the Law

Applicants and employees are protected under Federal law from discrimination. To learn more, Click here.

Pay Transparency Non-Discrimination Provision

Siemens follows Executive Order 11246, including the Pay Transparency Nondiscrimination Provision. To learn more, Click here.

California Privacy Notice

California residents have the right to receive additional notices about their personal information. To learn more, click here.","Capacidad de análisis y Gobierno de datos, Ciencias de la computación, Compartir datos, Diagrama de tuberías e instrumentación, Diseño de procesos, Expertos en la materia, Ingeniería de procesos, Ingeniería química y Privacidad de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3975879463/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=srdfgSg5VEf%2BK4FNgYd4gg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Mineápolis, MN","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Job Description

Utilimarc is a small, agile team driven by a shared passion for cutting-edge solutions and exceptional client experiences. We’re looking for a mid- to senior-level Data Engineer to join our forward-thinking team.

About the Role:

You will be responsible for creating data models that deliver cleaned, structured data for analysis and complex algorithm development, improve data quality, and derive new business insights for our customers. You will work collaboratively with other data engineers, data scientists, and analysts to model data from our near-real time vehicle data streams and augment it with other information, such as fuel transactions, vehicle maintenance data, mobile app data and other geo-spatial data.

What You'll Do:

Assemble, explore and clean data and build models for analysis and algorithm development. 
Collaborate with analysts and data scientists to understand business requirements and develop effective models. 
Identify and flag data quality issues and create methods to report and resolve. 
Create and maintain documentation around data models, including data definitions, relationships, and assumptions. 
Manage ad-hoc requests from internal and external customers and provide accurate results. 
Write and maintain performant code that meets our coding standards and best practices. 
Monitor, maintain, and tune performance of pipelines and models. 
Perform code reviews. 
Define and improve coding standards and best practices. 
Develop a strong understanding of fleet management practices, processes, and assumptions. 

Qualifications:

Ability to work autonomously as well as collaborate with team members. 
Minimum of a Bachelor’s degree in Statistics, Mathematics, Computer Science, Business, Economics or related field. 
At least 3-5 years of experience working in a data lake environment with large scale data processing. 
At least 3-5 years of experience in SQL and Python. 
Experience with git, dbt, and AWS infrastructure. 
Ability to multitask and handle multiple projects. 
Excellent communication skills with demonstrated ability to translate requirements into implementation. 
Experience with data science, statistics, data mining and/or digital analytics. 
Experience with statistical software packages (R, Python, Spark, etc.). 

Company Description

Utilimarc is leading the industry in analytics solutions for enterprise fleets. We work closely with our customers to ensure their data is actionable and reliable to inform sustainable change within their organization. 23 years of industry experience working with diverse data silos from the nation’s largest utility fleets has driven us to develop our analytics platform that connects and unifies fleet data sources into a single environment - expressed through our Fleet Analytics, Benchmarking, and Telematics applications.

Utilimarc is headquartered in Minneapolis and currently working alongside North America’s highest-performing fleet organizations. For more information, visit www.utilimarc.com

Utilimarc is leading the industry in analytics solutions for enterprise fleets. We work closely with our customers to ensure their data is actionable and reliable to inform sustainable change within their organization. 23 years of industry experience working with diverse data silos from the nation’s largest utility fleets has driven us to develop our analytics platform that connects and unifies fleet data sources into a single environment - expressed through our Fleet Analytics, Benchmarking, and Telematics applications. Utilimarc is headquartered in Minneapolis and currently working alongside North America’s highest-performing fleet organizations. For more information, visit www.utilimarc.com","Ciencia de datos, Python y SQL, Calidad de datos, Ciencias de la computación, Comunicación, Lagos de datos, Modelo de datos, Necesidades empresariales y Procesamiento de datos",Solicitar
https://www.linkedin.com/jobs/view/3971716790/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=2vJbetcqG5ywg0dIMge18w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Boston, MA","Acerca del empleo
Company Description

We believe in the power of ingenuity to build a positive human future.

As strategies, technologies and innovation collide, we create opportunity from complexity.

Our diverse teams of experts combine innovative thinking and breakthrough use of technologies to progress further, faster. Our clients adapt and transform, and together we achieve enduring results.

An innovation and transformation consultancy, we are over 4000 specialists in consumer and manufacturing, defence and security, energy and utilities, financial services, government and public services, health and life sciences, and transport. Our people are strategists, innovators, designers, consultants, digital experts, scientists, engineers and technologists. We operate globally from offices across the UK, US, Netherlands and Nordics.

PA. Bringing Ingenuity to Life

Job Description

Your day to day 

We’re an innovation and transformation consultancy that believes in the power of ingenuity to build a positive-human future in a technology-driven world. Our diverse teams of experts combine innovative thinking with breakthrough-technologies to progress further, faster.

Are you ready to harness the power of data to drive advancements in healthcare? Are you passionate about designing, building, and maintaining data infrastructure that plays a pivotal role in improving patient outcomes and shaping the future of medicine? If you're seeking a rewarding career at the intersection of healthcare and technology, we invite you to be part of our dynamic team. This is a unique, multi-year, project-based opportunity to build and grow a clinical data registry platform over many years working with a dedicated team of collaborators and customers. As a Data Engineer for our cutting-edge medical data registry, you'll be at the forefront of managing, optimizing, and expanding our data infrastructure, enabling critical insights that can positively impact patient outcomes. If you're excited about leveraging your data engineering skills to make a difference in the world of healthcare, we want to hear from you.

Qualifications

Minimum qualifications:


Advanced SQL and Python
Expertise in the design and construction of Big Data Lakes and Data Warehouses capable of ingesting, standardizing, and serving billions of data rows spanning diverse datasets ranging from tens to hundreds
Experience building dynamic, metadata driven pipelines and analyses
Building and managing fully automated data pipelines (ETL, ELT, ELTL) including:
Designing and building data interfaces to source systems
Combining and transforming data into the appropriate format for storage
Developing data sets for analytics purposes
Developing pipelines that can handle common issues/errors in a robust and automated way
Cloud experience in Azure, AWS or GCP

Preferred qualifications:


Spark / PySpark experience highly preferable
Working in Agile and DevOps environments
Basic Python, Bash, or PowerShell for automation
Data modelling – Kimball, Data Vault, Star/Snowflake schema, Query-first etc.
Data visualisation in Power BI, Tableau, Qlik or similar
Architecting Data Platforms - designing BI/MI/Analytics solutions using Big Data, Relational or Streaming technologies
One or more of the following certifications:
Microsoft Certified: Azure Data Engineer Associate
AWS Certified Data Analytics - Specialty
GCP Professional Data Engineers


Additional Information

Life At PA encompasses our peoples' experience at PA. It's about how we enrich peoples’ working lives by giving them access to unique people and growth opportunities and purpose led meaningful work.

We believe diversity fuels ingenuity. Diversity of thought brings exciting perspectives; diversity of experience brings a wealth of knowledge, and diversity of skills brings the tools we need. When we bring people together with diverse backgrounds, identities, and minds, embracing that difference through an inclusive culture where our people thrive; we unleash the power of diversity – bringing ingenuity to life. We are dedicated to supporting the physical, emotional, social and financial well-being of our people.

The Salary Range for this role is between $90,000 - $125,000","Almacenamiento de datos, Analítica de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud y Visualización de datos, Arquitectura técnica, Datasets y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982799061/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=HakTTDV2w3DC8eT1lO0HMw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Better Future Solutions, is seeking the following. Apply via Dice today!

Role: Data Engineer

Experience- 8+ years.

Location: Remote

Minimum Qualifications

Essential to perform duties without training

BS/BTech (or higher) in Computer Science, Engineering or a related field.
2+ years experience as a python engineer building data pipelines.
2+ years of experience working with SQL or other database querying language on large

multi-table data sets

Strong self-starter able to work independently and complete tasks in timely manner

Preferred KSA s

Nice to have knowledge, skills, and/or abilities

Experience working with health-tech systems, like Electronic Health Records, Clinical data, etc.

Domain Specific Experience

Data Infrastructure:

Experience in designing, building and optimizing data pipelines and ETL processes

Proficiency in working with large datasets and knowledge of data storage technologies

Experience working with data ingestion systems and optimizing performance for

handling large-scale data processing and analysis

In-depth knowledge of database systems

Familiarity with database replication, sharding and other techniques for scalability and

high availability of databases

Experience in performance monitoring and optimization of data systems and

infrastructure

Experience with containerization and orchestration technologies such as Docker and

Kubernetes

Experience building continuous integration and continuous deployment(CI/CD) pipelines

Experience with security and systems that handle sensitive data","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Datasets, Optimización y Sistemas de bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3838644190/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=vOPkEKm%2FUIc0sMHaM1zbyw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 meses,"Nueva York, NY","Acerca del empleo
12+ Months

PH and Video

Develop and manage data integration solutions, including Extract, Transform, Load (ETL) processes, to ensure seamless data flow between different systems and databases
Leverage Microsoft Azure cloud services for data integration, storage, and processing
Strong SQL and Transact-SQL (T-SQL) skills for writing and optimizing queries, stored procedures, and triggers
Ensure efficient data retrieval and manipulation
Integrate data from external sources and applications through API integration
Implement data governance practices to ensure data quality, security, and compliance
In depth technical knowledge and experience with SQL and SSIS
Critical thinking and problem-solving skills 
Must be hands-on with development and build. Need team members who are self-motivated and driven.
Must have deep experience with SSIS, Python, SQL, and ideally Azure and Azure Data Factory. Primary relevant experience would be coding complex SQL, building the integration packages (including logic development), flat file transfers, etc.
MUST must possess familiarity and working knowledge of Validated Systems","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Pensamiento crítico, SQL y SQL Server Integration Services (SSIS), Calidad de datos, Procedimientos de almacenado y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3962082634/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=SVD45iFRcEtuRf0fGebQNw%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Systems Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 3 días,"Laurel, MD","Acerca del empleo
Description

Are you passionate about defending our nation against challenging real-world air and missile threats?

Are you searching for an opportunity to apply your engineering background in an engaging, inquisitive environment?

If so, we're looking for someone like you to join our team at APL!

Our dedicated team is passionate about providing high-impact analysis and system engineering solutions to support our warfighters. As a part of our team, you will address real warfighter challenges, influence emerging system design, and impact fleet deployment decisions.

We are committed to taking on challenging problems with technical excellence and strive to foster an environment of collaboration, innovation, and sustained professional growth.

As a Data Analyst/Systems Engineer…

You will conduct highly technical analysis of the Aegis Weapon Control System (WCS) to evaluate system performance in support of fleet studies, flight test events, and system engineering studies.
You will characterize system capabilities and limitations, identify trends and outliers, and perform root cause analysis.
You will support concept and feasibility studies to assist in the development of future capabilities and requirements.
You will develop tools to analyze and interpret combat system performance and visualize data in innovative ways.
You will contribute to WCS system engineering efforts by assessing system design and evaluating algorithms.
You will have the opportunity to present your work through briefings to both technical staff, program leadership and pivotal decision makers.

Qualifications

You meet our minimum qualifications for the job if you…

Have a BS degree in Computer Science, Computer Engineering, Mathematics, Electrical Engineering, Physics, or related technical field.
Have 1+ years of related professional experience.
Are proficient in using Python and/or MATLAB to perform data manipulation and visualization.
Are detail-oriented with strong analytical and problem-solving skills.
Are highly self-motivated and driven to fully understand data trends and root causes.
Have strong technical written and oral communication skills.
Have the ability to work independently, as well as collaborate both within a team and with external partners.
Are able to obtain Interim Secret level security clearance by your start date and can ultimately obtain Secret level clearance. If selected, you will be subject to a government security clearance investigation and must meet the requirements for access to classified information. Eligibility requirements include U.S. citizenship.

You’ll go above and beyond our minimum requirements if you…

Have an MS degree in Computer Science, Computer Engineering, Mathematics, Electrical Engineering, Physics, or related technical field.
Have 3+ years of related professional experience.
Have proficiency with Python data science libraries (e.g. NumPy, Pandas).
Are familiar with object-oriented programming languages such as C++ or Java.
Have experience working with high or variable-fidelity models and simulations.
Are familiar with software version control systems (e.g. git).
Have strong interpersonal and communications skills with a desire to take on leadership roles for analysis studies.
Have a working knowledge of the Aegis Weapon System or other missile defense systems.

Why work at APL?

The Johns Hopkins University Applied Physics Laboratory (APL) brings world-class expertise to our nation's most critical defense, security, space and science challenges. While we are dedicated to solving complex challenges and pioneering new technologies, what makes us truly outstanding is our culture. We offer a vibrant, welcoming atmosphere where you can bring your authentic self to work, continue to grow, and build strong connections with inspiring teammates.

At APL, we celebrate our differences and encourage creativity and bold, new ideas. Our employees enjoy generous benefits, including a robust education assistance program, unparalleled retirement contributions, and a healthy work/life balance. APL's campus is located in the Baltimore-Washington metro area. Learn more about our career opportunities at www.jhuapl.edu/careers.

About Us

APL is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender identity or expression, sexual orientation, national origin, age, physical or mental disability, genetic information, veteran status, occupation, marital or familial status, political opinion, personal appearance, or any other characteristic protected by applicable law. APL is committed to promoting an innovative environment that embraces diversity, encourages creativity, and supports inclusion of new ideas. In doing so, we are committed to providing reasonable accommodation to individuals of all abilities, including those with disabilities. If you require a reasonable accommodation to participate in any part of the hiring process, please contact Accommodations@jhuapl.edu. Only by ensuring that everyone’s voice is heard are we empowered to be bold, do great things, and make the world a better place.","Analítica de datos, Capacidad de análisis, Visualización y Visualización de datos, Ciencias de la computación, Comunicación, Comunicación oral, Estudios de factibilidad, Manipulación de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982353644/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=ANZZpDT3jPAD2JZbPoToiA%3D%3D&trk=flagship3_search_srp_jobs,Data Pipeline Engineer :: On Site in DC or NYC. :: Pay Rate: 80hr W2 :: No C2C :: Clearance: Secret or Above,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 6 días,"Nueva York, NY","Acerca del empleo
Title: Data Pipeline Engineer

Clearance: Secret or Above

Location: On Site in DC or NYC.

IV Process: Phone Screen with Hiring Manager, followed by 90 minute Technical/Scenario Based Interview.

The Role Foundry Support Engineers use their technical and problem-solving skillset to enable users across all Foundry customers to succeed in integrating their data to facilitate applications that help their organizations make better decisions.

They are advocates for our customers’ success and collaborate with product engineers, implementation teams, and other support engineers to ensure users’ inquiries are resolved as effectively and quickly as possible. They’re comfortable using their problem-solving skills to find solutions to novel problems within a complex, rapidly evolving software product, rather than relying on copying and pasting precedent from previous answers.

Importantly, they’re responsible not only for assisting users but also for synthesizing the key trends they’re observing in the field into feature requests to partner with engineers to shape the continued evolution of the platform.

As Part Of This Role, They

Provide technical support to Data Engineers working in Foundry.
Develop a deep understanding of Foundry applications so they can leverage problem-solving skills to ensure user inquiries are resolved in an efficient and delightful way.
Collaborate with product engineers to identify, root cause, and ultimately resolve bugs surfaced by users.
Identify and drive forward various support initiatives to improve the effectiveness and efficiency of global Foundry user support at growing scale.
Make contributions to core documentation where context is currently missing.

The Ideal Candidate Would Have 

A passion for customer support and enablement - this will be key to success, as the predominant responsibility of this role and the core of its day-to-day work is handling customer support tickets.
B.S. or M.S degree in Computer Science or equivalent discipline
Required to have proficiency with Python and PySpark.
Nice to have proficiency with SQL, Java, TypeScript/JavaScript, or similar.
Understanding of APIs and RESTful endpoints
Understanding of development best practices (such as branching, testing, etc.)
Experience providing customer support, preferably on complex software platforms.
Experience working with analytical software platforms, using large-scale data to solve valuable business problems, in any industry or subject area.
Excellent English writing and communication skills with the ability to skillfully engage with customers on complex, sensitive topics.
Ability to operate in a fast-paced environment, where the product and support processes are often changing.
Ability to continuously learn and work independently, making decisions with minimal supervision.
Strong organizational skills and attention to detail through effective prioritization strategies and the use of task ticketing and tracking systems. Bonus: knack for streamlining and improving support processes.
Learning and growth mindset: ability and willingness to quickly ramp up on Foundry and project-specific workflows.
Comfortable working in a rapidly changing environment with dynamic objectives and iteration with user
If you have a candidate, they must complete the pre-submission questions to be considered. Thanks.

Tell me a little about yourself that isn’t on your resume, and what makes you interested in Palantir and this role?
Please describe any experience you have with supporting a technical system (L2/L3 support, pipeline support, user support, GHE repo maintenance, DevOPS, site reliability engineering) and on-call support rotations.
Please describe any experience or coursework you have with regards to setting up devices over a network, networking in general and/or distributed systems.
Please describe any experience you have, be it professional, academic, or casual, in working with query-based, code-based, or analytical software (including but not limited to: SQL, Python, Microsoft Excel, Java, R, Spark) to manipulate data or write pipelines.
Please describe any experience you have working with an existing architecture or code base.

Thanks and Regards,

Mohd Niyaz

Email : mniyaz@sibitalent.com

Linkedin ID:- linkedin.com/in/mohd-niyaz-362667220

Contact : +1 (936)-310-2126

Web: www.sibitalent.com

101 E. Park Blvd., Suite 600

Plano, TX - 75074","PySpark y Python, Ciencias de la computación, Comunicación, Construcción de gasoductos, Diseño de tuberías, English writing, Ingeniería submarina, Resolución de problemas y Root Cause",Solicitud sencilla
https://www.linkedin.com/jobs/view/3985590527/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=Soe2g1NqvxnX3FRqN2DOGg%3D%3D&trk=flagship3_search_srp_jobs,Analytics Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 días,"Utah, Estados Unidos","Acerca del empleo
Job description
We are building a World Class Service Center and invite you to join a team of people who are committed to a core objective of supporting life-changing service and providing professional expertise to the operations and leaders we support.

About the Company
Pennant Services is one of the most dynamic and progressive companies in the rapidly expanding senior living, home health, hospice, and home care industries. Affiliates of Pennant Services now operate 111 senior living, home health, hospice, and home care operations across 14 states and we are growing! These operations have no corporate headquarters or traditional management hierarchy. Instead, they operate independently with support from the “Service Center,” a world-class service team that provides the centralized clinical, legal, risk management, HR, training, accounting, IT, and other resources necessary to allow on-site leaders and caregivers to focus squarely on day-to-day care and business issues in their own agencies. Something else that sets us apart from other companies is the quality of our most valuable resources – our people! We are dedicated to living out our culture as defined by our core values,
“CAPLICO”:

Customer Second
Accountability
Passion for Learning
Love One Another
Intelligent Risk Taking
Celebrate
Ownership

By incorporating these principles at all levels of our organization, our employees feel valued and excited about their impact on our service center team members and operational partners. Our culture fosters excellence both personally and professionally and promotes development that leads to continued success.

Requirements
Know how to write SQL which is easy to understand, simple to troubleshoot, and highly performant.
Understand data warehouse design principles
Clear and direct communication skills about complex, technical topics.
Track record of working autonomously with organizational and time management skills.
Ability to build and maintain multi-functional relationships with various teams across the business.

Responsibilities
Utilize SQL + Git to build new datasets and support existing ones.
Build data warehouse in a modern data stack including Snowflake, Fivetran, dbt, and Tableau.
Communicate findings to a wide range of stakeholders.
Help drive a change in usage of data through the active surfacing of insights to stakeholders.

Once Hired
30 days: Working in Snowflake database, producing curated data sets.
60 days: Helping lead metrics alignment conversations across stakeholders
90 days: Producing proactive insights for the business

The above statements are only meant to be a representative summary of the major duties and responsibilities performed by incumbents of this job. The incumbents may be requested to perform job-related tasks other than those stated in this description.

Additional Information
We are committed to providing a competitive Total Rewards Package that meets our employees needs. From a choice of medical, dental, and vision plans to retirement savings opportunities through a 401(k), company match, and various other features, we offer a comprehensive benefits package. We believe in great work and we celebrate our employees' efforts and accomplishments both locally and companywide, recognizing people daily through our Moments of Truth Program. In addition to recognition, we believe in supporting our employees' professional growth and development. We provide employees a wide range of free e-courses through our Learning Management System as well as training sessions and seminars.

Compensation: Based on experience.
Type: Full Time
Location: Utah

About The Pennant Group
We are proud to be affiliated with the Pennant Group, Inc. (NASDAQ: PNTG). Pennant was created in 2019 in connection with The Ensign Group, Inc.’s (NASDAQ: ENSG) spin-off of its home health, hospice, and senior living businesses. We believe that through our innovative operating model, we can foster a new level of patient care and professional competence at our independent operating subsidiaries and set a new industry standard for quality home health and hospice and senior living services. You can learn more about The Pennant Group at www.pennantgroup.com","Analítica, SQL, Tableau y Visualización de datos, Aprendizaje, Compensation, Datasets, Fivetran ETL Tool, Modelado de bases de datos, Skill Development, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3963169708/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=7DJVC4pGJ7iz0SnemRm%2BDw%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer I,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 6 días,"Little Chute, WI","Acerca del empleo
Job Type

Full-time

Description

Position Summary:

This position would require a candidate to possess a strong technical background in developing and delivering BI solutions along with a strong understanding of SQL Server environments. Business intelligence (BI) is a set of technologies and practices for transforming business information into actionable reports and visualizations. The Senior Data Engineer transforms data into a useful format for analysis and is focused on the design and architecture.

A Senior Data Engineer is the data professional who prepares the data infrastructure to be leveraged by the HBS BI Data Developers. The Senior Data Engineer will design, build, integrate data from various resources and manage big data. The Senior Data Engineer ensures the operations of the data pipeline follow a consistent process of Ingestion, Processing, Storage and Access. The work involves tuning databases for fast analysis and creating table schemas.

The Senior Data Engineer is responsible for making data easily accessible, ensuring the process works smoothly and is optimized. The Senior Data Engineer is a critical firm member of the Data Team, The Senior Data Engineer will run Extract, Transform and Load (ETL) on top of datasets and create data warehouses that can be used for reporting and analysis. The Senior Data Engineer ensures the operations of the data pipeline follow a consistent process of Ingestion, Processing, Storage and Access.

Roles and Responsibilities/ Essential Functions:

 Meet with clients to understand their current business processes and needs to provide consulting services and direction on how to build or grow their current data strategy.
 Work with HBS Sales Solutions consultants to identify and grow opportunities within HBS client environments.
 Support and administer the underlining infrastructure and layout of a client data environment.
 Develop and design the process for the customer data collection process.
 Develop policies and procedures for the collection and analysis of data.
 Review customer sources to ensure integrity of the data collection process.
 Collaborate with the BI Data Developers to ensure the requirements are being met to build the right solution needed.
 Estimate development effort required to deliver data customer needs and requests.
 Use business analysis skillset to identify development needs for the purpose of streamlining and improving the operations of the organization for efficiency and profitability.
 Ability to work independently or as a team on project-based solutions for clients.
 Work with team mates to continue to grow and mature data services and delivery options for HBS clients.
 Based on experience, one may mentor other engineers in developing scalable, secure, high-performance BI and Data solutions.
 Billable goal expectation set on an annual basis. These charge hour requirements are prorated based on start date and will be balanced against professional development and on the job training.

Requirements

Competencies

 Accuracy – Ability to produce high quality work deliverables leveraging industry best practices.
 Analytical Skills - Strong abilities required to effectively interpret customer business needs and translate them into application and operational requirements, resolving complex technical and business problems.
 Communication – strong written, verbal, and non-verbal communication skills, especially conveying complex information in an understandable manner.
 Leadership – Ability to motivate and guide others to ensure performance is in accordance with clear expectations and goals.
 Learning – Ability to quickly learn new technologies to deliver solutions.
 Presentation Skills – Ability to effectively conduct formal and informal presentations in both small and large group settings within all levels of a company.
 Project Management – Ability to demonstrate an understanding of process engineering, planning, organizing, staffing, directing, and controlling work tasks.
 Time Management – Ability to effectively utilize available time for managing multiple tasks/projects simultaneously.

Required Experience:

 7 or more years in technology related role
 Data Model Design (Physical or Conceptual or both)
 Experience with needs analysis, software evaluation and selection, customization, and implementation
 Data Warehousing systems and architecture experience in 'real world', practical, successful implementations
 Understand multi-dimensional/relational database structures and schemas.
 Strong knowledge of system design, development, and deployment
 Microsoft BI Suite Experience
Microsoft Excel
Microsoft SQL Server Integration Services (SSIS)
Microsoft SQL Server Reporting Services (SSRS)
Microsoft SQL Server Analysis Services (SSAS)
 Programming and Processing Experience
T-SQL
ETL
Python
 Azure Experience
Azure SQL Database
Azure SQL Manage Instance
Azure Architecture
Azure Data Factory
 Strong knowledge of SQL utilizing MS SQL Server
 Expertise in Professional Services or similar client facing roles

Preferred Experience:

 Experience in multiple industry (Education, Healthcare, Retail, Manufacturing) verticals
 PowerShell knowledge and understanding
 Understanding of report writing and visualization
 GitHub Copilot experience
 Microsoft BI Suite Experience
Microsoft Power BI
Microsoft Fabric
 Microsoft certified: Data Analyst Associate
 Microsoft certified: DP-200 - Implementing an Azure Data Solution
 Microsoft certified: DP-201 - Designing an Azure Data Solution
 Microsoft certified: DP-300 - Administering Relational Databases on Microsoft Azure
 Other SQL platform knowledge (Oracle, MySQL, PostgreSQL, etc.)

Required Skills, Education and/ or Certifications:

 Bachelor’s degree in business or I.T. related discipline accepted or equivalent experience.

Equal Opportunity Employer - Including Disabled and Veterans

#HBS","Almacenamiento de datos, Base de datos relacional, Extraer, transformar y cargar (ETL) y Microsoft BI Suite, Análisis de las necesidades, Bases de datos, Comunicación, Necesidades empresariales, Requerimientos operacionales y SQL Server Analysis Services (SSAS)",Solicitar
https://www.linkedin.com/jobs/view/3982512415/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=QRlwuHsaPn0%2FXjwEhcyr1g%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Boston, MA","Acerca del empleo
Why Jerry?

Join a pre-IPO startup with capital, traction and runway ($240M funded | 40X revenue growth in 4 years | $2T market size)
Work closely with brilliant leaders and team members who have scaled companies like Nvidia, Better, Nerdwallet, Wayfair, Sofi, etc. 
Disrupt a massive market and take us to a $5B business in the next few years
Be immersed in a talent-dense environment and greatly accelerate your career growth

About the opportunity: 

Jerry is building the first AllCar™ super app to help people optimize all aspects of owning a car – insurance, buy/sell, registration, loans, safety, repairs, parking, etc – a $2T market in the U.S. We started with insurance in 2019, and since then we’ve launched loan refinancing, real-time driving insights, a repair marketplace, car diagnostics, and a GenAI-powered chatbot. We have amassed over 5M customers, raised $240MM in funding, scaled our revenue 40X and our team to 250 across 7 countries.

The Data team at Jerry is responsible for supporting intelligent business decision making through analytical insights and advanced models. As a senior data engineer, you bring your previous industry experience in infrastructure scaling to Jerry and lead the development of a new infrastructure solution that will continue to perform with great stability and efficiency even when the company grows 10x bigger. You will join a team of data engineers, data analysts, and data scientists who operate with urgency and passion for excellence in pursuit of our mission to build the first AllCar™ super app.

How you will make an impact:

Partner with data scientists, software engineers and cross-functional stakeholders (Marketing, Operations, Finance) to build data pipelines that will provide critical data and insights to drive growth for the business, while ensuring data availability and accuracy
Partner with data scientists and machine learning engineers to evolve, optimize, and integrate critical predictive models
Partner with software engineers and data scientists to build data pipelines to consume and refine application data
Define data engineering standards and best practices and drive operational excellence
Perform regular maintenance and optimization of our data infrastructure

Ideal profile:

Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related technical discipline. Master’s degree in Computer Science or Engineering preferred
5+ years of experience in data engineer or similar role
Proven success in communication with database users, software engineers and senior management to collect requirements, discuss data modeling decisions and define data engineering strategies
Outstanding communication and problem solving skills, eager to work on the most pressing business problems and drive impact for the company
Experience with data modeling, data warehousing, and ETL pipeline development
Experience with column-based databases (Amazon Redshift, ClickHouse, etc.) and infrastructure integration with analytical platforms (Tableau, Qlik, Power BI, etc.)
Hands-on experience and advanced knowledge of SQL
Experience with big data technologies (Spark, Clickhouse, Redshift, Snowflake etc.)
Experience in process automation and deployment of advanced machine learning models

Jerry is proud to be an Equal Employment Opportunity employer. We prohibit discrimination based on race, religion, color, national origin, sex, pregnancy, reproductive health decisions or related medical conditions, sexual orientation, gender identity, gender expression, age, veteran status, disability, genetic information, or other characteristics protected by applicable local, state or federal laws. 

Jerry is committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, please contact us at recruiting@getjerry.com

About Jerry:

Jerry is America’s first and only AllCar™ app. We are redefining and radically improving how people manage owning a car, one of their most expensive and time-consuming assets.

Backed by artificial intelligence and machine learning, Jerry simplifies and automates owning and maintaining a car while providing personalized services for all car owners' needs. We spend every day innovating and improving our AI-powered app to provide the best possible experience for our customers. From car insurance and financing to maintenance and safety, Jerry does it all.

We are the #1 rated and most downloaded app in our category with a 4.7 star rating in the App Store. We have more than 6 million customers — and we’re just getting started.

Jerry was founded in 2017 by serial entrepreneurs and has raised more than $242 million in financing.

Join our team and work with passionate, curious and egoless people who love solving real-world problems. Help us build a revolutionary product that’s disrupting a massive market.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Automatización de procesos, Bases de datos, Comunicación, Modelado de datos, Oil Pipeline Development y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982515265/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=4BsnVIMR6p%2F%2FKvYSehHMuw%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Cambridge, MA","Acerca del empleo
Why Jerry?

Join a pre-IPO startup with capital, traction and runway ($240M funded | 40X revenue growth in 4 years | $2T market size)
Work closely with brilliant leaders and team members who have scaled companies like Nvidia, Better, Nerdwallet, Wayfair, Sofi, etc. 
Disrupt a massive market and take us to a $5B business in the next few years
Be immersed in a talent-dense environment and greatly accelerate your career growth

About the opportunity: 

Jerry is building the first AllCar™ super app to help people optimize all aspects of owning a car – insurance, buy/sell, registration, loans, safety, repairs, parking, etc – a $2T market in the U.S. We started with insurance in 2019, and since then we’ve launched loan refinancing, real-time driving insights, a repair marketplace, car diagnostics, and a GenAI-powered chatbot. We have amassed over 5M customers, raised $240MM in funding, scaled our revenue 40X and our team to 250 across 7 countries.

The Data team at Jerry is responsible for supporting intelligent business decision making through analytical insights and advanced models. As a senior data engineer, you bring your previous industry experience in infrastructure scaling to Jerry and lead the development of a new infrastructure solution that will continue to perform with great stability and efficiency even when the company grows 10x bigger. You will join a team of data engineers, data analysts, and data scientists who operate with urgency and passion for excellence in pursuit of our mission to build the first AllCar™ super app.

How you will make an impact:

Partner with data scientists, software engineers and cross-functional stakeholders (Marketing, Operations, Finance) to build data pipelines that will provide critical data and insights to drive growth for the business, while ensuring data availability and accuracy
Partner with data scientists and machine learning engineers to evolve, optimize, and integrate critical predictive models
Partner with software engineers and data scientists to build data pipelines to consume and refine application data
Define data engineering standards and best practices and drive operational excellence
Perform regular maintenance and optimization of our data infrastructure

Ideal profile:

Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related technical discipline. Master’s degree in Computer Science or Engineering preferred
5+ years of experience in data engineer or similar role
Proven success in communication with database users, software engineers and senior management to collect requirements, discuss data modeling decisions and define data engineering strategies
Outstanding communication and problem solving skills, eager to work on the most pressing business problems and drive impact for the company
Experience with data modeling, data warehousing, and ETL pipeline development
Experience with column-based databases (Amazon Redshift, ClickHouse, etc.) and infrastructure integration with analytical platforms (Tableau, Qlik, Power BI, etc.)
Hands-on experience and advanced knowledge of SQL
Experience with big data technologies (Spark, Clickhouse, Redshift, Snowflake etc.)
Experience in process automation and deployment of advanced machine learning models

Jerry is proud to be an Equal Employment Opportunity employer. We prohibit discrimination based on race, religion, color, national origin, sex, pregnancy, reproductive health decisions or related medical conditions, sexual orientation, gender identity, gender expression, age, veteran status, disability, genetic information, or other characteristics protected by applicable local, state or federal laws. 

Jerry is committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, please contact us at recruiting@getjerry.com

About Jerry:

Jerry is America’s first and only AllCar™ app. We are redefining and radically improving how people manage owning a car, one of their most expensive and time-consuming assets.

Backed by artificial intelligence and machine learning, Jerry simplifies and automates owning and maintaining a car while providing personalized services for all car owners' needs. We spend every day innovating and improving our AI-powered app to provide the best possible experience for our customers. From car insurance and financing to maintenance and safety, Jerry does it all.

We are the #1 rated and most downloaded app in our category with a 4.7 star rating in the App Store. We have more than 6 million customers — and we’re just getting started.

Jerry was founded in 2017 by serial entrepreneurs and has raised more than $242 million in financing.

Join our team and work with passionate, curious and egoless people who love solving real-world problems. Help us build a revolutionary product that’s disrupting a massive market.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Automatización de procesos, Bases de datos, Comunicación, Modelado de datos, Oil Pipeline Development y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982517071/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=7WH7PB4tKpbLMkJ8pz24DQ%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Denver, CO","Acerca del empleo
Why Jerry?

Join a pre-IPO startup with capital, traction and runway ($240M funded | 40X revenue growth in 4 years | $2T market size)
Work closely with brilliant leaders and team members who have scaled companies like Nvidia, Better, Nerdwallet, Wayfair, Sofi, etc. 
Disrupt a massive market and take us to a $5B business in the next few years
Be immersed in a talent-dense environment and greatly accelerate your career growth

About the opportunity: 

Jerry is building the first AllCar™ super app to help people optimize all aspects of owning a car – insurance, buy/sell, registration, loans, safety, repairs, parking, etc – a $2T market in the U.S. We started with insurance in 2019, and since then we’ve launched loan refinancing, real-time driving insights, a repair marketplace, car diagnostics, and a GenAI-powered chatbot. We have amassed over 5M customers, raised $240MM in funding, scaled our revenue 40X and our team to 250 across 7 countries.

The Data team at Jerry is responsible for supporting intelligent business decision making through analytical insights and advanced models. As a senior data engineer, you bring your previous industry experience in infrastructure scaling to Jerry and lead the development of a new infrastructure solution that will continue to perform with great stability and efficiency even when the company grows 10x bigger. You will join a team of data engineers, data analysts, and data scientists who operate with urgency and passion for excellence in pursuit of our mission to build the first AllCar™ super app.

How you will make an impact:

Partner with data scientists, software engineers and cross-functional stakeholders (Marketing, Operations, Finance) to build data pipelines that will provide critical data and insights to drive growth for the business, while ensuring data availability and accuracy
Partner with data scientists and machine learning engineers to evolve, optimize, and integrate critical predictive models
Partner with software engineers and data scientists to build data pipelines to consume and refine application data
Define data engineering standards and best practices and drive operational excellence
Perform regular maintenance and optimization of our data infrastructure

Ideal profile:

Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related technical discipline. Master’s degree in Computer Science or Engineering preferred
5+ years of experience in data engineer or similar role
Proven success in communication with database users, software engineers and senior management to collect requirements, discuss data modeling decisions and define data engineering strategies
Outstanding communication and problem solving skills, eager to work on the most pressing business problems and drive impact for the company
Experience with data modeling, data warehousing, and ETL pipeline development
Experience with column-based databases (Amazon Redshift, ClickHouse, etc.) and infrastructure integration with analytical platforms (Tableau, Qlik, Power BI, etc.)
Hands-on experience and advanced knowledge of SQL
Experience with big data technologies (Spark, Clickhouse, Redshift, Snowflake etc.)
Experience in process automation and deployment of advanced machine learning models

Jerry is proud to be an Equal Employment Opportunity employer. We prohibit discrimination based on race, religion, color, national origin, sex, pregnancy, reproductive health decisions or related medical conditions, sexual orientation, gender identity, gender expression, age, veteran status, disability, genetic information, or other characteristics protected by applicable local, state or federal laws. 

Jerry is committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, please contact us at recruiting@getjerry.com

About Jerry:

Jerry is America’s first and only AllCar™ app. We are redefining and radically improving how people manage owning a car, one of their most expensive and time-consuming assets.

Backed by artificial intelligence and machine learning, Jerry simplifies and automates owning and maintaining a car while providing personalized services for all car owners' needs. We spend every day innovating and improving our AI-powered app to provide the best possible experience for our customers. From car insurance and financing to maintenance and safety, Jerry does it all.

We are the #1 rated and most downloaded app in our category with a 4.7 star rating in the App Store. We have more than 6 million customers — and we’re just getting started.

Jerry was founded in 2017 by serial entrepreneurs and has raised more than $242 million in financing.

Join our team and work with passionate, curious and egoless people who love solving real-world problems. Help us build a revolutionary product that’s disrupting a massive market.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Automatización de procesos, Bases de datos, Comunicación, Modelado de datos, Oil Pipeline Development y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3980905120/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=L1azxoMHzkdt7N%2FTAxWwkw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 6 días,"Cincinnati, OH","Acerca del empleo
About AMEND: 
AMEND is a management consulting firm based in Cincinnati, OH with areas of focus in operations, analytics, and technology, focused on strengthening the people, processes, and systems in organizations to generate a holistic transformation. Our three-tiered approach provides a distinct competitive edge and allows us to build strong relationships and create customized solutions for every client. We work each day to change lives and transform businesses, and we are constantly striving to make a positive impact on our community! The AMEND team continues to grow at a rapid pace, and our technical team will continue to be an important part of that journey. 

Overview:
The Data Engineer consultant role is an incredibly exciting position in the fastest growing segment of AMEND. You will be working to solve real-world problems by designing cutting edge analytic solutions while surrounded by a team of world class talent. You will be entering an environment of explosive growth with ample opportunity for development. We are looking for individuals who can go into a client and optimize (or re-design) companies data architecture, who are the combination of a change agent, technical leader and passionate about transforming companies for the better. We need someone who is a problem solver, a critical thinker, and is always wanting to go after new things; you’ll never be doing the same thing twice!

Job Tasks:
Create and maintain optimal data pipeline architecture
Assemble large, complex data sets that meet functional / non-functional business requirements
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics
Work with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs
Define project requirements by identifying project milestones, phases, and deliverables
Execute project plan, report progress, identify and resolve problems, and recommend further actions
Delegate tasks to appropriate resources as project requirements dictate
Design, develop, and deliver audience training and adoption methods and materials

 Qualifications:
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. Databricks and DBT experience is a plus
Experience building and optimizing data pipelines, architectures, and data sets
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
Strong analytic skills related to working with structured and unstructured datasets
Build processes supporting data transformation, data structures, metadata, dependency, and workload management
A successful history of manipulating, processing, and extracting value from large, disconnected datasets
Ability to interface with multiple other business functions (internally and externally)
Desire to build analytical competencies in others within the business
Curiosity to ask questions and challenge the status quo
Creativity to devise out-of-the-box solutions
Ability to travel as needed to meet client requirements

 What’s in it for you?
Competitive pay and bonus
Continued education and individual development plans
Unlimited Vacation
Full Health, Vision, Dental, and Life Benefits
Paid parental leave
PTO for your Birthday
3:1 charity match

All this to say – we are looking for talented people who are excited to make an impact on our clients. If this job description isn’t a perfect match for your skillset, but you are talented, eager to learn, and passionate about our work, please apply! Our recruiting process is centered around you as an individual and finding the best place for you to thrive at AMEND, whether it be with the specific title on this posting or something different. One recruiting conversation with us has the potential to open you up to our entire network of opportunities, so why not give it a shot? We’re looking forward to connecting with you.
*Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of employment Visa at this time.*","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Datasets, Establecer prioridades del trabajo, Modelado de datos, Necesidades empresariales y Planes de proyecto",Solicitar
https://www.linkedin.com/jobs/view/3982994513/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=s%2BTGTpXJRVQmO8Rspd8gvw%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer - Hybrid/Southfield, MI","Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 días,"Michigan, Estados Unidos","Acerca del empleo
Title: Data Engineer

Location: Hybrid/Southfield, MI, US

Job Type: Full-time with Benefits

Who We Are

For more than 25 years, MSX International has been a dedicated partner to leading automotive brands around the world. We support them in transforming their businesses and in managing their operations across the areas of Customer Experience, Repair Optimization, Learning and Insights. We focus on helping our clients generate more value for their customers. With over 5,000 employees based in more than 80 countries worldwide, our teams provide industry-leading expertise. We have combined our deep industry expertise with cutting-edge technology solutions to help our automotive clients increase revenue and reduce costs, while enhancing operational efficiency and improving customer satisfaction.  Our goal is to help our customers reach their full potential and to excel as their global partner of choice.

Summary

MSX is launching a new data driven startup team. This team will be built from individuals who want to be entrepreneurs and create new innovative digital products for the mobility industry. We are looking for individuals who exhibit curiosity, are extremely resourceful, enjoy being creative, bring lots of energy & have grit.

This position is responsible for designing, developing, and maintaining business data platforms that allow insights into customer data. The Data Engineer will use SQL skills to execute high quality projects and assignments with a high degree of accuracy in a timely manner.

Essential Duties And Responsibilities

Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result. 
Ensure data quality and implement tools and frameworks for automating the identification and notification of data quality issues. 
Profile data sources and develop ETL processes with knowledge of data modeling fundamentals, using both SQL and supporting ETL/ELT tools. 
Assists management in creating estimates and proposals for clients. 
Plan effective data storage, security, sharing and publishing within the organization. 
Create and update documentation of process flows and business rules. 
Maintain current workflows and create new ones for the automated processing of large amounts of data. 


Minimum Qualifications

Bachelor's degree in computer science, statistics, mathematics, or relevant discipline 
1-3 years of Data engineering and/or data warehousing experience 
Experience in designing and developing ETL/ELT data pipelines. 
Proficiency in writing Advanced SQL queries. 
Proficiency in ANSI SQL. 
Familiarity with T-SQL and PL/SQL. 
Strong analytical skills with ability to understand and communicate the meaning of the data being presented. 
Excellent Mathematical/Statistical and Logic skills. 
Excellent Reasoning skills. 
Enthusiastic, highly motivated and ability to learn quickly. 


Preferred Qualifications

1-3 years automotive operations experience, or similar business (quick-paced environment). 
Experience with Databricks. 
Experience building Azure cloud data solutions and migrating from on-prem to cloud. 
Previous external client facing experience. 
Familiarity with administering MS SQL Server or Oracle. 
Familiarity with GIT repositories. 
Strong communication skills and a working knowledge of agile development, including DevOps concepts. 
Experience with data visualization tools (Tableau, PowerBI, etc.) 


MSX VALUES 

We get it done – We are decisive, solution oriented, and results focused. Motion is our first instinct. 
We are exceptional teammates – We put collective success before individual achievement. We back each other, embrace diverse perspectives, and win as one team. 
We prove our words with our actions – We are proud of the journey. We are guided by our values to do what is right for our clients, our company, and our teammates. 
We deliver on our commitments – We share openly, question respectfully, and once a decision is made, own it fully. We are accountable to one another, acknowledge our mistakes, and take positive action. 
We are fearless explorers – We are unafraid to discover new roads. We harness our creativity to find better ways of navigating the future. 
We create a better tomorrow for our clients every single day – We guide, advise, and drive our clients towards superior outcomes. 


Language Skills

Full Professional Proficiency in English is required 


PHYSICAL DEMANDS 

While performing the duties of this job, the employee is regularly required to:

Call, video, email, message and communicate with dealers and co-workers 


The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

WORK ENVIRONMENT 

Hybrid Office environment (2-3 days per week) / Southfield, MI 
The noise level in the work environment is usually moderate. 


The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

MSX International is an Equal Employment Opportunity Employer committed to employing a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veteran status, age, or any other characteristic protected by law.

Please note, MSXi did not provide any salary data for this position. If there is a salary range included in the posting the data was estimated by the job posting site and does not reflect our company's actual salary ranges. Actual starting salaries are determined based on job requirements and level of experience.

Apply NOW!!!

#MSXNAJobs","Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, ANSI, Azure Databricks, Capacidad de razonamiento, Ciencias de la computación, Inglés como lengua extranjera, Modelado de datos y Reglas de empresa",Solicitar
https://www.linkedin.com/jobs/view/3955435939/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=HfpP5Y1nKwJJXsq4yPInrA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Analyst - Remote,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Sacramento, CA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos y Lenguajes de programación, Ciclo de vida de desarrollo de software (SDLC), Comunicación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3977516269/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=ojxtgvt6FVunNqArDp%2Fw2Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Remote,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Bloomington, IL","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Positions 

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación y Programación, Comunicación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3982511468/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=PBXP%2BqZc3DOBmBptUiXSKA%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Portland, OR","Acerca del empleo
Why Jerry?

Join a pre-IPO startup with capital, traction and runway ($240M funded | 40X revenue growth in 4 years | $2T market size)
Work closely with brilliant leaders and team members who have scaled companies like Nvidia, Better, Nerdwallet, Wayfair, Sofi, etc. 
Disrupt a massive market and take us to a $5B business in the next few years
Be immersed in a talent-dense environment and greatly accelerate your career growth

About the opportunity: 

Jerry is building the first AllCar™ super app to help people optimize all aspects of owning a car – insurance, buy/sell, registration, loans, safety, repairs, parking, etc – a $2T market in the U.S. We started with insurance in 2019, and since then we’ve launched loan refinancing, real-time driving insights, a repair marketplace, car diagnostics, and a GenAI-powered chatbot. We have amassed over 5M customers, raised $240MM in funding, scaled our revenue 40X and our team to 250 across 7 countries.

The Data team at Jerry is responsible for supporting intelligent business decision making through analytical insights and advanced models. As a senior data engineer, you bring your previous industry experience in infrastructure scaling to Jerry and lead the development of a new infrastructure solution that will continue to perform with great stability and efficiency even when the company grows 10x bigger. You will join a team of data engineers, data analysts, and data scientists who operate with urgency and passion for excellence in pursuit of our mission to build the first AllCar™ super app.

How you will make an impact:

Partner with data scientists, software engineers and cross-functional stakeholders (Marketing, Operations, Finance) to build data pipelines that will provide critical data and insights to drive growth for the business, while ensuring data availability and accuracy
Partner with data scientists and machine learning engineers to evolve, optimize, and integrate critical predictive models
Partner with software engineers and data scientists to build data pipelines to consume and refine application data
Define data engineering standards and best practices and drive operational excellence
Perform regular maintenance and optimization of our data infrastructure

Ideal profile:

Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related technical discipline. Master’s degree in Computer Science or Engineering preferred
5+ years of experience in data engineer or similar role
Proven success in communication with database users, software engineers and senior management to collect requirements, discuss data modeling decisions and define data engineering strategies
Outstanding communication and problem solving skills, eager to work on the most pressing business problems and drive impact for the company
Experience with data modeling, data warehousing, and ETL pipeline development
Experience with column-based databases (Amazon Redshift, ClickHouse, etc.) and infrastructure integration with analytical platforms (Tableau, Qlik, Power BI, etc.)
Hands-on experience and advanced knowledge of SQL
Experience with big data technologies (Spark, Clickhouse, Redshift, Snowflake etc.)
Experience in process automation and deployment of advanced machine learning models

Jerry is proud to be an Equal Employment Opportunity employer. We prohibit discrimination based on race, religion, color, national origin, sex, pregnancy, reproductive health decisions or related medical conditions, sexual orientation, gender identity, gender expression, age, veteran status, disability, genetic information, or other characteristics protected by applicable local, state or federal laws. 

Jerry is committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, please contact us at recruiting@getjerry.com

About Jerry:

Jerry is America’s first and only AllCar™ app. We are redefining and radically improving how people manage owning a car, one of their most expensive and time-consuming assets.

Backed by artificial intelligence and machine learning, Jerry simplifies and automates owning and maintaining a car while providing personalized services for all car owners' needs. We spend every day innovating and improving our AI-powered app to provide the best possible experience for our customers. From car insurance and financing to maintenance and safety, Jerry does it all.

We are the #1 rated and most downloaded app in our category with a 4.7 star rating in the App Store. We have more than 6 million customers — and we’re just getting started.

Jerry was founded in 2017 by serial entrepreneurs and has raised more than $242 million in financing.

Join our team and work with passionate, curious and egoless people who love solving real-world problems. Help us build a revolutionary product that’s disrupting a massive market.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Automatización de procesos, Bases de datos, Comunicación, Modelado de datos, Oil Pipeline Development y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3984144282/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=iTLk5kGsbFkz6DFAB%2Fj8HQ%3D%3D&trackingId=8RPUVSOpqTBFY%2BMT3b45eg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (IICS & GBQ),"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Atlanta, GA","Acerca del empleo
Job Description

The Data Engineer will analyze, design, develop, performance tuning, test their code as well as others code and support initiatives to build an enterprise integration framework while providing inputs for improving standards for data mapping, integration, and data traceability across the NAPA business functions.

"" Five or more years experience in software engineering.

"" Five or more years experience in large scale RDBMS environments or Google BigQuery

"" Two or more years of Exadata experience OR Google BigQuery

"" Four or more years experience with Informatica PowerCenter or IICS

"" One or more years experience in Erwin

"" Experience in code automation (e.g. pattern based integration)

"" Experience in advanced SQL and PL/SQL techniques

"" Experience in building re-usable Utility packages

"" Experience with testing the code

"" Focus on continuous improvement

"" Experience in Unix shell and Python scripting

"" Integration design & data modeling skills in Data lake and Data Warehousing environments

"" Experience with Streaming technologies is a plus( STRIIM, Kafka, etc)

"" Experience with other Informatica tools is plus e.g., Metadata manager, Analyst, DVO, Data Quality

"" Exposure to both on-prem and cloud Integration solutions

"" Familiarity with non-relational DB technologies is a plus

"" Experience with automated testing

"" Experience with both batch and real-time patterns for integrations

"" Ability to build and analyze complex integration workflows from heterogeneous data sources

"" Experienced in large Enterprise Data Warehouse & Integration projects.

"" Strong background in full lifecycle development using multiple platforms or languages.

"" Ability to interact at a technical and non-technical level with Infrastructure, Network, Development, BA and QA teams.

"" Development experience in high transaction/high availability systems.

"" Experience with analyzing and recommending solutions for Production is sues short-term and long term

Responsibilities

"" The ETL Engineer performs design, development and implementation of integration processes for both the Enterprise Data lake, Data Warehouse and Applications

"" Analyzes requirements and existing resources to create efficient database and integration designs that meet company IT standards.

"" Works with project and business analyst leads to develop and clarify in-depth technical requirements.

"" Participates in all phases of the integration development lifecycle, including unit testing, quality assurance(QA) and ongoing support.

"" Helps with Production support as needed","Google BigQuery, Lenguajes de programación, PL/SQL, PowerCenter y Sistema de gestión de bases de datos relacionales, Amdocs CRM, Erwin, Informatica PowerCenter, Modelado de datos y Requisitos técnicos",Solicitar
https://www.linkedin.com/jobs/view/3947018417/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=R0TxmziReCKr6R7ZZD%2F0Ag%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"150 US$K/año - 250 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,Estados Unidos,"Acerca del empleo
Are you interested in transforming climate tech?
Do you thrive in a fast-moving R&D environment with state-of-the-art technologies?
Would you like to build advanced petabyte-scale systems for AI models?

We're working with an open-source climate AI start-up focused on building transformative ML models to predict weather and global climate challenges. The team brings a depth of world-class experience from Google X, MIT, and Descartes Labs. They plan to democratize access to powerful, open-source climate information, enabling researchers, government agencies, academics, and other groups to leverage vast datasets for predictive modeling on a petabyte scale.

Role Responsibilities:
Architect and manage petabyte-scale infrastructure for processing geospatial data related to current/historic climate models (including satellite, radar, and other types).
Support ML research in an MLOps capacity.
Utilize Python for developing and maintaining data infrastructure.
Implement and manage big data technologies such as Apache Parquet, Beam, Google Cloud Dataflow.
Use cloud services like AWS/Azure for data processing and storage.

Required Qualifications:
Strong experience working with large-scale data infrastructure and processing (geospatial data an added bonus).
Proficiency in Python or equivalent.
Expertise with data tools (e.g. Apache Beam, Parquet, BigQuery).
Desire to wear hats and make a big impact with a world-class team.
Familiarity with cloud services (AWS/GCP/Azure)

Please note, this company is not yet considering sponsorship of visas (e.g. H-1B). US citizens and permanent residents will be prioritized, at least a the start of this search.

Big Cloud is a data science, machine learning and AI recruiting firm. We’re lucky enough to recruit the best candidates into the most exciting companies all over the world. We try to reply to all unsuccessful applications, but we’re only human (for now)!

In the meantime, check out our jobs page to see what else we’re recruiting for.","Almacenamiento de datos, Amazon Web Services (AWS), Big data, Infraestructura en la nube, Ingeniería de datos y Python, Bases de datos, Infraestructura de datos, Investigación y desarrollo (I+D), MLOps y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984135664/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=RAK1Ljfu4mSUlgPEmAlWiQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Phoenix, AZ","Acerca del empleo
Looking for a Data Engineer with SQL, Python, ADF, Databricks, Kafka and ideally Snowflake experience to work 100% remote.

 Data Engineer to develop CI/D data pipelines and ETL processes to curate and transform pharmacy data from different sources (both on premise and cloud).
 Build large scale databases that are robust and secure to be stored in a data lake or data warehouse.
 Automate data workflows and processes including ingestion, cleaning, structuring, formatting of data.
 Build engineering solutions that support Client/data science projects.
 Collaborate with Data Scientists and business partners to deploy machine learning models in production (preferred).
 Extensively work on different kinds of datasets including text, voice, images, unstructured, structured.

Requirements

 4-6 years of enterprise data warehouse development - preferably on SQL or Snowflake
 4-6 years of experience creating, enhancing, and maintaining ETL frameworks using ETL tools.
 Hands-on experience with SQL, PL/SQL, Python and/or Shell Scripting is a must.
 Experience migrating from RDBMS to Snowflake is a plus.
 End-to-end dataflow design and development experience is a plus.
 Hands-on experience with Azure Cloud, including Azure Data Factory and Databricks.
 Kafka experience to setup/attach to Databricks.

Aptitudes y experiencia deseables
DATA WAREHOUSE, SQL, PYTHON, AZURE, DATA FACTORY, DATABRICKS, KAFKA, SNOWFLAKE","Azure Data Factory, Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos , Lenguajes de programación, PL/SQL, Python y SQL, Guiones shell y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3921547388/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=LZ9vtenixOnDrk9BP2oFmw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"60 US$K/año - 70 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 meses,"Rochester, NY","Acerca del empleo
Description

We are on the lookout for a highly skilled Data Engineer to join our team! This role is based in Rochester, New York and will primarily involve defining and leading data projects, spearheading data management practices, and collaborating with various teams to improve data quality.

Responsibilities

 Lead and define data projects in collaboration with Data Scientists and Engineers to enhance data workflows.
 Implement advanced solutions to meet complex data challenges.
 Develop and enforce data management practices, ensuring the highest quality of data in our data lake.
 Ensure compliance with data privacy standards.
 Collaborate with the Business Intelligence and Analytics team to share knowledge and improve data quality.
 Utilize skills in Microsoft SQL Server, Azure Synapse Analytics, C Sharp Programming (C#), Java, and Python to execute job functions effectively.
 Monitor and optimize data systems to ensure their performance and reliability.
 Develop protocols for data acquisition, processing, and utilization.
 Collaborate with stakeholders to understand and meet data requirements.
 Maintain accurate documentation to facilitate future project developments and optimizations

Requirements

 A minimum of 2 years of experience in a Data Engineer role or similar capacity
 Proficiency in Microsoft SQL Server
 Experience with Azure Synapse Analytics
 Knowledge of C Sharp Programming (C#)
 Proficiency in Java programming language
 Proficiency in Python programming language
 Relevant experience in the Healthcare, Hospitals, and Social Assistance industry is a plus
 Excellent problem-solving skills and attention to detail
 Strong communication skills and ability to work in a team environment
 Bachelor's degree in Computer Science, Information Systems, or related field is preferred

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, Calidad de datos, Comunicación, Java, Optimización y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3980268047/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=5JIMPg6r40HORkVO07zbsQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Exciting Opportunity Alert! Join our Team as a Data Engineer!

hackajob has partnered with a cutting-edge company seeking a talented Data Engineer with expertise in Machine Learning and Python to join our innovative team!

Role: Data Engineer
Location: Remote
Salary: $150K - $240K depending on experience + benefits package

Required Skills:
Experience with Python, Data Mining, and SQL
Experience with Machine Learning, Artificial Intelligence, Unstructured Data
Experienced in Spark, Apache Airflow, ETL, CI/CD, Scala
Experience with multiple programming languages and statistical packages

If you're interested in finding out more about this fantastic opportunity please get your application in and we can arrange a call.

hackajob is a recruitment platform that will match you with relevant roles based on your preferences and in order to be matched with the roles you need to create an account with us.

*This role requires you to be based in the US*","Aprendizaje automático, Extraer, transformar y cargar (ETL), Procesamiento de lenguaje natural, Python y Scala, Azure Databricks y Large Language Models (LLM)",Solicitar
https://www.linkedin.com/jobs/view/3981806884/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=eo0n5m6Xog%2FCZ3ipSfnhhQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Charleston, WV","Acerca del empleo
iO Associates is hiring for a Data Engineer who can support a data and data pipeline optimization project.

This is a long-term contract position (6-12+ months) and is 100% remote, though applicants do need to be based in the US to be considered.

What You'll Do:
Identify stakeholders' data needs, define the project scope, and select the right implementation to address stakeholder pain points.
Bridge the gap between the domain requirements and technology during the tool selection process. Your role involves ensuring that the chosen technology aligns with those requirements and the overall domain strategy.
Maintaining constant communication with stakeholders regarding project progress, blockers, and expected timelines is crucial.
Collaborate with stakeholders to continuously automate and improve existing workflows and/or processes.
Develop, deploy and maintain workflow management tools such as Airflow, Jenkins etc in cloud environments.
Using cloud technology such as AWS (Lambda, S3, EC2, ECS, DynamoDB, SQS), Kubernetes, Docker, etc.
Work alongside the infrastructure, data ingestion, data warehouse and analytics teams to manage and optimize ETL and data pipeline parameters, scheduling, monitoring, and alerting.
Collaborate with other engineers and teams to implement large-scale data processing systems to map and standardize data.
Maintain and optimize data pipelines, architectures, and data sets
Build, develop and maintain data models, reporting systems, data automation systems, etc.

 Your Background:
8+ years of experience as a Data Engineer
Strong communication skills.
Strong Snowflake experience
Strong experience in Python development.
Experience with a workflow manager - ideally Airflow
Experience working with AWS (Lambda, S3, EC2, ECS, DynamoDB, SQS)","Airflow, Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, Bases de datos, Comunicación, Modelo de datos y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3976447679/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=%2Ftz%2BsxPT5WB7pWmVSk982Q%3D%3D&trk=flagship3_search_srp_jobs,AWS Data Engineer - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,Estados Unidos,"Acerca del empleo
Job Description

We currently have a career opportunity for a Lead Developer with streaming (Kafka, Kinesis), Postgres database and batch processing using Glue.

Job Overview:

As a Lead Technical Developer, you will be hands-on with knowledge of streaming (Kafka, Kinesis), SQL Postgres databases and experience with batch processing. You will participate in all aspects of the software development lifecycle which includes estimating, technical design, implementation, documentation, testing, deployment and support of application developed for our clients. As a member working in a team environment you will work with solution architects and developers on interpretation/translation of wireframes and creative designs into functional requirements, and subsequently into technical design.

Responsibilities

You will have expert knowledge of streaming services, SQL Postgres and batch processing along with AWS. 
Lead the technical planning & requirements gathering phases including estimate, develop, test, manage projects, architect and deliver. 
Serve as a technical lead and mentor. Provide technical support or leadership in the development and continual improvement of service. 
Develop and maintain effective working relationships with team members. 
Demonstrate the ability to adapt and work with team members of various experience level. 

Qualifications

Passionate coders with 3-5 years of application development experience using AWS. 
Expert knowledge of streaming services i.e. Kafka and Kinesis. 
Experience with batch processing. 
Client facing or consulting experience highly preferred. 
Skilled problem solvers with the desire and proven ability to create innovative solutions. 
Flexible and adaptable attitude, disciplined to manage multiple responsibilities and adjust to varied environments. 
Future technology leaders- dynamic individuals energized by fast paced personal and professional growth. 
Phenomenal communicators who can explain and present concepts to technical and non-technical audiences alike, including high level decision makers. 
Bachelor’s Degree in MIS, Computer Science, Math, Engineering or comparable major. 
Solid foundation in Computer Science, with strong competencies in data structures, algorithms and software design. 
Knowledge and experience in developing software using agile methodologies. 
Proficient in authoring, editing and presenting technical documents. 
Ability to communicate effectively via multiple channels (verbal, written, etc.) with technical and non-technical staff. 

The salary range for this position takes into consideration a variety of factors, including but not limited to skill sets, level of experience, applicable office location, training, licensure and certifications, and other business and organizational needs. The new hire salary range displays the minimum and maximum salary targets for this position across all US locations, and the range has not been adjusted for any specific state differentials. It is not typical for a candidate to be hired at or near the top of the range for their role, and compensation decisions are dependent on the unique facts and circumstances regarding each candidate. A reasonable estimate of the current salary range for this position is $7,300 to 170,080. Please note that the salary range posted reflects the base salary only and does not include benefits or any potential equity or variable bonus programs. Information regarding the benefits available for this position are in our benefits overview .

Applications will be accepted until the position is filled or the posting is removed.

Who We Are

Perficient is a leading global digital consultancy. We imagine, create, engineer, and run digital transformation solutions that help our clients exceed customers’ expectations, outpace competition, and grow their business. With unparalleled strategy, creative, and technology capabilities, our colleagues bring big thinking and innovative ideas, along with a practical approach to help our clients – the world’s largest enterprises and biggest brands succeed.

What We Believe

At Perficient, we promise to challenge, champion, and celebrate our people. You will experience a unique and collaborative culture that values every voice. Join our team, and you’ll become part of something truly special.

We believe in developing a workforce that is as diverse and inclusive as the clients we work with. We’re committed to actively listening, learning, and acting to further advance our organization, our communities, and our future leaders… and we’re not done yet.

Perficient, Inc. proudly provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, genetic information, marital status, amnesty, or status as a protected veteran in accordance with applicable federal, state and local laws. Perficient, Inc. complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. Perficient, Inc. expressly prohibits any form of unlawful employee harassment based on race, color, religion, gender, sexual orientation, national origin, age, genetic information, disability, or covered veterans. Improper interference with the ability of Perficient, Inc. employees to perform their expected job duties is absolutely not tolerated.

Disability Accommodations:

Perficient is committed to providing a barrier-free employment process with reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or accommodation due to a disability, please contact us.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time.

About Us

Perficient is always looking for the best and brightest talent and we need you! We’re a quickly growing, global digital consulting leader, and we’re transforming the world’s largest enterprises and biggest brands. You’ll work with the latest technologies, expand your skills, experience work-life balance, and become a part of our global community of talented, diverse, and knowledgeable colleagues.

 

Select work authorization questions to ask when applicants apply

1. Are you legally authorized to work in the United States?

2. Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?","Apache Kafka y SQL, Amazon Kinesis, Bases de datos, Ciencias de la computación, Diseño técnico, Planificación técnica, PostgreSQL, Procesamiento por lotes y Wireframing",Solicitar
https://www.linkedin.com/jobs/view/3942534963/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=ah0sMKZApDaGO1DMsGi9bA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Las Vegas, NV","Acerca del empleo
We are seeking an experienced Data Engineer to join the Business Intelligence team at PLAYSTUDIOS. As a Data Engineer, you will play a vital role in developing and maintaining robust data pipelines, optimizing data architecture, and ensuring data availability and quality for analysis and reporting. Your expertise in data integration, ETL processes, and database management will be instrumental in driving our data-driven initiatives and supporting the playAWARDS team. This is an exciting opportunity to join a world-class team and exploit the opportunity for PLAYSTUDIOS to become the world’s leader in rewarded play.

Duties and Responsibilities

 Design, develop, and maintain scalable and efficient data pipelines for collecting, processing, and storing structured and unstructured data from various sources. 
 Implement ETL (Extract, Transform, Load) processes to transform raw data into usable formats. 
 Collaborate closely with data scientists, analysts, and other cross-functional stakeholders to identify data requirements and integrate new data sources into the existing infrastructure. 
 Monitor data quality and implement data validation and cleansing processes to ensure accuracy and consistency. 
 Tune and optimize data pipelines, queries, and database performance to ensure efficient data processing and retrieval. 
 Create and maintain comprehensive documentation for data pipelines, processes, and database schemas. 
 Ensure data consumers have access to the data and reporting they need. 
 Take pride in the quality of your work output. Your code, analyses, and documentation all reflect that of someone who is extremely detail-oriented. 

Requirements

 Bachelor’s degree in Computer Science, Information Technology, or a related field. 
 3+ years prior experience in data engineering or related roles 
 Proficiency in designing, building, and maintaining data pipelines for large-scale data processing. 
 Experience with relational and non-relational databases (e.g., PostgreSQL, MySQL, Snowflake, MongoDB) and data warehousing concepts. 
 Knowledge of data management, and working in both structured and unstructured data 
 Strong programming skills in languages such as SQL and Python. 
 Knowledge of AWS technologies (EMR, EC2, S3, etc.). 
 Experience with data integration toolsets (e.g., Apache Airflow, Talend) and writing and maintaining data pipelines. 
 Experience in data engineering pipelines and building data warehouse systems with ability to understand ETL principles and write complex SQL queries to extract data and build performant datasets. 
 Analytical mindset with the ability to troubleshoot and resolve technical challenges. 

Benefits and Perks

 100% health benefit premiums for you and your dependents 
 401K match and Restricted Stock Units 
 Flexible vacation policy 
 Employee-driven entertainment, happy hours, and team-building events 
 Snacks and drinks are available in the kitchen 
 Casual work environment 

About PLAYSTUDIOS

PLAYSTUDIOS is the developer and operator of award-winni ng free-to-play casual games for mobile and social platforms. The company’s collection of original and published titles is powered by its groundbreaki ng playAWARDS loyalty marketi ng platform, which enables players to earn real-world rewards from a portfolio of global entertainment, retail, technology, travel, leisure, and gami ng brands across 17 countries and four continents. Founded by a team of veteran gami ng , hospitality, and technology entrepreneurs, PLAYSTUDIOS bri ng s together beautifully designed mobile gami ng content with an innovative loyalty platform in order to provide its players with an unequaled entertainment experience and its partners with actionable business insights. To learn more about PLAYSTUDIOS, visit playstudios.com

PLAYSTUDIOS is an equal opportunity employer. Applicants will be considered regardless of their sex (includes pregnancy, childbirth, breastfeedi ng and related medical conditions), race, age, religion, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sexual orientation, gender, gender identity, gender expression, military or veteran status, or other protected category under the law. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

California Residents: Please review our California HR Data Privacy Policy for information about our data practices.

Apply Now","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Datasets, Datos no estructurados y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3979166743/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=%2FQsI6qrKa%2FzxwxB2XOIWRw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 1 semana,"Nashville, TN","Acerca del empleo
Description

Data Engineer

Come build the future as a Data Engineer at Amazon, where you will be inspired working along best-in-class inventors and innovators! You will have the opportunity to create meaningful experiences that deliver on the ever-evolving needs of our customers, and your work will impact millions of people around the world.

As an Amazon Data Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. You will find that there is an unlimited number of opportunities within Amazon, where developing your career across a wide range of teams is highly supported. We are committed to making your work experience as enjoyable as the experiences you’ll be creating for our customers.

Apply now and you will be eligible for Amazon Data Engineer positions that are based on your preferred location, team, and more. We’re hiring across Amazon Stores in the United States and Canada.

Teams with available positions include, but are not limited to:

 Consumer Technology: Build new generation features and products for amazon.com, constantly improving the Customer and Seller experience for billions around the globe. Whether building site wide features such as reviews and recommendations, category specific software for the likes of Pharmacy, Electronics, Digital Software and Video Games or seller infrastructure, there are a variety of complex problems to tackle using a range of technologies in the design of your technical solutions.
 Operations Technology: Shape the future of transportation planning and execution on a global scale, that impacts hundreds of fulfillment centers, thousands of Amazonians, and millions of customers across the world. Your technology will support thousands of operators worldwide to design, build and run the best-in-class Amazon transportation network. We are building intelligent software to make transportation more reliable, faster, and less costly, providing a better and less expensive experience for our customers.
 Human Resources Technology: Create a seamless experience for millions of Amazonians and/or candidates. Whether supporting technologies for onboarding, time and attendance, compensation, amazon.jobs, or recruiting, you’ll deliver robust feature sets, elegant designs, intuitive user interfaces and systems that make it easy for Amazonians to excel at performing critical business functions.

About Us

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance be-tween your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign opportunities based on what will help each team member develop into a better-rounded contributor.

Inclusive Team Culture

Here at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Key job responsibilities

 Design, implement, and support a platform providing secured access to large datasets.
 Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
 Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
 Tune application and query performance using profiling tools and SQL.
 Analyze and solve problems at their root, stepping back to understand the broader context.
 Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
 Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
 Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
 Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com LLC - A03

Job ID: A2703397","AWS Lambda, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Datasets, Modelado de datos y Object Storage",Solicitar
https://www.linkedin.com/jobs/view/3978691313/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=gzYQc3mNy8r%2B5KK7amiYrQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Platform Team,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Applied Systems, Inc., a worldwide leader in insurance technology, is currently searching for a Data Engineer for its AI Engineering team. In this role, you will be instrumental in building and maintaining data pipelines to support the team’s AI/ML initiatives.

What You’ll Do

Implement scalable and efficient data solutions
Manage and optimize data storage, partitioning, and indexing strategies to ensure high performance and reliability of our data infrastructure
Develop and implement features and enhancements leveraging your SQL and ETL expertise, and cloud-based data warehousing technologies
Collaborate with cross-functional teams to implement AI data requirements and deliver solutions that align with security, legal, and data governance guidelines
Ensure data integrity and quality by implementing robust data validation and error-handling
Implement the full lifecycle of data management, from ingestion to ETL process, and data expiry layers
Stay current with industry trends and advancements in data engineering and data technologies


What You’ll Need To Succeed

5+ years as a data engineer, with a focus on cloud-based data solutions
Proficiency in SQL and development of high-level code with languages such as Python or Scala to manipulate, store, manage, or retrieve data assets.
Understanding of data modeling, data warehousing concepts, and big data architectures
Working knowledge of Infrastructure as Code (IaC) and cloud native data solutions such as BigQuery, Spark, Pub/Sub, and Object Storage
Practical experience with Agile frameworks, ideally Scrum, and tools like Jira and Confluence
Ability to communicate in a team-oriented environment to clarify requirements and deliver solutions within committed timelines
Strong analytical and problem-solving skills, with a detail-oriented mindset
Bachelor-level degree in Computer Science, MIS, or CIS, or equivalent experience 
We proudly support and encourage people with military experience, as well as military spouses, to apply
What You’ll Gain

Benefits from Day One

Health insurance plans, dental, and vision
Wellness incentives
401(k) and/or RRSP retirement savings plans with employer match


Work-Life Balance

Competitive paid vacation time and a free day for your birthday
Personal/sick time
Paid holidays
Flex Time
Paid parental leave (U.S. candidates)
Volunteer time off


Empowering Career Growth and Success – We invest in talent, care about our people and are empowered by the results of our work. We grow our teams from within and give our employees opportunities to advance.

What We Value

We strive for excellence at every turn to be the best at what we do. We invest in talent, care about our people and are empowered by the results of our work. We fulfil the promise of insurance – safeguarding and protecting what matters most in people’s lives. And there is no more important job than that.

Our focus on the workforce, workplace and marketplace gives us a qualified individual in an environment in which they can be productive while we maintain our position in the industry. To help drive that change toward a vibrant, modern workplace, we have employee-driven networks with commonalities in ethnicity, gender, sexual orientation and military status.

Who We Are

Applied Systems is the leading global provider of cloud-based software that powers the business of insurance. Recognized as a pioneer in insurance automation and the innovation leader, Applied is the world’s largest agency and brokerage management systems provider, serving customers throughout the United States, Canada, the Republic of Ireland, and the United Kingdom. By automating the insurance lifecycle, Applied’s people and products enable millions of people worldwide to safeguard and protect what matters most.

For 40 years, Applied Systems has led an industry we helped to create with a mission to continuously improve the business of insurance. From partnerships, acquisitions, and insurance innovation initiatives, Applied has focused on efforts to be the indispensable partner in our industry.

It’s an exciting time at Applied. You can do big things here, in an environment that supports creative thinking and bold ideas. Visit http//www.AppliedSystems.com for more information on how you can challenge what’s possible.

EEO Statement

Applied Systems is proud to be an Equal Employment Opportunity and Affirmative Action Employer. Diversity and Inclusion is a business imperative and is a part of building our brand and reputation. At Applied, we don’t discriminate, and we are committed to recruit, develop, retain, and promote regardless of race, religion, color, national origin, sexual orientation, gender identity, disability, age, veteran status, and other protected status as required by applicable law.","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Google BigQuery, Ingeniería de datos y Scala, Amdocs CRM, Ciencias de la computación, Modelado de datos, Resolución de problemas y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3943563748/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=iHV4EpX%2BkiMTmLKTOQxNeg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Nueva York, NY","Acerca del empleo
About Treeswift

Treeswift is revolutionizing decision-making for large-scale landscapes and critical infrastructure in the face of rising threats like severe storms and wildfires.

After getting our start developing AI models for forests in the timber sector, we are growing in the utility sector. In the U.S. alone, hundreds of thousands of miles of transmission lines and millions of miles of distribution lines need to be monitored and managed to reduce hazards, such as encroaching vegetation that can cause outages or fires. With the rapid growth of renewable energy this infrastructure footprint is expected to more than double in the coming years creating more assets to defend than ever before.

Treeswift’s mission is to deliver effective scalable technology to manage the landscapes around large-scale distributed infrastructure, such as power lines. We do so by innovating across the technical stack. Data is collected by our sensor packs mounted on backpacks and vehicles to seamlessly integrate into customer operations. We then leverage AI (i.e. computer vision, etc…) to transform collected data into actionable insights delivered to decision-makers in our software platform.

We are early in our journey and focused on finding ground truth by working closely with our customers and using this understanding to deliver real value. To tackle this challenge we have brought together mission-driven robotics experts from top academic institutions (Penn, CalTech, etc…) and professionals with deep industry experience (Palantir) in enterprise software development. We hope you’ll join us.

What You Will Do

Treeswift is looking for a highly skilled and motivated Data Engineer who will:

Build robust connective tissue between our raw data and our customer experience. When raw data is uploaded there are several steps before the final customer-facing product. You will take the lead on architecting, implementing and maintaining a pipeline that automates and optimizes these steps. You will consider cost and performance in your design, collaborate with other team members to implement and engage with users internal and external to scope current and future needs.
Help enable rapid model development. Aside from customer delivery, data pipelines and products are a key requirement of our internal machine learning workflows. You will help facilitate the development of training datasets and model result evaluation. You are not expected to be a machine learning expert, but you will contribute to early infrastructure that can accelerate these outcomes.
Contribute where you are most needed. We are at an early stage and expect everyone to wear a few hats. You might be asked to help annotate some features, contribute a new end-user software feature (with support of course) or join a field visit to collect some sample data. We want you to be excited about learning new things, leveling up others and pitching in where you’re needed!

This is a full-time, hybrid (2-3 day a week in person) role, based out of our NYC office.

Required Skills

4+ years of experience working with data products, pipelines and relevant tooling
You have thoughtful opinions about data architecture and tooling to support that architecture. You can guide Treeswift to make decisions that enable near-term outcomes, but also scale well to future needs. Where trade-offs between these objectives exist you can articulate them and make a recommendation. 
You think critically about database design, understand how it supports downstream workflows and potential implications for reliability, scale and performance. 
Experience with AWS technologies (S3, RDS, ECS, etc…) 
You are proficient in Python
Strong communication and collaboration. 

Preferred Skills

Experience designing and implementing basic security protocols, and effectively communicating those to customer counterparts.
Experience supporting machine learning workflows (i.e. managing training datasets)
Experience with geospatial and imagery data 

What We Value

Mission first. We value low ego team members who focus on working towards the best outcomes for the customer and the business, regardless of who gets the credit. 
Truth seeking. We don’t always have perfect information to make decisions, but we seek to constantly get closer to the ground truth and aren’t afraid to learn we were wrong in the process.
Owners mindset. If you see an opportunity for improvement, run with it. We believe that good ideas can come from anywhere, no matter your role.

Benefits

Comprehensive medical, dental and vision insurance 
Life insurance package and disability coverage 
Stock options
Paid leave for new parents 
Unlimited PTO
401K

Salary

The estimated salary range for this position is $135,000-$170,000. Total compensation for this position is determined by skills, qualifications, relevant work experience, location, and other factors. This salary estimate excludes the value of any potential bonuses; the value of any benefits offered; and the potential future value of any long-term incentives. This information is provided per the New York City Human Rights Law. Please note that the range provided is applicable only to New York City-based applicants. Base compensation may vary if the work location is outside of New York City.

Treeswift is proud to be an equal opportunity employer. We provide employment opportunities without regard to age, race, color, ancestry, national origin, religion, disability, sex, gender identity or expression, sexual orientation, veteran status, or any other protected status in accordance with applicable law.

If you require any accommodations during the recruitment process, whether it be alternate forms of material, accessible meeting rooms, etc., please let us know and we will work with you to meet your needs.","Almacenamiento de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Pensamiento crítico, Bases de datos, Comunicación, Datasets y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3963329416/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=3amP8OMruP8AeGGjn9kp8A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - ETL,"60 US$/h - 65 US$/h Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,"Jersey City, NJ","Acerca del empleo
About Client:

The client provides information technology (IT) services, including business outsourcing, infrastructure technology, and application services. The application service offered by the company includes application development, maintenance, and support. The markets served by the company are financial services and insurance, healthcare, manufacturing, government, transportation, communications, and consumer and retail industries.

Rate Range: $65/hr C2C $55/hr W2

Salary Range: $130K per annum + Benefits

Location: Plano TX and Jersey City, NJ Hybrid 3 days office in week - Only local candidates within 40 Miles

Job Description:

Strong knowledge of SQL and its use in building Large-Scale ETL solutions
Good to Moderate knowledge of Python
Good to moderate knowledge of AWS and Terraform
ETL tools - More on the dataware house side >>> Snowflake and Aurora
Dynamo DB (NoSQL) nice to have - (not essential)
Looking for SQL and Python hands on. 
The candidate should have SQL background and it is the most important skill to have. 

About ApTask:

ApTask is a leading global provider of workforce solutions and talent acquisition services, dedicated to shaping the future of work. As an African American-owned and Veteran-certified company, ApTask offers a comprehensive suite of services, including staffing and recruitment solutions, managed services, IT consulting, and project management. With a focus on excellence, collaboration, and innovation, ApTask provides unparalleled opportunities for professional growth and development. As a member of the ApTask team, you will have the chance to connect businesses with top-tier professionals, optimize workforce performance, and drive success across diverse industries. Join us at ApTask and be part of our mission to empower organizations to thrive while fostering a diverse and inclusive work environment.

Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.

Candidate Data Collection Disclaimer:

At ApTask, we prioritize safeguarding your privacy. As part of our recruitment process, certain Personally Identifiable Information (PII) may be requested by our clients for verification and application purposes. Rest assured, we strictly adhere to confidentiality standards and comply with all relevant data protection laws. Please note that we only collect the necessary information as specified by each client and do not request sensitive details during the initial stages of recruitment.

If you have any concerns or queries about your personal information, please feel free to contact our compliance team at businessexcellence@aptask.com.","Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos , NoSQL, Python y SQL, Dynamo, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3890303244/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=f5uESuoSV%2BLXUxShAdGd4Q%3D%3D&trk=flagship3_search_srp_jobs,Looking for Data Engineer- Remote- Contract(W2/1099),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 meses,Estados Unidos,"Acerca del empleo
Hi,

I hope you are doing well!

We have an opportunity for Data Engineer with one of our clients for  Remote 

Please see the job details below and let me know if you would be interested in this role.

If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.

Title: Data Engineer

Location: : Remote

Terms: Contract (W2/1099)

Job Description

Points to Remember:

We need someone who is very strong on SQL with Azure Data factory

good understanding of ETL and DWH concepts

Data Engineer

Mandatory skills: Strong SQL / Claims and NDW/VBP Experience preferred SQL Developer II 

The SQL Developer role will support several internal applications and participate in the modernization of HMA's internal systems.

This position will be responsible for the application of engineering to the design, development, implementation, testing and maintenance of software in a systematic method.

The roles in this function will cover all primary development activity across all technology functions that ensure we deliver code with high quality for our applications, products, and services and to understand customer needs.

Primary Responsibilities

Working with business users to gather requirements and design, create, and implement database systems / applications. 
Develop, design, and implement stored procedures, functions, and views using T-SQL.
Provide technical support and guidance to testing teams. 
Evaluating new tools, new technologies to help modernize existing applications. 
Must possess critical analytical and complex problem-solving skills. Required Qualifications: 
Strong proficiency with SQL (T-SQL preferred). 
Skilled at building and optimizing large, complicated SQL queries. 
Experience in analyzing and resolving performance issues with queries including query and database schema / index optimization. 
Knowledge of best practices when dealing with relational databases 
Capable of troubleshooting common database issues 
Bachelor's degree in IT or equivalent experience 
Excellent written and verbal communication skills Preferred Qualifications: 
Experience working in an agile environment with DevOps tools including Azure DevOps, Azure pipelines. 
Proven ability to work independently with less structured, more complex tasks 
EDI experience 
Knowledge / experience working in a Health Care / Health insurance environment. 
Experience integrating systems with external parties including Blue Cross and National Data Warehouse. 

Thanks & Regards

Priyanka tiwari

Extend Information System Inc

Phone: (703) 956-1120

Email: priyanka1@extendinfosys.com 

44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166","Capacidad de análisis y Extraer, transformar y cargar (ETL), Bases de datos, DWH, Intercambio electrónico de datos, Intercambio electrónico de datos de SAP (EDI), Lenguaje de consulta (query), Procedimientos de almacenado, Resolución de incidencias y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3839335764/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=2vBYHnbUsZ8P3mx9TVRdlw%3D%3D&trk=flagship3_search_srp_jobs,Remote Big Data Operations engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 meses,Estados Unidos,"Acerca del empleo
Big Data Operations Engineer

Good Linux skills bash scripting, administration etc. 3-5 years
Kubernetes, Spark, Docker working experience 1-2 years with real time experience
Understanding of Python scripting/Java basics required
on-prem experience is required

Additional Preferred Skills

Nice to have experience with cloud environments

Project Description

Supporting in-house batch data ingestion frameworks and tooling, including support for internal customers. Environment is hybrid on-premise hardware and AWS.","Apache Spark, Big data, Hive, Ingeniería de datos , Python y Sqoop, Java y Secuencia de comandos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3963027404/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=PmduaWu3kMWpYHY1ZnK4Jg%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, Rust","100 US$K/año - 500 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 días,Estados Unidos,"Acerca del empleo
Edge & Node is at the forefront of web3 innovation. Our mission is to establish The Graph as the unbreakable foundation of open data. Our pioneering subgraphs set the industry standard and solidify The Graph as the premier solution for organizing and accessing blockchain data.

At Edge & Node, we champion a decentralized future based on shared values. Dedicated to decentralizing power and resisting censorship, we aim for a robust, permissionless information era free from central control, thus eliminating the traditional vulnerabilities associated with misplaced trust.

The Graph Network Engineering team is focused on building and maintaining core software components that make The Graph work. We are especially focused on graph-node, an Open Source Rust project that provides the core indexing capabilities of The Graph.

We are looking for a Data Engineer with deep understanding of relational databases and modern data stacks, and a desire to code in Rust. A Rust Data Engineer at E&N will not just use Rust data tools, but build our general purpose blockchain data systems and also contribute to our dependencies.

What You’ll Be Doing

Working within the Graph Node team to improve Graph Node’s capabilities as a data processing and querying engine
Familiarizing yourself with the large and complex Rust codebase that is Graph Node
Using subgraphs to understand both the specification implemented by Graph Node as well as the developer experience of the builders who are our users.
Develop an understanding of existing code and design choices, and then analyze how they can be experimented with and improved upon
Setting up performance tests, doing quantitative assessments of any proposals and changes you make, and monitoring how the changes ultimately behave when rolled out
Experimentally validating, and if necessary, falsifying your own ideas.
Reviewing pull requests of your colleagues and taking responsibility for the reviewed code as if it were your own
Documenting important aspects of the software while understanding that Graph Node has a large community of independent operators that need to understand how to run, configure, and monitor Graph Node

What We Expect

You’ve contributed more than 10,000 lines of code to a production software product
Thinking about data in SQL terms comes naturally for you
You have an understanding of some of the theory behind data systems, such as relational algebra, and an understanding of fundamental DB data structures and data formats, such as BTrees, LSM Trees, and compression formats
You have an interest in higher-level query languages such GraphQL
You have an interest in federated data and federated query frameworks
You understand how small design choices can have large effects in big data systems
You are comfortable working with an existing system and codebase and making fundamental improvements to it while being mindful of backwards compatibility
You understand tracing and debugging requirements of complex software
You have an understanding of the academia and theory that backs the software libraries you use but also understand the pragmatic necessity of shipping usable software quickly

Compensation/Culture

The overall market range for roles at Edge & Node is typically $100k - $500k annually. This market range is based on total compensation (vs. only base salary). Edge & Node has a culture that values meritocracy, personal growth, and an ego-free environment.

About the Graph

The Graph is the source of data and information for the decentralized internet. As the original decentralized data marketplace that introduced and standardized subgraphs, The Graph has become web3’s method of indexing and accessing blockchain data. Since its launch in 2018, tens of thousands of developers have built subgraphs for dapps across 40+ blockchains - including Ethereum, Arbitrum, Optimism, Base, Polygon, Celo, Fantom, Gnosis, and Avalanche.

As demand for data in web3 continues to grow, The Graph enters a New Era with a more expansive vision including new data services and query languages, ensuring the decentralized protocol can serve any use case - now and into the future.

Discover more about how The Graph is shaping the future of decentralized physical infrastructure networks (DePIN) and stay connected with the community. Follow The Graph on X, LinkedIn, Instagram, Facebook, Reddit, and Medium. Join the community on The Graph’s Telegram, join technical discussions on The Graph’s Discord.

The Graph Foundation oversees The Graph Network. The Graph Foundation is overseen by the Technical Council. Edge & Node, StreamingFast, Semiotic Labs, The Guild, Messari, GraphOps, Pinax and Geo are eight of the many organizations within The Graph ecosystem.","Extraer, transformar y cargar (ETL) y SQL, Blockchain, Crypto, Ethereum, GraphQL, PostgreSQL, Rust (lenguaje de programación) y Web3",Solicitar
https://www.linkedin.com/jobs/view/3959056330/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=JkeG%2BdRtKJPGV29Dpaix4A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Databricks),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Bloomington, MN","Acerca del empleo
Title 

Data Engineer (Databricks)

Overview

This is a full-time, employee position. Please do not apply if you are seeking a C2C or 1099/W2 contract. 

 Must be located in one of the following: Atlanta, Chicago, Columbus, Dallas, Minneapolis, New York Area, St. Louis. This position is mostly Remote. 

Daugherty Business Solutions brings a fresh approach to data engineering by delivering results through unmatched innovation and world-class technology and talent. This is why many of the most well-known companies in the world trust us with their mission-critical projects. As a team member at Daugherty, you will play an integral role in our company’s success and are recognized and valued for your contributions. We have an entrepreneurial culture with the maturity and security of a 35+ year company helping you to create your best work and take your career to the next level.

We are seeking a skilled and experienced Data Engineer with expertise in Databricks, Azure, and Spark to join our dynamic team. The ideal candidate will play a key role in designing, developing, and maintaining scalable data pipelines and analytics solutions to support our growing business needs. This position offers an exciting opportunity to work with cutting-edge technologies and collaborate with cross-functional teams to drive actionable insights from data.

Responsibilities

 Design, develop, and deploy end-to-end data pipelines using Databricks, Azure Data Factory, and Spark to ingest, process, and analyze large volumes of structured and unstructured data. 
 Collaborate with data scientists, analysts, and other stakeholders to understand business requirements and translate them into technical solutions. 
 Optimize and tune data pipelines for performance, reliability, and scalability to ensure efficient processing of data. 
 Implement data governance and security best practices to ensure the integrity and confidentiality of data. 
 Develop and maintain documentation, including data flow diagrams, technical specifications, and user guides. 
 Stay up-to-date with emerging technologies and best practices in data engineering, cloud computing, and big data analytics. 


Qualifications

 Proven experience as a Data Engineer, with at least 5 years of hands-on experience in designing and building data pipelines. 
 Proficiency in Databricks, Azure services (e.g., Azure Data Lake Storage, Azure Synapse Analytics), and Spark for big data processing and analytics. 
 Strong programming skills in Python, Scala, or Java, with experience in writing complex SQL queries. 
 Experience with data modeling, ETL/ELT processes, and data warehousing concepts. 
 Excellent problem-solving skills and attention to detail, with the ability to troubleshoot and debug complex data engineering issues. 
 Strong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. 
 Able to articulate ideas clearly, present findings persuasively, and build rapport with clients and team members. 
 Ability to effectively communicate technical concepts to non-technical stakeholders .
 Relevant certifications in Databricks, Azure, or Spark (e.g., Azure Data Engineer, Databricks Certified Associate) are a plus. 
 Bachelor's degree in Computer Science, Engineering, or related field; Master's degree preferred. 


 What We Commit to YOU: 

 We provide many training opportunities like certifications, hackathons, lunch and learns and free access to Pluralsight, Udemy and other digital learning platforms. 
 You will get to work with some of the most innovative teams in the IT marketplace and solve real strategic problems. 
 We will invest in things that are important to you professionally and personally. 
 We will build a relationship with you to accelerate your career. 
 We will provide you with a team environment like no other. We are consistently ranked as a Top Workplace in many of our regions as voted by our employees. 
 We provide opportunities to build community, be social and have fun with your colleagues. 
 We provide a comprehensive compensation and benefits package. 


Daugherty Business Solutions is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.

If you require accommodations or assistance to complete the online application process, please inform any recruiter you are working with (or send an email to careers@daugherty.com) and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The recruiting team will respond to your email promptly.","Almacenamiento de datos, Apache Spark, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , SQL y Scala, Modelado de datos, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3925271464/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=%2Fsvf1l%2FeDE8bjI3QSyhuAg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",Publicado de nuevo hace 2 semanas,"Hoover, AL","Acerca del empleo
The Data Engineer is responsible for assisting with end-to end data consumption, from loading the data into the data warehouse to leveraging statistical models built with that data to gather critical insights to accommodate business needs.

Location listed is preferred office location but other locations and working remotely within the Renasant footprint may be considered based upon convenience and business necessity. 

RENASANT BANK IS AN EQUAL OPPORTUNITY EMPLOYER

Responsibilities 

Perform analyses of structured and unstructured data to solve basic business problems utilizing advanced statistical techniques and mathematical analyses
Document data gathering and processing and provides a detailed set of results and analytical metrics
Build, prototype, and deploy machine learning models in sandbox and production environments
Assist with developing and maintaining end-to-end ETL data pipelines
Develop SQL queries for data extraction from the data warehouse
Participate in requirements gathering discussions and collect the appropriate level of detail required to and drive towards immediate technical results to solve client problems
Design and develop data integration / transformation solutions
Develop ad hoc SQL queries to support internal requests for data
Create Entity Relationship Diagrams to design and document database solutions
Design, develop, and maintain SQL Server databases, tables, indexes, stored procedures, views, and triggers
Develop queries, views, and stored procedures to provide data to front end reporting solutions
Optimize slow running queries, views, and stored procedures
Troubleshoot and correct SQL Server performance issues
Perform other related duties as assigned

Qualifications

 High School Diploma or equivalent
Minimum of 5 years of developing MS SQL Server stored procedures, functions, tables, and view experience
Snowflake databases, developing front-end application and database administrator experience preferred
Experience working with a statistical programming language (R, Python or similar)
Experience working with Qlik Talend software
Knowledge of common unsupervised/supervised ML techniques
Hands-on experience using source control tools and incident tracking software
Working knowledge of all phases of the Software Development Life Cycle (SDLC) involving Systems Analysis, Design, Development, and Implementation
Proficiency with Microsoft Office products
Demonstrated ability to work and communicate well with coworkers and stakeholders

Physical Demands

The physical demands described are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is frequently required to stand or sit; kneel, stoop, or squat; use hands or fingers to handle or feel objects, tools or controls; reach with hands and arms, and talk or hear. The employee is occasionally required to walk. The employee must occasionally lift and /or move up to 25 pounds. Specific vision abilities required by this job include close vision, peripheral vision, depth perception and the ability to focus.

Work Environment

The Bank’s professional working environment requires employees to communicate effectively, both verbally and in writing. Employees must demonstrate strong interpersonal skills when working closely with internal business partners and external clients. Employees may be exposed to confidential and propriety information within the working environment, therefore, must uphold confidentiality at all times. Due to the possibility of being exposed to high risk situations (i.e. robbery), detailed instructions and procedures are required to be followed at all times to safeguard the Bank’s employees, customers, and assets.


The above is intended to describe the general content of and requirements for the performance of this job. It is not to be construed as an exhaustive statement of duties, responsibilities, or requirements. The principal duties and responsibilities enumerated are all essential job functions except for those that begin with the word “May”.

This job description is intended to describe the normal level of work required by the person performing the work. The principle duties outlined are the essential responsibilities and duties. Other duties may be assigned as needs arise. Job requirements and/or processes may be modified to reasonably accommodate persons with a disability as required by law.

This description is not intended as a contract and is subject to change. Any written contractual agreements supersede this job description.","Análisis de datos y SQL, Bases de datos y SQL Server Analysis Services (SSAS)",Solicitar
https://www.linkedin.com/jobs/view/3982955019/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=Z9Cw9m2pmXSAmhYxYNJrNQ%3D%3D&trk=flagship3_search_srp_jobs,GCP Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.",hace 5 días,Estados Unidos,"Acerca del empleo
Our client is seeking 3 new Data Engineers for a conversion project from Teradata to GCP.

Good interpersonal and communication skills are required as well as attention to detail.

Must Haves

8+ years of experience in Data Engineering working in Enterprise Environments. 
Strong GCP skills
Strong SQL skills
Data Conversion work: Ideally from Teradata to GCP experience 
Strong communication and collaboration skills, ability to communicate technical concepts and implications to business partners
Ability to handle multiple projects and activities in a timely, organized manner
High levels of self-motivation and attention to detail.
Healthcare expereince is preferred

Bethpage is an equal opportunity employer. We value a diverse workforce and encourage women, people of color, LGBTQIA individuals, people with disabilities, members of ethnic minorities, foreign-born residents, and others from minority groups and diverse backgrounds to apply. We do not discriminate on the basis of race, gender, religion, color, sexual orientation age, national origin, veteran status, marital status, or disabilities.","Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud , Ingeniería de datos y SQL, Atención al detalle, Comunicación, Comunicación interpersonal, Conversión de datos y Handle Multiple Projects",Solicitud sencilla
https://www.linkedin.com/jobs/view/3944486608/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=jTxrsqn1PClwcmyS8qhVIw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 2 semanas,"Austin, TX","Acerca del empleo
Who We Are

Apex Fintech Solutions (AFS) powers innovation and the future of digital wealth management by processing millions of transactions daily, to simplify, automate, and facilitate access to financial markets for all. Our robust suite of fintech solutions enables us to support clients such as Stash, Betterment, SoFi, and Webull, and more than 20 million of our clients' customers.

Collectively, AFS creates an environment in which companies with the biggest ideas in fintech are empowered to change the world. We are based in Dallas, TX and also have offices in Austin, New York, Chicago, Portland, and Belfast.

If you are seeking a fast-paced and entrepreneurial environment where you'll have the opportunity to make an immediate impact, and you have the guts to change everything, this is the place for you.

AFS has received a number of prestigious industry awards, including:

2021, 2020, 2019, and 2018 Best Wealth Management Company - presented by Fintech Breakthrough Awards
2021 Most Innovative Companies - presented by Fast Company
2021 Best API & Best Trading Technology - presented by Global Fintech Awards

About This Role

We are looking for a Data Engineer to help us create solutions for Apex Compliance and Regulatory team that leverage our next generation cloud-based data platform. Apex is recognized for disrupting the financial services industry, enabling fintech standouts like Stash, Webull and Betterment. You will design, develop, and deploy solutions on large scale trading data utilizing technologies such as Big Query, Looker, Cloud Functions, and more. You will build reporting infrastructure that enables users to access complex financial data with ease and deliver data sets that adhere to strict quality requirements to regulatory entities and vendors. Working alongside experienced engineers and product managers, you will be able to leverage your ideas and creativity to solve complex problems.

We are looking for someone who:

Is passionate - You have a genuine passion for technology. You love using technology differently to maximize opportunity and impact for customers and you have a way of bringing out that same fire in the people you work with. 
Is motivated - You are driven to be the best – whether that’s decreasing system down time or making an innovative change to “how it’s always been done” resulting in a more efficient way of supporting the customer. You challenge yourself by setting goals and exceeding them. 
Is collaborative - You are excited to work with fellow engineers and big thinkers. You know how to collaborate not only within the department, but also across the organization. 
Wants to make an impact - You are looking to do amazing work. You value preventing problems from occurring over being caught in the chaos zone putting out fires and looking for the “hero” spotlight. 
Strives for frictionless IT - You understand the importance of building great partnerships. You promote a seamless, smooth, user friendly and reliable environment. 

What you will do all day:

Be a data wizard - Create, manipulate, access, and deliver data in the most efficient ways possible to business users, end-customers, 3rd-party vendors, and application developers. 
Work the data - Create processes to load, transform, and deliver data to business users, end-customers, 3rd party vendors, and application developers. 
Learn the data - Utilize excellent analytical and problem-solving techniques to understand our complex data structures and put the data to work. Participate in all phases of the development process. 
See the data - Build reports, analytics and visualizations to help meet business initiatives and make decisions. 
Make us better - Identify, advocate for, and implement solutions to improve performance and efficiency across systems, APIs, and overnight batch processing. Develop quality code that is maintainable and avoids problems. Promote a culture for effective documentation and lessons learned. 
Be a great team member - Work as a member of an agile software development team to rapidly produce software. Balance both project-based and day-to-day support tasks. 
Show off your work - Embrace transparency and share metrics around our levels of service with the rest of the company and our customers. 
Live our culture - Embrace Apex’s values as our differentiator and be an example of them every day. 

The skills you will need to succeed: 

Bachelor’s degree in Computer Science, Computer Engineering or a similar field 
2+ years of data engineering and analysis 
2+ years of experience in ETL processes 
1+ years of experience with Python, Java 
1+ years of experience with Cloud Data technologies (Big Query, Looker, Google Cloud Storage, or similar) 
Advanced problem-solving, debugging, and troubleshooting skills 
Excellent client support skills 
Proficient with version control systems, ideally GitHub 
Ability and willingness to learn new things (languages, tools, frameworks) quickly 
Financial services background preferred 
Experience with the MS SQL Server experience nice to have 

Work Environment:

This job operates in an office environment. 

Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

#engineering #mid-senior #APEX

Our Rewards

We offer a robust package of employee perks and benefits, including healthcare benefits (medical, dental and vision, EAP), competitive PTO, 401k match, parental leave, and HSA contribution match. We also provide our employees with a paid subscription to the Calm app and offer generous external learning and tuition reimbursement benefits. At AFS, we offer a hybrid work schedule for most roles that allows employees to have the flexibility of working from home and one of our primary offices.

Diversity, Equity, Inclusion, and Belonging (DEIB) Commitment

We're looking for all kinds of people.

At Apex, we believe that wealth management and investing should be accessible to everyone, and we strive to create spaces to democratize investing for folks of all walks of life. Internally, we embrace diversity and are dedicated to creating an inclusive and equitable workplace, which reflects our company vision and mission. We value every team member's unique perspective and are committed to fostering a culture where everyone belongs. Join us in our mission to empower and celebrate individual differences.

Apex is committed to being an equal opportunity employer. We ensure that qualified applicants receive fair consideration for employment without discrimination based on sex, gender identity, gender expression, sexual orientation, race, color, natural or protective hairstyle, genetics, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law. Know your rights: workplace discrimination is illegal. We stand by this commitment to promote a diverse, equitable, and inclusive workforce.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery y Ingeniería de datos, Bases de datos, Ciencias de la computación, Lenguaje de consulta (query), Resolución de incidencias y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3980370032/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=AuwS8ScpKcw2EEQzpb6dYw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"70 US$/h - 75 US$/h Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 6 días,"El Segundo, CA","Acerca del empleo
Job Description:
 
 Pay Range: $70hr - $75hr
 
 Responsibilities: 
 Experience in Google Cloud Platform (GBQ, GCS, Kubernetes POD operator and Image creation, Composer, DAG creation). 
 Python Programming Expert. 
 Experience in Ascend tool. 
 Create Data flow, Data service, Read and Write connection for the various sources. 
 Failure Notification settings (Ascend Web hook notification, Scheduling in Ascend). 
 Big query merge to perform merge. 
 Custom Read and Write connector using python framework. 
 Good understanding in Error logging. 
 Experience in Airflow (Create DAG, Configure the variables in Airflow, Scheduling). 
 Experience in DBT to create the lineage in GCP. 
 Worked in Dev-Sec-Ops (CI/CD) environment. 
 Experienced with containerized execution/code containerization. 
 Interact with the platform leads and 3rd party application leads to understand and analyze the various interfaces and data integration needs to create technical user stories. 
 Contribute to the creation of technical design, estimation as applicable for the user stories and review with the EAC team and Product Owner. 
 Conduct and participate in code review sessions for the team deliverables. 
 Guides the team members and involves in development and unit testing. 
 Work with business users for UAT and assist QA team accordingly for defects fixing. 
 Help the team to troubleshoot critical issues in development, staging or prod Retail & Manufacturing Domain expertise.","Airflow, Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud y Ingeniería de datos, Diseño técnico y Lenguaje de consulta (query)",Solicitar
https://www.linkedin.com/jobs/view/3965969574/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=FnhwBjGrp%2F0EgwN9WXXnPw%3D%3D&trk=flagship3_search_srp_jobs,AWS Data engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 semanas,"Farmington, CT","Acerca del empleo
Title: AWS Data Engineer

Location: Farmington, CT

Duration: 6 months

Position type: W2 contract.

Required Skills & Experience

AWS (the dataset from the Cloud Engineer and Snowflake data lake), Snowflake, SQL, Python and PowerBI

Responsibilities

They will be designing, developing, and deploying a portfolio of Data & Analytics technology assets and platforms leveraging cloud-based tools and capabilities to capture, explore, transform, and deliver data. 
Additionally, they will analyze, document and develop solutions based on business needs and opportunities to deliver the intended outcomes in a timely manner. 
Key skills in AWS (the dataset from the Cloud Engineer and Snowflake data lake), Snowflake, SQL, Python and PowerBI willenable you to excel in this role. 
Put simply - We need a hands on Data Engineer that is really good with PowerBI and have experience working with AWS Data & Analytics Capabilities. 
They will like working with our business teams to rapidly build new datasets and dashboards.","Almacenamiento de datos, Análisis de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Datasets, Lagos de datos y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3970470210/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=DE%2Fc0pnds9ZHM2%2Bp79TLmQ%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer I, Professional Services, Google Cloud","Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 semanas,"Boulder, CO","Acerca del empleo
The application window will be open until at least July 18, 2024. This opportunity will remain online based on business needs which may be before or after the specified date.

Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Austin, TX, USA; Atlanta, GA, USA; Boulder, CO, USA; Chicago, IL, USA.Minimum qualifications:

Bachelor's degree in Computer Science, Mathematics, a related field, or equivalent practical experience.
3 years of experience with data processing software (e.g., Hadoop, Spark, Pig, Hive) and algorithms (e.g., MapReduce, Flume).
3 years of experience in Google Cloud.
Experience managing client-facing projects, troubleshooting technical issues, and working with Engineering and Sales Services teams.
Experience programming in Python and SQL.

Preferred qualifications:

Experience in technical consulting.
Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT, and reporting/analytic tools and environments.
Experience working with Big Data, information retrieval, data mining, or machine learning.
Experience in building multi-tier high availability applications with modern web technologies (e.g., NoSQL, MongoDB, SparkML, TensorFlow).
Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments.

About The Job

The Google Cloud Consulting Professional Services team guides customers through the moments that matter most in their cloud journey to help businesses thrive. We help customers transform and evolve their business through the use of Google’s global network, web-scale data centers, and software infrastructure. As part of an innovative team in this rapidly growing business, you will help shape the future of businesses of all sizes and use technology to connect with customers, employees, and partners.

As a Data Engineer, you will guide customers on how to ingest, store, process, analyze, explore, and visualize data on Google Cloud Platform. You will lead data migrations and transformations, partner with clients to architect scalable data processing systems, build efficient data pipelines, and resolve platform challenges.

In this role, you will collaborate with Google's strategic cloud customers and our team to successfully implement Google Cloud products.

Google Cloud accelerates every organization’s ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google’s cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.

The US base salary range for this full-time position is $118,000-$174,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.

Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.

Responsibilities

 Act as a trusted technical advisor to customers and solve complex Big Data challenges. 
 Create and deliver best practice recommendations, tutorials, blog articles, sample code, and technical presentations, tailoring approach and messaging to varied levels of business and technical stakeholders. 
Analyze on-premises and cloud database environments and consult on the optimal design for performance and deployment on Google Cloud Platform.
Travel regularly up to 30% of the time, in-region for meetings, technical reviews, and onsite delivery activities.
Communicate effectively via video conferencing for meetings, technical reviews, and onsite delivery activities.


Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .","Ciencia de datos, Computación en la nube, Extraer, transformar y cargar (ETL) y Google Cloud, Ciencias de la computación, Comunicación, Evaluaciones técnicas, Presentaciones técnicas, Resolución de incidencias y Tutoriales",Solicitar
https://www.linkedin.com/jobs/view/3932398932/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=nzfoFNH12BYsvAkx1%2Fc1%2BA%3D%3D&trk=flagship3_search_srp_jobs,Data Infrastructure Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,Estados Unidos,"Acerca del empleo
Codeworks is an IT Services firm headquartered in SE Wisconsin, known for our strong commitment to quality and for our direct client relationships.

Who We’re Looking For: A Data Infrastructure Engineer to partner with Data Scientists and Developers working on a cutting-edge, innovative team tasked with supporting clinical research, AI solutions, application development, and deployment of these solutions within a practice of over 2,500 physicians. 100% Remote, contract or permanent position!

Key Responsibilities:

Design, implement, and maintain data pipelines that collect, process, and store large volumes of data from various sources.

Ensure data quality, integrity, and consistency throughout the pipeline.

Create and manage scalable data storage solutions, such as data warehouses, data lakes, and databases.

Optimize data storage for efficient retrieval and processing.

Set up and manage computational environments and clusters, including cloud-based resources and on-premises hardware.

Ensure that data scientists have access to the necessary computational power for their analyses and model training.

Implement and enforce data security measures to protect sensitive information.

Ensure compliance with relevant regulations and industry standards (e.g., GDPR, HIPAA).

Develop infrastructure for deploying machine learning models into production environments.

Monitor the performance and health of deployed models, ensuring they remain accurate and efficient.

Provide tools and platforms (such as Jupyter Notebooks, version control systems, and collaborative workspaces) that enable data scientists to work efficiently and collaboratively.

Implement best practices for code management, experiment tracking, and reproducibility.

Continuously optimize data processing workflows for speed and efficiency.

Implement techniques to reduce computational costs and improve resource utilization.

Required Skills and Qualifications:

Strong knowledge of programming languages (such as Python, R, and SQL) and experience with big data technologies (such as Hadoop, Spark, and Kafka).

Expertise in cloud platforms like AWS, Google Cloud Platform, or Azure, including services for data storage, processing, and machine learning.

Proficiency in managing relational databases (like PostgreSQL, MySQL) and NoSQL databases (like MongoDB, Cassandra).

Skills in ETL (Extract, Transform, Load) processes, data warehousing, and building robust data pipelines.

Experience with DevOps practices, including CI/CD (Continuous Integration/Continuous Deployment) pipelines, containerization (using Docker), and orchestration tools (such as Kubernetes).

Understanding of data security practices, encryption methods, and regulatory compliance requirements.

Ability to analyze complex systems and workflows to identify bottlenecks and areas for improvement.

About Codeworks: Codeworks has over 25 years of experience serving Fortune 1000 companies in Wisconsin as well as our client's national locations. Our recruiting team excels at evaluating, advising, and connecting IT professionals with new opportunities that will satisfy their expectations regarding income and opportunity for growth. At Codeworks, we're committed to diversity, equity, and inclusion in our workforce and beyond. We believe in equal opportunities and value the unique perspectives that every individual brings to our team. Join us in creating an inclusive, innovative, and collaborative workplace where your talents can thrive.

Codeworks is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, disability, or national origin.","Analítica y Infraestructura en la nube, Administración de sistemas, Autenticación, Bases de datos, Cassandra y Resolución de incidencias",Solicitar
https://www.linkedin.com/jobs/view/3919919946/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=HRG%2FWAnRIJ2f9h8WQcpzfA%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist/Engineer - Junior,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"Tucson, AZ","Acerca del empleo
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this market also our candidates get multiple job offers and $100k + salaries.

 please check the below links to see success outcomes of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 https://www.youtube.com/watch?v=OFoqPTNORew

 https://www.youtube.com/watch?v=-HkNN1ag6Zk

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

 https://youtu.be/bJJl27D8bh0

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3984230847/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=s4nJHWleXqjnoOA4BQ9a8w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Lafayette, LA","Acerca del empleo
Position Description

The best version of us starts with You!

CGI is seeking a Data Engineer who can bring in expertise and industry standard methodologies define better development. This is an exciting opportunity to augment your current skills, as well as learn new technologies.

This role can be performed from Lafayette, LA, Bloomfield, CT, Nashville, TN and Plano, TX in a Hybrid model.

Your future duties and responsibilities

Drive automation pyramid and integrate with CI/CD tools for continuous validation.

Drive mentality of building well architected applications for Cloud

Can identify code defects and work with other developers to address quality issues in product code.

Passion for finding bottlenecks and thresholds in existing code through the use of automation tools.

Collaborate on architecture decisions and develop solutions in AWS – Including Python, Databricks, Redshift, elastic search, DynamoDB, Lambda, Kinesis, Glue and AI/ML tools.

Create Proof of concepts, innovate and integrate cloud-based software in a continuous delivery environment.

Required Qualifications To Be Successful In This Role

At least 5+ years of Data Engineering experience with web development frameworks experience.

Extensive experience in AWS Cloud Computing, AWS Services and Security.

Cypress or Angular CLI work experience.

5+ years of experience in Scripting language (Python, SQL).

Experience in Swagger, Postman and Docker experience.

Experience in Semantic Layer Design and implementation.

Experience in Data Governance and Data Modelling.

Experience with Frontend build tools.

Proficient in Java and Java based web frameworks.

Database management (relational and non-relational).

Data Virtualization and data federation experience.

Database management (relational and non-relational) experience.

Knowledge in Data Integration, Mapping and Data Migration tools.

Education: Bachelor's degree in computer science or related field.

#DICE

#CSG-TMC-F24

CGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to: skill set level; experience and training; and licensure and certifications. CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is $85,800 - 209,100.

At CGI we call our professionals “members” to reinforce that all who join our team are, as owners, empowered to participate in the challenges and rewards that come from building a world-class company. CGI’s benefits include:

 Competitive base salaries
 Eligibility to participate in an attractive Share Purchase Plan (SPP) in which the company matches dollar-for-dollar contributions made by eligible employees, up to a maximum, for their job category
 401(k) Plan and Profit Participation for eligible members
 Generous holidays, vacation, and sick leave plans
 Comprehensive insurance plans that include, among other benefits, medical, dental, vision, life, disability, out-of-county emergency coverage in all countries of employment;
 Back-up child care, Pet insurance, a Member Assistance Program, a 529 college savings program, a personal financial management tool, lifestyle management programs and more

Together, as owners, let’s turn meaningful insights into action.

Life at CGI is rooted in ownership, teamwork, respect and belonging. Here, you’ll reach your full potential because…

You are invited to be an owner from day 1 as we work together to bring our Dream to life. That’s why we call ourselves CGI Partners rather than employees. We benefit from our collective success and actively shape our company’s strategy and direction.

Your work creates value. You’ll develop innovative solutions and build relationships with teammates and clients while accessing global capabilities to scale your ideas, embrace new opportunities, and benefit from expansive industry and technology expertise.

You’ll shape your career by joining a company built to grow and last. You’ll be supported by leaders who care about your health and well-being and provide you with opportunities to deepen your skills and broaden your horizons.

Come join our team—one of the largest IT and business consulting services firms in the world.

Qualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status, political affiliation, genetic information, height, weight, or any other legally protected status or characteristics.

CGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the Position ID of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you. Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a Position ID will not be returned.

We make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members.

All CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. Dependent upon role and/or federal government security clearance requirements, and in accordance with applicable laws, some background investigations may include a credit check. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances.

CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGI’s legal duty to furnish information.","Extraer, transformar y cargar (ETL), Gobierno de datos, Ingeniería de datos y Python, API de Postman, API de Swagger, Amazon Redshift, Bases de datos, Ciencias de la computación y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983403326/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=VrLkpQOvlWTHbKnNrWDNig%3D%3D&trackingId=fJHbPVTcRto5fQU4ayqmCQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Birmingham, AL","Acerca del empleo
Job#: 2039228

Job Description:

Our Fortune 500 client is looking for a Data Engineer with 3+ years of experience!

Location: Birmingham, AL (must be within 50 miles of Birmingham)

Duration: 1 year contract with potential to convert

On-site requirements: 3x a week on-site and 2x remote

**Must be authorized to work to in the US and not require sponsorship- no C2C**

please apply directly by emailing a word copy of your resume and a brief write up of your relevant data engineering experience to [email protected]

Job Summary:

The Data Engineer plays a pivotal role in advancing data-driven decision-making and operational efficiency across business units. Working closely with data scientists and within a cloud environment, this role requires expertise in data engineering, cloud technologies, and collaborative problem-solving.

Key Responsibilities:

Data Pipeline Development:

 Assist in designing, building, and maintaining scalable data pipelines to ingest, transform, and store large volumes of AMI data from various sources. 
 Collaborate with data scientists to ensure seamless integration of data pipelines with analytical models and AI processes. 

Data Modeling and Architecture:

 Develop and implement data models and architectures optimized for AMI data analytics, ensuring efficiency, scalability, and data integrity. 
 Implement best practices for data storage, partitioning, and indexing to optimize performance and facilitate analysis. 

Cloud Environment Management:

 Work within cloud environments - Azure to deploy, manage, and optimize data engineering solutions. 
 Collaborate with cloud architects and administrators to ensure security, compliance, and cost-effectiveness of cloud infrastructure. 

Visualization and Reporting:

 Develop and maintain Power BI dashboards and reports to visualize AMI data insights and facilitate data-driven decision-making. 
 Create custom UI/UX applications 
 Ensure the accuracy, reliability, and usability of visualizations to meet business requirements. 

Data Cataloging and Documentation:

 Catalog and document AMI data sources, datasets, and metadata to facilitate data discovery, lineage, and governance. 
 Implement data cataloging best practices to ensure the availability and accessibility of AMI data assets. 

Collaboration and Support:

 Collaborate with cross-functional teams to understand data requirements and support analytical initiatives. 
 Provide technical support and troubleshooting for data-related issues, ensuring the reliability and availability of data infrastructure. 

Job Requirements:

Education/Experience:

 Bachelor’s degree in Computer Science, Information Technology, or related field. 
 3+ years of experience in data engineering or related roles, preferably in a cloud environment. 

Knowledge, Skills & Abilities:

 Proficiency in programming languages such as Python, SQL, and Scala. 
 Proficiency with Power BI for data visualization and reporting. 
 Experience with cloud-based data platforms (e.g., Azure Databricks, AWS EMR). 
 Strong understanding of data modeling, ETL processes, and data warehousing concepts. 
 Familiarity with big data technologies 

Behavioral Attributes:

 Innovative and adaptable, with a passion for continuous learning and improvement. 
 Strong communication and collaboration skills, with the ability to work effectively across teams. 
 Results-oriented and committed to delivering high-quality solutions that meet business needs. 
 Ethical conduct and commitment to safety in all aspects of work. 

EEO Employer

Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] or 844-463-6178 .

Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.

Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.

4400 Cox Road

Suite 200

Glen Allen, Virginia 23060

Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] (Do not submit resumes or solicit consultants to this email address). UnitedHealthcare creates and publishes the Transparency in Coverage Machine-Readable Files on behalf of Apex Systems.","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Ciencias de la computación, Datasets, Modelado de datos, Modelo de datos y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3971001821/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=RK6l3mw9hTa5B4BzcKo4Iw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.",hace 1 mes,"San Francisco, CA","Acerca del empleo
About Company:
Our client is a liquidity and technology provider of solutions for the crypto and foreign exchange (FX) industry. The company specializes in the sphere of B2B services and products, catering to a wide range of clients including large licensed brokers, crypto exchanges, crypto brokers, forex brokers, hedge and crypto funds, and professional managers. The company’s advanced base of ready-to-use technical solutions enables brokers to save time and money on consuming infrastructure projects and focus on enlarging their client base and increasing their revenues.
Responsibilities:
Design and implement scalable data pipelines, warehouses
Develop complex ETL/ELT processes to cleanse, transform, and model raw data into actionable formats.
Collaborate with data analysts and team to provide the data tools, infrastructure, and support they needs
Automate data workflows and build monitoring systems to ensure seamless operations and enhance efficiency.
Evaluate and implement new technologies and tools to improve data processing workflows.
Ensure data security and compliance with regulatory requirements.

Required candidate skills: 
5+ years of experience in data engineering
Experience with Big data technologies
Experience with Cloud platforms
Strong Proficiency in Python and SQL for data manipulation, analysis, and model development.
Experience working with Airflow, dbt, Databricks
Experience with data visualization tools (e.g., Tableau, Power BI) 

Languages:
Russian - fluent;
English - Upper intermediate.

Job Offer:
Hybrid work environment. 
Opportunity to work from offices (depending on the location of the candidate). 
21 paid holidays. 
Amazing networking events within the group. 
Growth opportunities within the group.","Airflow, Analítica, Analítica de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Automatización y Manipulación de datos",Solicitar
https://www.linkedin.com/jobs/view/3948949246/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=1uyxJsZ%2BtwtBsYcPdjBOHg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - 75% Remote,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 1 mes,"Hoffman Estates, IL","Acerca del empleo
Work at OMRON!

OMRON is a global leader in the field of automation, an $8 billion global technology company celebrating more than 80 years of success. OMRON’s business fields cover a broad spectrum, ranging from industrial automation and electronic components to social systems and healthcare. OMRON Management Center of America, Inc. is the regional headquarters for OMRON in the Americas.

Data Engineer

As a Data Engineer at OMRON, you will play a crucial role in designing, implementing, and maintaining robust data pipelines and infrastructure on Snowflake, leveraging your expertise in data integration tools and AWS cloud services. You will work closely with cross-functional teams to ensure the efficient flow of data across various systems and platforms, while also validating data integrity and adherence to best practices. Additionally, you will contribute to data modeling efforts and collaborate with data Analysts/scientists to support advanced analytics initiatives. Experience with Power BI visualizations will be considered a plus.

Our Commitment To Employees

Training and Career Development Program to give employees a learning path with the necessary tools and resources they need to help build their career at Omron.
Great financial opportunities with competitive compensation, immediate 401k match with 100% vesting, profit sharing, and Blue Cross Blue Shield for medical, dental, vision and prescription drug benefits.
Community Awareness that includes activities with local non-profit organizations and a Matching Gift Program.
Work-Life Balance with Flexible Work Arrangements, Flexible Work Hours, and Sick/Vacation/Holiday Pay.
Wellness Activities such as Walking Contests, Nutritional Learning Sessions, On Site Flu shots and Health Screenings.

Responsibilities

Design, develop, and maintain data pipelines on Snowflake using Appflow, IICS, or other data integration tools.
Ensure data quality and integrity by implementing rigorous validation processes and recommending best practices for data management.
Collaborate with stakeholders to understand data requirements and translate them into scalable and efficient solutions.
Implement and optimize data models to support analytics and reporting needs, considering performance and scalability requirements.
Work closely with AWS cloud services, leveraging various tools and services for data storage, processing, and analysis.
Collaborate with data experts to deploy machine learning models and support data science projects.
Develop Power BI visualizations and dashboards to provide insights to stakeholders and drive decision-making.
Stay abreast of emerging technologies and industry trends, continuously improving processes and solutions.
Other duties and miscellaneous projects as needed

Job Requirements

Bachelor’s degree in business or technical field; master’s degree preferred.
Proven experience as a Data Engineer, with a focus on Snowflake, data integration tools, and AWS cloud services.
Strong proficiency in SQL
Experience with programming languages such as Python is a big plus
Experience with data modeling concepts and tools.
Familiarity with data science techniques and tools is a plus.
Experience with Power BI or other data visualization tools is preferred.
Excellent problem-solving skills and attention to detail.
Strong communication and collaboration skills, with the ability to work effectively in a team environment

Don't meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Omron we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.

Omron is an Equal Opportunity Employer. We provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, we comply with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.","Analítica, Ciencia de datos, Ingeniería de datos y SQL, Calidad de datos, Comunicación, Modelado de datos, Modelo de datos, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3940491775/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=6j2jajgMImyvD5j7PVRz4A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Nueva York, NY","Acerca del empleo
About Kustomer

Kustomer is the industry leading conversational CRM platform perfecting every customer experience. Built with intelligent tools such as AI and Automation, no code-configuration and a connected data platform that unifies data from multiple sources through a single timeline, Kustomer empowers businesses to operate with greater efficiency and deliver more personalized service to customers across any channel, making every interaction more meaningful and memorable. Today, Kustomer is the core platform for some of the leading customer service brands like Ring, Glovo, Away Travel, Priceline and Sweetgreen.

Kustomer was founded in 2015 by serial entrepreneurs Brad Birnbaum and Jeremy Suriel and has raised over $200M in funding backed by leading VCs. Meta announced its intention to acquire Kustomer in 2020 and completed the transaction in 2022. Kustomer joined Meta’s Business Messaging Group to transform the way people and businesses communicate through modern messaging channels. In 2023, Kustomer spun out from Meta as a standalone company backed by original partners, Battery, Redpoint and Boldstart Ventures, who have invested $60M in capital, ensuring Kustomer’s growth and success for many years to come.

Our Krew is made up of passionate and collaborative people who really care about what they do and the people they help. We look for people who are passionate about enhancing the customer service experience for everyone involved, as it's the core of what we do. We're growing our business with no plans of slowing down. We actively seek individuals who want to learn and be challenged every day. We have also transitioned to a remote friendly company, with Krew members located throughout the U.S. coming together for Kamp Kustomer each year.

About The Role

As a data engineer on Kustomer’s Engineering team, you will be responsible for growing our business by leveraging advanced data-engineering techniques to enhance our data architecture and streamline data-ingestion processes. Your expertise will be instrumental in improving our data-driven product offerings and in providing valuable insights that drive strategic product decision-making and help us grow globally.

We believe in ownership and are looking for people driven to continuously deliver excellence across all dimensions of their role. This role requires a deep understanding of both the technical and strategic aspects of big data solutions, including data modeling, data warehousing, and data integration.

What You’ll Do:

Data Engineering: Write and adapt tools to classify, ingest and reconcile data. Manage ETL processes to move data among various systems. Onboard datasets, explore data and automate tasks using a modern data stack. 
Data Analysis/Business Intelligence: Parse, analyze and understand data sets. Work closely with key stakeholders to identify valuable insights that can be realized from the amalgamation of our data and integration of disparate data sources. Partner with stakeholders to effectively surface and visualize data. 
Data Debugging: Find anomalies in datasets and debug issues relating to data availability, access, integrity, privacy and security. 
Production Support: Provide proactive oversight of our data pipeline, handle inquiries from internal customers and resolve issues in a timely manner. 
Best Practices: Conduct architecture, systems and data reviews across the platform. Provide education and support to the engineering team in data architecture and design. Identify and resolve deficiencies and deviance from best practices. Participate in data scalability initiatives across the platform. 

Our Tech Stack:

MongoDB, Redis, BigQuery, Elasticsearch, DynamoDB, Aurora, Snowflake
GCP, AWS
JavaScript/TypeScript (React/Node.js), Python, Go
Datadog, Coralogix, ELK

Minimum requirements: 

Bachelor's degree in computer science, data engineering, relevant technical field or equivalent practical experience
5+ years of experience in all or most of the following:
MongoDB architecture, data management and ETL
Relational databases and caches (Redis)
BigQuery, data warehouses/virtualization tools, analytical and data visualization products such as Google Data Studio
BigQuery integration with AWS products such as S3, Kinesis and AWS Batch
Cloud environments, especially GCP and AWS, with a focus on the administration, architecture and configuration of relational and nonrelational data stores and the integration of S3 and Kinesis with BigQuery
SQL, MQL, shell scripting, database scripting and ETL processing
Securing data privacy (PII, encryption, regulatory compliance)
SRE/observability tools such as Datadog, ELK stack, Coralogix and Sentry
Data governance
Nice To Have:

Certification in cloud or database technologies
Working understanding of JavaScript/Node.js, Python and/or Go
Familiarity with SOLID principles
Big Data management and analysis
Elasticsearch and/or Rockset experience
Experience with infrastructure as code using Terraform

HIPAA Compliance

All roles at Kustomer may involve handling sensitive personal data.

Benefits

Kustomer offers an array of benefits including competitive salaries, stock options, 100% healthcare coverage, 401K, WiFi and Mobile reimbursement, and a generous vacation policy.

Diversity & Inclusion at Kustomer

Kustomer is committed to bringing together individuals from different backgrounds and perspectives.

We strive to create an inclusive environment where everyone can thrive, feel a sense of belonging, and do great work together.We are proud to be an equal opportunity employer open to all qualified applicants regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, Veteran status, or any other legally protected status.

Disclaimer: Kustomer only contacts candidates from company email addresses ending in kustomer.com and does not seek funds from candidates in any circumstances.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Ciencias de la computación, Database Scripting, Datasets y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3984136592/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=nF%2BdRfZQTw%2Fds72po%2BRQ5w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 4 días,"Burlington, MA","Acerca del empleo
Burlington, MA

Our client is a manufacturing company that is transitioning from an on-prem ERP to the cloud. The company is hiring a Data Engineer to support this transition and ultimately own the system. The ideal candidate has a strong background in PowerBI and Azure. This role is on-site everyday in Burlington, MA.

Responsibilities

Design, implement, and maintain Synapse (DW), Data Factory, Data Lake, Foundry, and Storage, ensuring data integrity and accuracy.
Create visually appealing, accurate, and interactive Power BI reports and dashboards to provide stakeholders with actionable insights into key performance indicators (KPIs) and trends.
Write complex SQL queries, stored procedures, and functions for efficient data manipulation and analysis.
Maintain and develop solutions using SQL, ColdFusion, HTML, CSS, and other programming languages for internal applications.
Ensure data accuracy through effective data management and interface maintenance among applications.
Gather requirements, build solutions for business applications, and work with end users to identify process improvements.
Support and resolve end-user requests for EDI transactions.
Prioritize and execute tasks effectively.
Demonstrate strong project and task management skills.

Qualifications:

Degree in Computer Science or Information Technology.
3-4 years of experience with the Azure technology stack
Proficiency in Power BI
Experience in web application development preferred","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Intercambio electrónico de datos, Manipulación de datos, Microsoft Azure, Procedimientos de almacenado y Stack",Solicitar
https://www.linkedin.com/jobs/view/3977056464/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=HdIrz%2FJ0DUQo%2FcZPew1HDg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Remote!,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Nueva York, Estados Unidos","Acerca del empleo
Artech is currently looking for experienced professional for the below position.

Job Diva # 24-18941

Title: Data Engineer 

Duration: 12+months 

Location: Remote

Job Description

Expert-level skills in Informatica tools and a strong command of Oracle databases and Linux shell scripting.
Proven experience with AWS or GCP cloud environments, along with experience working with PostgreSQL.
Ability to lead development projects and ensure adherence to best practices and quality standards.
Excellent analytical and development management skills to oversee end-to-end development lifecycles.

Please apply on our company website (www.artechinfo.com) with reference to job ID, or contact me at khushi.gupta@artech.com for more details regarding position including feedback.

Please include work location when submitting candidates.Required skills (maximum of 6): Lead poistion. Require very strong ETL informatica, Oracle and Linux Sheel Scripting skills Nice to have skills (maximum of 3): Lead poistion. Require very strong ETL informatica, Oracle and Linux Sheel Scripting skills Years of experience: Education requirements: Sector: Industry: Media & Entertainment Work location type: IBM Work location country/territory: United States Work location state/province: New York Work location city: Remote Work location street address: Work location comments: Work remotely: Yes Pay travel and lodging: No Project contact: shivarajbi@in.ibm.com Additional comments to supplier: Confidence in requirements:","Buena práctica clínica, Ciencia de datos, Google Cloud , Ingeniería de datos , Microsoft Power BI y SQL, Bases de datos, Guiones shell, Informatica (empresa) y PostgreSQL",Solicitar
https://www.linkedin.com/jobs/view/3960599705/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=4aBWoVd31irxKx6csNSqDA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,"Ohio, Estados Unidos","Acerca del empleo
Job Title: IT Data Engineer
Location/ Work Structure: Remote

Who we are:
Vernovis is a Total Talent Solutions company that specializes in Technology, Cybersecurity, Finance & Accounting functions. At Vernovis, we help these professionals achieve their career goals, matching them with innovative projects and dynamic direct hire opportunities in Ohio and across the Midwest.
Client Overview:
Vernovis partnering with a global IT Platform services company to onboard an IT Data Engineer. This resource will work heavily on enterprise migrations work and the creation/development of new data models and platforms.

If interested please contact Travis at tbrush@vernovis.com


What You'll Do:*Heavy data migration from previous platforms into new internal platforms (CRM)
*Heavy SQL coding for the retrieval of enterprise data
*Platform modeling
Work with the IT Data Engineering Team and assist in developing the next generation data and analytics infrastructure.
Must have strong SQL modeling skills (dbt is a plus)
Write high quality SQL code to retrieve and analyze data from database tables (primarily Databricks)
Develop high quality SQL models for ad-hoc requests, as well as ongoing reporting / dashboarding.
Work directly with business stakeholders to translate between data and business needs.
Continually improve SQL models through automating or simplifying self-service support for datasets
What Experience You'll Have:Python, SQL, Spark and Databricks experience
Experience with data migration and platform modeling
Visualization tools experience
Agile environment experience
What Experience is Nice to Have:Specific experience with Data Warehouse/Data Lake configuration and development using Databricks platform.
Experience with Tableau / Sigma Computing
Experience operating in an Agile development environment.
Familiarity with usage of Agile tools (JIRA / Confluence)
Understanding of CI/CD deployment models and release strategy as well as SCM tools (Git preferred) and code management best practices.
Experience in AWS environment.
Experience with cloud ELT platforms such as AWS Glue, Talend Stitch, or FiveTran","Almacenamiento de datos, Integración continua y entrega continua (CI/CD), SQL y Tableau, AWS Glue, Azure Databricks, Bases de datos, Lagos de datos, Migración de datos, Modelado de datos y Tuberías",Solicitud sencilla
https://www.linkedin.com/jobs/view/3967445321/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=k15JT5gKy3jasDPfwFRHpg%3D%3D&trk=flagship3_search_srp_jobs,Machine Learning / Data Scientist / AI Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 2 semanas,"Denver, CO","Acerca del empleo
Title: Machine Learning / Data Scientist / AI Data Engineer

Location: 100% REMOTE; however, may be asked to come to Colorado once a quarter (expenses paid)

Type: Direct Hire

Time Zone of Work: MST

Salary Range: 143 - 148,500 (may consider higher for commensurate skills)

Bonus: Up to 12% every quarter - Profit sharing (usually pays out an average of 8%)

Job Description: We're seeking a highly motivated Senior Machine Learning Engineer to optimize the deployment, integration, and maintenance of our internal AI chat and search services within a workman's compensation data ecosystem. You'll focus on API-based serving/consumption of internal models and public large language models (LLMs) as well as the robust CI/CD pipeline framework and container-based, cloud infrastructure that supports them. Our highly collaborative data science team will work closely alongside you, with your role emphasizing the software development and deployment aspects of machine learning solutions.

Please Note

This position is more deployment (GIT to server, not training modules)
Team of 5 that is very collaborative
Agile environment

Required Skills

Python
SQL
Deploying LLMs in production environments.
Deep understanding of container-based application deployment within a cloud architecture, ideally within Google Cloud Platform.
Experience streamlining CI/CD processes for machine learning projects, including model versioning, testing, and monitoring.
Passion for AI Applications: A genuine enthusiasm for transforming complex data into powerful AI-driven chat and search tools.

Preferred

Docker, Kubernetes, or equivalent
Vertex AI, Kubeflow, Mlflow, or equivalent
spaCy, Hugging Face Transformers, or equivalent
Healthcare or insurance domain
3+ years’ experience","Analítica de datos, Análisis predictivo, Aprendizaje profundo, Ciencia de datos, Ingeniería de datos , Inteligencia artificial, Procesamiento de lenguaje natural, Python y SQL, Implementación de aplicaciones",Solicitar
https://www.linkedin.com/jobs/view/3983888210/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=k6f6Wc3yUCcqOM9hVTQKIw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"100 US$K/año - 120 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Nashville, TN","Acerca del empleo
Orbis is working with a top local Software Development Agency in Nashville on a new role due to growth.

This full-time Data Engineer will join a new long-term project with an exciting client.

What you'll bring:
3+ years of SQL
2+ year of PySpark
Hands-on experience with Azure Data Factory

We cannot accept C2C or 3rd party candidates.

Apply to learn more!","Azure Data Factory, Herramientas ETL, PySpark y SQL, Microsoft Azure",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982869620/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=QJczsHKcKHu7tQos0PJjvw%3D%3D&trk=flagship3_search_srp_jobs,Data Infrastructure Engineer (US),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Sunnyvale, CA","Acerca del empleo
About Onehouse

Onehouse is a mission-driven company dedicated to freeing data from data platform lock-in. We deliver the industry’s most interoperable data lakehouse through a cloud-native managed service built on Apache Hudi. Onehouse enables organizations to ingest data at scale with minute-level freshness, centrally store it, and make available to any downstream query engine and use case (from traditional analytics to real-time AI / ML).

We are a team of self-driven, inspired, and seasoned builders that have created large-scale data systems and globally distributed platforms that sit at the heart of some of the largest enterprises out there including Uber, Snowflake, AWS, Linkedin, Confluent and many more. Riding off a fresh $35M Series B backed by Craft, Greylock and Addition Ventures, we're now at $68M total funding and looking for rising talent to grow with us and become future leaders of the team. Come help us build the world's best fully managed and self-optimizing data lake platform!

The Community You Will Join

When you join Onehouse, you're joining a team of passionate professionals tackling the deeply technical challenges of building a 2-sided engineering product. Our engineering team serves as the bridge between the worlds of open source and enterprise: contributing directly to and growing Apache Hudi (already used at scale by global enterprises like Uber, Amazon, ByteDance etc) and concurrently defining a new industry category - the transactional data lake. The Data Infrastructure team is the grounding heartbeat to all of this. We live and breathe databases, building cornerstone infrastructure by working under Hudi's hood to solving incredibly complex optimization and systems problems.

The Impact You Will Drive:

As a foundational member of the Data Infrastructure team, you will productionize the next generation of our data tech stack by building the software and data features that actually process all of the data we ingest.
Accelerate our open source enterprise flywheel by working on the guts of Apache Hudi's transactional engine and optimizing it for diverse Onehouse customer workloads.
Act as a SME to deepen our teams' expertise on database internals, query engines, storage and/or stream processing.

A Typical Day:

Design new concurrency control and transactional capabilities that maximize throughput for competing writers.
Design and implement new indexing schemes, specifically optimized for incremental data processing and analytical query performance.
Design systems that help scale and streamline metadata and data access from different query/compute engines.
Solve hard optimization problems to improve the efficiency (increase performance and lower cost) of distributed data processing algorithms over a Kubernetes cluster.
Leverage data from existing systems to find inefficiencies, and quickly build and validate prototypes.
Collaborate with other engineers to implement and deploy, safely rollout the optimized solutions in production.


What You Bring to the Table:

Strong, object-oriented design and coding skills (Java and/or C/C++ preferably on a UNIX or Linux platform).
Experience with inner workings of distributed (multi-tiered) systems, algorithms, and relational databases.
You embrace ambiguous/undefined problems with an ability to think abstractly and articulate technical challenges and solutions.
An ability to prioritize across feature development and tech debt with urgency and speed.
An ability to solve complex programming/optimization problems.
An ability to quickly prototype optimization solutions and analyze large/complex data.
Robust and clear communication skills.
Nice to haves (but not required):
Experience working with database systems, Query Engines or Spark codebases.
Experience in optimization mathematics (linear programming, nonlinear optimization).
Existing publications of optimizing large-scale data systems in top-tier distributed system conferences.
PhD degree with 2+ years industry experience in solving and delivering high-impact optimization projects.


How We'll Take Care Of You

Competitive Compensation; the estimated base salary range for this role is $150,000 - $220,000
Equity Compensation; our success is your success with eligible participation in our company equity plan
 Health & Well-being; we'll invest in your physical and mental well-being with up to 90% health coverage (50% for spouses/dependents) including comprehensive medical, dental & vision benefits
 Financial Future; we'll invest in your financial well-being by making this role eligible to contribute to our company 401(k) or Roth 401(k) retirement plan
 Location; we are a remote-friendly company (internationally distributed across N. America + India), though some roles will be subject to in-person requirements in alignment with the needs of the business
 Generous Time Off; unlimited PTO (mandatory 1 week/year minimum), uncapped sick days and 11 paid company holidays
 Company Camaraderie; Annual company offsites and Quarterly team onsites @Sunnyvale HQ
 Food & Meal Allowance; weekly lunch stipend, in-office snacks/drinks
 Equipment; we'll provide you with the equipment you need to be successful and a one-time $500 stipend for your initial desk setup
 Child Bonding!; 8 weeks off for parents (birthing, non-birthing, adoptive, foster, child placement, new guardianship) - fully paid so you can focus your energy on your newest addition


House Values

One Team

Optimize for the company, your team, self - in that order. We may fight long and hard in the trenches, take care of your co-workers with empathy. We give more than we take to build the one house, that everyone dreams of being part of.

Tough & Persevering 

We are building our company in a very large, fast-growing but highly competitive space. Life will get tough sometimes. We take hardships in the stride, be positive, focus all energy on the path forward and develop a champion's mindset to overcome odds. Always day one!

Keep Making It Better Always

Rome was not built in a day; If we can get 1% better each day for one year, we'll end up thirty-seven times better. This means being organized, communicating promptly, taking even small tasks seriously, tracking all small ideas, and paying it forward.

Think Big, Act Fast

We have tremendous scope for innovation, but we will still be judged by impact over time. Big, bold ideas still need to be strategized against priorities, broken down, set in rapid motion, measure, refine, repeat. Great execution is what separates promising companies from proven unicorns.

Be Customer Obsessed

Everyone has the responsibility to drive towards the best experience for the customer, be an OSS user or a paid customer. If something is broken, own it, say something, do something; never ignore. Be the change that you want to see in the company.

Pay Range Transparency

Onehouse is committed to fair and equitable compensation practices. Our job titles may span more than one career level. The pay range(s) for this role is listed above and represents the base salary range for non-commissionable roles or on-target earnings for commissionable roles. Actual compensation packages are dependent upon several factors that are unique to each candidate, including but not limited to: job-related skills, depth of transferable experience, relevant certifications and training, business needs, market demands and specific work location. Based on the factors above, Onehouse utilizes the full width of the range; the base pay range is subject to change and may be modified in the future. The total compensation package for this position will also include eligibility for equity options and the benefits listed above.","Base de datos relacional, Administración de sistemas, Arquitectura de sistemas, Bases de datos, Comunicación, Lenguaje de consulta (query), Nonlinear Optimization, Optimización, Programación lineal y Resolución de incidencias",Solicitar
https://www.linkedin.com/jobs/view/3983740083/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=%2BuwJZXC0mRXEA25mz88SgQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Seattle, WA","Acerca del empleo
Company Overview

Docusign brings agreements to life. Over 1.5 million customers and more than a billion people in over 180 countries use Docusign solutions to accelerate the process of doing business and simplify people’s lives. With intelligent agreement management, Docusign unleashes business-critical data that is trapped inside of documents. Until now, these were disconnected from business systems of record, costing businesses time, money, and opportunity. Using Docusign’s Intelligent Agreement Management platform, companies can create, commit, and manage agreements with solutions created by the #1 company in e-signature and contract lifecycle management (CLM).

What you'll do

Docusign is seeking a dedicated and results-oriented Data Engineer to focus on delivering trusted data to the business. As a member of the Global Data Analytics (GDA) Team, the Data Engineer uses a variety of technologies to design, develop and deliver new features in addition to loading, transforming and preparing data sets of all shapes and sizes for teams around the world. During a typical day, the Engineer will spend time developing new features to analyze data, develop solutions and load tested data sets into the Snowflake Enterprise Data Warehouse. The ideal candidate will demonstrate a positive approach, a passion for learning and growing, and the drive to work hard and get the job done in a timely fashion. This position provides plenty of room to grow, with a mix of challenging assignments, a chance to work with an extraordinary team, and the opportunity to use innovative technologies such as AWS, Snowflake, dbt, Airflow and Matillion.

This position is an individual contributor role reporting to the Senior Manager of Data Warehouse.

Responsibility

Design, develop and maintain scalable and efficient data pipelines
Analyze and develop data quality and validation procedures
Work with partners to understand the data requirements and provide solutions
Troubleshoot and resolve data issues in a timely manner
Collaborate with cross-functional teams to ingest data from various sources
Evaluate and improve data architecture and processes continuously
Own, monitor, and improve solutions to ensure SLAs are met
Develop and maintain documentation for Data infrastructure and processes
Complete projects using Agile Scrum methodologies

Job Designation

Hybrid: Employee divides their time between in-office and remote work. Access to an office location is required. (Frequency: Minimum 2 days per week; may vary by team but will be weekly in-office expectation)

Positions at Docusign are assigned a job designation of either In Office, Hybrid or Remote and are specific to the role/job. Preferred job designations are not guaranteed when changing positions within Docusign. Docusign reserves the right to change a position's job designation depending on business needs and as permitted by local law.

What you bring

Basic

Bachelor’s degree in Computer Science, Data Analytics, Information Systems, or equivalent
Experience developing data pipelines in one of the following languages: C#, Java or Python
5+ years dimensional and relational data modeling experience
Experience with SQL and database management

Preferred

5+ years in data warehouse engineering (OLAP) Snowflake, BigQuery, Teradata and Redshift
5+ years with transactional databases (OLTP) Oracle, SQL Server, and MySQL
5+ years with big data, Hadoop, Data Lake, or Spark in a cloud environment (AWS)
5+ years with commercial ETL tools including dbt, Matillion, and SSIS/ADF
5+ years delivering ETL solutions from source systems, databases, APIs, flat-files, and JSON
Experience building applications and services with C# .NET Framework
Experience working with job scheduling and monitoring systems (e.g., Airflow, Datadog, AWS SNS)
Experience building data pipelines with Azure Data Lake Storage (ADLS) and Azure Data Factory (ADF)
Experience in any of the financial, Marketing, Sales, accounts payable, accounts receivable, or invoicing domains
Experience managing work assignments using tools like Jira and Confluence
Experience with Scrum/Agile methodologies
Ability to work independently and as part of a team
Excellent analytical and problem solving skills

Wage Transparency

Based on applicable legislation, the below details pay ranges in the following locations:

Washington and New York (including NYC metro area): $116,500 - $168,200 base salary

This role is also eligible for bonus, equity and benefits.

Global Benefits Provide Options For The Following

Paid Time Off: earned time off, as well as paid company holidays based on region
Paid Parental Leave: take up to six months off with your child after birth, adoption or foster care placement
Full Health Benefits Plans: options for 100% employer paid and minimum employee contribution health plans from day one of employment
Retirement Plans: select retirement and pension programs with potential for employer contributions
Learning and Development: options for coaching, online courses and education reimbursements
Compassionate Care Leave: paid time off following the loss of a loved one and other life-changing events

Life at Docusign

Working here

Docusign is committed to building trust and making the world more agreeable for our employees, customers and the communities in which we live and work. You can count on us to listen, be honest, and try our best to do what’s right, every day. At Docusign, everything is equal.

We each have a responsibility to ensure every team member has an equal opportunity to succeed, to be heard, to exchange ideas openly, to build lasting relationships, and to do the work of their life. Best of all, you will be able to feel deep pride in the work you do, because your contribution helps us make the world better than we found it. And for that, you’ll be loved by us, our customers, and the world in which we live.

Accommodation

Docusign is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application procedures. If you need such an accommodation, or a religious accommodation, during the application process, please contact us at accommodations@docusign.com.

If you experience any issues, concerns, or technical difficulties during the application process please get in touch with our Talent organization at taops@docusign.com for assistance.

Applicant and Candidate Privacy Notice

States Not Eligible for Employment

This position is not eligible for employment in the following states: Alaska, Hawaii, Maine, Mississippi, North Dakota, South Dakota, Vermont, West Virginia and Wyoming.

Equal Opportunity Employer

It's important to us that we build a talented team that is as diverse as our customers and where all employees feel a deep sense of belonging and thrive. We encourage great talent who bring a range of perspectives to apply for our open positions. Docusign is an Equal Opportunity Employer and makes hiring decisions based on experience, skill, aptitude and a can-do approach. We will not discriminate based on race, ethnicity, color, age, sex, religion, national origin, ancestry, pregnancy, sexual orientation, gender identity, gender expression, genetic information, physical or mental disability, registered domestic partner status, caregiver status, marital status, veteran or military status, or any other legally protected category.

EEO Know Your Rights poster","Extraer, transformar y cargar (ETL), Google BigQuery y Herramientas ETL, Amazon Redshift, Bases de datos, Calidad de datos, Ciencias de la computación, Modelado de datos, Modelado de datos relacionales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3971447644/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=6lCaDr2xgFPrjDHaYwhCfw%3D%3D&trk=flagship3_search_srp_jobs,Imagery Data Engineer,"80 US$/h - 110 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
Job Title: Imagery Data Engineer
Location: Remote 
Duration: 1 year contract

Job Description:
In our team we believe that the location of things, and the relationships between them in time and space, are of fundamental importance to creating transformational digital products. We are passionate about enabling teams to seamlessly incorporate spatial and location data into their applications, analyses, operations, and models. We do this by ingesting and stewarding much of the location related data for client, by integrating models that enrich that data, and by building platforms and interfaces that make it easy for teams to integrate that data into their work. Our Imagery team is looking for an experienced and innovative Full Stack Engineer with cloud proficiency to join us.

As an Imagery Data Engineer, you will play a crucial role in designing, implementing, and maintaining Imagery Team data assets and capabilities in a growing multi-cloud environment to ensure the efficient and reliable operation of clients Imagery pipelines and services. You will collaborate closely with data engineers, data stewards, data scientists, platform engineers and software developers across the organization to deploy large-scale cloud-based solutions, implement and maintain scalable cloud infrastructures, optimize data pipelines, monitor, and identify issues in the cloud environment, enhance system performance and ensure data integrity and security.

Responsibilities:
Work with team and project management to manage priorities, deadlines, and deliverables including collaborative identification and assignment of tasks.
Implement data solutions according to design documentation using a variety of tools and programming languages, like GIT, Kafka, SQL and no-SQL databases, object storage, Python, Typescript/Javascript, C++, Go etc., and following the team’s established processes and methodologies, like SCRUM or Kanban.
Participate in incident response and troubleshooting issues for Imagery data pipelines and API services.
Learn and share code quality practices, and participate in code reviews, retrospectives, functional and integration testing, and other team activities focused on improving quality of delivery.
Assist in developing any technical documentation needed to accurately represent application design and code.
Gain understanding of the business operations and functions for the product(s) owned within the team.
Collaborate with data steward, stake holders, and platform engineers to optimize data pipelines for performance, reliability, and cost-effectiveness.
Actively seek opportunities to discover new and better solutions.

Experience:
Experience engineering data intensive solutions using streaming and/or resource based (i.e. API) design principles
Experience developing pipeline solutions for deploying to cloud environments like Amazon Web Services, Google Cloud Platform, Azure, etc using respective cloud services like GCS/S3, GCE/EC2, Cloud Functions/Lambda, Pub/Sub/SQS/SNS, GKE/EKS, etc.
Demonstrated understanding of data architecture and modeling, including designing both logical and physical models for datasets
Proven experience writing queries and building data structures from cloud-based datastores like AWS Aurora Postgres, Google BigQuery, Elasticsearch, etc.
Knowledge of at least one NoSQL database such as Elasticsearch, Neo4j, Cassandra, DynamoDB, Spanner, etc. 
Experience with containerization technology and orchestration platforms such as Docker and Kubernetes.
Experience with monitoring and logging tools such as Grafana, Prometheus, ELK stack, Datadog or equivalent.
Strong interpersonal skills and desire to work in a fast-paced and highly collaborative environment.

Skills:
Highly proficient in Golang or Python and the respective geospatial libraries associated with each (i.e. GoDAL/GDAL, Rasterio, PyProj, PDAL, PySTAC, etc.)
Hands on experience developing HTTP APIs (Open API, REST, gRPC, and/or GraphQL) 
Experience with processing UAV (drone) or satellite imagery for modeling and analysis
Experience with cloud-based machine learning platforms (i.e. AWS SageMaker , Vertex AI, etc.) and familiarity with data science practices, tools, and libraries like Jupyer Notebooks, TensorFlow, PyTorch, Scikit-Learn, Pandas, etc.
Demonstrated experience with Continuous Integration, Continuous Delivery (CICD) concepts and applications like GitHub Actions, Argo, etc.
Proven interest/curiosity in agriculture, life sciences, bioinformatics, biochemistry, environmental sciences, biology, or other STEM related disciplines
Experience working with varied geospatial datasets and formats like WKT, GeoJSON, cloud-optimized geotiffs(COGs), OGC services, etc.
Familiarity with SpatioTemporal Asset Catalog (STAC) specification and implementations
Experience building Software Development Kit (SDKs) that advance the adoption of specific software or resource capabilities
Hands-on experience with Infrastructure as Code (IaC) tools such as Terraform.
Experience with standard cloud authentication and authorization patterns


Education:
Bachelor’s degree in Computer Science, Software Engineering, Data Science or related field including geospatial, environmental, remote sensing/earth observation or other STEM related disciplines

About US Tech Solutions:
US Tech Solutions is a global staff augmentation firm providing a wide range of talent on-demand and total workforce solutions. To know more about US Tech Solutions, please visit www.ustechsolutions.com.

US Tech Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity,
national origin, disability, or status as a protected veteran.

Recruiter Details:
Name: Peruka
Email: pjames@ustechsolutionsinc.com
Internal Reference Id: 24-15861","Arquitectura de datos, Ciencia de datos y Ingeniería de datos, Ciencias de la computación, Datasets, Documentación técnica, Imágenes satelitales, Modelado de datos, Protocolo de transferencia de hipertexto (HTTP) y Revisión de código",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984581317/?eBP=BUDGET_EXHAUSTED_JOB&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=u62qLQLErzWn6ILnKOjQgw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer III,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Chicago, IL","Acerca del empleo
Be essential at Cars Commerce

At Cars Commerce, we're fanatical about simplifying everything about car buying and selling. We do right by our customers and consumers to better connect the industry with simplified and tierless technology to enhance, measure and drive local automotive retail. Whether through our No.1 most recognized marketplace, Cars.com, our industry-leading digital experience, Dealer Inspire, our trade and appraisal technology, AccuTrade, or our new Cars Commerce Media Network, Cars Commerce is essential for success in the automotive industry.

No one ever travels alone here: at its core, Cars Commerce is collaboration. In fact, it's built into the very fabric of our shared values. We like to say we Rise Together – putting people at the center of what we do, from consumer to customer to community. Life at Cars Commerce makes it easy when we share the ethos to be Open to All, encouraging open-minded communication because we know diverse thinking yields better outcomes. But critical to our success is Caring to Challenge and Taking Ownership, fueling a competitive spirit in a respectful environment where we think about tomorrow but act today. At our foundation, we have integrity, Doing the Right Thing, even when it's hard. It's our shared commitment to these values that makes Cars Commerce a place where growth becomes not only possible, but downright unavoidable.

But don't take our word for it. As a U.S. News & World Report Best Company to Work For in 2024, we're obsessive about the employee experience. We are among the top 20% being declared ""Best"" of our industry based on six critical factors that are important to employee wellbeing, like quality of pay, benefits, work life balance and more.

About the Role:

Data is the driver for our future at Cars. We're searching for a collaborative, analytical, and innovative engineer to build scalable and highly performant platforms, systems and tools to enable innovations with data. If you are passionate about building large scale systems and data driven products, we want to hear from you.

Responsibilities Include:

Build data pipelines and deriving insights out of the data using advanced analytic techniques, streaming and machine learning at scale
Work within a dynamic, forward thinking team environment where you will design, develop, and maintain mission-critical, highly visible Big Data applications
Build, deploy and support data pipelines into production.
Work in close partnership with other Engineering teams, including Data Science, & cross-functional teams, such as Product Management & Product Design
Opportunity to mentor others on the team and share your knowledge across the Cars.com organization

Required Skills

Experience with one or more query language (e.g., SQL, PL/SQL, DDL, HiveQL, SparkSQL)
Experience with data modeling, warehousing and building ETL pipelines
Proficiency in at least one programming language commonly used within Data Engineering, such as Python, Scala, or Java.
Solid understanding of Spark and ability to write, debug and optimize Spark code in Python.
Solid understanding of various file formats and compression techniques.
Experience with any of ETL schedulers such as Airflow or similar frameworks.
Experience with source code management systems such as Github and developing CI/CD pipelines with tools such as Jenkins for data.
Excellent communication and collaboration skills. 
Ability to design, develop and debug at a cross-project level.

Required Experience

Data Engineering | 3+ years of designing & developing complex, real-time applications at enterprise scale; specifically Python and/or Scala. 
Big Data Ecosystem | 2+ years of hands-on, professional experience with tools and platforms like Spark Streaming, EMR, Kafka.
AWS Cloud | 2+ years of professional experience in developing Big Data applications in the cloud, specifically AWS.

Preferred:

Experience with developing REST APIs
Experience to develop Spark streaming jobs to read data from Kafka.
Experience working with Google Analytics
Experience working with digital marketing platforms such as Facebook, Google Ads, and so on.

In the spirit of pay transparency, we are excited to share the base salary range for this position which is not inclusive of bonuses, benefits or other forms of compensation that the position may be eligible for. If you are hired at Cars Commerce, your final base salary compensation will be determined based on factors such as skills and/or experience. If the salary range is close to what you're seeking, then we encourage you to apply and learn more about the total compensation package for this position.

Salary Range

$101,100—$140,000 USD

Our Comprehensive Benefits Package includes:

Medical, Dental & Vision Healthcare Plans
401(k) with Company Match + Immediate Vesting
New Hire Stipend for Home Office Set-Up
Employee Stock Purchase Program
Generous PTO
Refuel - a service based recognition program where employees receive additional paid time away to learn grow and reset
Paid Holidays, Floating Holiday, Volunteer Day, Recharge Day
Learn more about our Benefits, Perks, & Culture on our LinkedIn Life Pages!

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. California Applicants: Click here to review our California Privacy Policy for Applicants. For current employees, please click here to review our California Privacy Policy for Employees.","Airflow, Apache Kafka, Apache Spark, Big data, Extraer, transformar y cargar (ETL), Ingeniería de datos , Lenguajes de programación y Scala, Comunicación y Modelado de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3889323873/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=t3xym1oKGgHwhfuWVt37wQ%3D%3D&trk=flagship3_search_srp_jobs,Epic Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 meses,"San Francisco, CA","Acerca del empleo
Prominence is looking for a Data Engineer to assist with dimensional data modeling development for healthcare data architecture project.

Who We Are

Prominence is a healthcare technology strategy and implementation firm, focused on helping the nation's leading healthcare organizations to do more with their data. Founded by former Epic managers, we understand the technology landscape in healthcare and provide IT staffing, advisory services, and analytics solutions to create robust data ecosystems that support clinical workflows, automate operational processes, and expedite research. Whether it's guiding a technology implementation, establishing governance principles, or developing leading edge analytics, we help our customers make sense out of the mountain of data at their fingertips in order to deliver higher quality care at a lower cost.

Ranked as a best place to work over 27 times (and counting!), Prominence's culture provides consultants with a supportive environment that allows you to innovate and grow your career in healthcare IT. Additional information is available on our website.

Your Role

Our consultants guide our customers through complex technology requirements to summit the challenge at hand. You will need to be able to create order out of chaos, and compile ambiguous information into tactical action plans.

Our ideal team members are humble, smart, and driven to ensure our customer's success. This includes a passion to deliver high-quality results, while teaching our counterparts how to fish and grow the skills needed to support and expand upon the deliverables of our projects.

If this sounds like you, and you meet the requirements below, we encourage you to apply. If you know of someone else how would be a great fit, let us know!

Requirements

As a member of our Epic Consulting team, you'll work closely with our customers to implement and optimize their Epic workflows. In addition to your Epic project work, you will help mentor and grow our customer's teams, escalate issues, and guide projects to a successful outcome.

Key Responsibilities

Perform Epic-related consulting and advisory services, including but not limited to the following:

Apply technical expertise to implement and optimize EMR workflows and data capture
Mentor customers to up-level their system knowledge and analyst skills
Analyze operational and business requirements, and translate into system configuration
Create build documentation and workflow diagrams
Track and resolve project risks and issues
Lead meetings and participate in ongoing work-product coordination. 
Transparently report on project status and deliverables. 
Develop robust knowledge transfer documentation to hand-off deliverables to customer teams. 
Additional duties as may be required to successfully deliver a project
May be invited to participate in corporate functions, events, and meetings


Desired Qualifications

Active Certification(s): Cogito Data Model (Clinical or Revenue Cycle), Cogito Tools, Caboodle Developer
5+ years of experience as an Epic BI Developer or Data Engineer
Dimensional data modeling experience
Caboodle development experience preferred
SSIS, Azure Data Factory, or Data Lake development preferred
Demonstrated ability to deliver successful projects remotely

Success Criteria

Successful team members at Prominence display the following:

High degree of professionalism; treats others with respect, keeps commitments, builds trust within a team, works with integrity, and upholds organizational values. 
Highly organized; able to manage multi-faceted work streams
Self-motivated; able to maintain schedule, meet deadlines, and monitor your personal work product
Highly adaptable; able to acclimate quickly to new project assignments and work environments. 
Creative; not paralyzed by problems and able to work collaboratively to find novel solutions
Clear communication skills; ability to clearly convey messaging that resonates with your audience, in clear and concise written and verbal communications
Can smell smoke and anticipate issues before they arise, ability to escalate effectively
Passion to mentor and guide others


Benefits

Prominence is dedicated to hiring the best and brightest minds in healthcare and maintaining a culture that rewards our employees for following their passion. We are excited to offer the following benefits for this position:

Competitive Salaried and Hybrid Compensation Plans
Health Care Plan (Medical, HSAs, Dental & Vision)
Retirement Plan (401k)
Life Insurance (Basic, Voluntary & AD&D)
Dependent & Health Savings Accounts
Short Term & Long Term Disability
Paid Time Off (Vacation/Sick & Public Holidays)
Training & Development Fund
Technology Stipends (for Qualifying Roles)
Work From Home
Charitable Giving to Causes You Believe In

Employment Eligibility

Must be legally authorized to work in the United States without sponsorship.

Commitment to Equal Opportunity

The world's most talented professionals come from every background. All applicants will be considered for employment without attention to age, race, color, religion, gender identity and/or expression, sexual orientation, national origin, marital status, veteran or disability status, or any other characteristic protected by law. In addition, Prominence will provide reasonable accommodations for qualified individuals with disabilities.

If you are smart and good at what you do, come as you are. All qualified candidates are encouraged to apply.

Partnership Eligibility

Our partnerships are extremely important to us. This online application is not intended for anyone who is currently under a non-compete agreement or has an arrangement that precludes employment at Prominence. We appreciate your help in respecting our partners.

Interested in learning more? Apply below to connect with our Talent team about immediate openings and future consulting projects.","Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Configuración de sistemas, Diagramas de flujo de trabajo, Modelado de datos y Necesidades empresariales",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984530268/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=yvq7MQAYXWEVl9aGxF4M1g%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"Presencial Media jornada
Coincide con tus preferencias de empleo. El tipo de empleo es Media jornada.
Intermedio",hace 4 días,"Washington, DC","Acerca del empleo
Responsibilities

""ALL our jobs are US based and candidates must be in the US with valid US Work Authorization. Please apply on our website directly."" Join a dynamic team in Washington DC as a Senior Data Engineer, where you'll play a pivotal role in designing and maintaining scalable data pipelines for batch and streaming data. This position involves leveraging advanced analytics tools to enhance the organization's analytics platform, collaborating with both business and technical teams to design data solutions that align with strategic goals, and implementing complex data operations with Python, Java, Spark, and SQL. Additionally, you'll manage data across various platforms using cloud services, big data technologies, and non-relational databases to ensure seamless data integration and accessibility. The role also requires expertise in DevOps processes, scripting, and creating CI/CD pipelines for automation and configuration. Job Responsibilities:

Design, develop, and maintain scalable data pipelines for batch and streaming data, handling both structured and unstructured formats effectively. Utilize advanced analytics tools and methodologies to enhance the analytics platform. Collaborate with business and technical teams to gather requirements and design comprehensive data solutions. Implement complex data operations using Python, Java, Spark, and SQL, optimizing ETL/ELT processes for performance and efficiency. Manage data across various platforms, leveraging cloud services, big data technologies, and non-relational databases to ensure comprehensive data integration and accessibility. Develop and maintain DevOps processes, creating pipelines for automation and configuration. Script and manage CI/CD pipelines to support continuous integration and deployment.

Required Skills : Data Analysis,Data WarehouseAdditional Skills : Data Engineer","Analítica de datos, Análisis de datos, Apache Spark, Big data, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Procesamiento por lotes",Solicitar
https://www.linkedin.com/jobs/view/3970629003/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=ZeWqLW1MfJ5sTmMMt2jX0g%3D%3D&trk=flagship3_search_srp_jobs,Data Reporting/Analytics Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",Publicado de nuevo hace 2 semanas,"Greenwood Village, CO","Acerca del empleo
Data Reporting/Analytics Engineer

Requirements:

5+ years experience as data analytics engineer
AWS QuickSight is a must have
AWS Glue is a must have
Terraform experience
Experience with SQL (queries, extractions, etc...)
Experience with Python or R and AWS RedShift will move candidates to the top
Bachelors degree


Do you have a passion for transforming data into actionable insights? Are you an expert in building reports and dashboards that empower business decisions? If so, we want to hear from you! We are seeking a Senior Reporting & Analytics Engineer to join our growing team. In this role, you will play a key part in designing, developing, and implementing robust reporting and analytics solutions using AWS QuickSight and Tableau.

Horizontal facilitates valuable and productive conversations between you and potential employers. We can assist you in growing your career by partnering you with employers that offer challenging assignments. For those that join the team, we offer competitive compensation and benefits including medical, dental, vision, and retirement. Check out all we have to offer and how you can become part of the Horizontal Talent Team. The pay range for this role is $76 - $83 per hour. This is not a guarantee of compensation, as final offer amount may vary based on factors including but not limited to experience and geographic location.","Amazon QuickSight, Amazon Web Services (AWS), Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Microsoft Power BI, AWS Glue, Amazon Redshift y Pegamento",Solicitar
https://www.linkedin.com/jobs/view/3945738250/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=SbLTAD3V%2FBS7M8EU2007qA%3D%3D&trk=flagship3_search_srp_jobs,"Python/Java Data Engineer_W2_McLean, VA","En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
Required Skills

Python, will accept Java but Python is preferred Spark, PySpark AWS, Lambda, S3 SQL ETL PREVIOUS CAPITAL ONE EXPERIENCE WILL BE PREFERED!!!

Additional Skills

Job Description

Job Title: Python/Java Data Engineer

Duration of project: 6 months

Location: McLean – Hybrid 2-3 days

 

Must Haves

Python, will accept Java but Python is preferred.
Spark, PySpark
AWS ,lambda, S3
SQL
ETL

 

PREVIOUS CAPITAL ONE EXPERIENCE WILL BE PREFERED!!!

 

Org/Team

2 teams under HM
7 engineers within core team

 

Project Details/Day2Day

ETL data transformation project
Self-service data transformation – onboard their use cases and data transformations
Maintaining existing application
Feature developments – here and there
API’s – data brick clusters
New developments on the Python side

 ","AWS Lambda, Extraer, transformar y cargar (ETL), Ingeniería de datos , Microsoft Power BI, PySpark, Python y SQL, Amazon S3, Java y Transformación de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3985593232/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=p%2Bg3%2FyWQQZ2mYg288D5%2B1w%3D%3D&trk=flagship3_search_srp_jobs,Data Integration Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 2 días,"Nueva York, Estados Unidos","Acerca del empleo
About Us

Riskified empowers businesses to unleash ecommerce growth by taking risk off the table. Many of the world’s biggest brands and publicly traded companies selling online rely on Riskified for guaranteed protection against chargebacks, to fight fraud and policy abuse at scale, and to improve customer retention. Developed and managed by the largest team of ecommerce risk analysts, data scientists and researchers, Riskified’s AI-powered fraud and risk intelligence platform analyzes the individual behind each interaction to provide real-time decisions and robust identity-based insights. Riskified is proud to work with incredible companies in virtually all industries including Wayfair, Acer, Gucci, Lorna Jane, GoPro, and many more.

We thrive in a collaborative work setting, alongside great people, to build and enhance products that matter. Abundant opportunities to create and contribute provide us with a sense of purpose that extends beyond ourselves, leaving a lasting impact. These sentiments capture why we choose Riskified every day.

About The Role

As a Data Integration Engineer at Riskified, you will be using technical skills, analytical insights, and business perspective to ensure we have the most accurate and complete data from each client to feed our data science models. You will be responsible for helping new and existing customers ensure they are receiving the best performance possible and leading the charge on processing the data of the largest eCommerce merchants in the world.

Riskified’s clients come in many different sizes and industries, use different technical platforms, and each one has their own unique data concerns and order flows that have to be uniquely assessed. Our team helps scope, implement, validate and monitor these data integrations from both a technical and business perspective.

What You'll Be Doing

In your first 60 days, you will be expected to complete a global and team-specific onboarding program to ground yourself in the knowledge, tools, processes, and people necessary to succeed on the job.

After Onboarding, You Will Be Expected To Begin Supporting Data Flows For Merchant Integrations And Pilots. Typically, This Includes, But Is Not Limited To

Perform due diligence on merchant websites, business models and data flows
Generate tailored data templates (JSON, CSV) to help guide merchant developers / data teams in building high-quality online (API) and offline (file transfer) integrations
Write queries and scripts (R, Python, SQL, Spark, Databricks) to: 
Test, validate, and detect data issues in API calls in sandbox and production environments
Test, validate, transform, and upload large historical data files to Riskified APIs
Maintain / manage merchant data configuration layer of Riskified’s platform (automated data tagging, transformation, filtering, etc)
Combine technical, business and analytical objectives to come up with tailored solutions for onboarding merchants
Ad hoc data wrangling and sanitization
Monitor automated data alerts - prioritize, track, and communicate merchant data issues / irregularities with internal and external stakeholders
Work closely with analysts, research and data science teams to accommodate changes in industry data
Identify, research, and offer system-wide solutions for common data issues
Work closely with client-facing teams to engage customers regarding their data integrations
Support R&D, Product Management, Data Science, Integrations Engineering, Account Management, and Sales as a subject matter expert of Riskified’s product, technology and merchants
Contribute to advancing the team’s tooling, processes, and documentation
Qualifications

3+ years of experience in a data-centric industry role (Analytics, Data Engineering, ETL, DBA, Consulting, etc.)
2+ years with SQL or NoSQL databases
2+ years experience with data wrangling (R, Python for data processing)
2+ years optimizing technical processes
Strong analytical skills and attention to detail
Strong verbal and written communication skills

The base salary range for this position is $110,000 - $130,000. This range is applicable to candidates who will perform the job either wholly, or in part, within New York City. Actual salary will be based on qualifications, competencies, and location. If you feel this range is not attractive, we encourage you to let us know through the application process.

Base salary is just one part of the pay package at Riskified. All full-time regular employees receive a bonus target and are eligible to receive stock-based awards. Also, our value proposition goes way beyond compensation: our perks and benefits package, culture, community, and learning and development programs are just some of the elements we provide to bring value to our employees. We invite you to apply at www.riskified.com to learn more about what we have to offer.

Life at Riskified

We are a fast-growing and dynamic tech company with 750+ team members globally. We value collaboration and innovative thinking. We’re looking for bright, driven, and passionate people to grow with us.

Some Of Our NYC Benefits & Perks

Our NYC team is currently working in a hybrid of remote and in-office for all our team members
Fully-covered medical, dental, and vision insurance from your first day
Equity for all employees, 401(k) + matching, commuter benefits
Catered lunch, fully-stocked kitchen, team events, happy hours, birthday celebrations
Yoga, pilates, soccer league, wellness classes 
Wide-ranging opportunities to volunteer and make an impact in local communities 
Commitment to your professional development with global onboarding, sales bootcamp, skills-based courses, full access to Udemy, lunch & learns 
Awesome Riskified gifts and swag! 

In the News

Reuters: General Atlantic-backed Riskified valued at $4.3 bln in NYSE debut

Fortune Magazine: Riskified named Best Workplaces in New York for 2022

The Muse: Q&A with Sales Enablement Manager Benedikt Parstorfer

Globes: Riskified is among Israel’s fastest growing companies 

TechCrunch: Riskified Prevents Fraud on Your Favorite E-commerce Site

CTech: Riskified’s VP HR on Post-COVID Flexible Work Routines

Riskified is deeply committed to the principle of equal opportunity for all individuals. We do not discriminate based on race, color, religion, sex, sexual orientation, national origin, age, disability, veteran status, or any other status protected by law.","Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Integración de datos y Validación del sistemas informáticos (CSV), Bases de datos, Comunicación, Comunicación escrita, Disputas de datos y Procesamiento de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982283132/?eBP=BUDGET_EXHAUSTED_JOB&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=%2B4in2m4wUdeiPlxbA%2FBqdQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Atlanta, GA","Acerca del empleo
External Job Title: Data Engineer  (hybrid)

Internal Job Title: Senior Data Engineer

 Location:  US-GA-Atlanta (Sandy Springs)

 FLSA  : Exempt or Non-exempt

 Job Overview  :

Safe-Guard’s Risk Department is expanding and adding a new Senior Data Engineer role within the Claims Analytics team. Processing millions of claims every year generates significant amounts of data and we are currently making investments in analytical infrastructure which will better enable business users to take advantage of the data we’re collecting. Safe-Guard is the leading provider of automotive warranties in North America, providing warranties to a wide variety of clients. Our clients include BMW, Mercedes-Benz, General Motors, and many more.

As a Senior Data Engineer, you will be responsible for building and maintaining an enterprise-wide analytical data warehouse. This includes but is not limited to: gathering data from different systems using SQL/Python, creating complex data models, developing proprietary reporting metrics, and driving necessary change across IT systems to help support the company’s analytical priorities. This role will be directly responsible for supporting the highest visibility initiatives at the company.

 Job Responsibilities: 

 Serve as the technical expert for creating Safe-Guard’s analytical data infrastructure. 
 Develop extensive ETL processes to take data from across different systems and bring them into a centralized data model. 
 Develop and implement automated data quality control processes. 
 Collaborate with cross-functional teams to understand business requirements and translate them into technical solutions. 
 Define and enforce data standards, policies, and best practices to ensure data integrity, security, and quality. 
 Other duties, as dictated by departmental priorities 


The above statements are intended only to describe the general nature of the job and should not be construed as an all-inclusive list of position responsibilities.

 Job Requirements: 

 5+ years working in data or analytics role. 
 Extensive experience in data modeling and data infrastructure. 
 B.S. in mathematics, statistics, engineering, or equivalent. 
 Expert proficiency in SQL, Python, or equivalent. 
 Knowledge of common database platforms such as Snowflake, Postgres, and AWS. 
 Experience in Looker/ Power BI/Qlik/ Tableau preferred 
 Strong written and verbal communication skills. 
 Must be authorized to work in the U.S 
 Must be able to successfully pass a background check. 


 Company Benefits: 

 Medical, Dental, and Vision Insurance 
 Flexible Spending Account 
 Health Savings Account 
 401(k) Plan with Company Match 
 Company-paid Short-Term and Long-Term Disability 
 Company-paid Life Insurance 
 Paid Holidays and Vacation 
 Employee Referral Program 
 Employee Assistance Program 
 Wellness Programs 
 Paid Community Service Opportunities 
 Tuition Reimbursement 
 Ongoing Training & Personal Development 
 And More! 


 About Safe-Guard Products International: 

Safe-Guard serves Original Equipment Manufacturers (OEMs), top retailers, and independent agents in the automotive finance and insurance industry with the leading Protection Products Platform. Our platform delivers innovative protection products and solutions that protect consumers from the perils of ownership, while providing Finance &Insurance professionals the tools to ignite scalable and sustainable business growth. Safe-Guard’s success is driven by over 850 employees, who serve more than 12,000 dealers and support contract holders across the U.S. and Canada.

For 30 years and counting, our team continues to transform the motor vehicle space, earning a stellar reputation from our partners and peers by providing: 1) the highest quality protection products in the industry, 2) a broad platform of branded product, technology, marketing, and training solutions, and 3) an unwavering commitment to uncomplicated care and customer service.

Safe-Guard Products International is an equal opportunity employer and gives consideration for employment to qualified applicants without regard to age, race, color, religion, creed, sex, sexual orientation, gender identity or expression, national origin, marital status, disability or protected veteran status, or any other status or characteristic protected by federal, state, or local law.","Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Comunicación, Comunicación oral, Estándares de datos, Modelado de datos, Necesidades empresariales y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3982463656/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=IpBQL9T0%2F1PUDQFEAWOliQ%3D%3D&trk=flagship3_search_srp_jobs,Analytics Engineer (L5) - Regional Understanding,"170 US$K/año - 720 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 4 días,"Los Gatos, CA","Acerca del empleo
Netflix is one of the world’s leading entertainment services with 278 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.

The Role

About

Netflix is one of the world's leading entertainment services, with 270 million paid memberships in over 190 countries, enjoying TV series, films, and games across a wide variety of genres and languages. Members can play, pause, and resume watching as much as they want, anytime, anywhere, and can change their plans at any time. In this role, you will dive into different regions and countries to holistically answer questions like “Who are our members?”, “How satisfied are current members with our service?” and “How do the needs and expectations of non-members differ from those of members?” The questions you answer will help shape how senior leadership thinks about opportunities and challenges in these different regions, and enable us to evaluate and determine how our country-level strategies are performing. As a Senior Analytics Engineer, you will be responsible for developing and maintaining data pipelines, conducting in-depth analyses, and developing tools and products to create a single source of truth around Netflix's greatest consumer-facing opportunities and challenges—at both a company and country level. The ideal candidate will excel in data analytics, storytelling with data, cross-functional collaboration, clear communication with executive stakeholders, and share a passion for continuously improving the way we use data to make Netflix better. To learn more about analytics engineering at Netflix, read here. In this role, you will:

Become an expert in country-level performance by diving deep into our data to understand both our members and non-members.
Spearheaded research to understand the consumer-facing opportunities and challenges facing Netflix.
Scale these insights—Netflix is a global company, and we need to be able to efficiently zoom in on different regions throughout the world and ladder this up to a global view. 
Share your learnings with high-level leaders in a way that is digestible, actionable, and clear.

To be successful in this role, you have:

At least 5 years of experience as an analytics professional.
Expertise in SQL, programming skills (e.g. Python, Scala), and some exposure to ETL and data warehousing concepts.
A proven track record of data analysis, reporting and visualization (e.g. Tableau, D3).
Strong communication with the ability to build meaningful stakeholder relationships.
Enthusiasm for creative thinking and innovation in a fast-paced data and analytics space.
Excitement to learn about new fields, with the ability to be scrappy as needed.
Comfort with ambiguity; able to thrive with minimal oversight and process.

Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here. Netflix is a unique culture and environment. Learn more here.

We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.

Job is open for no less than 20 days and will be removed when the position is filled.","Almacenamiento de datos, Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Gestión de partes interesadas y Zoom",Solicitar
https://www.linkedin.com/jobs/view/3983056863/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=scN0qA4vJvZXK56qSY%2FAlw%3D%3D&trk=flagship3_search_srp_jobs,ETL Tool Developer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 días,"Phoenix, AZ","Acerca del empleo
Job Description:
Design of complex ETL interfaces with agnostic tool set for various source/target types
SAP BODS is preferred. If not, expert level in any major ETL tool
Performance Tuning & Troubleshooting skills across all technologies used
Strong in DB and SQLs with decent data modelling skills
Ability to lead the developers and QA and adhere to timelines committed. Agile experience is preferred.
Ability to work with both upstream, downstream and reporting system stakeholders and convert the business requirements into technical requirements / JIRA stories.
5+ years of experience in ETL tools, repository maintenance, migration of jobs/workflows/dataflows, job monitoring, break fix, user maintenance, CDC, Oracle Golden Gate, worked with different sources targets to load the data (Oracle, SAP, Files as source; SQL Server, file uploads etc. as targets).
Responsible for Planning, Developing, Implementing and Managing Data Warehouse Project Deliverables, Design Documents and High-Level Data Mapping for ETL Specifications
Create transformations in ETL jobs to achieve data cleansing and standardizations during initial data load
Provide support for integration testing, defect fixing and deployment. Proficient knowledge in transformations like Query, Push down, SQL, Table Comparison, Pivot, Look up () etc.
Proficient Knowledge in Databases like Oracle (SQL Queries), SQL Server
Proficient Knowledge in Dimensional modelling, Data Mining and Data warehouse concepts
Should have Excellent Analytical Skills
Knowledge to transfer data from Oracle, SQL Server tables and Web Services either Incrementally (Delta Load) or full Load to the Data warehouse on periodic basis. Knowledge in Trouble shooting existing ETL jobs and improve performance of the existing jobs
Should be able to create and load data into Aggregate Tables using transformations. Should be able to perform tasks individually and independently","Ciencia de datos, Extraer, transformar y cargar (ETL) y Reconocimiento de patrones, Asignación de datos, Bases de datos, Documentos de diseño, JIRA, Modelado de datos, Necesidades empresariales y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3978774108/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=IyfZAIQSNhN94Ync8NRNXw%3D%3D&trk=flagship3_search_srp_jobs,Sr. Data Operations Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Chicago, IL","Acerca del empleo
At Beyond Finance, we've made it our mission to help everyday Americans escape the endless cycle of crippling debt and step into a brighter financial future. Through compassionate, individualized care, a culture focused on compliance and ethics, supportive user-centric technology, and customized financial solutions, we've helped over 300,000 clients on their path to a brighter future.

While we're proud of what we've already accomplished, we're searching for new collaborators to help us get to the next level! If you're looking to join a forward-thinking, rapidly growing organization with helping people as its number one goal, we want to hear from you.

About The Role

As a Data Operations Engineer you will be, for the Beyond Finance data ecosystem, a firefighter, an architect, an analyst, and a detective. You will support the Data Engineering and Business Intelligence teams, and interact with and support the data needs of nearly every segment of the company. You’ll work with stakeholders across the company, vendors and other third parties. You’ll learn to use new tools as they become available, working continuously to keep our data operations at best practice levels for security, functionality, and data quality.

What You'll Do

Ensure the regular sharing of data within and outside the company
Build efficient SQL queries and work to improve SQL efficiency across the organization
Assess and manage the ongoing performance and usage of databases and BI tools
Respond to support questions and propose solutions
Create and deactivate accounts, expand privileges when appropriate
Identify and remove unused objects
Archive data from systems when needed
Develop and maintain ETL transformation code
Create data audit alerts and respond to them
Maintain data security procedures and guidelines

What We Look For

Bachelor's degree in Computer Science or other technical field or equivalent work experience
Strong experience using analytics platforms and BI tools
Stellar ability to troubleshoot issues and attempt to resolve the issue. Coordinate issues with the other teams so a root cause can be identified and created.
Demonstrated good understanding of how data flows throughout a company, solid understanding of data governance at each phase.
Excellent SQL experience with working through complex queries.
Willingness to be be on a on-call rotation
Strong multitasking skills with the ability to balance competing priorities
Ability to work in a fast-paced environment where continuous innovation is desired, and ambiguity is the norm
Python language experience preferred

Why Join Us?

Benefits

While you make a difference for others, we’ll work to make a difference for you, providing an uplifting, collaborative work environment and benefits that reflect your value to us. For eligible full-time employees, we offer:

Considerable employer contributions for health, dental, and vision programs
Generous PTO, paid holidays, and paid parental leave
401(k) matching program
Merit advancement opportunities
Career development & training

And finally, our team spirit and culture! We cultivate an environment of community, connection, and belonging across our entire organization.

Beyond Finance does not accept unsolicited resumes from individual recruiters or third-party recruiting agencies in response to job positions. No fee will be paid to their parties who submit unsolicited candidates directly to Beyond Finance employees or the Beyond Finance HR team. No placement fee will be paid to any third party unless such a request has been made by the Beyond HR team.","Ciencia de datos, Gobierno de datos y SQL, Administración de sistemas, Auditoría de datos, Bases de datos, Ciencias de la computación, Multitarea, Resolución de incidencias y Root Cause",Solicitud sencilla
https://www.linkedin.com/jobs/view/3945905059/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=vzmSKcKSmOItw%2BeG4WdV2A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 5 días,"Atlanta, GA","Acerca del empleo
The ETL Engineer performs design, development and implementation of integration processes for both the Enterprise Data lake, Data Warehouse and Applications
Analyzes requirements and existing resources to create efficient database and integration designs that meet company IT standards.
Works with project and business analyst leads to develop and clarify in-depth technical requirements.
Participates in all phases of the integration development lifecycle, including unit testing, quality assurance(QA) and ongoing support.
Helps with Production support as needed
Excellent communication skills (both verbal and written) 
Proven ability to provide strong problem solving skills.
Must be self-motivated and know when to seek guidance
Must be flexible, be able to change priorities quickly, and handle multiple tasks concurrently
Individual must be a self-starter and capable of working independently as well as part of a team
Capable of learning new tools and technologies

Aptitudes y experiencia deseables
SQL","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amdocs CRM, Comunicación, Requisitos técnicos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3981402482/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=KYXbmqt%2BKcs4aLTnftFqqw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Full Remote!,"100 US$K/año - 150 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 4 días,"Dallas, TX","Acerca del empleo
Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!

Job details

Full Remote, Strong Pay!

This Jobot Job is hosted by Evan Cornutt

Are you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.

Salary $100,000 - $150,000 per year

A Bit About Us

Based out of Southern California, we are a Software company. Our product is a SaaS type platform that connects users to small businesses. We have been in business over 5 years and have been growing rapidly. Due to growth, we have a need for several Data Engineers to join our team. Our ideal candidates would be strong in Python and have experience with AWS. This is a fully remote role!



Why join us?


 Full Remote! 
 Strong Pay! 
 Great Benefits!

Job Details

 2+ years as a Data Engineer
 Python experience 
 Degree in a related field

Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.

Want to learn more about this role and Jobot?

Click our Jobot logo and follow our LinkedIn page!","Almacenamiento de datos, Análisis de datos, AutoCAD, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos y Python, Bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3983544078/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=7JWF8sr9M%2BK988tfpzD%2BJw%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer - W2 - Hybrid,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 4 días,"Washington, DC","Acerca del empleo
Join a dynamic team in Washington DC as a Senior Data Engineer, where you'll play a pivotal role in designing and maintaining scalable data pipelines for batch and streaming data. This position involves leveraging advanced analytics tools to enhance the organization's analytics platform, collaborating with both business and technical teams to design data solutions that align with strategic goals, and implementing complex data operations with Python, Java, Spark, and SQL. Additionally, you'll manage data across various platforms using cloud services, big data technologies, and non-relational databases to ensure seamless data integration and accessibility. The role also requires expertise in DevOps processes, scripting, and creating CI/CD pipelines for automation and configuration.

Job Responsibilities

Design, develop, and maintain scalable data pipelines for batch and streaming data, handling both structured and unstructured formats effectively.
Utilize advanced analytics tools and methodologies to enhance the analytics platform.
Collaborate with business and technical teams to gather requirements and design comprehensive data solutions.
Implement complex data operations using Python, Java, Spark, and SQL, optimizing ETL/ELT processes for performance and efficiency.
Manage data across various platforms, leveraging cloud services, big data technologies, and non-relational databases to ensure comprehensive data integration and accessibility.
Develop and maintain DevOps processes, creating pipelines for automation and configuration.
Script and manage CI/CD pipelines to support continuous integration and deployment.","Analítica, Analítica de datos, Apache Spark, Big data, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Obtención de requisitos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982792763/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=UMigavP%2F%2BBJ4sCvrKUS4ww%3D%3D&trackingId=xBXwy1wSNpJ6ke7eaPe3gw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Snowflake),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, TechHuman, is seeking the following. Apply via Dice today!

Title: Data Engineer (Snowflake)

Pay: up to $70/hr.

Location: 100% Remote

Job Summary:

We are seeking an experienced Data Engineer to join our client's team to facilitate a crucial migration project. The consultant will be responsible for reviewing SAP Business Objects reports, analyzing SQL in raw Oracle tables, and converting it to Snowflake SQL. This role requires the ability to reverse engineer previous data entry intentions and develop more efficient solutions.

Key Responsibilities:

Analyze and review SAP Business Objects reports to understand data requirements and transformation needs.
Examine SQL queries and raw Oracle tables stored in Snowflake to determine the best approach for conversion to Snowflake SQL.
Reverse engineer data entry processes in Datameer, identifying inefficiencies or redundant elements.
Rewrite and optimize complex SQL queries and database objects from PL/SQL to Snowflake SQL.
Provide recommendations for improvements and implement solutions that enhance efficiency and performance.
Collaborate with technical and non-technical stakeholders to ensure clear understanding and effective implementation of solutions.

Must Haves:

4+ years of experience in developing complex SQL queries and database objects.
Proven experience with Datameer or Alteryx for analyzing, cleansing, and transforming datasets.
Professional experience with Snowflake SQL, including writing and optimizing queries.
Strong experience in analyzing Oracle tables and developing with PL/SQL, with a focus on conversion to Snowflake SQL.

Nice to Haves:

Experience analyzing reports in SAP Business Objects.
Familiarity with Agile methodologies.","Lenguajes de programación, PL/SQL, SAP BusinessObjects y SQL, Bases de datos, Contabilidad exigida por ley, Datameer, Datasets, Snowflake y Snowflake cloud",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3945985895/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=YJuFnYm%2Bg4kzWdSU98636A%3D%3D&trk=flagship3_search_srp_jobs,Machine Learning Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,"Los Ángeles, CA","Acerca del empleo
About Us

We're on a mission is to revolutionize the CAD industry by developing the world's most advanced hardware design infrastructure and tools. Mechanical CAD is in the dark ages, and hardware demands have outpaced today's hardware design infrastructure. The industry is due for a refresh, and we're laying the foundation for a modern hardware design toolkit so that you can create new design tools never before possible. We were founded and incubated by Embedded Ventures in 2021, and we're a fully remote team.

About You

We're looking for curious, innovative, and ambitious self-starters to join our lean and growing team to help us bring our mission to life. We think you'll thrive on our team if you're:

Passionate about making an impact on the ground floor of something big!
Curious at your core, with an eagerness to learn and do things differently
Customer focused, always thinking about ways to improve the user experience
Able to operate autonomously while also being an effective team player 
Agile and thrive in a fast-paced, startup environment 

About The Role

We are seeking a highly skilled Machine Learning Data Engineer to join our growing team to support our foundational data engineering processes. In this role, you'll build, implement, and manage our ML data ecosystem to support Zoo's ML initiatives as we scale. The right candidate will have a strong background in Python programming, data integration, data warehousing, data munging/cleaning, and ETL for ML datasets. The ideal candidate will also have an understanding of Computer-Aided Design (CAD) with knowledge of the relevant data required for hardware design.

What You'll Do

Develop and maintain automated processes for data collection from various sources
Design and implement data warehousing solutions to store and manage large datasets
Clean and pre-process data to ensure its quality and usability for ML models
Create and manage ETL pipelines to transform raw data into structured datasets for ML applications
Utilize vector databases to enhance RAG processes for advanced data retrieval
Apply CAD knowledge to integrate and manipulate design data within our systems
Collaborate with cross-functional teams to understand data requirements and deliver robust data solutions
Continuously optimize and improve Zoo's data workflows and processes

What You'll Need

B.S. Computer Science or a related field, or equivalent professional experience
Demonstrated proficiency in Data Engineering for ML datasets
Expertise in data extraction, data warehousing, and data munging best practices 
Proficiency with Python
Proficiency with SQL
Experience with Git
Familiarity with vector databases and their application in RAG
CAD knowledge and its integration with data systems

Nice to Have

Experience with PyTorch and training machine learning models
Strong background in data analytics, statistics, and data visualization techniques
Experience working on Generative AI applications
Proficiency in Rust

What We Offer

Competitive compensation & equity packages 
Medical, Dental, and Vision coverage for you and your dependents 
401K match (for US-based employees)
Flexible vacation policy
Home office stipend & wifi reimbursement to set you up for success working remotely
Pet insurance reimbursement for your animal friends

Zoo is proud to be an equal opportunity employer. We're committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.","Aprendizaje automático, Ciencia de datos, Ingeniería de datos , PyTorch y Reconocimiento de patrones, Bases de datos, Ciencias de la computación, Data Munging, Datasets y Rust (lenguaje de programación)",Solicitar
https://www.linkedin.com/jobs/view/3895816950/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=31gp1KBRl3R5nhxe%2FtlNEg%3D%3D&trk=flagship3_search_srp_jobs,Data Warehouse Engineer,"140 US$K/año - 210 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,"San Francisco, CA","Acerca del empleo
Company Overview: Welcome to the forefront of data-driven decision-making! At our company, we're committed to harnessing the power of data to drive insights and innovation. Our mission is to develop scalable and efficient data warehouse solutions that empower organizations to make informed decisions based on high-quality data. Join us and be part of a dynamic team dedicated to shaping the future of data warehouse engineering.

Position Overview: As a Data Warehouse Engineer, you'll play a crucial role in designing, building, and maintaining our data warehouse infrastructure and systems. Working closely with cross-functional teams of data analysts, business intelligence developers, and software engineers, you'll ensure the reliability, performance, and scalability of our data warehouse solutions. If you're passionate about data engineering and eager to drive innovation through scalable data warehouse solutions, we want you on our team.

Requirements

Key Responsibilities:

 Data Warehouse Design: Design and architect scalable and efficient data warehouse solutions, including data models, schemas, and storage structures
 ETL Development: Develop and maintain ETL (Extract, Transform, Load) processes to extract data from source systems, transform it to meet business requirements, and load it into the data warehouse
 Data Modeling: Design and implement dimensional and normalized data models to support reporting, analytics, and business intelligence applications
 Data Integration: Integrate data from diverse sources, including databases, applications, APIs, and flat files, ensuring data consistency and integrity
 Performance Optimization: Optimize data warehouse performance for query speed, concurrency, and scalability, tuning SQL queries, indexing strategies, and partitioning schemes
 Data Quality and Governance: Implement data quality checks and validation rules to ensure the accuracy, completeness, and consistency of data in the warehouse
 Monitoring and Maintenance: Monitor data warehouse performance and health, proactively identifying and addressing issues to minimize downtime and optimize resource utilization
 Documentation and Collaboration: Document data warehouse architecture, processes, and best practices, and collaborate with cross-functional teams to ensure alignment and transparency


Qualifications:

Bachelor's degree or higher in Computer Science, Engineering, or related field
Strong background in data engineering, with hands-on experience in designing, building, and maintaining data warehouse solutions
Proficiency in SQL and database technologies such as PostgreSQL, MySQL, SQL Server, or Oracle
Experience with data warehouse platforms and technologies such as Amazon Redshift, Google BigQuery, Snowflake, or Microsoft Azure SQL Data Warehouse
Familiarity with ETL tools and technologies such as Apache Airflow, Talend, Informatica, or AWS Glue
Strong problem-solving abilities and analytical thinking, with a keen attention to detail and a passion for tackling complex technical challenges
Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams and communicate technical concepts to non-technical stakeholders


Benefits

Competitive salary: The industry standard salary for Data Warehouse Engineers typically ranges from $140,000 to $210,000 per year, depending on experience and qualifications. Exceptional candidates may be eligible for higher compensation packages
Comprehensive health, dental, and vision insurance plans
Flexible work hours and remote work options
Generous vacation and paid time off
Professional development opportunities, including access to training programs, conferences, and workshops
State-of-the-art technology environment with access to cutting-edge tools and resources
Vibrant and inclusive company culture with team-building activities and social events
Opportunities for career growth and advancement within the company
Exciting projects with real-world impact across diverse industries
Chance to work alongside top talent and industry experts in the field of data warehouse engineering


Join Us: Ready to shape the future of data warehouse engineering? Apply now to join our team and be part of an exciting journey of innovation and discovery!","Almacenamiento de datos y Extraer, transformar y cargar (ETL), Amazon Redshift, Bases de datos, Ciencias de la computación, Comunicación, Modelado de datos, Modelo de datos, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3890998604/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=lOeKyoZhNeWjQtw73Pc%2Blw%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist/ ML Engineer | Remote | Long Term,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 meses,Estados Unidos,"Acerca del empleo
Data Scientist/ ML Engineer

Remote

Long Term

Note: Data scientist we are looking for needs to be 1) hands-on and 2) have a good history of building data-based models and being able to evaluate their accuracy and improve it.

Job Description

We are seeking a highly skilled AI/ML Engineer to join our dynamic team. As an AI/ML Engineer, you will be responsible for analyzing underlying data sources and sets, identifying relationships and correlations, building machine learning models, and conducting thorough feasibility checks for patient scheduling solutions. You will play a pivotal role in the design, development, testing, and deployment of AI-powered solutions aimed at optimizing patient scheduling processes.

Responsibilities

Analyze Data: Collaborate with cross-functional teams to understand data requirements and identify relevant data sources. Analyze and preprocess data to extract valuable insights and ensure data quality.
Model Development: Develop and implement machine learning models to address patient scheduling challenges. Utilize advanced algorithms and techniques to optimize scheduling processes and improve efficiency.
Hypothesis Testing: Test hypotheses and validate assumptions through rigorous experimentation and analysis. Conduct feasibility checks to assess the viability and effectiveness of proposed solutions before deployment.
Solution Design: Work closely with stakeholders to define requirements and translate business objectives into technical specifications. Design scalable and robust AI solutions tailored to meet specific client needs.
Evaluation and Optimization: Evaluate model performance using appropriate metrics and iterate on solutions to enhance performance and accuracy. Continuously optimize algorithms and models to adapt to evolving business requirements.
Collaboration: Collaborate with data engineers, software developers, and domain experts to integrate AI/ML solutions into existing systems and workflows. Ensure seamless deployment and integration of solutions within client environments.
Documentation and Reporting: Document methodologies, findings, and outcomes in clear and concise reports. Communicate results effectively to technical and non-technical stakeholders, and provide actionable insights for decision-making.

Requirements

Bachelor's or Master's degree in Computer Science, Engineering, Statistics, or a related field. Ph.D. preferred.
Proven experience in data analysis, machine learning, and AI model development, preferably in the healthcare domain.
Proficiency in programming languages such as Python, R, and familiarity with python-notebooks for model development and libraries such as TensorFlow, PyTorch, scikit-learn, etc.
Strong understanding of statistical methods, data mining techniques, and predictive modeling.
Experience with data preprocessing, feature engineering, and model evaluation.
Excellent problem-solving skills and the ability to think critically and analytically.
Strong communication and interpersonal skills, with the ability to collaborate effectively in a team environment.
Proven track record of delivering high-quality solutions on time and within budget.
Knowledge of healthcare standards and regulations (e.g., HIPAA) is a plus.
Ability to adapt to a fast-paced, dynamic work environment and learn new technologies quickly.

If you're looking for new opportunities Kindly share your updated resume and contact details to Mail: ashwin.kumar@stiorg.com; Call: 6092328706*106.","Ciencia de datos, Inteligencia artificial, Modelos predictivos, Pensamiento crítico y Reconocimiento de patrones, Diseño de soluciones empresariales, Diseño de soluciones técnicas, Especificaciones técnicas, Ingeniería de características y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3848908130/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=CDbgVB9IuS3J3kqZ4zxXhw%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 4 meses,"Houston, TX","Acerca del empleo
Responsibilities
 •
 Develop, maintain, and optimize data pipelines to extract, transform, and load large datasets from diverse sources into our data ecosystem
 •
 Design and implement efficient and scalable data models that align with business requirements, ensuring data integrity and performance
 •
 Collaborate with cross-functional teams to understand data needs and deliver solutions that meet those requirements
 •
 Work closely with data scientists, analysts, and software engineers to ensure seamless integration of data solutions into larger systems
 •
 Identify and resolve data quality issues, ensuring accuracy, reliability, and consistency of the data infrastructure
 •
 Continuously monitor and improve data pipelines and processes, identifying opportunities for automation and optimization
 
 
  Qualifications
 •
 Bachelor's or Master's degree in Computer Science, Engineering, or a related field
 •
 5+years of hands-on experience as a Data Engineer, working on complex data projects and implementing data modeling solutions
 •
 Solid understanding of SQL and expertise in working with relational databases (e.g., PostgreSQL, MySQL)
 •
 In-depth knowledge of data modeling techniques and experience with data modeling tools
 •
 Working knowledge on Data warehousing
 •
 Familiarity with cloud-based data platforms and services (e.g., Snowflake, AWS, Google Cloud, Azure)
 •
 Experience with version control systems (e.g., Git) and agile software development methodologies
 •
 Strong communication skills to effectively convey technical concepts to both technical and non-technical stakeholders
 
 •
 Primary Skillset: Data Engineering

Aptitudes y experiencia deseables
DATA ENGINEER","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Comunicación, Herramientas de modelización, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3984640536/?eBP=BUDGET_EXHAUSTED_JOB&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=RMzn45jXaosyiWWELPL5wA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"50 US$/h - 55 US$/h Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 días,"Orlando, FL","Acerca del empleo
We are seeking a Data Engineer to join our team and lead the design and implementation of our data strategy. This role is critical in ensuring the success of our data architecture in Azure, as well as in governing, managing, and maintaining high data quality standards. The ideal candidate will have extensive experience in Azure and knowledge of relevant data management practices and tools. Specifically, in Data Storage, Data Integration/ETL, Big Data and Analytics, Data Governance and Security, Data Management, Business Intelligence, Azure Monitoring/Management, and Azure DevOps.
About the Opportunity:
Hybrid: 2-3 days a week on-site
Schedule: M-F
Key Responsibilities:
* Data Asset Management: Shape how data is stored, managed, and utilized within the organization, ensuring that data assets are effectively leveraged to drive business success.
* Design and Implementation: Lead the design, implementation, and maintenance of our hub & spoke data architecture in Azure.
* Data Governance: Develop and enforce data governance policies and procedures to ensure data integrity, security, and compliance.
* Data Quality Management: Establish and oversee data quality standards and processes to ensure the accuracy, consistency, and reliability of our data.
* Collaboration: Work closely with data developers, junior data developers, and other stakeholders to ensure seamless integration and alignment with business goals.
* Mentorship: Provide guidance and support to the data team, fostering a culture of continuous learning and improvement.
* Vendor Management: Assist in the evaluation and selection of data management tools and technologies.
* Documentation: Create and maintain comprehensive documentation for data architecture, processes, and standards.
Qualifications:
* Education: Bachelor's degree in Computer Science, Information Systems, or a related field, or equivalent experience.
* Experience:
o 3 to 5 years of experience in data architecture, data management, or a related field.
o Proven experience with Azure data services.
* Technical Skills:
o 3 to 5 years' experience in data modeling, database design, and data warehousing.
o 3 years' experience in SQL and other database query languages.
o 2 to 3 years' experience with big data technologies and cloud computing.
o 1 to 2 years' experience understanding of data governance and data quality management principles.
o Experience in designing data architectures.
o 2 to 3 years' experience in data management, including evaluating and implementing data management tools and technologies.
o 3 to 5 years' experience in Strategic Planning.
o 3 to 5 years Performance Optimization of data systems
* Soft Skills:
o Excellent communication and interpersonal skills.
o Strong analytical and problem-solving abilities.
o Ability to work collaboratively in a team environment.
Preferred Qualifications:
* Knowledge of data management tools such as Informatica or Qlik Talend.
* Experience with IBMi main frame.
* Experience with MS-SQL, DB2, Oracle databases.
* Experience with ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) processes.
* Experience as Data Administrator, Date Engineer, Data Analyst, and Software Developer.
* A master's degree is a plus.","Almacenamiento de datos, Arquitectura de datos, Big data, Extraer, transformar y cargar (ETL) y Query Languages, Bases de datos, Calidad de datos, Comunicación, Lenguaje de consulta (query) y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3976355216/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=ieEARZZ08Gw6xovG5BFCHw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Platform Team,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,,", ",Solicitar
https://www.linkedin.com/jobs/view/3984546692/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=tn39GL%2BRypDqvd8HOJf%2BEA%3D%3D&trk=flagship3_search_srp_jobs,GCP Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,Estados Unidos,"Acerca del empleo
Skill Requirements:

 Experience 7+ years with at least 2 years using GCP 
 Big Query 
 SQL 
 Tekton and Terraform (IAC) 
 Experience working on Data Pipelines 
 Good Communication Skills 
 Ready to learn

Aptitudes y experiencia deseables
GCP","Almacenamiento de datos, Buena práctica clínica, Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud , Ingeniería de datos y SQL, Bases de datos, Comunicación y Lenguaje de consulta (query)",Solicitar
https://www.linkedin.com/jobs/view/3917069903/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=td3tx4scrLAGmA6QcRbong%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 meses,"Littleton, CO","Acerca del empleo
Come Join Our Team At Anteriad and innovate the way B2B marketers make data-driven business decisions.

About Anteriad

We are not just another B2B solution provider. We're problem solvers. We believe that data is the key to unlocking effective solutions that span a range of marketing challenges - from customer acquisition to demand generation to account-based marketing. Data is at the core of everything we do. Our team works tirelessly to create powerful solutions that drive real results for our clients. Whether it's through innovative technology or deep analysis, we're committed to finding the best path to growth for every one of our customers.

 Why Join Our Database SolutionsTeam? 

Join our  Database Solutions division   in  Littleton, Colorado as a  Data Engineer!  This is an excellent opportunity for an intelligent, energetic, and self-motivated recent graduate looking to start a data engineering career. As our Data Engineer you will have an important role in receiving, organizing, and loading data into Anteriad’s Data Warehouse. This data will then be used by our clients and throughout the Anteriad organization.

 Anteriad means “always moving forward” and we apply that to our company culture by tirelessly promoting an environment that allows our employees to thrive: 

 Flexible PTO 
 Training & development with unlimited access to Cornerstone Learning System 
 Mix of collaborative & independent work 
 Community outreach via Anteriad Cares - encouraging staff to take time to volunteer 
 Professional mentoring program - career guidance from leadership 
 Employee Resource Groups - collaborate with others that share your passions! 
 Great benefits for you and your family 

 Benefits We Bring To You: 

 Comprehensive medical (choice of 3 plans), dental and vision coverage 
 Company paid short-term disability, long term disability and life Insurance 
 Optional supplemental life, accident and critical illness insurance plans 
 401K with company match 
 Flexible PTO and generous holiday schedule 
 Fully paid primary caregiver leave (12 weeks) & parental bonding leave (2 weeks) 

 What You’ll Do: 

 Primary responsibility is to receive, organize, coordinate and load data using Anteriad’s proprietary Multi-file Automated Processing System (MAPS) 
 Work with others to actively manage the flow of over 1,000 weekly files destined for the Anteriad Data Warehouse 
 Develop strong relationships with key personnel in other Anteriad departments, communicating as needed to facilitate quality, receipt, and timely processing of data. 
 Understand and apply special client-specific business rules 
 Troubleshoot data issues with data providers and other applicable parties to successfully remedy problems 

 What You’ll Bring: 

 4-year college degree 
 1-2 years of professional experience or internships preferred 
 Microsoft Office proficiency, with emphasis on Excel and Outlook 
 Highly organized 
 Detail oriented 
 Quick learner 
 Good at multitasking 
 Strong written and verbal communication skills 
 Passion for problem-solving 
 Data experience is preferred 
 Prior SQL experience is a plus 

Our Values:

Lead & Learn We lead with unrivaled vision, innovation and execution, always learning and embracing new ways of doing things to stay out in front 
Collaborate & Celebrate We build great things when we work together as one Anteriad team, celebrating our achievements – both great and small – along the way 
Innovate & Inspire We are always looking for bold new ways to exceed the expectations of our customers and to inspire each other to even greater success 
Do More & Do Good We go above and beyond in the service of our clients and colleagues, and the communities where we live","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Comunicación oral, Multitarea, Resolución de problemas y Skilled Multi-tasker",Solicitar
https://www.linkedin.com/jobs/view/3970972247/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=QhlCts47X6ksXRHnTmohkQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Databricks),"130 US$K/año - 140 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Ejecutivo",hace 2 semanas,"Nueva Jersey, Estados Unidos","Acerca del empleo
What Working at Hexaware offers:
Hexaware is a dynamic and innovative IT organization committed to delivering cutting-edge solutions to our clients worldwide. We pride ourselves on fostering a collaborative and inclusive work environment where every team member is valued and empowered to succeed.
Hexaware provides access to a vast array of tools that enhance, revolutionize, and advance professional profile. We complete the circle with excellent growth opportunities, chances to collaborate with highly visible customers, chances to work alongside bright brains, and the perfect work-life balance.
With an ever-expanding portfolio of capabilities, we delve deep into and identify the source of our motivation. Although technology is at the core of our solutions, it is still the people and their passion that fuel Hexaware’s commitment towards creating smiles.
“At Hexaware we encourage to challenge oneself to achieve full potential and propel growth. We trust and empower to disrupt the status quo and innovate for a better future. We encourage an open and inspiring culture that fosters learning and brings talented, passionate, and caring people together.”
We are always interested in, and want to support, the professional and personal you. We offer a wide array of programs to help expand skills and supercharge careers. We help discover passion—the driving force that makes one smile and innovate, create, and make a difference every day.

The Hexaware Advantage: Your Workplace Benefits
Excellent Health benefits with low-cost employee premium.
Wide range of voluntary benefits such as Legal, Identity theft and Critical Care Coverage
Unlimited training and upskilling opportunities through Udemy and Hexavarsity

Role: Data Engineer (Databricks)
Location: Morristown, New Jersey
Work Mode: Hybrid
Salary Range: $130K - $140K
Role: Databricks Developer
Responsible for designing, developing, and maintaining data processing pipelines using Databricks platform. Working closely with data engineers and data scientists to implement data solutions that meet business requirements, to help client with their cloud migration journey.
It includes the following responsibilities:
Designing and developing data pipelines using Databricks platform.
Writing efficient and optimized code in languages such as Python, Scala, or SQL.
Collaborating with data engineers to ensure data quality and integrity.
Implementing data transformations and aggregations to support analytics and reporting.
Working with data scientists to deploy machine learning models on Databricks.
Troubleshooting and resolving issues related to data pipelines and Databricks environment.
Optimizing performance and scalability of Databricks jobs.
Documenting technical specifications and maintaining code repositories.
Keeping up to date with the latest Databricks features and best practices.
Participating in code reviews and providing feedback to improve code quality.
He/she should have a strong understanding of distributed computing concepts and experience with cloud platforms such as Azure. They should also possess good problem-solving skills and be able to work in a collaborative team environment.

Privacy Statement:
The information you provide will be used in accordance with the terms of our Privacy Policy and will be used specifically for the business/processing purpose of the event. You should be aware that we may share your details with our approved vendors for this event to be handled successfully.","Canalizaciones de datos, Python, SQL y Scala, Azure Databricks",Solicitud sencilla
https://www.linkedin.com/jobs/view/3954201082/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=qlPLAU48NavLHfnGwqSSKw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Chicago, IL","Acerca del empleo
What do incubators and Predictive Sales AI (PSAI) have in common? Well, they’re great for growth, and even better for stability. As an innovative software development and digital marketing company pioneering the field of artificial intelligence, we can offer our newest Data Engineer the best of both words – a high energy, forward-thinking start-up culture, inside of a well-established, profitable, and stable structure . if you’re interested in getting your hands dirty and inciting change into a larger organization with a vision to change the world uses data today, then please read on.

Responsibilities

Beyond working with state of the art technology you will have many different fantastic projects to work on as a Data Engineer at PSAI. Here are just a few different responsibilities you can expect off the bat:

Design, build and maintain both new and existing data infrastructures using tools including (but not limited to) Microsoft SQL Server, Azure Tables, Azure Blobs, and Azure Cosmos
Determine the right database system for a provided data set
Drafting database/table schemas
Properly index tables when applicable to ensure query performance
Create data pipelines for transforming new datasets into consistent, reliable data systems that are usable across multiple platforms
Identify optimizations and improvements in existing data pipelines
Create solutions for data validation to ensure data integrity, accuracy, and consistency
Ensure data is readily available for analysis and development usage
Shifting existing big data pipelines
Come up with data validation solutions for both initial data imports as well as results
Audit the automation process to make sure there are no gaps, redundancies, or faults in the data pipeline that would cause damage to data integrity
Planning query efficient data store schemas with respect to the data requirements of our internal data warehouse
Communicate with data scientists and other software engineers to build proper data pipelines for efficient and cost-effective querying
Develop, construct, test, and maintain architectures with the help and feedback of software engineers
Determine and implement proper data schemas that align with PSAI's and Spectrum’s client’s business objectives
Recommend and implement ways to optimize data reliability, efficiency, and quality
Consult and suggest different technologies for our team to test and utilize together
Incorporate the business objectives of key stakeholders in the architecture of our data warehouse
Research new and interesting data acquisition opportunities

Some Characteristics That Define You

We Understand That As a Data Engineer For PSAI, You Have Many Different Professional Goals And Personal Interests. As Such Here Are Just a Few Different Things That Typically Define Our Team Members On The Data Science Team

Self-Starter. Building a data warehouse is no simple task. You will need to come in with a self-starter attitude to not only make this data engineering role what you want it to be, but make a stellar and efficient data warehouse while you’re at it.
Analytical. In order to solve problems and build innovative new digital marketing campaigns, it is essential that you know how to take an idea and analyze it from all of its angles.
Developer. In the ever changing world of artificial intelligence, it’s not enough just to build models. You bring to the table an eye for data patterns along with a knack for relational database engineering.
Patient. As a data engineer, you know that you work with extremely large data sets on a daily basis. As such we are looking for someone who is not only meticulous, but patient enough to sit and sift through that data in a thorough way.
Creative. Beyond just analyzing data sets, you are an explorer and a puzzle solver. Pulling insights out of your data and understanding how those insights can better shape our tools is something that you live to do.
Student. More so than most industries, the field of data science is always changing and evolving. As such, you are always looking to learn new things and gain new skills.

Required Skills And Experience

On top of the many intangible skills you bring to the table, there are many skills that can help improve the efficiencies and success of your work at PSAI. Here are a few of those required skills and experience that you will come in with as a Data Engineer on our team:

A bachelor’s degree/pursuing a bachelor’s degree in computer science, mathematics, statistics, information systems, or a related field
Experience with statistical modeling
1-3 years working experience with Python and/or SAS languages
1-3 years working experience with SQL databases and database querying languages
1-3 years working experience with C#/.NET
Familiarity with Microsoft Azure
Familiarity with Graph database structures
Experience with both RDBMS and TDMS
Experience with data mining and data cleaning
Experience with data visualization and reporting techniques
Written and verbal expression

Benefits

As a Data Engineer at PSAI there are a ton of fantastic perks and benefits that come along with your work. Here are just a few of the benefits you can expect when joining the PSAI family:

Comprehensive medical & dental insurance
Retirement planning & company matching
Generous PTO, including sick days & holidays
Coworking space access for remote and hybrid work options
Year-round gym memberships
Paid continuing education
Hybrid work from Home , meetings a few times a month to strategize new projects

About Us

Our mission at PSAI has always been the same— growth. Whether that be helping our customers find quality new business, developing and challenging our team members, or evolving our products and services with advancements in technology and best practices, we have always been looking towards bettering ourselves for the future. Now as we continue to grow, we find ourselves as not only the nation’s leading digital marketing and software provider for the home services industry, but an innovator and ground-shaker for the world of artificial intelligence as well. From marketing automation software powered by AI, to top notch digital marketing services via those same AI insights, we love what we do and are excited to continue to innovate for the future together.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, SQL, Sistema de gestión de bases de datos relacionales y Visualización de datos, Bases de datos, Lenguaje de consulta (query), Limpieza de datos y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3984742612/?eBP=BUDGET_EXHAUSTED_JOB&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=lkjUy6fafHzVySlmY5LUPQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Oakland, CA","Acerca del empleo
In this role as a Data Engineer on the Square Banking Data Science team, your primary responsibility will be to build scalable and robust data solutions to comply with global regulatory requirements and build data automation mechanisms that are critical to operating Square Banking in licensed states and countries. You will be expected to partner with regulatory, compliance and operations teams across Square to prepare for banking license application or renewal, providing guidance to stakeholders on the effective use of technologies and performance optimization.

You Will

Build and maintain reliable data pipelines to meet regulatory and compliance data requirements for all Square banking products, including but not limited to Global Loans, Credit Cards, Square Card, and Savings, etc.

Deploy data quality checks to ensure data integrity and scalability, improving data validation and monitoring processes to proactively prevent and identify data issues.

Constantly improve banking regulatory processes with advanced data streaming and reporting tools to enable quick and easy consumption of data for our legal partners.

Manage communication with regulatory teams for request intakes, data clarification and report delivery.

3+ years of data engineering experience or other quantitative field

Advanced proficiency in SQL to produce summary data reports.

Experience in designing and implementing ETL (Extract, Transform, Load) pipelines for data ingestion with ETL scheduling technologies such as Airflow and Prefect.

Have experience building visualization dashboards (e.g. Looker), automating frequently used queries to generate data insights.

Understanding of data quality management and data governance best practices.

Excellent problem solving skills, being a fast learner by understanding business requirements and data architecture within a short period of time.

Can work under pressure and prioritize multiple tasks concurrently in order to meet deadlines and clarity.
Aptitudes y experiencia deseables
DATA ENGINEER, DATA ENGINEERING, ETL, PIPELINE, PIPELINES, SQL, LOOKER, REGULATORY, COMPLIANCE","Arquitectura de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Calidad de datos, Comunicación, Modelado de datos, Necesidades empresariales, Resolución de problemas y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3967189457/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=8hU513LU%2BAy4B8DJusMwnA%3D%3D&trk=flagship3_search_srp_jobs,Sr. Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Nevada, Estados Unidos","Acerca del empleo
Clear Capital is building the future of real estate data, and we need your help! We are seeking experienced product builders: with your talent as a Senior Data Engineer, help us reach our goals of knowing more about a property than anybody else and in the process making home ownership valuations more fair and equitable for millions of people.

Become part of an innovative team supporting and developing the data and machine learning products that will shape Clear Capital’s future. The Senior Data Engineer role at Clear Capital will work closely with product teams to build next generation data products. Working alongside Software Engineers, Data Quality Analysts, ML Engineers, Data Scientists, and ML Ops Engineers who, like you, are dedicated to build exceptional software.

We are looking for a Senior Data Engineer to assist with the development and implementation of systems leveraging structured and unstructured data to deliver data and data science solutions at scale. As Senior Data Engineer at Clear Capital, you are committed to enabling the best work of others on the team. You help yourself and your team to consistently “level up.” You think ahead to anticipate the needs of others and provide concise information for decision-making.

About Us

Clear Capital is a national real estate valuation technology company with a simple purpose: build confidence in real estate decisions to strengthen communities and improve lives. Our goal is to provide customers with a complete understanding of every U.S. property through our field valuation services and analytics tools, and improve their workflows with our platform technologies. Our commitment to excellence — wherever it leads, whatever it takes® — is embodied by team members.

Clear Capital is an equal opportunity employer.

To all recruitment agencies: Clear Capital does not accept agency resumes. Please do not forward resumes to our jobs alias, Clear Capital employees, or any other company location. Clear Capital is not responsible for any fees related to unsolicited resumes.","Almacenamiento de datos, Apache Spark, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Calidad de datos",Solicitar
https://www.linkedin.com/jobs/view/3964952002/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=6JxvR%2BDZMJd%2FxLEgO8x2kA%3D%3D&trk=flagship3_search_srp_jobs,Business Data Engineer,"90 US$K/año - 110 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 3 semanas,"Wenatchee, WA","Acerca del empleo
Business Data Engineer
 Full-Time 
Planning and Analytics
Hybrid-Remote 
 Are you interested in driving change? Do you have a curiosity-driven mindset to discover what is possible? Are you a self-driven individual with integrity? Do you want to be part of a department that is on the path to become uniquely extraordinary? If so, join us as Stemilt’s Business Data Engineer. Stemilt, a vertically integrated Company that brings wholesome and earth friendly products to families around the world, and the largest employer and pillar for the Wenatchee valley community, is looking to become a leading employer, not just in its home valley but, around Washington, and the world. If you are an open-minded, continuously learning individual that looks to push the limits, and is beyond ego… This Company is for you. We are World Famous! Join us, you will love it here!
 As Stemilt’s Business Data Engineer you are charged with implementing new technologies and methods, affecting change, and being a key team member within our Analytical Center of Excellence (ACOE) reporting directly to our Director of Planning & Analytics. Stemilt is undergoing a transformation to turn data into analytical gold! As our Data Engineer, you will be at the forefront of innovation supporting our analytics team transform to data story tellers and insight makers. You are the innovator that makes this transformation and journey possible! This position delivers extraordinary customer service at all levels, and is responsible for the design, build, and implementation of data pipelines that support analysts and reporting across the enterprise. You will be responsible for data extracts and transformations that can be used for the import and export of data, ensuring existing data import jobs run successfully, staying highly organized and knowing when to prioritize different tasks. Ability to work and adapt in a multi-project environment is a must. 
 Additional Responsibilities 
 Help build and maintain our analytical cloud architecture 
 Develop and execute ETL/ELT strategies 
 Participate in our ACOE’s project planning process 
 Assemble large, complex data sets that meet business requirements 
 Tackle data-related technical issues and assist with data infrastructure needs 
 Test data pipelines to ensure data quality and accuracy
 Help our ACOE identify data patterns, create data tools, and reports 
 Build multi-dimensional cube data models used in ERP embedded analytics
 Work with our ACOE to strive for greater functionality within our data systems 
 Prepare data for predicative and prescriptive modeling 
 Support the development planning, analytics and reporting standards 
 Execute data governance strategies 
 Partner with technical and non-technical Stemilters to understand data and reporting requirements 
 Identify areas where improvements can be made and find/suggest solutions 
 Understand and apply advanced mathematical and/or statistical concepts practically to business situations 
 Create actionable and optimal communication plans for the delivery of all information related to the areas of responsibility. 
 Ensure all communications reflect the Employer Brand along with all legal requirements and reach our entire team. 
 Contribute to the team and the Stemilt in various other ways when requested or required. 
 Reports to: Director of Planning & Analytics
 What we bring to your table:  
 A World Famous! benefits package that includes: 
 Medical/Dental/Vision insurance 
 Short- and Long-term Disability insurance 
 FSA 
 Life insurance 
 Matched 401(k) 
 Paid holidays 
 Paid-time-off 
 Performance Incentive Plan
 An amazing opportunity to create new performance standards and help lead a new team, develop knowledge and new career growth paths. 
 What you bring to our World Famous! Table:  
Experience: 
Experience working in a cloud environment (e.g. AWS, Azure, IBM Cloud) 
 Cleansing, manipulating data sets, and anomaly detection 
 Experience building relational data models 
 Experience creating and managing ETL/ELT pipelines 
 Experience performing data analysis 
 Proficiency in connecting to data sources, modeling data, and creating visuals 
 Advanced knowledge of SQL, writing queries, and database design 
 Experience with multiple programming, mathematical, or statistical languages (e.g. Python, R, SAS, Stata, Java, C# etc) 
 Not Required but a plus: 
 Working knowledge of Azure Synapse, Azure DevOps, and/or understanding of software development methods and processes 
 Experience authoring reports using business intelligence tools (e.g. Power BI, Tableau, SAP Analytics Cloud, etc.) 
 Experience with automation tools (e.g. Power Automate)
 Knowledge of application development production and deployment 
 Statistical and mathematical programming or ML tools used to train data (e.g. Python, Stata, SAS, R, MATLAB etc.) 
 Experience working in the tree fruit or agricultural industry 
 Experience working in a manufacturing industry
  Qualifications: 
Bachelor’s degree and 2 or more years’ experience in Computer Science, IT, Economics, Statistics, Engineering, Applied Mathematics, or related field, or an equivalent combination of experience, training, and education. 
 Analytical, able to handle data effectively, efficiently, and with extreme confidentiality 
 Resourceful, self-starter with little to no direct supervision 
 Good communicator, with the willingness to engage other Stemilters
 Ability to work collaboratively across different business functions and effectively influence senior leaders 
 An intellectual curiosity and a drive toward continuous improvement through process enhancement and change 
 Tactful team player who knows how to collaborate with other technical professionals 
 Ability to read, understand, compose, and collaborate on software requirements and training documents 
 Enjoys learning and willing to transfer knowledge 
 Ability to deliver high quality results with an exceptional transparent, consultative, and partnering attitude. 
 A commitment to understand that change is the only constant and a practice of change with purpose, flexibility, and adaptability is a must 
 Ability to interact and connect with all Stemilters 
 Success defined: 
 Improved data systems and insights for Stemilt!","SQL, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3985593529/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=8xAhF68XVLpvTI4O6r0Ekg%3D%3D&trk=flagship3_search_srp_jobs,Tableau developer,"60 US$/h Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 días,"Cupertino, CA","Acerca del empleo
Title: Sr. BI Engineer/Tableau developer 
Potential contract to hire.
Hybrid Role at Cupertino, CA
 Looking for an 8-10 yrs. experienced 
SQL/Postgres, tableau, dbt and basic python 
Snowflake and tableau must have.","Python y Tableau, PostgreSQL",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984524135/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=ouD%2Fn6w%2FN3%2BAPnNGYn09Ow%3D%3D&trk=flagship3_search_srp_jobs,Big Data Developer/Big Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Massachusetts, Estados Unidos","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Job Title: Big Data Developer/Big Data Engineer

Location: Boston, MA (Onsite)

Duration: Long Term

Job Description

We are seeking an experienced Big Data Developer to design, develop, and optimize large-scale data processing systems. The ideal candidate will have a strong background in Hadoop or Scala and experience with Databricks.

Key Responsibilities

Design, develop, and maintain scalable data processing pipelines using Hadoop or Scala.

Implement data transformation and processing workflows on the Databricks platform.

Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver efficient solutions.

Optimize and tune data processing jobs for performance and cost-effectiveness.

Ensure data quality and integrity through thorough testing and validation.

Troubleshoot and resolve issues related to data processing and workflows.

Stay updated with the latest trends and technologies in big data and data engineering.","Analítica, Big data, Hadoop, Hive, Ingeniería de datos , Minería de datos, Scala y Sqoop, Calidad de datos y Procesamiento de datos",Solicitar
https://www.linkedin.com/jobs/view/3965820028/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=N2IazG1m7CpE739boeoSbQ%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Scientist/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 semanas,"Garland, TX","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3925995889/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=aC%2Fwnj6u62CXPdv7pmAI4Q%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Analyst/Engineer/Scientist - Remote,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"Portland, OR","Acerca del empleo
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this market also our candidates get multiple job offers and $100k + salaries.

please check the below links to see success outcomes of our candidates .

https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

https://www.youtube.com/watch?v=OFoqPTNORew

https://www.youtube.com/watch?v=-HkNN1ag6Zk

https://www.youtube.com/watch?v=OAFOhcGy9Z8

https://youtu.be/bJJl27D8bh0

We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3955467499/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=FvS82gKKloga8PQykQwTng%3D%3D&trk=flagship3_search_srp_jobs,Developer/Data Engineer with Security Clearance,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 meses,"McLean, VA","Acerca del empleo
We are looking for an entry level or junior full stack developer to join our high functioning Data Engineering team. This is a great chance to learn and grow your skillset. We are looking for someone who can demonstrate an aptitude or willingness to learn some or all of the following technologies. AWS - S3, IAM, RDS, EMR, EC2, etc

Linux Commands

Trino

Apache Spark

Node.js

JavaScript

Preact.js

Postgres

MySQL

HTML

CSS Target Salary Range is $125k-$150k or more depending on experience. We recognize this skillset is in high demand and will provide opportunities for continued career development and significant future salary growth.","Almacenamiento de datos, Apache Spark, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , MySQL y SQL, Bases de datos y HTML",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3980111564/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=fvSXI87XVNYFctRNhE%2BP0A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Azure Cloud - Remote | WFH,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Grand Prairie, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
We are excited to invite applications for a Data Engineer with a strong background in Azure Cloud technologies. In this role, you will be instrumental in designing, implementing, and maintaining data solutions in a dynamic and evolving environment. If you're passionate about cloud technologies and data engineering, and enjoy working in a collaborative, remote setting, we'd love to hear from you!

Key Responsibilities

Design and Build Data Solutions: Develop and implement scalable data pipelines and architectures using Azure Cloud services, including Azure Synapse.

Collaborate with Teams: Work closely with cross-functional teams to understand data needs and deliver efficient solutions.

Optimize Performance: Monitor and enhance the performance of data systems, ensuring high availability and reliability.

Manage Infrastructure: Utilize Terraform for infrastructure as code (IaC) to manage and automate cloud resources.

What We're Looking For

Strong Communication Skills: Ability to clearly articulate technical concepts to both technical and non-technical stakeholders.

Azure Cloud Expertise: Hands-on experience with Azure Cloud services and tools, including Azure Synapse.

Experience with Terraform: Proficiency in using Terraform for managing and automating cloud infrastructure.

Problem-Solving Skills: Ability to troubleshoot issues and provide innovative solutions.

Why Join Us?

Flexible Remote Work: Enjoy the flexibility of working from anywhere while being part of a supportive and engaging team.

Long-Term Opportunity: Engage in a 12+ month contract with the potential for further opportunities.

Innovative Projects: Contribute to impactful projects and stay ahead with cutting-edge technology.

How To Apply

If you're ready to take on this exciting challenge and make a significant impact, please submit your resume and a brief cover letter outlining your relevant experience.

Employment Type: Full-Time","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Infraestructura en la nube, Ingeniería de datos y Python, Comunicación, Microsoft Azure, Resolución de problemas y Terraform",Solicitar
https://www.linkedin.com/jobs/view/3833867273/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=Rwg9gUl80dERu8DWm63t9Q%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Programmer/Coder/Developer/Data Scientist/Analyst/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Lakeland, FL","Acerca del empleo
The Job Market is Challenging due to almost 300,000 Tech Layoffs since October 2022 due to which thousands of laid-off Techies are competing with existing Jobseekers. Entry-level Job seekers struggle to get responses to their applications and forget about getting client interviews. As the Saying goes ""When the Going gets tough the Tough get going” Candidates who want to make a tech career they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.

Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy, every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs. Clients now post covid can also hire remote workers which increases even more competition for jobseekers.

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's candidates get jobs at technology clients like Apple, google, Paypal, Western Union, Client, visa, Walmart labs etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the IT Industry

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

We also offer Skill and technology enhancement programs for candidates who are either missing skills or lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who want to improve their skill portfolio. If they are qualified with enough skills and have hands-on project work at clients then they should be good to be submitted to clients. Shortlisting and selection are totally based on clients' discretion, not ours.

please check the below links to see the success outcomes of our candidates

https://www.synergisticit.com/candidate-outcomes/

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

Oracle CloudWorld Event (OCW) Las Vegas 2023/ 2022 | SynergisticIT - YouTube

https://youtu.be/-HkNN1ag6Zk

https://youtu.be/Rfn8Y0gnfL8

https://www.youtube.com/watch?v=OAFOhcGy9Z8

https://www.youtube.com/watch?v=EmO7NrWHkLM

https://www.youtube.com/watch?v=NVBU9RYZ6UI

https://www.youtube.com/watch?v=Yy74yvjatVg

For preparing for interviews please visit https://www.synergisticit.com/interview-questions/

We are looking for the right matching candidates for our clients

Please apply via the job posting

REQUIRED SKILLS For Java /Full Stack/Software Programmer

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Project work on the skills
Knowledge of Core Java , javascript , C++ or software programming
Spring boot, Microservices, Docker, Jenkins and REST API's experience
Excellent written and verbal communication skills

For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Project work on the technologies needed
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Programación, Ciencias de la computación, Comunicación, Desarrollo de software, Java y Transferencia de Estado Representacional (REST)",Solicitar
https://www.linkedin.com/jobs/view/3818360624/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=NiAsGB6HHOT3doIZh52Xeg%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Engineer (Remote),"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Augusta, GA","Acerca del empleo
The Job Market is Challenging due to almost 300,000 Tech Layoffs since October 2022 due to which thousands of laid off Techies are competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications forget about getting client interviews. As the Saying goes ""when the Going gets tough the Tough get going” Candidates who want to make a tech career they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.

Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs. Clients now post covid can also hire remote workers which increases even more competition for jobseekers.

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, PayPal, western union, Client, visa, Walmart labs etc. to name a few.

We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT Industry

We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.

please check the below links to see success outcomes of our candidates

https://www.synergisticit.com/candidate-outcomes/

We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrn

https://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Q

https://www.youtube.com/watch?v=OAFOhcGy9Z8

https://www.youtube.com/watch?v=EmO7NrWHkLM

https://www.youtube.com/watch?v=NVBU9RYZ6UI

https://www.youtube.com/watch?v=Yy74yvjatVg

For preparing for interviews please visit https://www.synergisticit.com/interview-questions/

We are looking for the right matching candidates for our clients

Please apply via the job posting

REQUIRED SKILLS For Java /Full Stack/Software Programmer

Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
Highly motivated, self-learner, and technically inquisitive 
Experience in programming language Java and understanding of the software development life cycle 
Project work on the skills 
Knowledge of Core Java, JavaScript, C++ or software programming 
Spring boot, Microservices, Docker, Jenkins and REST API's experience 
Excellent written and verbal communication skills 

For data Science/Machine learning Positions

Required Skills

Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
Project work on the technologies needed 
Highly motivated, self-learner, and technically inquisitive 
Experience in programming language Java and understanding of the software development life cycle 
Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools 
Excellent written and verbal communication skills 

Preferred skills: NLP, Text mining, Tableau, PowerBI, TensorFlow

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos y Lenguajes de programación, Ciencias de la computación, Comunicación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3982794853/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=N2IkmLCKA2uom7aH4fFtSg%3D%3D&trk=flagship3_search_srp_jobs,Power BI Developer with Qlik Experience,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.",hace 2 días,"Nueva York, NY","Acerca del empleo
Job Title: Power BI Developer with Qlik Experience
Location: NY/NJ/CT (Hybrid)
Job Type: Contract
Visa: GC and US Citizens only
Experience Required: 12+ years overall experience

Job Summary: We are seeking a highly skilled and experienced Power BI Developer with a strong background in Qlik. The ideal candidate will have a minimum of 12 years of overall experience in business intelligence and data analytics, with a proven track record of leveraging Power BI and Qlik to deliver actionable insights and drive business decisions. The candidate will work closely with cross-functional teams to design, develop, and implement data-driven solutions that meet the organization’s strategic objectives.
Key Responsibilities:
Design, develop, and deploy advanced Power BI reports and dashboards.
Utilize QlikView/Qlik Sense to create interactive visualizations and complex data models.
Collaborate with stakeholders to gather requirements and translate them into technical specifications.
Perform data extraction, transformation, and loading (ETL) processes using various data sources.
Ensure data accuracy and integrity by implementing robust data validation and quality checks.
Optimize performance of Power BI and Qlik applications for scalability and efficiency.
Conduct data analysis to identify trends, patterns, and insights that support business strategies.
Provide training and support to end-users on Power BI and Qlik applications.
Stay updated with the latest industry trends and best practices in business intelligence and data visualization.
Participate in project planning, management, and execution, ensuring timely delivery of solutions.
Qualifications:
Bachelor’s degree in Computer Science, Information Technology, Data Science, or a related field.
12+ years of overall experience in business intelligence, data analytics, or a related field.
Proven experience with Power BI and Qlik (QlikView/Qlik Sense) in a professional setting.
Strong proficiency in SQL and data modelling.
Experience with ETL tools and processes.
Solid understanding of database management systems and data warehousing concepts.
Excellent analytical and problem-solving skills.
Strong communication and interpersonal skills, with the ability to work effectively with cross-functional teams.
Ability to manage multiple projects and prioritize tasks in a fast-paced environment.
Experience with other BI tools (e.g., Tableau, SAP BusinessObjects) is a plus.
Power BI and/or Qlik certifications are a plus.","Almacenamiento de datos, Analítica de datos, Herramientas ETL, Herramientas de inteligencia de negocios, Inteligencia empresarial, Microsoft Power BI y SQL, Modelado de datos, Qlik Sense y QlikView",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3963386870/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=s8%2BHs4SA5ZNTKmQI%2BTVMcA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - SSIS/SSRS & SQL,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 semanas,"Dallas, TX","Acerca del empleo
Job Description

 Build and maintain E TL processes in SSIS and Azure Data Factory
 Engineer scalable, reliable and performant systems to manage data
 Develop, implement and optimize stored procedures and functions
 Research and analyze data issues and provide automated solutions
 Implement new technologies to enhance the optimization of current practices
 Provide valuable suggestions regarding new ideas and technologies

 Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","Análisis de datos, Apache Kafka, Azure Data Factory, Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos y SQL Server Integration Services (SSIS), Optimización, Procedimientos de almacenado y SQL Server Reporting Services (SSRS)",Solicitar
https://www.linkedin.com/jobs/view/3909364792/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=CphU9ad1Fs7VtLLypK8s5Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer / Analyst,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",Publicado de nuevo hace 2 semanas,Chicago y alrededores,"Acerca del empleo
Job Description

Our ambitious Chief Data Office is spearheading the creation of a top-tier data team dedicated to tackling intricate business challenges and enhancing both our business operations and customer experiences through insightful analysis. We seek a creative data analytics developer adept at navigating complex datasets related to loan and deposit products, with a talent for crafting intuitive user interfaces and a strategic mindset for generating valuable business insights. Collaborating closely with key stakeholders such as Business Chief Information Officers and Chief Financial Officers, you will assess their reporting requirements and devise tailored solutions. Utilizing a range of data query and visualization tools such as SQL, Power BI, and SAS, you will access diverse data sources to craft impactful products for our business partners. Working alongside teams of diverse backgrounds and perspectives, you will drive solutions that deliver tangible benefits for both the bank and our customers.

Work authorization: US Citizen or Green Card

Key Accountabilities

Execute primary responsibilities of a data analytics developer:

Construct, enhance, and sustain data models, dashboards, reports, automation systems, and metrics assistance to facilitate critical business determinations. 
Actively assess data and trends to advise business collaborators on emerging prospects. 
Uphold data precision and standardized reporting through the development and implementation of efficient analytics code adhering to prescribed policies, procedures, and standards. 
Manage time-sensitive tasks and deliverables, necessitating a proactive stance to consistently meet deadlines. 

Design And Deploy Power BI Reports

Establish a uniform dashboard, reporting, and analytics interface tailored for business users. 
Grasp the business needs within the BI framework and devise a data model to translate raw data into actionable insights. 
Translate business requisites into technical blueprints and execute them through reports that facilitate decision-making. 
Execute DAX queries and functions within Power BI to enhance data analysis capabilities. 

Generate Data Extracts

Collaborate with business users to gather requirements, pinpoint data sources, manipulate data through SQL and/or SAS to generate file extracts for integration into PowerBI Dashboards or dissemination to other systems. 
Craft personalized visualizations and user-defined computations as required. 
Devise, implement, and roll out business intelligence solutions utilizing SQL queries, filters, and graphs to comprehensively analyze data at various levels, thereby enhancing performance and offering recommendations. 

Qualifications And Education Requirements

Completion of a Bachelor’s degree in a relevant field; completion of advanced coursework/training in computer science, management information systems, and mathematics/statistics 
Over 3 years of experience in the banking and finance sector 
Over 3 years of experience in developing analytics reports and dashboards, preferably utilizing PowerBI 
Over 3 years of experience working with and generating data extracts 
Over 3 years of experience in programming languages such as SQL, SAS, R, Python, etc. 
Over 3 years of experience in data visualization 
Proficiency in integrating various components of the Microsoft BI Stack, with a particular emphasis on Power BI, Power Apps, Power Automate, and Azure services 
Utilization of analytics techniques to contribute to company growth endeavors, including revenue increase and other significant business outcomes 
Familiarity with industry-leading practices in Analytics 
Strong written, verbal, and interpersonal communication skills 
Robust project management capabilities, with a preference for experience with agile methodologies 
Demonstrated ability to communicate and support initiatives aimed at fostering a data-centric culture 
Client-focused approach in collaborating with line of business stakeholders 
Demonstrated capability to engage directly with C-level executives when presenting data, reports, and presentations 
Strong preference for expertise in the banking domain 
Working knowledge of banking systems and tools (MeridianLink/LoansPQ, Encompass, Velocify, Fiserv Signature, Marquis Executrax MCIF) is advantageous 

**We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, color, ethnic origin, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.**","Analítica, Analítica de datos, Ciencia de datos, Microsoft PowerApps y Visualización de datos, Automatización, Ciencias de la computación, Comunicación, Microsoft Power Automate y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3984319313/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=vuoZCd0gFDaR4kJGps6PsA%3D%3D&trackingId=q9wwE8i4m1yDMGHiazcTSg%3D%3D&trk=flagship3_search_srp_jobs,"Data Center Software Engineer, Cybersecurity, Cloud","161 US$K/año - 239 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Sunnyvale, CA","Acerca del empleo
info_outline

XInfo Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Sunnyvale, CA, USA; Kirkland, WA, USA.Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Sunnyvale, CA, USA; Kirkland, WA, USA.

Minimum Qualifications

Bachelor’s degree in Computer Science or equivalent practical experience.
5 years of experience in programming languages such as Java/C++ and architecture and design of large systems.
3 years of experience working with Software-as-a-Service focused in cybersecurity.
3 years of experience in source control and CI/CD continuous integration and continuous delivery/continuous deployment in scalable compute infrastructure such as Kubernetes as well with stateless services and database such as Bigtable, Spanner, or DynomoDB.

Preferred Qualifications

Master's degree or PhD in Computer Science, or a related technical field.
1 year of experience in a technical leadership role.
Experience developing accessible technologies.

About The Job

Google Cloud's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. We're looking for engineers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Google Cloud's needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. You will anticipate our customer needs and be empowered to act like an owner, take action and innovate. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.

Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible.

The US base salary range for this full-time position is $161,000-$239,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.

Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.

Responsibilities

Review code developed by other developers and provide feedback to ensure best practices (e.g., style guidelines, checking code in, accuracy, testability, and efficiency).
Contribute to existing documentation or educational content and adapt content based on product/program updates and user feedback.
Triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on hardware, network, or service operations and quality.
Write and test product or system development code and

participate in, or lead design reviews with peers and stakeholders to decide amongst available technologies. 

Information collected and processed as part of your Google Careers profile, and any job applications you choose to submit is subject to Google's Applicant and Candidate Privacy Policy.

Google is proud to be an equal opportunity and affirmative action employer. We are committed to building a workforce that is representative of the users we serve, creating a culture of belonging, and providing an equal employment opportunity regardless of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or related condition (including breastfeeding), expecting or parents-to-be, criminal histories consistent with legal requirements, or any other basis protected by law. See also Google's EEO Policy, Know your rights: workplace discrimination is illegal, Belonging at Google, and How we hire.

If you have a need that requires accommodation, please let us know by completing our Accommodations for Applicants form.

Google is a global company and, in order to facilitate efficient collaboration and communication globally, English proficiency is a requirement for all roles unless stated otherwise in the job posting.

To all recruitment agencies: Google does not accept agency resumes. Please do not forward resumes to our jobs alias, Google employees, or any other organization location. Google is not responsible for any fees related to unsolicited resumes.","Bigtable, Ciencias de la computación, Comunicación, Dirección técnica, Infaestructura del centro de datos y Java",Solicitar
https://www.linkedin.com/jobs/view/3942965372/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=BD28CtbT7oLUPAsqgCEdcA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 mes,Estados Unidos,"Acerca del empleo
NO C2C and NO SPONSORSHIP AVAILABLE

The Data Engineer plays a leading role designing and running multiple data platforms across the enterprise. This is a ‘hands on’ role and is responsible for leading the transformation and execution of data solutions from disparate legacy systems to modern cloud-based solutions with a focus on enterprise data management. This position leverages cloud-based (Azure) infrastructure to implement technology solutions that are scalable, resilient, and efficient. This role collaborates with Technology Product Owners, Solution Architects, DBAs, and other cross-functional teams and business leaders. In addition, the Lead Data Engineer/Architect will plan, design, implement and
operate data solutions, using an Agile methodology.

RESPONSIBILITIES
Leads the development of future state data architecture designs, standards, guidelines, and principles.
Works with Technology Product Owners to understand functional requirements and interact with other cross-functional teams to architect data solutions.
Executes design sessions to gather requirements, review, approve, and communicate design artifacts with stakeholders.
Utilizes Microsoft Azure technologies to solve business problems with a focus on enterprise data management.
Designs and manages data models associated with Relational DBs and optionally NoSQL DBs (Azure Cosmos, MongoDB, etc.).
Designs, implements, and maintains database solutions, manages data access, and resolves database performance, capacity, and security issues.
Performs problem-solving of application issues and production errors, including high level critical production issues that require immediate attention.
Develops Data Flow Diagrams (DFDs), Data Dictionaries, and database schemas with a focus on enterprise data management.
Designs and codes SQL queries and maintenance of SSIS packages.
Participates in an on-call support rotation and provide non-standard work hour support
Participates in brainstorming and discussion sessions to help development teams better understand and utilize data technologies.

SKILLS
Bachelor's Degree preferably in Computer Science, Information Technology, or related IT discipline or equivalent experience.
7+ years of experience developing Data Flow Diagrams, Data Models, and Data Architectures both on-prem and in the cloud, with at least 3-5 years utilizing Microsoft Azure.
5-7 years of experience designing and supporting Azure hosted data services.
Successful completion of the Microsoft Azure Data Engineer Associate Certification. Additional Azure Certifications and/or Exams are preferred.
3-5 years of experience working in a multi-business unit organization.
Experience with API code integrations with external vendors to push/pull data.
Experience with SQL Server and Microservices.
Ability to create and maintain organized technical documentation.
Advanced organizational skills with the ability to manage multiple assignments.
Strong interpersonal, written, and verbal communication and presentation skills.
Reasoning ability to solve practical problems and deal with a variety of variables in many different situations.
Experience with Azure Cloud Technologies including a track record of learning new technologies and architecting them to solve business problems.
Demonstrated commitment to continuous learning within the enterprise architecture field.","MongoDB y SQL, Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3980230452/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=WjpdDFyZ%2FxpkAqpFU5xHUA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"90 US$K/año - 100 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 1 semana,Estados Unidos,"Acerca del empleo
JOB SUMMARY:
The Data Engineer will sift through structured and unstructured data from diverse sources to design and build solutions to support BI, data science and operational needs. Will work closely with business leaders, product managers and data scientists to understand the potential business value of data sets and ultimately build data processing pipelines around those data sources. In addition to development around new data sources, the candidate will deliver provisioning capabilities to end users as well as core frameworks and infrastructure that supports a rapidly growing number of use cases.

REPORTS TO:
Director

Responsibilities/Skills/Experience Requirements
JOB DUTIES/RESPONSIBILITIES:
Inform, influence, support, and execute our product decisions and product launches
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Partner with Product and Engineering teams to solve problems and identify trends and opportunities.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data extraction, transformation and loading processes in production. Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
Build platform infrastructure and interfaces between and within GCP, AWS, and other big data platforms in support of BI and operational needs
Provide guidance and support to big data ETL engineer on performance and facilitate ETL performance tuning
Performs other duties as assigned

JOB REQUIREMENTS:
Bachelors Degree
5-10 years of related experience
Valid Driver License for the State of employment
18 years of age or older (except some locations which may allow for 16 and 17 year old individuals)

REQUIRED SKILLS:
BS/BA in Technical Field, Computer Science or Mathematics.
Experience with DBT (data build tools) required
Experience and focus on operations data stores to build out fundamental data models
4+ years experience in the data warehouse space.
4+ years experience in custom ETL design, implementation and maintenance.
4+ years experience working with either a Map Reduce or an MPP system.
4+ years experience with schema design and dimensional data modeling.
4+ years experience in writing SQL/ NoSQL statements.
4+ years experience using Python or Java
Ability to analyze data to identify deliverables, gaps and inconsistencies.
Communication skills including the ability to identify and communicate data driven insights.
Ability in managing and communicating data warehouse plans to internal clients
Experience or demonstrated interest in big data technologies
Experience with Hadoop or other big data platforms
Experience in coding
Strong communication and presentation skills

Years Experience
5 - 10 Years Experience
Travel Requirements
None
Country
United States
Work-In City
REMOTE

Business
Transformco Corporate
Job Function
Engineering/Quality

Employment Category
Regular, Full-time
Compensation Range
$90,000 - $100,000","Almacenamiento de datos, Big data, Extraer, transformar y cargar (ETL), Ingeniería de datos , NoSQL y SQL, Ciencias de la computación, Comunicación, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3921625824/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=nFHaOUpokSRPu0GWWsrngw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,"McLean, VA","Acerca del empleo
Overview

As the largest and leading value-based kidney care company, Somatus is empowering patients across the country living with chronic kidney disease to experience more days out of the hospital and healthier at home.

It takes a village of passionate and tenacious innovators to revolutionize an industry and support individuals living with a chronic disease to fulfill our purpose of creating More Lives, Better Lived. Does this sound like you?

Showing Up Somatus Strong

We Foster An Inclusive Work Environment That Promotes Collaboration And Innovation At Every Level. Our Values Bring Our Mission To Life And Serve As The DNA For Every Decision We Make

Authenticity: We believe in real dialogue. In any interaction, with patients, partners, vendors, or our teammates, we are true to who we are, say what we mean, and mean what we say.
Collaboration: We appreciate what every person at Somatus brings to the table and believe that together we can do and achieve more.
Empowerment: We make sure every voice gets heard and all ideas are considered, especially when it comes to our patients’ lives or our partners’ best interests.
Innovation: We relentlessly look for ways to improve upon the status quo to continuously deliver new solutions. 
Tenacity: We see challenges as opportunities for growth and improvement — especially when new solutions will make a difference for our patients and partners.

Showing Up for You

We Offer More Than 25 Health, Growth, And Wealth Work Perks To Help Teammates Learn, Grow, And Be The Best Version Of Themselves, Including

Subsidized, personal healthcare coverage (medical, dental vision)
Flexible PTO
Professional Development, CEU, and Tuition Reimbursement
Curated Wellness Benefits supporting teammates physical and mental well-being
Community engagement opportunities
And more!

Somatus, the leader in value-based kidney care, has an immediate opening for a Data Engineer to create impact among Somatus’ clients and leadership by developing and maintaining ETL solutions to drive clinical operations, advanced analytics, and machine learning models. This position will work collaboratively with the technology team to support our clinical, operational, and finance teams to integrate data from diverse sources for consumption by internal and external stakeholders.

Responsibilities

Responsible for the development, testing, maintenance, and optimization of cloud-native data and ETL solutions.
Work on small to mid-sized and cross-functional IT and business intelligence solutions.
Participate in the workstream planning process including inception, requirements gathering, technical design, development, testing and delivery of ETL solutions.
Collaborate with Analytics & Reporting, Data Science, Machine Learning, Analytics Engineering, IT Infrastructure, and other Technology teams in solution design, development, and deployment.
Practice business guidelines to protect PHI and ensure secure communication channels for transfer of such data.
Exercise best practice Agile communication and documentation through channels like JIRA, Confluence
Follow DevOps/DataOps best practices throughout software development lifecycle (SDLC).

Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Qualifications

Bachelor's degree in Computer Science, Information Technology, Engineering, Mathematics, or equivalent.
2 - 5 years professional experience in Data Engineering (or similar) role
Experience in designing and implementing data applications and data architectures.
Experience with open-source data frameworks like Spark and/or experience with cloud data platforms is preferred. 
Healthcare experience in a Payer or Provider/Hospital Organization preferred.
Experience in Azure data technologies is a bonus (Azure Data Factory, Synapse, Cosmos DB, Azure SQL).
Experience with DevOps/DataOps practices is a bonus. 
Experience or familiar with Agile or a similar process.
Experience in implementing ELT and ETL solutions. 

Knowledge, Skills, And Abilities

Experience with at least one database/data warehouse solution (e.g., MySQL, MSSQL, Synapse, Snowflake, RedShift).
Strong coding proficiency in at least one programming language (preferably Python)
Experience using industry standard Python libraries for data exploration, analysis, and transformation (e.g., Pandas, Numpy, etc.)
Experience using REST APIs
Proficient in writing SQL Code for SQL queries, views, stored procedures, etc. 
Experience working with data housed in file formats including TXT, CSV, JSON, YAML, Parquet, XLSX
Problem-solving aptitude and critical thinking skills
Excellent communication and presentation skills 

Other Duties

Please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice. Our priority is the health and safety of our members, colleagues, partners, and community. Proof of COVID-19 Vaccination is required for employment. If you are unable to be vaccinated for medical reasons or sincerely-held religious beliefs, we will consider requests for reasonable accommodations consistent with our policy, and where we are able to provide such accommodations without undue hardship to the company pursuant to applicable law. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. Somatus, Inc. provides equal employment opportunity to all individuals regardless of their race, color, creed, religion, gender, age, sexual orientation, national origin, disability, veteran status, or any other characteristic protected by state, federal, or local law. Further, the company takes affirmative action to ensure that applicants are employed, and employees are treated during employment without regard to any of these characteristics. Discrimination of any type will not be tolerated.","Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Validación del sistemas informáticos (CSV), Amazon Redshift, Ciencias de la computación, Comunicación, Diseño técnico, Optimización y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3929064524/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=IfYHYVTwxd4olYe3LIdxwQ%3D%3D&trk=flagship3_search_srp_jobs,Remote Work - Need Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,Estados Unidos,"Acerca del empleo
100% remote

Need valid LinkedIn with picture

Resume must be under 5 pages

Job Summary: As a Data Engineer you will play a critical role in designing, building, and maintaining our data infrastructure. You will work closely with data analysts, data scientists, and other stakeholders to ensure the seamless extraction, transformation, and loading (ETL) of data from various sources into our Snowflake data warehouse. Your expertise in Fivetran, dbt, and Snowflake will be pivotal in helping us create a scalable and efficient data ecosystem.

Key Responsibilities 

ETL Development and Maintenance:
Design, implement, and manage ETL processes using Fivetran to automate data extraction from various sources.
Ensure data is accurately and efficiently transformed and loaded into the Snowflake data warehouse.
Data Modeling and Transformation:
Utilize dbt (data build tool) to develop and maintain data models that support business requirements.
Write, test, and maintain SQL code for data transformations and ensure data quality and consistency.
Data Warehouse Management:
Maintain and optimize our Snowflake data warehouse, ensuring high performance and availability.
Implement best practices for data warehousing, including schema design, indexing, and query optimization.
Collaboration and Communication:
Work closely with data analysts, data scientists, and other stakeholders to understand data needs and deliver solutions.
Communicate complex technical concepts to non-technical stakeholders in a clear and concise manner.
Monitoring and Troubleshooting:
Monitor ETL pipelines and data workflows to ensure they are running smoothly and efficiently.
Troubleshoot and resolve any issues that arise in the ETL processes or data warehouse.
Qualifications 

Educational Background:
Bachelor’s degree in Computer Science, Information Technology, Data Science, or a related field.
Technical Skills:
Proven experience with Fivetran for automated data extraction and loading.
Strong proficiency in dbt for data modeling and transformation.
Expertise in Snowflake, including data warehouse architecture, performance tuning, and security.
Advanced SQL skills and experience with data manipulation and querying.
Familiarity with Python or other programming languages for scripting and automation is a plus.
Professional Experience:
Minimum of 3-5 years of experience in a data engineering role or similar position.
Experience with cloud platforms (e.g., AWS, GCP, Azure) and their data services.
Soft Skills:
Strong analytical and problem-solving skills.
Excellent communication and teamwork abilities.
Ability to work independently and manage multiple tasks in a fast-paced environment.","Ingeniería de datos y SQL, Extracción de datos, Manipulación de datos, Modelado de datos, Modelo de datos, Necesidades empresariales, Optimización de consultas, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3953760111/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=3azF980xFc16AHoWs3yjJQ%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,"Filadelfia, PA","Acerca del empleo
About Us:

We believe everyone deserves a chance to improve their financial future. We’re dedicated to building simple and inclusive financial products that help our members create healthy habits and achieve economic stability.

Some things we’re excited about:

$500 million in spending power used by our members
Increasing members’ credit by 36 points (on average) within the first 3 months
Backed by First Round Capital and JP Morgan

Products we’ve built to make an impact:

Perpay Marketplace: Combines interest-free payments and modern e-commerce to reduce cost of ownership and promote healthy repayment behavior. 
Perpay+: Leverages Marketplace repayment history to help members monitor and build credit with all 3 credit bureaus. 
Perpay Credit Card: Expands access to the flexibility and benefits of a World Mastercard by removing common barriers like high security deposits and low approval odds. 

The Perpay team is a motivated group of creative problem solvers who love getting things done and making an impact. Located in Center City, Philadelphia, our one-of-a-kind space promotes a collaborative work environment, unites our team, and feels like a home away from home.

About the Role:

As a Junior Data Engineer at Perpay, you will assist in building and maintaining our data pipelines and architectures, supporting our data products and insights. You will work alongside data scientists, analysts, and other team members to help drive Perpay’s mission of creating inclusive financial products that improve the lives of our members. With the launch of our credit card, the need for effective data engineering resources has increased to meet our growing modeling, reporting, and analytical requirements.

You will gain exposure to a wide variety of projects across different business domains, ensuring our data infrastructure supports essential functions in risk, commerce, marketing, operations, and more. Your work will directly impact our customers by enabling automated and efficient data-driven services.

We are looking for a Junior Data Engineer who is a quantitative, eager learner with a passion for data and a willingness to develop new skills. The ideal candidate has a foundational understanding of data engineering principles and is excited to grow their expertise in building and maintaining data pipelines, implementing ETL processes, and supporting data governance initiatives. You should be comfortable working in a fast-paced, entrepreneurial environment and handling multiple tasks with various stakeholders.

Why You’ll Love It Here:

Learning Opportunities: Gain hands-on experience and learn from experienced professionals in the field. 
Variety: Work on a diverse set of projects that will expose you to different areas of data engineering and business functions. 
Growth: Opportunities for career advancement and professional development. 
Collaborative Environment: Join a team that values collaboration and continuous improvement. 

Our greatest strength is our people and we’d love for you to be one of them!

Responsibilities:

Assist in the development and maintenance of ETL pipelines using tools like AWS Glue, Apache Airflow, and Fivetran
Support data producers in understanding data sources and contribute to the design and implementation of data models using Redshift and Snowflake
Implement basic data governance practices, including metadata management and data lineage tracking with tools such as Apache Atlas
Collaborate with team members to develop scalable data solutions, ensuring data quality and reliability
Help identify and resolve data-related issues, applying optimization techniques like indexing and partitioning
Learn and contribute to the ongoing development of a modern data architecture, gaining exposure to advanced data engineering practices
Stay current with industry trends and best practices, continuously developing technical skills in data engineering

What You’ll Bring:

Bachelor’s degree in a quantitative/technical field (Computer Science, Statistics, Engineering, Mathematics, Physics, Chemistry)
0-2 years of experience in data engineering or related fields, with a strong eagerness to learn and grow
Basic proficiency in SQL and Python, with a willingness to learn cloud data platforms such as AWS, GCP, or Azure
Familiarity with data warehouse solutions like Redshift or Snowflake and data orchestration tools like Apache Airflow is a plus
Understanding of data modeling and ETL processes, with a keen interest in data governance and quality practices
Strong problem-solving skills and the ability to work collaboratively in a team environment
Excellent communication skills and a proactive approach to learning and development

Hey, we know not everybody checks all the boxes, so if you’re interested, please apply because you could be just what we’re looking for!

What We’ll Bring:

Competitive salary + company equity
401k with company match
Medical / Dental / Vision insurance
Flexible Spending Account (FSA)
Relocation assistance
Pre-tax commuter benefit
Student loan repayment match
Gym subsidy with City Fitness
Cell phone plan
Paid parental leave
Unlimited PTO

Additional Perks:

Opportunity to gain experience at one of the fastest-growing financial startups in the country
Work on both e-commerce & fintech customer-facing products
Collaborate cross-functionally with product, design, marketing, operations, data teams, and more

This is not a remote opportunity; it is 100% onsite () () ()

Perpay is proud to be an equal opportunity employer. We value diversity in all its forms and are committed to creating an inclusive environment. We do not discriminate on the basis of race, religion, color, national origin, gender identity, sexual orientation, sex (including pregnancy), marital status, political affiliation, age, veteran status, disability status or other non-merit factor. Please contact us at careers@perpay.com to request accommodation.","Extraer, transformar y cargar (ETL), Gobierno de datos, Google Cloud y Ingeniería de datos, Amazon Redshift, Apache Atlas, Gestión de metadatos, Metadatos, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3959571166/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=l%2B5VRVBjjEn2pbFn%2B%2BNjaA%3D%3D&trk=flagship3_search_srp_jobs,Data (Power BI) Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"California, Estados Unidos","Acerca del empleo
E2 Consulting Engineers, Inc. (E2) is a professional services firm established in 1988 specializing in a full spectrum of engineering services including, project engineering and design, federal base operations and infrastructure support services, gas pipeline construction and inspection services, environmental consulting and remediation, and information technology services. At E2, we value safety, innovation and collaboration, and we are dedicated to excellence.

Overview

E2 is seeking a highly motivated Data (PowerBI) Engineer to join our team and partner with our client to design, develop, and implement highly functional and scalable Power BI data analytics solutions. The successful candidate will thrive on delivering production-quality dashboards, process and workflow automation, automated reporting solutions, and complex calculations and measures. The right person for this role has a strong background in data engineering/data visualization/data warehousing, loves problem-solving, and is a great communicator. This role is remote with travel to California every month to the client location.

Responsibilities

 Design, develop, and implement Power BI dashboards and reports to meet business requirements. 
 Work closely with stakeholders to gather and understand reporting requirements. 
 Utilize DAX (Data Analysis Expressions) to create complex calculations and measures. 
 Develop and maintain Power Automate flows to automate data workflows and processes. 
 Integrate Power BI with Power Automate and other external data sources for seamless data updates and synchronization. 
 Utilize Rest API for data extraction and integration into Power BI. 
 Write and optimize SQL queries to retrieve and manipulate data from various databases. 
 Implement and maintain data security measures within Power BI, ensuring compliance with organizational policies. 
 Develop role-based access control and manage user access to reports and dashboards. 

Qualifications

Education

 Bachelors degree in Computer Science, Information Technology or related field. 

Experience

 5+ Years in Data analysis, business intelligence or related fields (required). 

Power BI Experience And Skills Required

 Experience with Power Automate, SharePoint it strongly preferred. 
 Data Modeling and Transformation. 
 Apply advanced data visualization techniques. 
 Data integration with External sources. 
 Implement version control for Power BI artifacts to track changes and facilitate collaboration. 
 Advanced understanding of data warehousing concepts, including star and snowflake schemas. 
 Proficiency in optimizing data models for large datasets and complex relationships. 
 Ability to troubleshoot and resolve issues related to data refresh, connectivity, and performance. 
 Conduct training sessions for end-users on Power BI functionality and best practices. 
 In-depth knowledge of Power BI features such as row-level security, custom connectors, and Power Query M language. 
 Experience with Power BI Premium. 
 Capability to design and implement complex data transformations using Power Query Editor. 
 Strong proficiency in SQL queries, SQL Agent, and Store procedures. 
 Knowledge of Rest API. 
 Understanding of data security principles and best practices. 
 Excellent communication and collaboration skills. 
 Certification in Power BI is a plus. 

Benefits

E2 Consulting Engineers, Inc. offers an excellent benefits package including health, dental, vision, and life insurance, 401(k) with employer match, paid time off.

Wage Data Per State Requirements

Salary range for this position is $85,000 - $115,000. The starting salary will be commensurate with skill, education, experience, and working environment.

 Work Environment/Physical Demands 

Work Environment

This job operates in a professional office environment and uses standard office equipment such as computers and phones.

Physical Demands

The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

 Ability to sit/stand for up to 8 hours per day. 
 Ability to move freely for up to 8 hours per day. 

 Travel 

Travel to California every month to the client location.

 Drug Free Workplace 

E2 Consulting Engineers, Inc. is a Drug Free Workplace. After accepting an offer of employment, applicants may be required to undergo background checks, drug testing, and/or fit-for-duty physical examination. Drug screens will include, but not be limited to, Amphetamines, Cocaine Metabolites, Marijuana Metabolites (THC), Opiates, and Phencyclidine (PCP). As a federal contractor, E2 cannot permit employees in certain positions to use medical marijuana, even if prescribed by an authorized physician.

 Solicitation 

Please no solicitation of any kind from agencies, staffing, or recruiting firms.

 EEO Statement 

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.","Almacenamiento de datos, Analítica de datos, Análisis de datos y Inteligencia empresarial, Expresiones de análisis de datos (DAX), Modelo de datos, Panel de control, Requisitos de información, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3641784622/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=eEzTU3sWFdKjrM8cokPsgQ%3D%3D&trk=flagship3_search_srp_jobs,Associate Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 año,"Nueva York, NY","Acerca del empleo
About Vivanti 

At Vivanti, we are building a better consultancy, one consultant at a time. We believe very strongly in the ability of empowered technically-minded individuals to collaborate on solutions to business problems. We are a modern data and cloud consultancy focusing on the rising demands of cloud-first, data-driven enterprises. We provide services across Data & Analytics, Cloud Technologies, Customer Engagement, and Artificial Intelligence.

About the role

You will be working on exciting projects identifying our client's needs and providing solutions across many major data platforms (Snowflake, Databricks, and more) atop all major cloud platforms.

As a Data Consultant you will:

Deliver solutions to our customers' pressing data problems and most urgent data needs.
Work on a variety of projects involving data architecture, data models, data migration, data integration, data analysis, and visualization.
Develop end-to-end data pipelines.
Implement solutions for the establishment of data management capabilities including data models and structures, database and data storage infrastructure, master and metadata management, data quality, data warehousing, data transformation, data analysis and data governance.

About you

We are looking for talented technologists who would like to further enhance their data engineering, analytics and AI/ML skills. Your background may be in data engineering or application development (with strong SQL skills). We are open to graduates who have done a good amount of work with SQL as well as handling data sets. You should be willing to learn & master the leading cloud data platforms (Snowflake, Databricks, AWS, GCP, Azure) and engineering techniques.

Our consultants are adaptable, natural problem solvers, and driven technical people who want to build a better consultancy based on empowerment, autonomy, and doing right by the customer. You bring the technical chops and we will teach you how to consult, how to weave creative solutions to on-the-ground problems, and how to propel your career to the next level. We invest extensively in growth and development providing our consultants with a genuine opportunity for career growth.

What are the technical chops?

We don't expect all of the following skills, but experience in some of the areas below is desirable:

Strong SQL skills
Experience in any of the programming languages (Python, Java, Scala, Go, etc.)
Working knowledge of supporting data structures - SQL or NoSQL databases
Understanding of one of the data platforms ( Snowflake, Databricks, MS SQL Server, Oracle, GCP BigQuery, Azure Synapse)
Working knowledge of Cloud Platforms

Soft Skills

Ability to learn quickly
Ability to communicate effectively across teams, and build positive relationships
Ability to work on teams, collaborate on projects, and contribute to the overall successful delivery of projects
Proactiveness to get things done, be solutions–oriented, and enjoy solving complex problems
Resourcefulness, curiosity, creativity, courage, critical thinking, and a desire to use technology to search for new and innovative solutions to problems
Ability to execute, manage competing priorities, and deliver to deadlines
US Citizenship
Must be based in one of the following locations: New York, Washington DC, or Atlanta.

What about the stock standard benefits that everybody wants?

Medical / Dental / Vision plans picked specifically for both young single professionals and those with families to support.
401(k) match, dollar-for-dollar, up to 4% of your salary.
Flexible paid time-off, designed for adults — take time as you need it when you need it (and you do need at least two weeks off–preferably contiguous–per year!).
Top-of-the-line hardware, whether you want to optimize for horsepower or mobility (mostly MacBook M1s right now).
Vivanti Institute of Technology – an expansive and pervasive learning platform to keep the skills sharp and the mind keen. Learn more about your chosen specialization, whether that's data science, dev/ops, cloud automation, or software engineering. Branch out and pick up a new specialization, or even just dabble in a lot of different arcane technical topics. Whatever you do, we want you to learn and grow.
Sponsored certifications.

We are Vivanti. You can be too.

Above all, we're looking for fun people, with fresh perspectives, and a complete inability to pass up an interesting puzzle or problem to solve. We offer you the opportunity to make a workplace that suits you, with colleagues you love and celebrate, support and encourage. Come help us build a better consultancy, and have a blast along the way!","Analítica de datos, Arquitectura de datos, Google BigQuery, Google Cloud y Ingeniería de datos, Bases de datos, Comunicación, Modelado de datos, Modelo de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3952744203/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=ETAjnDEnHcy9vwCYpBD4Xg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Miami, FL","Acerca del empleo
Analytic Partners Recognized as a Leader by The Forrester Wave, Marketing Measurement & Optimization, Q3 2023 Report “Analytic Partners dominates with data-driven marketing insights.”

We are the Commercial Intelligence industry leader and the first to do it! We help brands use data to drive business growth, and we do this using a combination of technology and expert guidance that helps companies make the best possible decisions. Our Commercial Decision Platform GPS-Enterprise is the machine that turns data into actionable insights and our ROI Genome is our proprietary intelligence layer comprised of 20+ years of Analysis, Insights, and Learning. We measure the impact of Product, Sales, Operations, Customer Data and outside factors such as competitors, economic changes, seasonality, consumer mobility, and more to deliver impact. Our customers drive business growth by leveraging our technology and expert analyst!

We’re growing fast, with global operations across our full-service offices in New York City, Miami, Broomfield, Charlottesville, Dallas, Dublin, London, Paris, Hamburg, Munich, Sydney, Melbourne, Singapore, and Shanghai.

What You'll Be Doing

Data Engineers at Analytic Partners help turn data into expertise. You will work closely with Data Science, Software Engineering and Product Management in an agile development environment to deliver high performance data solutions for data science, application, and analytic workloads.
Responsibilities include:
Analyzing business and product requirements and recommending solutions
Collaborating with data scientists and software engineers on advanced analytics and modeling platforms
Designing database schemas
Building data pipelines
Monitoring data feeds and operations 
Advising cross-functional teams on data best practices

What We Look For In You

Required technologies:
Python
SQL 
Cloud computing (AWS & Azure)
Linux shell scripting
Bachelor's degree: Computer Science, Electrical Engineering, Mathematics, Statistics or related field.
Master’s degree preferred
3+ years of professional experience working with big data.
Experience working in an agile environment.
Excellent written and verbal communication skills.
Excellent understanding of data security and privacy best practices.
Familiarity with data regulations including CCPA and GDPR.
Familiarity with data science, machine learning, and modeling concepts.
Familiarity with the following technologies is a plus:
Apache Spark
Apache Kafka
Snowflake

Our differentiator is – Our People! We hire the brightest talent and develop them into leaders. We foster a culture of PEOPLE, PASSION and GROWTH.

People: We value our people, customers, and partners

Passion: We love what we do

Growth: Unlimited growth means unlimited potential

AP is a customer-focused, team-oriented organization where innovation and results are rewarded, and individuals can chart the course of their own careers.

As a woman founded and led company, this has meant supporting a meritocracy where everyone has opportunities to achieve their best and ensure we foster an environment of diversity, equity, and inclusion. In practice this means we will not only work to recruit a diverse workforce, but also maximize the full potential of all of our people. You can read more about our commitment to DEI here

Additionally, Analytic Partners participates in the E-Verify program in certain locations, as required by law.","Apache Kafka, Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Ciencias de la computación, Comunicación, Comunicación oral y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3955745886/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=0fhIOc1882%2Fnvn6aKICHAA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",Publicado de nuevo hace 1 semana,"Nueva York, NY","Acerca del empleo
A financial firm is looking for a Data Engineer to join their team in NYC or Chicago.

Pay: $900-1000/day

Responsibilities

Develop and implement data ingestion processes using Python, SQL, and Spark in a cloud-first environment
Build robust data models and infrastructure to accommodate a diverse array of market and alternative data sets
Maintain alerting systems to ensure seamless operations for extensive dataset collection
Resolve urgent data incidents to maintain the integrity and reliability of our data services

Qualifications

A Bachelor's or Master's degree in Computer Science, Engineering, or a related field
Experience in data engineering, particularly with big data technologies and environments
Proficiency in Python, SQL, and Apache Spark
Experience with cloud services and architectures, preferably with hands-on experience in AWS, Azure, or GCP
Understanding of data modeling, ETL processes, and data warehousing principles
Excellent problem-solving skills
Strong communication skills
Experience with real-time data processing and streaming technologies is a plus

24-02119","Almacenamiento de datos, Apache Spark, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, Comunicación, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3984293876/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=wxaor52hp3thwtSdwQlLeA%3D%3D&trk=flagship3_search_srp_jobs,Training for DATA ENGINEER,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Prácticas",hace 4 días,"Edison, NJ","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Hi We at Vforce Infotech are actively recruiting candidates for free training and placement opportunities in Data Engineer positions. The training batches will start soon. Please reach out to me if interested.

Qualifications

 Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
 0-3 years of experience in data engineering or related roles.
 Proficiency in SQL and experience with relational databases such as PostgreSQL, MySQL, or SQL Server.
 Experience with big data technologies such as Hadoop, Spark, or Apache Beam is a plus.
 Strong programming skills in languages such as Python, Java, or Scala.
 Familiarity with cloud platforms such as AWS, Azure, or Google Cloud Platform.
 Excellent problem-solving skills and attention to detail.
 Strong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.

Hiring Process

Send your resume to gvamshi@vforceinfotech.com. We will review your application and schedule a meeting to discuss the role and expectations.

Note:We are an E-Verify company and can sponsor H1, Green Card, and H1 transfer. We provide virtual training to ensure you have the necessary skills and confidence to excel in your role.

Join us at VForce InfoTech and be part of a dynamic team driving innovation and excellence in data engineering!","Base de datos relacional, Ingeniería de datos , SQL y Scala, Atención al detalle, Bases de datos, Ciencias de la computación, Comunicación, PostgreSQL y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982033064/?eBP=BUDGET_EXHAUSTED_JOB&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=4x6fv7Heu65eVhf%2F92ftfA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Scottsdale, AZ","Acerca del empleo
CBTS is currently seeking a Data Engineer for a position located in Scottsdale, AZ.
 
Responsibilities:

Collaborate with fellow data engineering and cross-functional data teams to build and deliver custom enterprise data warehousing and database applications.
Accurately estimate tasks, bugs, and spikes in your domain.
Develop software that meets code quality standards and metrics.
Build productive internal/external working relationships.
Effectively manage risk, change, and uncertainty with support from your leader and peers.
Build a solid understanding of inter-team functional data dependencies and navigate appropriate communication channels for solving issues as they arise.
Design and implement new features while continuously improving quality of technical data products and database applications.
Act as a technical expert and engineer on an agile squad.
Create and execute long term vision of technical data products and database applications.
Drive the architectural solutions across data warehousing and database applications.
Develop and Lead communities of practice by mentoring engineers, creating, and enforcing standards, reviewing others code, and sharing knowledge and know-hows.
Remain current on new cloud data solutions, design patterns, products, and development trends.
Assist with problem resolution for customers and end users.


Skills and Experience:

Strong knowledge of data engineering, database solution design, solid principles of data modeling.
Experience 10+ years in data engineering and database technologies
Strong in developing reusable common component data artifacts.
Experience with Data Engineering and Cloud products
Strong in various Data Engineering Methodologies
Data security best practices experience
Strong in data accessibility best practices
SQL and Enterprise Data Warehousing experience
Strong in Azure or similar cloud technologies
Proficient in building out ELT (Extract Load Transform) solutions
Proficient in SQL Server, Azure Synapse or similar.
Proficient in writing SQL Queries

Preferred Experience:

Knowledge of one or more modern database design and Reporting Data frameworks such as ELT, ETL or similar.
Git HUB – Enterprise or similar
Cloud Technologies – Azure Resources for Data Engineering
 
Cincinnati Bell Technology Solutions provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a protected veteran in accordance with applicable federal, state and local laws.

“Notice of Collection for California Employees, Applicants, and Contractors
https://www.cbts.com/privacy-policy/california-privacy-policy/”","Integración continua y entrega continua (CI/CD), ASP.NET, Automatización de pruebas, C#, Continuous Delivery, Enterprise Design Patterns, Integración continua, Microsoft Azure, Patrones de diseño y Revisión de código",Solicitud sencilla
https://www.linkedin.com/jobs/view/3919794192/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=HERWsrExpM4v%2B6amZArN%2Bg%3D%3D&trk=flagship3_search_srp_jobs,Data Warehouse/Modeling Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,Estados Unidos,"Acerca del empleo
Data Warehouse/Modeling Engineer

100% Remote

06 Months Contract with possibility of Extension

US Citizens required on this position.

Description

The successful candidate will be a part of the team that is responsible for Enterprise Data Warehouse. 
This position requires the candidate understands and applies technical standards, principles, theories, concepts, techniques and provides solutions to a variety of technical problems of moderate to high scope and complexity. 
The candidate develops deep knowledge and understanding of the data warehouse and data management assets available within our data platforms and act as the subject matter expert on the data and its utility for operational use. 
The candidate will regularly interact with the customers, data management teammates, and product development partners, your expertise in large volume data in data lakes, data warehouses, or data marts using structured, semi-structured, and unstructured formats - will deliver on the BAE’s Data and Analytics organization’s mission to enable data-driven decisions within any action. 
The candidate will follow established procedures, and contribute to the completion of milestones associated with specific projects and keep up-to-date with the technology shifts through training and development.

Required Skills

Bachelor’s Degree in Computer Science, Information Systems or similar 
7+ years of experience
 Strong understanding of Normalized/Dimensional model disciplines and similar data warehousing techniques. 
Strong experience working with ETL/ELT concepts of data integration, consolidation, enrichment, and aggregation in large volume data sets. 
Expert knowledge of AWS redshift database. 
Expert in SQL and/or SQL-based languages and performance tuning of SQL queries. 
Working knowledge of Data Pipelines, stream processing and highly scalable ‘big data’ data stores. 
Experience with various data storage technologies - e.g. RDBMS (Oracle, SQL Server, etc.), columnar, tabular, document store, NoSQL 
Experience with various ETL/ ETL tools – e.g. ODI, SSIS, Informatica Power Center, Talend, IICS, etc. 
Create supporting documentation, such as metadata and diagrams of entity relationships, business processes, and process flow. 
Collaborate with business users on normalizing and aggregating large data sets based on business needs and requirements. 
Data validation skills to verify data integrity, understand discrepancies, and resolve them with the highest sense of urgency.
Demonstrated ability to share findings with non-technical or business stakeholders. 
Familiarity with Analytical/Reporting Solutions like Tableau, Power BI is a plus 
Virtual team collaboration tools such as WebEx, Zoom etc. 
Experience with developing detailed migration plans, including data profiling, source/target mapping, transformation, data validation, deployment, data load and rollback strategies. 
Create documentation covering all aspects of migration projects, including technical design, architecture diagrams, configurations, data lineage, migration procedures. 
Experience working and leading large scale data migration projects ensuring clarity, accuracy, completeness and alignment with project objectives and compliance requirements. 
Excellent written, verbal communication and Interpersonal skills
 Excellent problem solving skills
Working knowledge of cloud-based data warehouse RedShift is a must have.. 
Strong experience working with ETL/ELT concepts of data integration, consolidation, enrichment, and aggregation in large volume data sets. 
Expert in SQL and/or SQL-based languages and performance tuning of SQL queries. 
Working knowledge of Data Pipelines, stream processing and highly scalable ‘big data’ data stores. 
Experience with various data storage technologies - e.g. RDBMS (Oracle, SQL Server, etc.), columnar, tabular, document store, NoSQL

Preferred Skills

Working knowledge of cloud-based data warehouse RedShift is a must have.. 
Strong experience working with ETL/ELT concepts of data integration, consolidation, enrichment, and aggregation in large volume data sets. 
Expert in SQL and/or SQL-based languages and performance tuning of SQL queries. 
Working knowledge of Data Pipelines, stream processing and highly scalable ‘big data’ data stores. 
Experience with various data storage technologies - e.g. RDBMS (Oracle, SQL Server, etc.), columnar, tabular, document store, NoSQL
10 to 15 years of Data Warehousing and Data Modeling experience.
Position is working in an AWS environment 
Work individually and in a matrix environment 
Work creatively and analytically in a problem-solving environment 
Communicate (written and oral) effectively and demonstrate professional interpersonal skills 
Interact professionally with clients and vendors 
Build positive working relationships with employees at all levels within the organization 
Meet deadlines as necessary 
Effectively work with minimal supervision 
“Can-do” attitude, pro-active and resourceful 
Knowledge of CI/CD pipeline. 
Experience with scripting languages – e.g. PowerShell, Python, etc
For more jobs: https://www.linkedin.com/company/trispoke-managed-services-pvt-ltd/jobs/?viewAsMember=true

#remotejobs #allusajobs #opentowork #fortune500jobs #data #warehousing #modeling #powerBi #modelingengineerjobs","Arquitectura de datos, Amazon Redshift, Ciencias de la computación, Diseño técnico, Linaje de datos, Modelado de datos, Normas técnicas, Perfiles de datos, Proyectos de migración y Validación de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983072180/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=QbOygFSTLxLZvunbPfjToA%3D%3D&trk=flagship3_search_srp_jobs,Python Developer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 días,"Jersey City, NJ","Acerca del empleo

Skill: Python
Design and build horizontally scalable components within the major platform that the team is developing. At least of 7 years’ experience.
Review and provide code feedback in terms of best practices with keen eye towards performance and stability optimizations.
Become subject matter expert on code deployed on our platform and support to other developers.
Write reusable and extendable code and become a key contributor of the core platform.
Consistently work to make our software simpler.
Challenge yourself and your peers to always improve.
Required Skills:
Expertise in functional and object-oriented programming, specifically in Python.
Experience in databases (relational/document/etc.) including NoSQL databases.
Strong in Algorithms and Data Structures.
Experience building distributed and scalable complex services as well as robust micro services.
Experience in writing unit tests in pytest or unit test.
Working knowledge of CI/CD pipelines and automation.
Strong sense of ownership, urgency, and drive.
Self-motivated with a strong work ethic and a passion for learning and problem solving.
 ","Desarrollo web back end, JSON, Programación y Python, Arquitectura orientada a servicios (SOA), Estructuras de datos, Hojas de estilos en cascada (CSS), Programación orientada a objetos (POO), Prueba unitaria y Pytest",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3893516250/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=729Hj2wO22isEwfQPJY3Ww%3D%3D&trk=flagship3_search_srp_jobs,Junior Data Analyst/Scientist/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 meses,"Oakland, CA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

In this market also we have been able to help our candidates get multiple job offers and $100k + salaries.

 please check the below links to see the success outcomes of our candidates  our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage see us exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, Google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry?

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 https://www.youtube.com/watch?v=OFoqPTNORew 

 https://www.youtube.com/watch?v=-HkNN1ag6Zk 

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://youtu.be/bJJl27D8bh0 

 We are looking for the right matching candidates for our clients 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Programmer 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java , javascript, C++, or software programming 
 Spring boot, Microservices, Docker, Jenkins, and REST API experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning Positions 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, and data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow 

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3982987854/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=nqaeBX8W43p49BTy2Q7tBA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Atlanta, GA","Acerca del empleo
Employment Type: Full-Time, Mid-level

Department: Business Intelligence

CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges.

CGS brings motivated, highly skilled, and creative people together to solve the government’s most dynamic problems with cutting-edge technology. To carry out our mission, we are seeking candidates who are excited to contribute to government innovation, appreciate collaboration, and can anticipate the needs of others. Here at CGS, we offer an environment in which our employees feel supported, and we encourage professional growth through various learning opportunities.

Skills and attributes for success:

Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists


Qualifications:

Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency in developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories


Ideally, you will also have:

Experience creating and consuming APIs
Experience with DHS and knowledge of DHS standards a plus
Candidates will be given special consideration for extensive experience with Python
Ability to develop visualizations utilizing Tableau or PowerBI
Experience in developing Shell scripts on Linux
Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions
Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences


Our Commitment:

Contact Government Services (CGS) strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting-edge technology with world-class personnel to deliver customized solutions that fit our client’s specific needs. We are committed to solving the most challenging and dynamic problems.

For the past seven years, we’ve been growing our government-contracting portfolio, and along the way, we’ve created valuable partnerships by demonstrating a commitment to honesty, professionalism, and quality work.

Here at CGS we value honesty through hard work and self-awareness, professionalism in all we do, and to deliver the best quality to our consumers mending those relations for years to come.

We care about our employees. Therefore, we offer a comprehensive benefits package:

Health, Dental, and Vision
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off and Observance of State/Federal Holidays


Contact Government Services, LLC is an Equal Opportunity Employer. Applicants will be considered without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Join our team and become part of government innovation!

Explore additional job opportunities with CGS on our Job Board:

https://cgsfederal.com/join-our-team/

For more information about CGS please visit: https://www.cgsfederal.com or contact:

Email: info@cgsfederal.com","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Datos no estructurados, Estrategia de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3960514298/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=U%2BcnyhRK0Y7cMTWK09B92A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Analyst,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",Publicado de nuevo hace 1 semana,"Irving, TX","Acerca del empleo
Job Description
Epsilon Analytics partners with both internal and external clients, and data providers, leveraging various analytics to drive strategic thought and effective decision making. The Data Analyst is responsible for conducting data analyses using SQL and other tools in support of a variety of analytic solutions.

Roles & Responsibilities
Collaborate with internal/external stakeholders to manage data logistics – including data specifications, transfers, structures, and rules.
Access and extract data from a variety of sources of all sizes (including client marketing databases) via SAS Access, SQL, etc.
Master and perform all steps required to create analysis-ready data sets, including data integration/merging, variable preparation, and quality control (QA/QC)
Develop and execute SQL (or related) programs with detailed direction and supervision.
Provide problem-solving and data analysis, derived from programming experience.
Demonstrate proficiency with desktop and UNIX toolsets (SAS, SAS ODS, SQL, MS Office) to create pivot tables and/or report content such as tables, reports, graphs, etc. (some positions require proficiency in digital analytic tools including Google and/or Adobe Analytics and familiarity with digital data, in addition to or in lieu of SAS/SQL)
Document and articulate steps taken in an analysis to project managers.
Answer questions about data sets and analyses
Follow all policies and procedures for programming, project documentation, and system management.
Become familiar with…
all offerings outlined in the Insider’s Guide to ACG
various statistical offerings and methods (CHAID, logistic/multiple regression, cluster analysis, factor analysis)
Epsilon data assets
the SAS macro library
Participate in the design, planning & execution of projects
Effectively manage time and resources in order to deliver on time / correctly on a limited number (1-4) of concurrent projects
Proactively communicate with supervisor regarding workload and the status of assignments
Prepare basic report content (Word, Excel, PowerPoint) in support of deliverables
Perform two tasks related to the role of Sr. Data Analyst during the year

Minimum Qualifications
Bachelor’s degree in a quantitative discipline (e.g., Statistics, Economics, Mathematics, Marketing Analytics) or significant relevant coursework
1-2 years of experience in the marketing analytics field
Demonstrated proficiency in SQL programming; minimum 2 years of experience
Strong analytic thought process and ability to interpret findings
Acute attention to detail (QA/QC)
Working knowledge of MS Office; including PowerPoint, Word, Excel and Outlook
Ability to work on multiple assignments concurrently
Excellent verbal and written communication skills
Highly motivated and collaborative team player with strong interpersonal skills
Effective organization and time management skills

Desirable Qualifications
Advanced degree (Master’s/PhD) in Statistics, Economics or other quantitative discipline
Database marketing experience/knowledge
Automotive industry knowledge
Ability to program in newer and emerging languages such as SAS, R, and Python

Additional Information
About Epsilon
Epsilon is a global advertising and marketing technology company positioned at the center of Publicis Groupe. Epsilon accelerates clients’ ability to harness the power of their first-party data to activate campaigns across channels and devices, with an unparalleled ability to prove outcomes. The company’s industry-leading technology connects advertisers with consumers to drive performance while respecting and protecting consumer privacy. Epsilon’s people-based identity graph allows brands, agencies and publishers to reach real people, not cookies or devices, across the open web. For more information, visit epsilon.com.
When you’re one of us, you get to run with the best. For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC
Our Culture https //www.epsilon.com/us/about-us/our-culture-epsilon
Life at Epsilon https //www.epsilon.com/us/about-us/epic-blog
DE&I https //www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR https //www.epsilon.com/us/about-us/corporate-social-responsibility
Great People Deserve Great Benefits
We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.
Epsilon is an Equal Opportunity Employer. Epsilon’s policy is not to discriminate against any applicant or employee based on actual or perceived race, age, sex or gender (including pregnancy), marital status, national origin, ancestry, citizenship status, mental or physical disability, religion, creed, color, sexual orientation, gender identity or expression (including transgender status), veteran status, genetic information, or any other characteristic protected by applicable federal, state or local law. Epsilon also prohibits harassment of applicants and employees based on any of these protected categories. Epsilon will provide accommodations to applicants needing accommodations to complete the application process.

REF235548W","Analítica de datos, Análisis de datos, Análisis de marketing, Canalizaciones de datos, Gestión de datos, Ingeniería de datos , Integración de datos, Microsoft SQL Server, Python y SQL",Solicitar
https://www.linkedin.com/jobs/view/3981406382/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=0EuRwy4RuQ8%2Buqb810Vjmg%3D%3D&trk=flagship3_search_srp_jobs,BigQuery Data Engineer - Dallas TX - W2 - Remote | WFH,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Addison, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
We're looking for an experienced and enthusiastic BigQuery Data Engineer to join our team. In this role, you'll act as a key consultant, bringing your expertise to both the development and design of data solutions. You'll primarily work with Google's stack, including BigQuery, Python, and SQL, to handle and analyze ecommerce data.

Key Responsibilities

Consulting and Development: Provide expert consulting services while developing and designing data solutions. Utilize Python to create effective data processing workflows.

Data Analysis: Work with Google Analytics data stored in BigQuery, and apply your skills to ensure accurate and insightful analysis.

Collaboration: Share your senior-level expertise with the team, providing training and guidance as needed.

Visualization: Use tools like PowerBI to create meaningful visualizations that help drive business decisions.

What We're Looking For

Senior-Level Expertise: You have extensive experience in BigQuery and are comfortable teaching and mentoring others.

Google Analytics Experience: Proven experience working with Google Analytics data and analyzing it within BigQuery.

Hands-On BigQuery Experience: 4-5 years of practical experience with BigQuery, demonstrating strong technical skills.

Strong SQL and Python Skills: Proficient in SQL and Python, with a track record of effective data processing and analysis.

Visualization Tools: Skilled in using PowerBI or similar tools to create clear and impactful data visualizations.

Ecommerce or Retail Background: Experience with ecommerce or retail data, particularly in a B2B or B2C context, is highly desirable.

Why Join Us?

Flexible Work Environment: Enjoy a hybrid work model with onsite days in Dallas and remote work for the rest of the week.

Long-Term Opportunity: Be part of a long-term project where your expertise will be valued and your impact felt.

If you're a data-driven problem solver with a passion for working with cutting-edge technologies, we'd love to hear from you!

Employment Type: Full-Time","Analítica de datos, Análisis de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery, Ingeniería de datos , Python y SQL, Bases de datos y Procesamiento de datos",Solicitar
https://www.linkedin.com/jobs/view/3984660274/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=b7ggfPn5oZUkYCFZmcMo7w%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Plano, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Location Plano, TX (Onsite) Need on W2

Duration 18 months+

Data Engineer

10 + years in Data Engineering and Analytics.

Expertise in data analytical skills and handling big data along with real time streaming.

Graph Ontology and semantic modeling with GraphQL or SPARQL experience is a must.

Proactive, self-driven, works independently and collaborates well.

Expertise in Python, Pyspark.

Use of databricks is a must.","Almacenamiento de datos, Analítica, Analítica de datos, Apache Spark, Big data, Extraer, transformar y cargar (ETL), Ingeniería de datos y PySpark, Azure Databricks y Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3983226164/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=VJmzFVyzdkvDcCFZk8De%2Bg%3D%3D&trk=flagship3_search_srp_jobs,Data Cloud Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,"Cherry Creek, CO","Acerca del empleo
Role: Data Cloud Engineer
Location: Cherry Creek, CO
Must be able to sit in Cherry Creek, CO (Denver, CO) – Hybrid 3 days a week
Must have experience with MCSI for the Data loud Engineer. 
Rate: $Market

Must have Perfect communication skills

Data Cloud Engineer (2 positions)
Experience with writing complex SQL and Stored Procedures
Snowflake – the work is very heavy with Snowflake – they should know Informatica or Azure Data Factory or Python Script.
PySpark
Complex Data modeling
MSCI systems
Complex reporting and performance tuning
Needs this person to have financial industry experience – asset management would be a plus
API experience is a nice to have",Comunicación y Procedimientos de almacenado,Solicitud sencilla
https://www.linkedin.com/jobs/view/3982992212/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=N0y0o4Ybv1mXjWNlAKRKLw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Norfolk, VA","Acerca del empleo
Employment Type: Full-Time, Mid-level

Department: Business Intelligence

CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges.

CGS brings motivated, highly skilled, and creative people together to solve the government’s most dynamic problems with cutting-edge technology. To carry out our mission, we are seeking candidates who are excited to contribute to government innovation, appreciate collaboration, and can anticipate the needs of others. Here at CGS, we offer an environment in which our employees feel supported, and we encourage professional growth through various learning opportunities.

Skills and attributes for success:

Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists


Qualifications:

Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency in developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories


Ideally, you will also have:

Experience creating and consuming APIs
Experience with DHS and knowledge of DHS standards a plus
Candidates will be given special consideration for extensive experience with Python
Ability to develop visualizations utilizing Tableau or PowerBI
Experience in developing Shell scripts on Linux
Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions
Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences


Our Commitment:

Contact Government Services (CGS) strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting-edge technology with world-class personnel to deliver customized solutions that fit our client’s specific needs. We are committed to solving the most challenging and dynamic problems.

For the past seven years, we’ve been growing our government-contracting portfolio, and along the way, we’ve created valuable partnerships by demonstrating a commitment to honesty, professionalism, and quality work.

Here at CGS we value honesty through hard work and self-awareness, professionalism in all we do, and to deliver the best quality to our consumers mending those relations for years to come.

We care about our employees. Therefore, we offer a comprehensive benefits package:

Health, Dental, and Vision
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off and Observance of State/Federal Holidays


Contact Government Services, LLC is an Equal Opportunity Employer. Applicants will be considered without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Join our team and become part of government innovation!

Explore additional job opportunities with CGS on our Job Board:

https://cgsfederal.com/join-our-team/

For more information about CGS please visit: https://www.cgsfederal.com or contact:

Email: info@cgsfederal.com","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Datos no estructurados, Estrategia de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982990386/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=9SwF5V5IPSd9AsqRYWF8hw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Columbia, SC","Acerca del empleo
Employment Type: Full-Time, Mid-level

Department: Business Intelligence

CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges.

CGS brings motivated, highly skilled, and creative people together to solve the government’s most dynamic problems with cutting-edge technology. To carry out our mission, we are seeking candidates who are excited to contribute to government innovation, appreciate collaboration, and can anticipate the needs of others. Here at CGS, we offer an environment in which our employees feel supported, and we encourage professional growth through various learning opportunities.

Skills and attributes for success:

Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists


Qualifications:

Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency in developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories


Ideally, you will also have:

Experience creating and consuming APIs
Experience with DHS and knowledge of DHS standards a plus
Candidates will be given special consideration for extensive experience with Python
Ability to develop visualizations utilizing Tableau or PowerBI
Experience in developing Shell scripts on Linux
Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions
Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences


Our Commitment:

Contact Government Services (CGS) strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting-edge technology with world-class personnel to deliver customized solutions that fit our client’s specific needs. We are committed to solving the most challenging and dynamic problems.

For the past seven years, we’ve been growing our government-contracting portfolio, and along the way, we’ve created valuable partnerships by demonstrating a commitment to honesty, professionalism, and quality work.

Here at CGS we value honesty through hard work and self-awareness, professionalism in all we do, and to deliver the best quality to our consumers mending those relations for years to come.

We care about our employees. Therefore, we offer a comprehensive benefits package:

Health, Dental, and Vision
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off and Observance of State/Federal Holidays


Contact Government Services, LLC is an Equal Opportunity Employer. Applicants will be considered without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Join our team and become part of government innovation!

Explore additional job opportunities with CGS on our Job Board:

https://cgsfederal.com/join-our-team/

For more information about CGS please visit: https://www.cgsfederal.com or contact:

Email: info@cgsfederal.com","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Datos no estructurados, Estrategia de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3955434970/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=cggF9S5vC9WEJGSi83rQ4A%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Engineer - Remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Indianápolis, IN","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación y Programación, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3982987917/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=KWRr2ysPRJaF8fH4mhOb2A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Nueva Orleans, LA","Acerca del empleo
Employment Type: Full-Time, Mid-level

Department: Business Intelligence

CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges.

CGS brings motivated, highly skilled, and creative people together to solve the government’s most dynamic problems with cutting-edge technology. To carry out our mission, we are seeking candidates who are excited to contribute to government innovation, appreciate collaboration, and can anticipate the needs of others. Here at CGS, we offer an environment in which our employees feel supported, and we encourage professional growth through various learning opportunities.

Skills and attributes for success:

Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists


Qualifications:

Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency in developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories


Ideally, you will also have:

Experience creating and consuming APIs
Experience with DHS and knowledge of DHS standards a plus
Candidates will be given special consideration for extensive experience with Python
Ability to develop visualizations utilizing Tableau or PowerBI
Experience in developing Shell scripts on Linux
Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions
Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences


Our Commitment:

Contact Government Services (CGS) strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting-edge technology with world-class personnel to deliver customized solutions that fit our client’s specific needs. We are committed to solving the most challenging and dynamic problems.

For the past seven years, we’ve been growing our government-contracting portfolio, and along the way, we’ve created valuable partnerships by demonstrating a commitment to honesty, professionalism, and quality work.

Here at CGS we value honesty through hard work and self-awareness, professionalism in all we do, and to deliver the best quality to our consumers mending those relations for years to come.

We care about our employees. Therefore, we offer a comprehensive benefits package:

Health, Dental, and Vision
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off and Observance of State/Federal Holidays


Contact Government Services, LLC is an Equal Opportunity Employer. Applicants will be considered without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Join our team and become part of government innovation!

Explore additional job opportunities with CGS on our Job Board:

https://cgsfederal.com/join-our-team/

For more information about CGS please visit: https://www.cgsfederal.com or contact:

Email: info@cgsfederal.com","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Datos no estructurados, Estrategia de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982793633/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=AeqNNpF5%2BV01V1v3REz9Ug%3D%3D&trk=flagship3_search_srp_jobs,Tableau PowerBI Developer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Proventus Metrics, is seeking the following. Apply via Dice today!

Hi

Position: Tableau PowerBi Developer

Location: Austin, Texas

Duration: Long Term

Skills/Experience

experience Creating reports or data analysis, using Tableau, PowerBI, SQL, Cognos, Business Objects, QlikView or similar

experience working with very large scale data sets in Oracle, SQL Server, MySQL, Snowflake, or similar.

experience working with Tableau creating dashboards and reports

experience working with PowerBI creating dashboards and reports

Thanks & Regards

Ashok

Austin, TX 78717, USA","Analítica de datos, Extraer, transformar y cargar (ETL), SAP BusinessObjects y SQL, Expresiones de análisis de datos (DAX), Panel de control, QlikView, SQL Server Analysis Services (SSAS), Sistema de gestión de bases de datos (SGBD) y Snowflake",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3981017316/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=hehKh4Mtj8EfElMrkI22XA%3D%3D&trackingId=gXwTnS5txA69UnrDO0briA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Intermediate level,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 4 días,"San Antonio, TX","Acerca del empleo
Why USAA?

At USAA, we have an important mission: facilitating the financial security of millions of U.S. military members and their families. Not all of our employees served in our nation’s military, but we all share in the mission to give back to those who did. We’re working as one to build a great experience and make a real impact for our members.

We believe in our core values of honesty, integrity, loyalty and service. They’re what guides everything we do – from how we treat our members to how we treat each other. Come be a part of what makes us so special!

The Opportunity

As a dedicated Data Engineer, you are engaged in all phases of the data management lifecycle. This includes gathering and analyzing requirements, collecting, processing, storing, securing, and archiving data. You will develop and maintain technical systems for data reporting and technical solutions utilizing emerging technologies. You will also partner with the business to ensure data management solutions align to business objectives.

To fully deliver these imperatives, we have developed technical training curriculum to establish base-level knowledge essential to data engineering standards and practices used within data engineering teams at USAA. After successfully completing the training curriculum, you will be assigned to an engineering team.

We offer a flexible work environment that requires an individual to be in the office 4 days per week. This position can be based in one of the following locations: San Antonio, TX, or Phoenix, AZ. Relocation assistance is not available for this position.

What You'll Do

Participates in the full life cycle of data engineering to include analysis, solution design, data pipeline engineering, testing, deployment, scheduling, and production support with guidance from senior team members.
Assists in the implementation of technical solutions for data reporting and analytic systems.
Assists with designing and writing test scripts to verify data integrity and application of functionality. Reviews functionality of existing test scripts for understanding.
Demonstrates familiarity with IT Change and Release Management best practices. Deploys data pipeline code with assistance from senior team members.
Participate in design and code review sessions.
Actively participates in Agile ceremonies such as daily standup, iteration planning, backlog grooming, and retrospective sessions.
Develops intermediate familiarity of data management best practices by participating in trainings, reviewing documentation, and reading code from existing solutions.
Demonstrates knowledge and understanding of business products and processes.
Assists senior team members in breaking down business features into technical stories and approaches.
Actively learns about new and emerging technologies in the data engineering space. Seeks to apply learnings in current and future projects.
Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled in accordance with risk and compliance policies and procedures.

What You Have

Bachelor’s degree; OR 4 years of related experience (in addition to the minimum years of experience required) may be substituted in lieu of degree; OR Approved certification from CodeUp, Galvanize, VetFIT (Veterans for IT) or eFIT (Employees for IT)
2 years of data engineering, data analysis or software development experience implementing data solutions.
Working Experience in SQL and Relational Databases.
Basic understanding of Agile methodology practices.
Strong analytical and problem-solving skills.
Working experience in Cloud technologies and tools.

What Sets You Apart

Experience with cloud modernization efforts.
Experience with Snaplogic.
Experience with DBT.
Experience with Snowflake.

The above description reflects the details considered necessary to describe the principal functions of the job and should not be construed as a detailed description of all the work requirements that may be performed in the job.

What We Offer

Compensation: USAA has an effective process for assessing market data and establishing ranges to ensure we remain competitive. You are paid within the salary range based on your experience and market data of the position. The actual salary for this role may vary by location. The salary range for this position is: $74,210.00 - $141,830.00.

Employees may be eligible for pay incentives based on overall corporate and individual performance and at the discretion of the USAA Board of Directors.

Benefits: At USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.

For more details on our outstanding benefits, please visit our benefits page on USAAjobs.com.

Applications for this position are accepted on an ongoing basis, this posting will remain open until the position is filled. Thus, interested candidates are encouraged to apply the same day they view this posting. 

USAA is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.","Analítica de datos, Base de datos relacional y Ingeniería de datos, Bases de datos, Elaboración de informes de datos, Integridad de la información, Resolución de problemas, Revisión de código, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3956070603/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=CPuGxa8gls9twvtdpxhVNQ%3D%3D&trk=flagship3_search_srp_jobs,Cloud Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 mes,Estados Unidos,"Acerca del empleo
About Us

Sela is a global cloud partner, dedicated to guiding our clients in their cloud journey. With our deep relationships with top cloud platforms (AWS, GCP, Azure, Alibaba) and our expertise in building resilient cloud infrastructure, we enable our clients to 'Cloud Better.' Our team has over three decades of experience delivering excellence and thought leadership to clients. Visit our website to learn more about what we do.

What We Are Looking For

We're looking for a Cloud Data Engineer to join our cloud development team. Your primary role will be to help design, build, and optimize data solutions for our clients. You'll work with large datasets, leveraging the latest cloud technologies to build robust data pipelines and analytics platforms. You take ownership of your work, are a great communicator, and thrive in environments with changing requirements.

Our team includes network architects, cloud architects, software developers, and security engineers. You will have the chance to gain significant real-world experience and work on high-impact projects. We also provide many opportunities for growth and will cover costs of relevant education and certification.

What You'll Do:

Architect and develop data pipelines for batch processing, streaming, and event processing
Build and manage data lakes/lakehouses
Implement and optimize data warehousing solutions
Collaborate with data analysts, scientists, and clients to understand requirements
Implement data governance, security, and compliance controls
Contribute to planning new data architectures and evaluating emerging technologies

Relevant Skills:

Batch Processing and Data Processing
Distributed frameworks: Apache Spark, Databricks, Flink, Ray
Programming: Python, Scala, Java
Streaming and Event Processing
Streaming platforms: Kafka, Kinesis
Real-time data ingestion and processing
Data Warehousing and Analytics
Cloud data warehouses: Snowflake, BigQuery, Redshift
Databases: OLAP, OLTP
Query engines: SQL, Presto
ETL/ELT: AWS Glue
Data Lakehouses: Delta Lake
Additional Skills:
Cloud platforms: AWS, GCP, Azure
Data governance, lineage, security
Containerization: Docker, Kubernetes

Great to Have
Experience with Docker, containerization, and container orchestration
Experience building serverless applications and workflows
Experience with Python, Node.js, JavaScript, and SQL
Experience making infrastructure design decisions and managing client requirements
AWS certifications

Benefits
Remote-first workplace
We offer top medical, dental, and vision insurance for you and your dependents (with options for 100% covered)
FSA and HSA plans available
Retirement matching (3%)
Unlimited PTO
Home office stipend
Annual team get-togethers
Modern tools to get your job done (MacBook, etc)
Collegial workplace focused on individual growth

We don't expect candidates to be familiar with all relevant skills but do expect them to have comprehensive experience in the domains in question and eagerness to learn. If you geek out on data infrastructures and want to work on cutting-edge projects, we'd love to hear from you! This is a full-time position with a generous benefits package and a quarterly performance bonus. Pay for this position will be commensurate with experience. Due to regulatory requirements, only candidates located in the US will be considered for this position. This is a fully remote role.","Almacenamiento de datos, Apache Spark, Extraer, transformar y cargar (ETL), Gobierno de datos, Ingeniería de datos y Scala, Bases de datos, Diseño de infraestructuras de TI, JavaScript y Procesamiento de transacciones online",Solicitud sencilla
https://www.linkedin.com/jobs/view/3974250615/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=d1vO%2B2IdHf7Jkw2qnw4aeA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"72 US$K/año - 84 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
The company Parabola Solutions was founded by technologists with specialised knowledge in business intelligence, CRM, mobile, and web solutions. They had an incredibly effective synergy, and the business quickly rose to prominence as a top supplier of comprehensive technical solutions.

The Role

You Will Be Responsible For

Designing, maintaining and supporting the network infrastructure.
Monitoring system performance and ensuring reliability and availability.
Recommending infrastructure solutions to meet business requirement in compliance with IT policy & procedure.
Providing Level 2 support and troubleshooting as and when required.

Ideal Profile

You possess a degree in Computer Science, Applied Mathematics, Engineering or related field.
You have at least 4 years experience, ideally within a Data Engineer role.
Demonstrated experience working with large and complex data sets as well as experience analyzing volumes of data.
You have working knowledge of Full understanding of ETL and Data Warehousing concepts. Experience with version control software. Strong understanding of Agile Principles, particularly Scrum. Experience with Azure. Experience with Databricks Delta Tables, Delta Lake, and Delta Live Tables. Experience with Relational Data Modeling. Experience with Structured Streaming (Spark or otherwise). Experience with GitHub SaaS and GitHub Actions.
You are a strong networker & relationship builder
You possess strong analytical skills and are comfortable dealing with numerical data
You are a self-starter and demonstrate a high level of resilience

What's on Offer?

Work alongside & learn from best in class talent
Leadership Role
Opportunity to make a positive impact","Almacenamiento de datos, Apache Spark, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Microsoft Azure, Modelado de datos, Modelado de datos relacionales, Possess strong analytical y Resolución de incidencias",Solicitar
https://www.linkedin.com/jobs/view/3876799957/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=XpGocnvPZEJcDhwukVfADw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"115 US$K/año - 150 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 meses,"San Diego, CA","Acerca del empleo
Fig is building a digital food-as-medicine platform to help anyone with complex dietary needs live an easier, healthier life around food. We're a fast-growing, early-stage startup supporting over 1,000,000+ users and backed by top VCs including Sequoia, Artis Ventures, Goodwater Capital, and Correlation Ventures. Our users love us (4.7 stars on app stores) as Fig changes their lives: ""THANK YOU!! It is sometimes such a struggle to find PCOS-compliant snacks, ingredients, etc. This app has truly changed my life in the 2 weeks I’ve had it!”

Fig’s mission is to help millions of people with dietary restrictions more easily find food. Our core product, the Fig phone app, already helps hundreds of thousands of people navigate food at grocery stores and restaurants each month.

Our company is seeking a highly skilled Data Engineer with extensive experience in building data ingestion, processing, and instrumentation systems at scale.

What You Will Do

As a Data Engineer at Fig, you will:

Design, develop, and maintain our data architecture, data models, ETL pipelines, and data warehouse.
Collaborate with cross-functional teams to identify business needs and translate them into data solutions.
Implement and optimize data ingestion processes from multiple data sources for real-time analytics and business intelligence.
Set up orchestration engines to run data ingestion processes in the cloud.
Monitor, troubleshoot, and optimize the performance of data pipelines and database systems.
Assist in developing data governance and data quality processes and ensure compliance with data privacy and security policies.
Stay up-to-date with the latest technologies and trends in the data engineering field.

Qualifications

Who you are:

At least 3-5 years of professional experience in data engineering or related roles.
Proficiency in SQL and Python and experience with ETL tools.
Experience with cloud platforms (AWS, GCP, Azure)
Solid understanding of database design, data warehousing concepts, and data modeling.
Proven ability to build and maintain data pipelines and deliver high-quality data solutions.
Strong problem-solving skills, analytical capabilities, and attention to detail.
Excellent communication skills to collaborate with diverse teams.
EXTREME attention-to-detail, efficiency, competence and a desire to get things right the first time (our users are counting on you!)
Stellar references

Nice To Haves

Experience with web scraping at scale
Personal experience with dietary restrictions (either yourself or a loved one); navigating tricky dietary needs is a daunting task, and prior understanding of these challenges will be helpful.

How We Work

Fig is a fully remote team. We generally work US hours, with most of our meetings happening between 12-5 PM EST. We speak and write in English and generally expect employees to be proficient in communicating in English. That said, we don’t have a strict geographic requirement if you are willing to work within those constraints. We expect this role to integrate into our software development lifecycle and join team meetings for sprint planning, retrospectives, and review.

What We Offer

Competitive salary
Be part of building company culture
For full-time employees:
Meaningful equity in the form of stock options
401(k) plan
Medical, Dental, and Vision Insurance
Flexible time off and paid holidays
How To Apply

Please reach out to recruiting@foodisgood.com with a resume or any questions. If we are interested, we will schedule an introductory call to get to know you more!","Almacenamiento de datos, Analítica de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, Comunicación, Modelado de datos, Modelo de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3881962539/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=VT9zeGI7Ufb5%2BVO%2BRSMf%2Bw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (CONTRACT) REMOTE,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 meses,Estados Unidos,"Acerca del empleo

Data Engineer 
Remote
Until 6/30 but will extend 
 
Work-Mode: This is a remote role where you will work off-site. Travel is limited and generally used for team learning and collaboration meetings. 
We are seeking a hardworking, motivated, and innovative Data Engineer to join our growing team on contract basis. In this role, you will focus on working on business requirement analysis, Application Design, Data Modeling, Development, testing and documentation. In addition, you will partner with internal and client teams to help craft and translate business requirements into technical specs, assure realization of business benefits and support the business process.
 
Job Responsibilities:
Snowflake Expertise:
Design, develop, and optimize data pipelines within the Snowflake data platform. Implement best practices for Snowflake data modeling, warehouse architecture, security and performance optimization.
Proficient in using Snowflake Snowpipe streaming for real-time data streaming and ingestion and different batch load ingestion.
AWS Proficiency:
Utilize AWS services for data integration, storage, and processing. Hands-on experience with AWS Data Migration Service (DMS) for seamless and secure data migration.
Implement and manage streaming data solutions using AWS Kinesis. Design and develop ETL processes using AWS Glue, MSK for efficient data transformation.
Airflow Workflow Orchestration:
Implement and maintain data workflows using Apache Airflow. Ensure the reliability and scalability of data workflows for efficient data processing. Data Aggregation and Transformation:
Develop and implement data aggregation strategies for large-scale datasets. Collaborate with data scientists and analysts to understand data requirements and ensure data quality.
Architecture Design:
Collaborate with cross-functional teams to design and implement scalable and reliable data architecture. Ensure adherence to best practices for data security, privacy, and compliance.
Qualifications:
Minimum – 7 to 10 years of Data Engineering experience
Strong Experience with Snowflake, AWS and ETL
Hands-on experience with snowflake security, roles, encryption, masking, replication, snowpipe streaming.
Hands-on experience with AWS Data Migration Service (DMS), Apache AirFlow, MSK, Kinesis Firehose for seamless and secure data migration.
Hands-on experience with RESTapi, java script, Python, Scala, SQL and Java.
Computer skills – a thorough knowledge of computer programming, coding, and various operating and database systems is a must
Time management – developers should have the ability to quickly develop data warehousing systems and solve any issues to ensure the continued accuracy of business data
Creativity – the ability to create mappings from scratch often calls for strong creative skills on the part of a developer
Analytical thinking –Developers should be able to analyze data needs and options and understand the needs of various clients
Troubleshooting – when data warehousing systems are down, Data engineer should be quickly assess the problem and provide a solution
Team collaboration – Data engineer rarely work alone; they typically interact closely with database managers and other IT specialists when maintaining, storing, and retrieving data

Aptitudes y experiencia deseables
SNOWFLAKE","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Bases de datos, Programación informática, Réplica, Sistemas de bases de datos, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3977908341/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=0PA43stD7GXXkQeudVQzJg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Contract),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Position: Data Engineer (Contact)
Mode: Contract - long term
Location: Remote

Tech Stack Grid:
NO Awareness of technology (0)
LITTLE Awareness - read/heard of technology (1)
EXPOSURE to technology in environment (2)
SOME development in technology (3)
Very COMFORTABLE developing in technology (4)
EXPERTISE in technology i.e. could teach a class (5)

SQL, Level – 5
Python, Level - 5
AI Tools, Level – 3
Redshift, Level – 5
msExcel, Level - 4
AWS - S3, Level - 3
Glue, Level - 3
AWS - Athena, Level - 2
AWS - Lambda, Level - 1
AWS - SNS/SQS, Level - 1
Tableau Desktop /Creator, Level - 2
QuickSight, Level - 2


Responsibilities
Expertise to understand complex queries, proof test, and tweak for the intended use. Critical analysis skills to extract data and analyze the data for inconsistencies and issues.
Good hands-on experience of working with python.
Must have led implementation of any market popular AI tools for analyzing the data and creating trend reports.
Good analytical skills and experience in handling large sets of data
Demonstrate the ability to access, trace, analyze, and remediate data issues leveraging SQL and applicable AWS tools such as Athena and Redshift.
Strong SQL skills and ability to work with different data sources.
Automate the data extract pull and able to visualize these on charts/graphs/Tableau
Gather, analyze, and interpret data to solve specific use cases. Develop and participate in the review of requirements, data mappings, database specifications documentation and other artifacts and clearly communicate to both the business and technical team members the interpretation of the data.
Design and manage information systems, conduct analysis, and generate accurate and comprehensive reports on operational data.
Very strong in documentation of data artifacts, design and reports details.
Support Architects, Leads and product owners to ensure that all aspects of the information analysis and requirements gathering process are completed with the highest degree of accuracy and quality, including the development and socialization of key project artifacts.
Support ad hoc data requests as required as part of DevOps team.
Review, extract, and analyze data and information on system processes and procedures.
Experience developing testing strategies and identifying comprehensive test scenarios based on business requirements.
Demonstrated experience with AWS Lambda, NodeJS, SNS/SQS, S3, IAM, CloudWatch, RDS, DynamoDB, Redshift and Athena.
Willingness and ability to perform in multiple roles on a team, including testing and production support in addition to data analysis.
Strong ability to understand and internalize the big picture and broader implications.
Communicate and understand business goals and requirements, and work to create data solutions that add value to the business.
Support team with functional, regression and end to end testing efforts with a focus on flow of data ensuring that features deliver the expected functionality with high quality.
Self-starter with ability to set priorities, work independently and attain goals.
Develop data-driven reporting solutions from concept to deployment.
Focus on designing and developing effective user-friendly reporting assets including interfaces and outputs.
Cultivate and maintain knowledge of College Board products, data systems and information assets.
Work with a highly proficient team using Tableau/Tableau Server, SQL, Relational/Columnar Databases, Data Modeling, MS Office
Develop scripts to extract data in Athena, Timestream and Redshift.
Exhibit good communication and practical decision-making skills, a believer in good feedback and documentation.


About You
BA/BS required (major in an analytical field desired)
A minimum of 5 to 10+ years work-related experience.
Experience in hands-on development of reporting applications for the Web in a professional environment
Passion for new technology and adapting to the latest tools in data analysis and reporting.
The Ethos of continuous improvement and interest in learning new things.


More about you
Strong analytical thinking and structured problem-solving ability
Ability to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment.
Versed on the agile methodology and best practices.
Excellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc.
Self-starter, ability to set priorities, work independently and attain goals


Pay Rate Range: $60-$70 per hr (Depending on Experience)


About the College Board
We are a mission-focused, not-for-profit membership organization that believes in promoting innovation, equity, and excellence for generations of students. Our members include more than 6,000 of the world’s leading colleges, schools, and other educational organizations. We have over 1,600 employees in offices across the continental U.S. and Puerto Rico.
We are advocates for children and parents; we empower teachers and educators, and we are a strong presence in thousands of schools and communities across the country through programs and services - the SAT, Advanced Placement (AP®) and Pre-AP are just a few. Our work falls broadly into four categories: College Readiness, College Connection & Success, Student Opportunities, and Advocacy.
Named by Fast Company as one of the most innovative education companies, the College Board is a mission-focused organization. This job requires a strong focus on improving educational opportunities and outcomes, particularly for disadvantaged students, in the context of a competitive business environment.
Mission
Our mission is to clear a path for all students to own their future.


Selected candidates will be employed as a contingent worker of nextSource which is College Board’s Managed Staffing Solutions Provider unless role is listed as “direct hire”. nextSource is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status, marital status, parental status, cultural background, organizational level, work styles, tenure and life experiences. Or for any other reason. nextSource is committed to providing reasonable accommodations for qualified individuals with disabilities in our job application procedures. If you need assistance or an accommodation due to a disability, you may contact us at talent@nextSource.com or you may call us at +1 (917) 818-2313.","Analítica de datos, Amazon CloudWatch, Amazon Redshift, Amazon Simple Notification Service (SNS), Amazon Simple Queue Service (SQS), Análisis de la información, Bases de datos, Escenarios de prueba, Herramientas de generación de informes y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3982035632/?eBP=BUDGET_EXHAUSTED_JOB&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=YSwdCq5cOSTPZQmLav436A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 días,"Madison, WI","Acerca del empleo
Element6's client is looking for a Data Engineer to join their team in Madison, WI. This is a key role on their IT Data Team requiring a broad range of skills and the ability to step into different roles depending on the size and scope of the business need. The self-motivated candidate will have proven experience architecting successful data solutions on key projects in a collaborative environment. Success will come from being able to prioritize, deliver value incrementally, problem solve, and manage changing priorities. You will work closely with the Company's business partners and interface with both technical and non-technical colleagues.

Responsibilities: 
Data Architecture
Specialize in data modeling, both 3NF and dimensional, with experience in conceptual, logical, physical, and industry data modeling. Strong knowledge and experience with data architecture methodologies.
Apply the appropriate level of modeling theory, pattern recognition, and abstractions to architect and design a pragmatic solution that functionally meets the business and technical requirements.
Partner with internal business units to define information requirements and translate them into appropriate data solutions.
Collaborate with IT and business partners to lead data discovery, profiling, analysis, and quality assessments in order to obtain clear information requirements.
Develop and validate source to target mappings and transformation logic required to support business needs. Understand the importance of capturing data lineage.
Architect, implement and verify end-to-end data solutions.
Develop test plans needed to ensure a quality deliverable. Participate in validation testing, coordinate user acceptance testing and training to ensure the final implementation enables the user to solve their business problem.
General Data Management
Play a critical role in architecting our data and analytics solution landscape
Demonstrate competence, experience, knowledge, understanding, and advocacy of data management concepts, data warehousing, BI, and analytics.
Demonstrate ability to perform appropriate level of strategic thinking by viewing initiatives both within the immediate project context as well as the overall architectural vision.
Participate and/or Lead in data architectural design and strategy discussions.
Data Delivery
Work with the business users to conduct data discovery engagements and can quickly identify, and prototype, a solution that brings together multiple data sources into one coherent concept and understanding. (data blending)
Leverage existing tools to create data visualizations and mentors the business to be self-sufficient.
Collaborate
Identify and communicate project risks and impediments and proactively work with other members of the Analytics team to complete high-value deliverables as identified by business partners and team leadership.
Partner with Analytics team members to translate business and functional requirements into technical designs
Strive to understand the data consumption needs of the business community, as well as the problems faced by business users involving the access and use of data
Help Analytics teams develop solutions that enable businesses to capitalize on business insights and drive toward gaining a competitive advantage

Requirements: 
Minimum of 3-5 years of experience in Data Solution delivery in a complex environment working collaboratively in a team setting
Proficient in Data Solution tools and concepts such as:
Business Intelligence tools: Microsoft tools (SQL Server Management Studio, SSRS, SSAS, Power Pivot, Power Query, PowerBI), Alteryx
Database: SQL Server
Data Query tools: SQL, T-SQL
Data Management and Quality: data mapping, data profiling, metadata repository, relational data modeling, master data management
Data Modeling: ER/Studio Data Architect, 3NF and dimensional modeling
Data Warehousing concepts: Inmon, Kimball, Data Lake
Data Integration concepts and strategies: EII, ETL, EL-T and EAI","Almacenamiento de datos, Arquitectura de datos, Integración de datos, SQL y SQL Server Integration Services (SSIS), ER/Studio, Inmon, Integración de aplicaciones empresariales (EAI), Linaje de datos, Modelos dimensionales, Necesidades empresariales, Perfiles de datos y SQL Server Reporting Services (SSRS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3945491629/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=Nwb%2F18RXyg4iNoM4K3oXhg%3D%3D&trk=flagship3_search_srp_jobs,Data and Software Engineer,"62 US$K/año - 93 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,Estados Unidos,"Acerca del empleo
About Ovative Group:

Ovative Group is the premier independent media and measurement firm in the United States. We help change-makers, in fast-growing, customer-centric organizations across industries reinvent their marketing and measurement programs. We leverage our media, measurement, and consulting capabilities to help brands like Coach, Kate Spade, Stuart Weitzman, Facebook, The Home Depot, CVS, Disney, and UnitedHealth Group transform their media and marketing programs. Our proprietary approach to measuring and optimizing marketing investment decisions, Enterprise Marketing Return (EMR), is disrupting the industry and setting the gold standard for customer and marketing strategy, activation, and measurement. 

Recognized eight consecutive years on Star Tribune’s list of Top 150 Workplaces and five years on Inc. 5000’s list of the fastest-growing private companies in America, we pride ourselves in always overdelivering for our clients, our teams, and our communities. 

About the Role:

We are seeking a Data & Software Engineer to join our rapidly growing product and engineering development team. Our company specializes in enterprise-level media measurement and optimization across various industries. In this role, you will be an integral part of a cross-functional team responsible for creating, optimizing, and maintaining scalable software and data solutions to enhance performance, stability, and scalability. You will work under the guidance of experienced team members and closely collaborate with stakeholders throughout the entire software development lifecycle, from concept to deployment.

The ideal candidate will have a strong foundation in iterative development practices, familiarity with version control systems like GitHub, and a passion for developing both conceptual and pragmatic problem-solving skills. Your attention to detail and communication skills, both written and oral, will enable you to work directly with a variety of users, understand their objectives, and contribute to translating them into technical requirements and solutions. As a valuable team member, you will be encouraged to learn and grow in a supportive environment, actively participate in team activities, and set the foundation for your professional development.

Responsibilities:

Assist in designing, developing, testing, and deploying software solutions that align with business and technical requirements.
Contribute to the effort of identifying opportunities for automation with a focus on the operational stability of software applications and systems
Collaborate with your team to support translating business goals and user requirements into detailed, actionable technical requirements
Contribute to documentation and technical requirements, including software designs, evaluation plans, test results, technical manuals and formal recommendations and reports
Contribute to the creation and maintenance of robust, performant solutions that provide high customer impact at scale
Provide technical assistance to teammates as needed through collaboration and by sharing your expertise
Follow established software and product engineering best practices, including code quality, documentation, deployment, and testing.
Explore new technologies and tools under guidance to enhance software development.
Troubleshoot tier 1 software issues with support from senior team members as needed.
Participate in technical knowledge sharing with other team members.
Stay current with emerging technology and software engineering innovations and continuously enhance skills.


Requirements:

3+ years of relevant data & software engineering development experience
Proficient working with ETL/ELT tooling for large data sets (e.g., Airbyte)
Proficient utilizing SQL, Python, and command line
Familiarity with cloud-based platforms (i.e. GCP, AWS)


Preferred:

Experience working with APIs for data retrieval
Experience working with data warehouses and big data tools (e.g., BigQuery, Databricks)
Experience creating data/table architecture
Experience implementing QA processes and QA automation
Experience integrating data models within software
Experience working with marketing, analytics and customer data


Pay Transparency

At Ovative, we offer a transparent view into three core components of your total compensation package: Base Salary, Annual Bonus, and Benefits. The salary range for this position below is inclusive of an annual bonus. Actual offers are made with consideration for relevant experience and anticipated impact. Additional benefits information is provided below.

For our Sr. Analyst positions, our compensation ranges from $62,000 to $93,000, which is inclusive of a 15% bonus.

Benefits Of Working At Ovative Group

We provide strong, competitive, holistic benefits that understand the importance of your life inside and out of work.

Culture:

Culture matters and we’ve been recognized as a Top Workplace for eight years running because of it. We demand trust and transparency from each other. We believe in doing the hard and complicated work others put off. We’re open in communication and floor plan. We’re flat – our interns sit next to VPs, our analysts work closely with senior leaders, and our CEO interacts with every single person daily. Put together, these elements help foster an environment where smart people can support each other in performing to their highest potential.

Compensation and Insurance:

We strive to hire and retain the best talent. Paying fair, competitive compensation, with a large bonus incentive, and phenomenal health insurance is an important part of this mix.

We’re rewarded fairly and when the company performs well, we all benefit.

Tangible amenities we enjoy:

Access to all office spaces in MSP, NYC, and CHI 
Frequent, paid travel to our Minneapolis headquarters for company events, team events, and in-person collaboration with teams. 
Flexible paid vacation policy 
401k match program 
Top-notch health insurance options 
Monthly stipend for your mobile phone and data plan 
Sabbatical program 
Charitable giving via our time and a financial match program 
Shenanigan’s Day 


Working at Ovative won’t be easy, but if you like getting your hands dirty, driving results, and being surrounded by the best talent, it’ll be the most rewarding job you’ll ever have. If you think you can make us better, we want to hear from you!","Aseguramiento de la calidad, Comunicación, Desarrollo de software, Intercambio de conocimientos, Manuales, Manuales técnicos, Modelo de datos, Requisitos del usuario, Requisitos técnicos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982452469/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=%2FfUtJ73N4A1%2BR89gKwrMCA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Irving, TX","Acerca del empleo
Buzz About Us:

Welcome to Hunt Hive Talent Recruiting Agency, where were all about connecting amazing talent with fantastic opportunities! Our mission? To help businesses and professionals soar to new heights. We specialize in matching skilled professionals with roles that challenge and inspire them. If youre ready to take your expertise in data management and cloud environments to the next level, look no further. Join our vibrant team and be part of something extraordinary!

About the Role:

Were on the hunt for a dynamic Data Engineer to join the hive on behalf of our clients. This role requires a seasoned professional with extensive experience in data management and cloud environments, particularly AWS and Snowflake. If youre passionate about technical troubleshooting, development, and automation work, we want to meet you!

Your Mission:

Perform all necessary data-related tasks, including data design, data quality, data triage, data governance, and data architecture
Develop scripts and automation tools to detect and correct data issues
Develop monitoring and alerting capabilities to proactively detect data issues
Work directly on complex application/technical problem identification and resolution, including responding to off-shift and weekend support calls
Participate in technical sync-up meetings with internal teams, including US and offshore teams
Collaborate with cross-functional teams (data engineering, data solutions, service, and tier 1 support teams) via Teams meetings, chat, and email for issue triaging and resolution


What You Bring:

Bachelor's degree in a relevant field
5-7 years of experience in data management, data engineering, or data operations (data design, data quality, metadata, governance, etc.)
3 or more years of experience in a Cloud environment (AWS, Snowflake)
Strong proficiency in SQL, Python, PySpark, and Pandas
Strong technical troubleshooting and problem-solving skills
Experience developing scripts and automation tools
Strong collaboration and communication skills, with the ability to work effectively in cross-functional teams


Additional Requirements:

Must be a United States citizen or authorized to work in the United States.
Must be at least 18 years of age.


Ready to Join the Hive?

Don't miss out on this exciting opportunity! Apply now by visiting our website at Hunt Hive Talent and submit your application. We can't wait to hear from you!

Hunt Hive Talent Recruiting Agency is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.","Arquitectura de datos, Gobierno de datos, Ingeniería de datos , Pandas (Software) y PySpark, Calidad de datos, Modelado de datos, Resolución de problemas, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3984549207/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=aUQZkb4DxJi9wvfXDTfrxg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Java & Big Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,"Sterling, VA","Acerca del empleo
We have the following urgent position with Capgemini. Please send all qualified candidates.

Job Title: Data Engineer - Java & Big Data Engineer

No. of Positions: 14

Location: Sterling, VA (Remote)

Duration: 12 Months

Rate: DOE C2C All Inclusive

Skills & Experience Required:

 Java/J2ee candidate with spring, cleanse, transform, etl and hadoop and pipelines. 
 Use Core Java techniques to modify data to improve quality. 
 Building data pipelines using Java
 Experience of creating ETL pipeline - In Java, Pig, and activated in Oozie,
 Using Spark in Legacy system
 Using Flink in streaming prospective
 Experience of Coding in Java or Scala, EMR and Spark as well
 Java, Hadoop, Spark, Oozie,
 Java Developer with Oozie, with Data Engineer background
 Pure Data Engineer resources with Java and Hadoop
 Experience of Git to promote code
 Java has SDK for Spark, if Spark is not available, can submit with Data Engineer with Java
 Programming in EMR AWS
 Scala is nice to have.","Apache Pig, Apache Spark, Extraer, transformar y cargar (ETL), Hadoop, Ingeniería de datos , Scala y Sqoop, Java, Oozie y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3970518952/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=42W%2BYWLZTldwZHpwikw0hg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"63,37 US$/h - 73,37 US$/h Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 semanas,"Santa Clara, CA","Acerca del empleo
Job Title: Data Engineer
Job Location: Santa Clara California 95051 (Hybrid)
Onsite Requirements:
 Python
 AWS
 Snowflake
Job Description:
Develop electric vehicle user model or surrogate models using telematic data.
Conduct extensive descriptive analytics on collected data Analyze, summarize and present research findings and results through presentations, reports/documents and publications
Job Knowledge and Skills: 
Qualifications and Education Requirements Graduate degree in Data Analytics, Data Science, Computer Science or related technical subject area.
Experience with a wide range of data analysis, machine learning and statistical modeling algorithms/techniques Expertise in Python with hands-on experience with machine learning frameworks like Tensor Flow, and packages for data sciences such as Pandas, SciPy, NumPy, etc.
Experience with Big Data and cloud environments such as AWS Attention to detail, flexibility, creativity, and excellent communication and teamwork skills
A strong passion for empirical research and for answering hard questions with data Excellent written and verbal communication skills.
** 3rd party and subcontract staffing agencies are not eligible for partnership on this position. 3rd party subcontractors need not apply.
This position requires candidates to be eligible to work in the United States, directly for an employer, without sponsorship now or anytime in the future. **","Almacenamiento de datos, Analítica de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Atención al detalle, Ciencias de la computación, Comunicación y Comunicación oral",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984254084/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=3DYUlOkccSi5pGuJiU%2F%2BlQ%3D%3D&trk=flagship3_search_srp_jobs,AWS Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Scottsdale, AZ","Acerca del empleo
Role: AWS Data Engineer

Location: Scottsdale AZ (Day 1 onsite)

 Type: Long Term Contract

 About VLink: Started in 2006 and headquartered in Connecticut, VLink is one of the fastest-growing digital technology services and consulting companies. Since its inception, our innovative team members have been solving our global clients' most complex business and IT challenges.

 Must Have : 

 IAM , AWS , Glue , S3 Redshift , Kinesis , Python/Java , Scala RDS SQL Server.  AWS Certified Data Analytics Specialty or AWS Certified Solutions Architect. 

 Needs someone who can understand and work on AWS echo system - Glue , Kinensis, DynamoDB etc. ) and the access permissions and especially terraform infra as code. 

 It is combination of skills AWS DE + some hands-on knowledge in AWS infra. 

 Position Summary: 

We are seeking a highly skilled AWS Data Engineer with extensive experience in AWS technologies to join our team as a contractor. The ideal candidate will have a strong background in designing, building, and maintaining data pipelines, and must be capable of contributing immediately to our ongoing projects.

 Key Responsibilities:

 Utilize AWS services such as Kinesis, S3, Glue, Redshift, and RDS SQL Server for data processing and storage.
 Implement data ingestion processes to handle streaming and batch data.
 Ensure data quality and integrity through robust ETL processes.
 Collaborate with other data engineers and the Cloud engineering team to develop and deploy data pipelines in AWS.
 Optimize and tune data processing workflows for performance and cost efficiency.
 Monitor and troubleshoot data pipeline issues to ensure continuous data flow and reliability.
 Document data architecture, processes, and workflows.

 Qualifications:

 Bachelor's or master's degree in computer science, Engineering, or a related field.
 Minimum of 5 years of experience in data engineering, with a focus on AWS technologies.
 Proven experience with AWS services including Kinesis, S3, Glue, Redshift, and RDS SQL Server.
 Strong proficiency in SQL and experience with database design and optimization.
 Expertise in ETL/ELT processes and tools.
 Familiarity with data warehousing concepts and best practices.
 Experience with data modeling and schema design.
 Proficiency in programming languages such as Python, Java, or Scala.
 Knowledge of data governance and security best practices in a cloud environment.
 Excellent problem-solving skills and the ability to work independently with minimal supervision.
 Strong communication and collaboration skills.

 Qualifications:

 AWS Certified Data Analytics Specialty or AWS Certified Solutions Architect.
 Experience with other AWS services such as Lambda, Cloudwatch, Kinesis, Firehose, Event bridge, Redshift, DynamoDB, IAM, RDS SQL Server
 Familiarity with big data technologies like Apache Spark or Hadoop.
 Experience with reporting and visualization tools like Tableau
 Knowledge of DevOps practices and tools for CI/CD such as Jira and Harness.

 Employment Practices:

EEO, ADA, FMLA Compliant

VLink is an equal opportunity employer. At VLink, we are committed to embracing diversity, multiculturalism, and inclusion. VLink does not discriminate on the basis of race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. All aspects of employment including the decision to hire, promote, or discharge, will be decided on the basis of qualifications, merit, performance, and business needs.
Aptitudes y experiencia deseables
AWS, DATA ENGINEER, S3, GLUE, KINESIS, IAM, TERRAFORM","Arquitectura de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Amazon Redshift, Bases de datos, Diseño de bases de datos, Modelado de datos, Optimización y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3959055517/?eBP=BUDGET_EXHAUSTED_JOB&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=oSni6tKh8ftQeZBM864fVQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Databricks),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Dallas, TX","Acerca del empleo
Title 

Data Engineer (Databricks)

Overview

This is a full-time, employee position. Please do not apply if you are seeking a C2C or 1099/W2 contract. 

 Must be located in one of the following: Atlanta, Chicago, Columbus, Dallas, Minneapolis, New York Area, St. Louis. This position is mostly Remote. 

Daugherty Business Solutions brings a fresh approach to data engineering by delivering results through unmatched innovation and world-class technology and talent. This is why many of the most well-known companies in the world trust us with their mission-critical projects. As a team member at Daugherty, you will play an integral role in our company’s success and are recognized and valued for your contributions. We have an entrepreneurial culture with the maturity and security of a 35+ year company helping you to create your best work and take your career to the next level.

We are seeking a skilled and experienced Data Engineer with expertise in Databricks, Azure, and Spark to join our dynamic team. The ideal candidate will play a key role in designing, developing, and maintaining scalable data pipelines and analytics solutions to support our growing business needs. This position offers an exciting opportunity to work with cutting-edge technologies and collaborate with cross-functional teams to drive actionable insights from data.

Responsibilities

 Design, develop, and deploy end-to-end data pipelines using Databricks, Azure Data Factory, and Spark to ingest, process, and analyze large volumes of structured and unstructured data. 
 Collaborate with data scientists, analysts, and other stakeholders to understand business requirements and translate them into technical solutions. 
 Optimize and tune data pipelines for performance, reliability, and scalability to ensure efficient processing of data. 
 Implement data governance and security best practices to ensure the integrity and confidentiality of data. 
 Develop and maintain documentation, including data flow diagrams, technical specifications, and user guides. 
 Stay up-to-date with emerging technologies and best practices in data engineering, cloud computing, and big data analytics. 


Qualifications

 Proven experience as a Data Engineer, with at least 5 years of hands-on experience in designing and building data pipelines. 
 Proficiency in Databricks, Azure services (e.g., Azure Data Lake Storage, Azure Synapse Analytics), and Spark for big data processing and analytics. 
 Strong programming skills in Python, Scala, or Java, with experience in writing complex SQL queries. 
 Experience with data modeling, ETL/ELT processes, and data warehousing concepts. 
 Excellent problem-solving skills and attention to detail, with the ability to troubleshoot and debug complex data engineering issues. 
 Strong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. 
 Able to articulate ideas clearly, present findings persuasively, and build rapport with clients and team members. 
 Ability to effectively communicate technical concepts to non-technical stakeholders .
 Relevant certifications in Databricks, Azure, or Spark (e.g., Azure Data Engineer, Databricks Certified Associate) are a plus. 
 Bachelor's degree in Computer Science, Engineering, or related field; Master's degree preferred. 


 What We Commit to YOU: 

 We provide many training opportunities like certifications, hackathons, lunch and learns and free access to Pluralsight, Udemy and other digital learning platforms. 
 You will get to work with some of the most innovative teams in the IT marketplace and solve real strategic problems. 
 We will invest in things that are important to you professionally and personally. 
 We will build a relationship with you to accelerate your career. 
 We will provide you with a team environment like no other. We are consistently ranked as a Top Workplace in many of our regions as voted by our employees. 
 We provide opportunities to build community, be social and have fun with your colleagues. 
 We provide a comprehensive compensation and benefits package. 


Daugherty Business Solutions is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.

If you require accommodations or assistance to complete the online application process, please inform any recruiter you are working with (or send an email to careers@daugherty.com) and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The recruiting team will respond to your email promptly.","Almacenamiento de datos, Apache Spark, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , SQL y Scala, Modelado de datos, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3976468546/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=aVnl1hLYkq7qqKyK9hn%2FeA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 semana,"Dallas, TX","Acerca del empleo
Who We Are

Apex Fintech Solutions (AFS) powers innovation and the future of digital wealth management by processing millions of transactions daily, to simplify, automate, and facilitate access to financial markets for all. Our robust suite of fintech solutions enables us to support clients such as Stash, Betterment, SoFi, and Webull, and more than 20 million of our clients' customers.

Collectively, AFS creates an environment in which companies with the biggest ideas in fintech are empowered to change the world. We are based in Dallas, TX and also have offices in Austin, New York, Chicago, Portland, and Belfast.

If you are seeking a fast-paced and entrepreneurial environment where you'll have the opportunity to make an immediate impact, and you have the guts to change everything, this is the place for you.

AFS has received a number of prestigious industry awards, including:

2021, 2020, 2019, and 2018 Best Wealth Management Company - presented by Fintech Breakthrough Awards
2021 Most Innovative Companies - presented by Fast Company
2021 Best API & Best Trading Technology - presented by Global Fintech Awards

About This Role

Job Title:  Data Engineer

Location: Dallas, TX or Chicago, IL

Job Summary:

The Data Engineer will operate, maintain, and improve the data quality, accuracy, timeliness, accessibility, and processing speed to meet Apex Fintech Solution's goal in enhancing financial services through technology. By working on innovative data solutions, this role supports the company's mission to empower dynamic fintech initiatives and change the industry landscape. The Data Engineer will bridge the complex world of data with the company's strategic objectives, making information a powerful tool for business transformation.

Duties/Responsibilities:

 Design, build, and optimize data pipelines, ensuring robustness and scalability for handling vast amounts of data
 Develop and maintain real-time database systems and data warehousing solutions that align with business needs
 Collaborate with stakeholders to understand data requirements and implement systems that provide critical business insights through data analytics
 Enhance data quality and reliability by implementing modern data management techniques and technologies
 Drive continuous improvement by identifying and implementing system optimizations to reduce downtime and improve data processing efficiency
 Document all data architecture processes and systems while ensuring compliance with industry standards and company policies
 Support data platform operations in all environments during business hours. 

Required Skills/Abilities:

 Proficient in Python and minimum of 2+ years’ experience with Cloud Data technologies. 
 Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
 Experienced in ETL processes using micro services

Education and/or Experience:

 Bachelor's degree in Computer Science, Computer Engineering, or a related field. 
 At least 2 years of experience in data engineering with a proven track record in data management systems
 Experience in the financial services industry preferred but not required

 

Work Environment:

This job operates in a modern hybrid office environment
Collaborative team settings with open communication channels across various departments
Fast-paced atmosphere that values creativity, innovation, and results, with a strong emphasis on data-driven decision making

#engineering #full-time #APEX #associate

Our Rewards

We offer a robust package of employee perks and benefits, including healthcare benefits (medical, dental and vision, EAP), competitive PTO, 401k match, parental leave, and HSA contribution match. We also provide our employees with a paid subscription to the Calm app and offer generous external learning and tuition reimbursement benefits. At AFS, we offer a hybrid work schedule for most roles that allows employees to have the flexibility of working from home and one of our primary offices.

Diversity, Equity, Inclusion, and Belonging (DEIB) Commitment

We're looking for all kinds of people.

At Apex, we believe that wealth management and investing should be accessible to everyone, and we strive to create spaces to democratize investing for folks of all walks of life. Internally, we embrace diversity and are dedicated to creating an inclusive and equitable workplace, which reflects our company vision and mission. We value every team member's unique perspective and are committed to fostering a culture where everyone belongs. Join us in our mission to empower and celebrate individual differences.

Apex is committed to being an equal opportunity employer. We ensure that qualified applicants receive fair consideration for employment without discrimination based on sex, gender identity, gender expression, sexual orientation, race, color, natural or protective hairstyle, genetics, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law. Know your rights: workplace discrimination is illegal. We stand by this commitment to promote a diverse, equitable, and inclusive workforce.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Gestión de datos, Ingeniería de datos y Python, Calidad de datos, Ciencias de la computación y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3963335869/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=3nieRaO1Jd6yqVvjJByvrA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"St. Louis Park, MN","Acerca del empleo
We are looking for a Data Engineer to join our Technology Team at Bridgewater Bank in St Louis Park, MN. In this role, this individual will work with our Data Architect to establish and define ETL processes, data integrations, and automation workflows to support a centralized data warehouse. This is an excellent opportunity for someone with experience in data science and analytics to build and expand their application of engineering tooling for driving our data strategy.

***Candidate must be local to Minnesota***

Responsibilities

Data modeling and building meaningful optimized, reusable data sets leveraging SQL
Building, maintaining, and optimizing ETL pipelines that power insightful data visualizations
Ensure data quality and integrity across multiple data sources and systems
Work with cross-functional teams to identify and solve business problems through data-driven insights
Continuously improve data quality and accuracy through data cleansing and validation
Clearly, sharply articulate and define necessary data architecture to meet the requirements of the organization
Develop and document best practices for governance of data sets


Qualifications

Proven experience with SQL for database querying
Familiarity with data modeling concepts
Excellent problem-solving skills and attention to detail
Effective communication, requirements gathering and collaboration with stakeholders
Ability to work independently and as part of a team
Bachelor’s degree in Computer Science, Information Systems, related field or 1-2 years of equivalent experience


Preferred Skills

Proficiency in Snowflake or Microsoft SQL Server 
Proficiency in Power BI administration
Experience in the banking or financial industry
Experience in data tooling implementations


About Bridgewater Bank

It all started with a vision in 2005. This vision was to create a full-service, entrepreneurial bank where clients would notice a difference, team members would be challenged to grow, and the culture would be optimistic. Over a decade later, this unconventional attitude laid the foundation of Bridgewater Bank, a nationwide top-performing bank with an award-winning culture.

We’re on a mission to become the finest entrepreneurial bank in the Twin Cities. And it’s working. Join our team and you will be surrounded by remarkable people who want to challenge the status quo and redefine what it means to work in this industry. This journey began in 2005, and it’s just getting started. Will you join us?

Please Note

The above is intended to describe the general content of and requirements for this position. It is not to be construed as an exhaustive list of duties, responsibilities, or requirements. It is Bridgewater Bank’s policy to promote equal employment opportunities. All personnel decisions, including, but not limited to, recruiting, hiring, training, promotion, compensation, benefits and termination, are made without regard to race, creed, color, religion, national origin, sex, age, marital status, sexual orientation, gender identity, citizenship status, veteran status, disability or any other characteristic protected by applicable federal, state or local law.

STATUS: Exempt","Arquitectura de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Limpieza de datos, Modelado de datos, Resolución de problemas, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3946618073/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=%2B%2BBncBtOZRH8FrJdVfvRVg%3D%3D&trk=flagship3_search_srp_jobs,Remote Work - Need Azure Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,Estados Unidos,"Acerca del empleo
100% Remote

Need valid LinkedIn

Must be W2 or 1099

Must Have Experience

Azure Data Services – Azure Data Factory, Databricks, Data Streaming.

Experience in handling big data sets.

Experience working in a matrixed environment.

Excellent communication and collaboration skills.

Healthcare experience (must be strong and recent)","Analítica, Analítica de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos y Minería de datos, Bases de datos y Comunicación",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982994034/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=UohFlPr8UYXAd3107JU8UQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"100 US$K/año - 165 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Santa Ana, CA","Acerca del empleo
Employment Type: Full-Time, Mid-level

Department: Business Intelligence

CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges.

CGS brings motivated, highly skilled, and creative people together to solve the government’s most dynamic problems with cutting-edge technology. To carry out our mission, we are seeking candidates who are excited to contribute to government innovation, appreciate collaboration, and can anticipate the needs of others. Here at CGS, we offer an environment in which our employees feel supported, and we encourage professional growth through various learning opportunities.

Skills and attributes for success:

Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists


Qualifications:

Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency in developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories


Ideally, you will also have:

Experience creating and consuming APIs
Experience with DHS and knowledge of DHS standards a plus
Candidates will be given special consideration for extensive experience with Python
Ability to develop visualizations utilizing Tableau or PowerBI
Experience in developing Shell scripts on Linux
Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions
Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences


Our Commitment:

Contact Government Services (CGS) strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting-edge technology with world-class personnel to deliver customized solutions that fit our client’s specific needs. We are committed to solving the most challenging and dynamic problems.

For the past seven years, we’ve been growing our government-contracting portfolio, and along the way, we’ve created valuable partnerships by demonstrating a commitment to honesty, professionalism, and quality work.

Here at CGS we value honesty through hard work and self-awareness, professionalism in all we do, and to deliver the best quality to our consumers mending those relations for years to come.

We care about our employees. Therefore, we offer a comprehensive benefits package:

Health, Dental, and Vision
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off and Observance of State/Federal Holidays


Contact Government Services, LLC is an Equal Opportunity Employer. Applicants will be considered without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Join our team and become part of government innovation!

Explore additional job opportunities with CGS on our Job Board:

https://cgsfederal.com/join-our-team/

For more information about CGS please visit: https://www.cgsfederal.com or contact:

Email: info@cgsfederal.com","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Datos no estructurados, Estrategia de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3974320392/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=KYOBmkIyQr8Gixbu8d8WJw%3D%3D&trk=flagship3_search_srp_jobs,Business Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Wenatchee, WA","Acerca del empleo
Brief Description

Business Data Engineer

Full-Time Planning And AnalyticsJob DescriptionHybrid-Remote

Are you interested in driving change? Do you have a curiosity-driven mindset to discover what is possible? Are you a self-driven individual with integrity? Do you want to be part of a department that is on the path to become uniquely extraordinary? If so, join us as Stemilt’s Business Data Engineer. Stemilt, a vertically integrated Company that brings wholesome and earth friendly products to families around the world, and the largest employer and pillar for the Wenatchee valley community, is looking to become a leading employer, not just in its home valley but, around Washington, and the world. If you are an open-minded, continuously learning individual that looks to push the limits, and is beyond ego… This Company is for you. We are World Famous! Join us, you will love it here!

As Stemilt’s Business Data Engineer you are charged with implementing new technologies and methods, affecting change, and being a key team member within our Analytical Center of Excellence (ACOE) reporting directly to our Director of Planning & Analytics. Stemilt is undergoing a transformation to turn data into analytical gold! As our Data Engineer, you will be at the forefront of innovation supporting our analytics team transform to data story tellers and insight makers. You are the innovator that makes this transformation and journey possible! This position delivers extraordinary customer service at all levels, and is responsible for the design, build, and implementation of data pipelines that support analysts and reporting across the enterprise. You will be responsible for data extracts and transformations that can be used for the import and export of data, ensuring existing data import jobs run successfully, staying highly organized and knowing when to prioritize different tasks. Ability to work and adapt in a multi-project environment is a must.

Additional Responsibilities

 Help build and maintain our analytical cloud architecture 
 Develop and execute ETL/ELT strategies 
 Participate in our ACOE’s project planning process 
 Assemble large, complex data sets that meet business requirements 
 Tackle data-related technical issues and assist with data infrastructure needs 
 Test data pipelines to ensure data quality and accuracy
 Help our ACOE identify data patterns, create data tools, and reports 
 Build multi-dimensional cube data models used in ERP embedded analytics
 Work with our ACOE to strive for greater functionality within our data systems 
 Prepare data for predicative and prescriptive modeling 
 Support the development planning, analytics and reporting standards 
 Execute data governance strategies 
 Partner with technical and non-technical Stemilters to understand data and reporting requirements 
 Identify areas where improvements can be made and find/suggest solutions 
 Understand and apply advanced mathematical and/or statistical concepts practically to business situations 
 Create actionable and optimal communication plans for the delivery of all information related to the areas of responsibility. 
 Ensure all communications reflect the Employer Brand along with all legal requirements and reach our entire team. 
 Contribute to the team and the Stemilt in various other ways when requested or required. 


Reports to: Director of Planning & Analytics

What We Bring To Your Table

 A World Famous! benefits package that includes: 
 Medical/Dental/Vision insurance 
 Short- and Long-term Disability insurance 
 FSA 
 Life insurance 
 Matched 401(k) 
 Paid holidays 
 Paid-time-off 
 Performance Incentive Plan
 An amazing opportunity to create new performance standards and help lead a new team, develop knowledge and new career growth paths. 


What you bring to our World Famous! Table: 

Experience

Experience working in a cloud environment (e.g. AWS, Azure, IBM Cloud) 
 Cleansing, manipulating data sets, and anomaly detection 
 Experience building relational data models 
 Experience creating and managing ETL/ELT pipelines 
 Experience performing data analysis 
 Proficiency in connecting to data sources, modeling data, and creating visuals 
 Advanced knowledge of SQL, writing queries, and database design 
 Experience with multiple programming, mathematical, or statistical languages (e.g. Python, R, SAS, Stata, Java, C# etc) 


Not Required But a Plus

 Working knowledge of Azure Synapse, Azure DevOps, and/or understanding of software development methods and processes 
 Experience authoring reports using business intelligence tools (e.g. Power BI, Tableau, SAP Analytics Cloud, etc.) 
 Experience with automation tools (e.g. Power Automate)
 Knowledge of application development production and deployment 
 Statistical and mathematical programming or ML tools used to train data (e.g. Python, Stata, SAS, R, MATLAB etc.) 
 Experience working in the tree fruit or agricultural industry 
 Experience working in a manufacturing industry


Qualifications

Bachelor’s degree and 2 or more years’ experience in Computer Science, IT, Economics, Statistics, Engineering, Applied Mathematics, or related field, or an equivalent combination of experience, training, and education.

 Analytical, able to handle data effectively, efficiently, and with extreme confidentiality 
 Resourceful, self-starter with little to no direct supervision 
 Good communicator, with the willingness to engage other Stemilters
 Ability to work collaboratively across different business functions and effectively influence senior leaders 
 An intellectual curiosity and a drive toward continuous improvement through process enhancement and change 
 Tactful team player who knows how to collaborate with other technical professionals 
 Ability to read, understand, compose, and collaborate on software requirements and training documents 
 Enjoys learning and willing to transfer knowledge 
 Ability to deliver high quality results with an exceptional transparent, consultative, and partnering attitude. 
 A commitment to understand that change is the only constant and a practice of change with purpose, flexibility, and adaptability is a must 
 Ability to interact and connect with all Stemilters 


Success Defined

 Improved data systems and insights for Stemilt!","Analítica de datos, Arquitectura de datos y Extraer, transformar y cargar (ETL), Ciencias de la computación, Conceptos de estadística, Modelado de datos, Modelo de datos, Necesidades empresariales, Requisitos de información y Requisitos de software",Solicitar
https://www.linkedin.com/jobs/view/3982994029/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=Lp3dVQh9z%2BcGPJ6ru%2B7r0Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Atlanta, GA","Acerca del empleo
Employment Type: Full-Time, Mid-level

Department: Business Intelligence

CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges.

CGS brings motivated, highly skilled, and creative people together to solve the government’s most dynamic problems with cutting-edge technology. To carry out our mission, we are seeking candidates who are excited to contribute to government innovation, appreciate collaboration, and can anticipate the needs of others. Here at CGS, we offer an environment in which our employees feel supported, and we encourage professional growth through various learning opportunities.

Skills and attributes for success:

Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists


Qualifications:

Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency in developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories


Ideally, you will also have:

Experience creating and consuming APIs
Experience with DHS and knowledge of DHS standards a plus
Candidates will be given special consideration for extensive experience with Python
Ability to develop visualizations utilizing Tableau or PowerBI
Experience in developing Shell scripts on Linux
Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions
Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences


Our Commitment:

Contact Government Services (CGS) strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting-edge technology with world-class personnel to deliver customized solutions that fit our client’s specific needs. We are committed to solving the most challenging and dynamic problems.

For the past seven years, we’ve been growing our government-contracting portfolio, and along the way, we’ve created valuable partnerships by demonstrating a commitment to honesty, professionalism, and quality work.

Here at CGS we value honesty through hard work and self-awareness, professionalism in all we do, and to deliver the best quality to our consumers mending those relations for years to come.

We care about our employees. Therefore, we offer a comprehensive benefits package:

Health, Dental, and Vision
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off and Observance of State/Federal Holidays


Contact Government Services, LLC is an Equal Opportunity Employer. Applicants will be considered without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Join our team and become part of government innovation!

Explore additional job opportunities with CGS on our Job Board:

https://cgsfederal.com/join-our-team/

For more information about CGS please visit: https://www.cgsfederal.com or contact:

Email: info@cgsfederal.com","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Datos no estructurados, Estrategia de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979924645/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=x07dqBkYxgjVwdv6ktqLCA%3D%3D&trk=flagship3_search_srp_jobs,Azure Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Jersey City, NJ","Acerca del empleo
Job Title : Azure Data Engineer

Location : Jersey city, New Jersey (Hybrid)

Duration : Long Term Contract

Client : TEKsystems / SMBC Bank

Job Description

(ADF, Databricks, ADLS gen2), 
SQL Server with trading/investment banking experience.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3965028133/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=pq2xb%2Fvc2p0FsCDdhgKStQ%3D%3D&trk=flagship3_search_srp_jobs,BI Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 5 días,"Bellevue, WA","Acerca del empleo
Technical/Functional Skills

bi data; Data Structures; spark; PL/SQL; azure

Experience Required

6

Roles & Responsibilities

Net Technologies(C#,Asp.Net, MVC, .Net Core), Web API, Microsoft Azure Services, Azure Dev Ops,Function app, Web app , Storage, SQL, COSMOS, ARM templates , Powershell and CICD pipelines BI Data Engineering SQL Spark Azure DEVOPS (CI/CD & Infrastructure as a code), ARM ADF, ADW, Azur Networking, Azure Security Power BI Python, Pyspark, Scala Apache Nifi is good to have.
Generic Managerial Skills 

Digital : PySpark; Digital : BI Data Visualization - Tableau


Aptitudes y experiencia deseables
ASP.NET , Azure , Tableau , Spark","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , PL/SQL y Visualización de datos, Bases de datos, Modelado de datos y SQL Server Analysis Services (SSAS)",Solicitar
https://www.linkedin.com/jobs/view/3955062416/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=QhRm5BkWOHamjODZvcNDFg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,"Maple Grove, MN","Acerca del empleo
Grow with Us

This position is in our Maple Grove, MN 

ProAg has an exciting opportunity for a Data Engineer to join our data team. We are looking for individuals who want to embrace the advantages of data and dedicate importance to ensure our data structures are the easiest and most efficient in the industry. Bring your passion for data to help data solutions to help as we serve our farmers, agents, and re-insurers.

You will be primarily responsible for the analysis, design, development, testing, implementation and maintenance of new and existing data structures. Responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. Supports our software developers, data architects, data analysts and data scientists on data initiatives.

In This Exciting Opportunity You Will

Build the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources.
Maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that utilize the data to provide actionable insights into operational efficiency and other key business performance metrics
Work with the stakeholders and business teams to assist with data-related technical issues and support

What You’ll Bring

2-3 years of relevant and progressive professional experience in data analysis, design and development.
Bachelor’s degree in a related field or equivalent education and/or experience.
Ability to work in a dynamic problem-solving environment and synthesize strategy, plans, and solutions.
Demonstrated ability to deliver in a complex business environment.

What We Represent

Part of something bigger: We offer a career with purpose as you support the farmers and ranchers who create food, fuel and fiber for the world.
Personal connections: We are built on strong relationships and appreciation of your individuality.
A team who cares: We look out for each other personally and professionally because we care about each other.
Innovators by trade: We’re committed to a brighter tomorrow for our team members and for agriculture.
The best of both worlds: We combine personal connections with powerful resources, thanks to our culture and the backing of Tokio Marine HCC.

While our nation weathers economic storms, ProAg, a member of the Tokio Marine HCC group of companies, is positioned as a financially strong and well-capitalized insurer. We’re known for our quick response and fast, accurate claims settlement. We understand how important this is because many of us are farmers and ranchers ourselves. With more than 90 years of service to our agents & insureds, we stand committed to continuing the principles that ProAg was founded on: Integrity, Loyalty and Customer Service.

The Tokio Marine HCC Group of Companies offers a competitive salary and employee benefit package. We are a successful, dynamic organization experiencing rapid growth and are seeking energetic and confident individuals to join our team of professionals. The Tokio Marine HCC Group of Companies is an equal-opportunity employer. Please visit www.tokiomarinehcc.com for more information about our companies.","Almacenamiento de datos, Analítica de datos, Análisis de datos, Canalizaciones de datos, Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3962426768/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=SF4aLMRqGutf%2FQJUHBgkcg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer Investment Tech,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Denver, CO","Acerca del empleo
Why work for us?

A career at Janus Henderson is more than a job, it’s about investing in a brighter future together .

Our Mission at Janus Henderson is to help clients define and achieve superior financial outcomes through differentiated insights, disciplined investments, and world-class service. We will do this by protecting and growing our core business, amplifying our strengths and diversifying where we have the right.

Our Values are key to driving our success, and are at the heart of everything we do:

Clients Come First - Always | Execution Supersedes Intention | Together We Win | Diversity Improves Results | Truth Builds Trust

If our mission, values, and purpose align with your own, we would love to hear from you!

Your opportunity

The Data Engineer is a crucial member of the Investments Data & Engineering Technology team, tasked with developing, managing, and optimizing data pipelines within our data ecosystem. Reporting directly to the Lead Investments Data Analyst & Architect, this role is instrumental in implementing the data strategy that supports front office stakeholders, systems, and clients. The Data Engineer will leverage cutting-edge technology in Snowflake, Python, SQL, and Azure to enhance our data capabilities and support the investment decision-making process. Key responsibilities include:

 Design, build, and maintain efficient, reliable data pipelines using ETL and ELT processes. Ensure the seamless flow and availability of high-quality data across the organization. 
 Utilize Snowflake for data storage, processing, and analytics. Optimize data structures and queries to support analytics and BI initiatives. 
 Develop scripts in Python and SQL to automate data processes, integrate data from various sources, and support data analytics and reporting. 
 Leverage Azure cloud-native technologies to enhance data infrastructure, ensuring scalability, security, and performance. 
 Work closely with data analysts, BI developers, and the enterprise data management group to understand data needs, contribute to the data governance framework, and deliver on data initiatives. 
 Apply knowledge of data domains within the finance industry to improve data models, enhance data accuracy, and ensure relevance to business needs. 
 Evaluate industry developments in cloud and data related technologies, assessing their impact and applicability to the existing enterprise architecture. 
 Follow existing Janus Henderson standards and procedures for data movement, as well as provide suggestions for new standards and procedures as the move toward cloud technologies matures. 
 Participate in all phases of the agile systems development life cycle, including requirements analysis, design (including code design and code reviews), development, testing and deployment, picking up data development tasks within a 2-week sprint, providing work estimates for tasks to project management, and participating in agile ceremonies. 
 Carry out other duties as assigned. 


What to expect when you join our firm

 Hybrid working and reasonable accommodations 
 Generous Holiday policies 
 Paid volunteer time to step away from your desk and into the community 
 Support to grow through professional development courses, tuition/qualification reimbursement and more 
 All-inclusive approach to Diversity, Equity and Inclusion 
 Maternal/paternal leave benefits and family services 
 Complimentary subscription to Headspace – the mindfulness app 
 Corporate membership to ClassPass and other health and well-being benefits 
 Unique employee events and programs including a 14er challenge 
 Complimentary beverages, snacks and all employee Happy Hours 


Must have skills

 Proven experience as a Data Engineer, with a strong background in data pipeline construction, data architecture, and data warehousing. 
 Expertise in Snowflake, Python, SQL, and cloud-native ETL/ELT tools. 
 Familiarity with Azure and other cloud-native technologies. 
 Understanding of finance industry data domains and their application in data engineering. 
 Strong problem-solving skills and the ability to work collaboratively across teams. 
 Excellent communication and stakeholder management abilities, capable of articulating complex technical concepts to non-technical audiences. 
 Experience with modern technology stacks, agile methodologies, and cloud computing 
 Experience of working with geographically disperse development teams 


Nice to have skills

 Experience of micro-service development and API development 
 Degree educated in mathematics or scientific/engineering discipline. 
 CFA or similar industry qualification 


Potential for growth

 Mentoring 
 Leadership development programs 
 Regular training 
 Career development services 
 Continuing education courses 


For those in scope of Knowledge & Competence (MiFID II)

 Knowledge of financial markets, financial markets function and the impact of economic figures and national/regional/global events on markets 
 Understanding of issues relating to market abuse and anti-money laundering 
 Annual attestation 


You will be expected to understand the regulatory obligations of the firm, and abide by the regulated entity requirements and JHI policies applicable for your role.

At Janus Henderson Investors we’re committed to an inclusive and supportive environment. We believe diversity improves results and we welcome applications from all backgrounds. Don’t worry if you don’t think you tick every box, we still want to hear from you! We understand everyone has different commitments and while we can’t accommodate every flexible working request we’re happy to be asked about work flexibility and our hybrid working environment. If you need any reasonable accommodations during our recruitment process, please get in touch and let us know at

Compensation information

The base salary range for this position is $135,000-150,000. This range is estimated for this role. Actual pay may be different. This role will remain open through July 30, 2024.

,

Annual Bonus Opportunity: Position is eligible to receive an annual discretionary bonus award from the profit pool. The profit pool is funded based on Company profits. Individual bonuses are determined based on Company, department, team and individual performance.

Benefits: Janus Henderson is committed to offering a comprehensive total rewards package to eligible employees that includes; competitive compensation, pension/retirement plans, and various health, wellbeing and lifestyle benefits. To learn more about our offerings please visit the Why Join Us section on the career page

Janus Henderson Investors is an equal opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status.

Janus Henderson (including its subsidiaries) will not maintain existing or sponsor new industry registrations or licenses where not supported by an employee’s job functions (as determined by Janus Henderson at its sole discretion).

You should be willing to adhere to the provisions of our Investment Advisory Code of Ethics related to personal securities activities and other disclosure and certification requirements, including past political contributions and political activities. Applicants’ past political contributions or activity may impact applicants’ eligibility for this position.

You will be expected to understand the regulatory obligations of the firm, and abide by the regulated entity requirements and JHI policies applicable for your role.","Arquitectura de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Comunicación, Diseño de código, Modelado de datos, Modelo de datos, Resolución de problemas, Revisión de código y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3982993157/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=R5zhvj4cQmeomV6rANw3PQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Anniston, AL","Acerca del empleo
Employment Type: Full-Time, Mid-level

Department: Business Intelligence

CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges.

CGS brings motivated, highly skilled, and creative people together to solve the government’s most dynamic problems with cutting-edge technology. To carry out our mission, we are seeking candidates who are excited to contribute to government innovation, appreciate collaboration, and can anticipate the needs of others. Here at CGS, we offer an environment in which our employees feel supported, and we encourage professional growth through various learning opportunities.

Skills and attributes for success:

Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists


Qualifications:

Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency in developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories


Ideally, you will also have:

Experience creating and consuming APIs
Experience with DHS and knowledge of DHS standards a plus
Candidates will be given special consideration for extensive experience with Python
Ability to develop visualizations utilizing Tableau or PowerBI
Experience in developing Shell scripts on Linux
Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions
Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences


Our Commitment:

Contact Government Services (CGS) strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting-edge technology with world-class personnel to deliver customized solutions that fit our client’s specific needs. We are committed to solving the most challenging and dynamic problems.

For the past seven years, we’ve been growing our government-contracting portfolio, and along the way, we’ve created valuable partnerships by demonstrating a commitment to honesty, professionalism, and quality work.

Here at CGS we value honesty through hard work and self-awareness, professionalism in all we do, and to deliver the best quality to our consumers mending those relations for years to come.

We care about our employees. Therefore, we offer a comprehensive benefits package:

Health, Dental, and Vision
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off and Observance of State/Federal Holidays


Contact Government Services, LLC is an Equal Opportunity Employer. Applicants will be considered without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Join our team and become part of government innovation!

Explore additional job opportunities with CGS on our Job Board:

https://cgsfederal.com/join-our-team/

For more information about CGS please visit: https://www.cgsfederal.com or contact:

Email: info@cgsfederal.com","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Datos no estructurados, Estrategia de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3851866694/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=3xw2vZLZ231fqUT%2BU0rNXg%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer: Implementation Consultant : Irving, TX   ( Local )","Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 meses,"Irving, TX","Acerca del empleo
Job Title: Data Engineer: Implementation Consultant

Location: Irving, TX 

Type: Contract 

Description

Responsibilities:

Job Responsibilities

Highly skilled and motivated Data Engineer
As a Data Engineer, you will be responsible for validating the data between two systems, ensuring accuracy and consistency.
Implement data quality checks and monitoring processes.
Design and Develop data entity validation scripts using python.
Validate and reconcile data between two systems, ensuring accuracy and consistency.
Collaborate with cross-functional teams to identify and resolve data discrepancies and issues.
Develop and implement data quality checks and monitoring processes to ensure data integrity.
Design and maintain data pipelines and ETL processes to extract, transform, and load data from various sources.
Optimize data storage and retrieval processes to improve performance and efficiency.
Perform data analysis and profiling to identify data quality issues and recommend solutions.
Develop and maintain documentation for data validation processes, data mappings, and data lineage.
Stay up to date with industry trends and best practices in data engineering and data validation.
Support migration of North American OPCOs, enabling data validation on the new platform.
Gather business requirements from key client stakeholders and translate these into technical tracking specifications based on standards and industry best practice.

Required Skills

Strong knowledge of IPAAS and data validation for migration of applications.
Demonstrable experience in delivering end to end data validation.
Confident understanding of web development and its languages and possess Python proficiency.
Proactive and highly organised with strong time management and planning skills. Able to change direction and work on multiple projects across a wide range of topics.
[Desired] Knowledge of Python/Google BigQuery architecture.

Application Process: If you're interested and available for this above role and responsibilities then, apply now!

Please send us your updated resume in a word document with your expected hourly Rate at suhil@anveta.com

If you are engaged in any project and not available in the market then feel free to share this post with in your network. Thank You","Analítica de datos, Análisis de datos, Ingeniería de datos , Microsoft Power BI y Python, Calidad de datos, Desarrollo web, Linaje de datos, Necesidades empresariales y Validación de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3980335934/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=rV3E6mRW%2B83RRn%2BQkRzDGA%3D%3D&trackingId=Q3D1xEwjcTOajlLDJ9RsMw%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,Syracuse-Auburn y alrededores,"Acerca del empleo
Why Jerry?

Join a pre-IPO startup with capital, traction and runway ($240M funded | 40X revenue growth in 4 years | $2T market size)
Work closely with brilliant leaders and team members who have scaled companies like Nvidia, Better, Nerdwallet, Wayfair, Sofi, etc.
Disrupt a massive market and take us to a $5B business in the next few years
Be immersed in a talent-dense environment and greatly accelerate your career growth

About The Opportunity

Jerry is building the first AllCar™ super app to help people optimize all aspects of owning a car – insurance, buy/sell, registration, loans, safety, repairs, parking, etc – a $2T market in the U.S. We started with insurance in 2019, and since then we’ve launched loan refinancing, real-time driving insights, a repair marketplace, car diagnostics, and a GenAI-powered chatbot. We have amassed over 5M customers, raised $240MM in funding, scaled our revenue 40X and our team to 250 across 7 countries.

The Data team at Jerry is responsible for supporting intelligent business decision making through analytical insights and advanced models. As a senior data engineer, you bring your previous industry experience in infrastructure scaling to Jerry and lead the development of a new infrastructure solution that will continue to perform with great stability and efficiency even when the company grows 10x bigger. You will join a team of data engineers, data analysts, and data scientists who operate with urgency and passion for excellence in pursuit of our mission to build the first AllCar™ super app.

How You Will Make An Impact

Partner with data scientists, software engineers and cross-functional stakeholders (Marketing, Operations, Finance) to build data pipelines that will provide critical data and insights to drive growth for the business, while ensuring data availability and accuracy
Partner with data scientists and machine learning engineers to evolve, optimize, and integrate critical predictive models
Partner with software engineers and data scientists to build data pipelines to consume and refine application data
Define data engineering standards and best practices and drive operational excellence
Perform regular maintenance and optimization of our data infrastructure

Ideal Profile

Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related technical discipline. Master’s degree in Computer Science or Engineering preferred
5+ years of experience in data engineer or similar role
Proven success in communication with database users, software engineers and senior management to collect requirements, discuss data modeling decisions and define data engineering strategies
Outstanding communication and problem solving skills, eager to work on the most pressing business problems and drive impact for the company
Experience with data modeling, data warehousing, and ETL pipeline development
Experience with column-based databases (Amazon Redshift, ClickHouse, etc.) and infrastructure integration with analytical platforms (Tableau, Qlik, Power BI, etc.)
Hands-on experience and advanced knowledge of SQL
Experience with big data technologies (Spark, Clickhouse, Redshift, Snowflake etc.)
Experience in process automation and deployment of advanced machine learning models

Jerry is proud to be an Equal Employment Opportunity employer. We prohibit discrimination based on race, religion, color, national origin, sex, pregnancy, reproductive health decisions or related medical conditions, sexual orientation, gender identity, gender expression, age, veteran status, disability, genetic information, or other characteristics protected by applicable local, state or federal laws. 

Jerry is committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, please contact us at recruiting@getjerry.com

About Jerry

Jerry is America’s first and only AllCar™ app. We are redefining and radically improving how people manage owning a car, one of their most expensive and time-consuming assets.

Backed by artificial intelligence and machine learning, Jerry simplifies and automates owning and maintaining a car while providing personalized services for all car owners' needs. We spend every day innovating and improving our AI-powered app to provide the best possible experience for our customers. From car insurance and financing to maintenance and safety, Jerry does it all.

We are the #1 rated and most downloaded app in our category with a 4.7 star rating in the App Store. We have more than 6 million customers — and we’re just getting started.

Jerry was founded in 2017 by serial entrepreneurs and has raised more than $242 million in financing.

Join our team and work with passionate, curious and egoless people who love solving real-world problems. Help us build a revolutionary product that’s disrupting a massive market.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Automatización de procesos, Bases de datos, Comunicación, Modelado de datos, Oil Pipeline Development y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3979927204/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=jVLwU1%2BPJJcrnEyL8ruf5g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"50 US$/h - 75 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Los Ángeles, CA","Acerca del empleo
The Los Angeles Department of Building and Safety (LADBS) Technology Services Bureau (TSB) is requesting to solicit the services of a Data Engineer who will primarily focus on building data pipelines. They will be expected to leverage a variety of advanced tools and technologies such as Kafka/Kinesis for real-time data processing/streaming, Relational/No-SQL databases for robust data storage and management, and other Integration tools for seamless data flow across various cloud and on-premises platforms. The Data Engineer will also utilize ETL processes to extract data from various sources, transform the data to fit operational and business needs, and load it into an end target. In addition to these, they will be expected to have familiarity with Pub-Sub messaging patterns or similar data dissemination models to ensure efficient data distribution and consumption. One of the primary goals is to create a real-time bidirectional data pipeline from Oracle transactional databases to a data lake in the cloud.

Work Products and Outcomes:

The Data Engineer shall meet the following key high-level work products and outcomes as identified by LADBS: (Note: this list is not exhaustive.)

Develop, construct, test, and maintain data architectures and pipelines. 
Create best-practice ETL frameworks; repeatable and reliable data pipelines that convert data into powerful signals and features. 
Handle raw data (structured, unstructured, and semi-structured) and align it into a more usable, structured format that is better suited for reporting and analytics. 
Work with the cloud solutions architect to ensure data solutions are aligned with company platform architecture and all aspects related to infrastructure. 
Collaborate with business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. 
Ensure data pipeline architecture will support the requirements of the business. 
Document processes and perform periodic system reviews to ensure adherence to established standards and processes. 
Evaluate and advise on technical aspects of open work requests in the product backlog with the project lead. 
Define Cloud infrastructure Reference Architectures for common solution archetypes
Real-time bidirectional data pipeline from Oracle transactional databases to a data lake in the cloud. 
Clear, comprehensive documentation related to the data pipeline. 
Regular reports on the status of data pipeline development. 
Other related tasks identified by LADBS

Performance Specifications:

The qualified candidate must possess the following skills and experience in the following areas:

A bachelor's degree in Computer Science, Data Science, Software/Computer Engineering, or a related field. 
Proven experience as a data engineer or in a similar role, with a track record of manipulating, processing, and extracting value from large disconnected datasets. 
Demonstrated technical proficiency with data architecture, databases, and processing large data sets. 
Proficient in Oracle databases and comprehensive understanding of ETL processes, including creating and implementing custom ETL processes. 
Experience with cloud services (AWS, Azure), and understanding of distributed systems, such as Hadoop/MapReduce, Spark, or equivalent technologies. 
Knowledge of Kafka, Kinesis, OCI Data Integration, Azure Service Bus or similar technologies for real-time data processing and streaming. 
Experience designing, building, and maintaining data processing systems, as well as experience working with either a MapReduce or an MPP system. 
Strong organizational, critical thinking, and problem-solving skills, with clear understanding of high-performance algorithms and Python scripting. 
Experience with machine learning toolkits, data ingestion technologies, data preparation technologies, and data visualization tools is a plus. 
Excellent communication and collaboration abilities, with the ability to work in a dynamic, team-oriented environment and adapt to changes in a fast-paced work environment. 
Data-driven mindset, with the ability to translate business requirements into data solutions. 
Experience with version control systems e.g. Git, and with agile methodologies/scrum. 
Certifications in related field would be an added advantage (e.g. Google Certified Professional Data Engineer, AWS Certified Big Data, etc.). 

Work hours and location:

Estimated Start Date: 5/8/2024 ** The candidate proposed must be available on the estimated start date. 
Estimated Completion Date: 3/22/2025
This is a Remote role

Evaluation Criteria:

LADBS will review the TOS Responses received and select up to 5 of the most qualified candidates to be interviewed based on a review of the resumes provided and the criteria below.

Education
Relevant degree in Computer Science, Engineering, Information Technology, or related field
Advanced degrees or certifications related to data engineering
Experience
Previous work experience with data migration and engineering
Hands-on experience with data warehouses
Demonstrated experience in managing and optimizing data pipelines and architectures
Technical Knowledge
Strong understanding of streaming data platforms and pub-sub models
In-depth knowledge of data warehousing concepts, including data storage, retrieval, and pipeline optimization

Evaluation Criteria Points Education and experience30Technical Knowledge50Cost10Qualifications based on a review of the candidate's resume10Total100

Spruce Technology, Inc. is a mid-size, award-winning (Inc 5000, SmartCEO, Entrepreneur of the Year) technology services firm with a steadily growing portfolio of commercial and government clients. Spruce provides innovative technology solutions, specialized IT staff, and IT strategy consulting nationwide. Spruce maintains partnerships with major technology vendors and continually develops leading-edge offerings in service areas such as digital experience, data services, application development, infrastructure, cyber security, and IT staffing.

Spruce Technology, Inc. is an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, disability, age, sexual orientation, gender identity, national origin, veteran status, or genetic information. Consistent with the Americans with Disabilities Act, it is the policy of Spruce Technology, Inc. to provide reasonable accommodation when requested by a qualified applicant or employee with a disability, unless such accommodation would cause an undue hardship. The policy regarding requests for reasonable accommodation applies to all aspects of employment, including the application process.","Almacenamiento de datos, Analítica de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y MapReduce, Ciencias de la computación, Necesidades empresariales y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3929020106/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=kqT3wO%2BhVDLailOY0P24eQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,Estados Unidos,"Acerca del empleo
Sr Data Engineer with Python 

12+ Years of experience

Remote

Mandatory Skills: Python, SQL

Job Description

 Must have 5-7+ years of experience
 Building a brand new internal app for accuracy dept
 SQL strong
 Database modeling skills
 Data movement from different data sources
 Demonstrate technical and creative skills
 Data storage
 Design of data environment
 Python
 Google cloud platform experience preferred
 Cloud platform experience preferred
 Cloud architecture and development preferred
 R-shiny skills preferred","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Aptitudes creativas, Bases de datos y Modelado de bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984552103/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=O%2FjNLO0VACYBy68R6nwepA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - remote,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,Estados Unidos,"Acerca del empleo
Data Engineer - Fully Remote

The Job:

Collaborate as part of a cross-functional Agile team to create and enhance software that enables state of the art, next generation Big Data & Fast Data applications. 
Build software and frameworks to automate high-volume and real-time data delivery between our cloud based data platforms and applications. 
Build data APIs and data delivery services that support critical operational and analytical applications for our internal business operations, customers and partners
Leverage DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of software utilizing tools like Jenkins, Maven, Nexus, Chef, Terraform, Ruby, Git and Docker
Perform unit tests and conduct reviews with other team members to make sure the code is rigorously designed, elegantly coded, and effectively tuned for performance
Develop and deploy distributed computing Data applications using Spark or Pyspark
Utilize programming languages like Python and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake
Sql experience
ETL experience 

Basic Qualifications:

Bachelor's Degree or military experience
At least 3 years of professional work experience in data engineering
At least 3 years of experience with Python
At least 3 years of experience with Sql
At least 3 years of experience with Spark or Pyspark
At least 3 years of experience with ETL development 
At least 2 year of experience working with cloud data capabilities - AWS","Almacenamiento de datos, Amazon Web Services (AWS), Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , PySpark y Python, Amazon Redshift y Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3969530536/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=Fi7XLb7em5JTHfdVl77BdQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,Estados Unidos,"Acerca del empleo
Overview

TekWissen Group is a workforce management provider throughout the USA and many other countries in the world. Our client is the brand name of Deere & Company, an American corporation that manufactures agricultural machinery, heavy equipment, forestry machinery, diesel engines, drivetrains used in heavy equipment, and lawn care equipment.

Job Title: Data Engineer 

Location: Johnston, IA, 50131 

Duration: 12 Months 

Job Type: Contract 

Work Type: Remote (Mon-Fri) 

Visa sponsorship is not available, now or in the near future, for this position. 

Job Description

Required Skills: 

SQL Query experience (1-3 years) 
Combine raw data into useful summarizations 
Python (3-5 years) 
Pyspark (3-5 years) 
Experience working with large scale data 
Experience creating automated unit tests 
Education Degree- BA in Data Science, Data Engineering, Computer Science, Business Analytics 

Top Three

Python (3-5 years) 
Pyspark (3-5 years) 
Experience working with large scale data 
And a candidate that is good with working in uncertain environments 

Major Purpose

Collaborates with business and analytics leaders to generate insights and answer business questions by using analytics techniques such as advanced data visualization, statistical analysis, randomized testing, predictive modeling, forecasting, optimization, and/or machine learning. 
Proposes innovative ways to look at problems by using these approaches on available enterprise data as well as customer third party data and information. 
Validates findings using experimental and iterative approaches. 
This level performs basic statistical analysis of low to moderately complex data from a single source, with heavy reliance on industry/standardized tools and existing models. 
Output is reviewed by higher-level Data Scientists or Analytics Manager for execution. 

Major Duties

Works with data sets and performs appropriate analytical methodology to provide insights and decision modeling for the business. 
Creates and implements algorithms which manage the data to enable it to be analysed in an efficient manner. 
Supports the communication of derived insights, especially through appropriate visualization techniques. 
Supports the identification of the required data sources and works with Data Wranglers and/or IT to implement methodologies to retrieve and use this data. 
Stays abreast of the latest appropriate analytical techniques and recommends these where needed. 
Explains implementation and usage in business terms. 
Applies analytical techniques to prospect for business insights and find patterns in data which could be valuable for the business 

Skills, Abilities, Knowledge

Quantitative analytical skills 
Knowledge of appropriate industry 
Good interpersonal, negotiation and conflict resolution skills. 
Excellence in verbal and written communication forms with emphasis on persuasive communication, tact and negotiation. 
Business process knowledge of assigned area(s) and/or function(s). 
Knowledge of advanced data gathering and analysis techniques, including statistical analysis. 

Education

Degree in a Math discipline or equivalent experience. University Degree (4 years or equivalent) 
Economics - University Degree (4 years or equivalent) 
Statistics - University Degree (4 years or equivalent) 

Work Experience

Internal or external industry specific experience in relevant discipline. (1 - 3 years) 
Data analytics experience. (1 - 3 years) 
Background or proven experience in mining data for analytics insights. (1 - 3 years) 
Good exposure to enterprise statistical tools like SAS, Statistica, SPSS or SAS E Miner (1 - 3 years) 

TekWissen® Group is an equal opportunity employer supporting workforce diversity. 

TekWissen is an emerging global human capital, recruitment and IT services organization. Operating since 2009, we draw upon more than a decade of staffing experience to deliver critical talent acquisition solutions and IT engagements for our clients. We’re founded on a culture that is passionate about delivering tailored solutions, that create lasting partnerships.

Our global footprint covers six countries: United States, Canada, Australia, India, United Kingdom and the Philippines. This allows us to work in close partnership with organizations and manage everything from global talent needs with demanding resourcing strategies, to single sites with lower recruitment volumes.

TekWissen® is an equal opportunity employer supporting workplace diversity.","Analítica de datos, Ciencia de datos, Ingeniería de datos y PySpark, Análisis estadístico, Ciencias de la computación, Comunicación, Comunicación escrita, Conflictología y Lenguaje de consulta (query)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3932083364/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=GbKoa800iLqZnz0FD310OQ%3D%3D&trk=flagship3_search_srp_jobs,Data Analytics Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 meses,Estados Unidos,"Acerca del empleo
At Colibri, culture is a critical part of our collective success, and we live our values everyday: Love, Joy,

Boldness, Teamwork and Curiosity. These values guide our interactions with each other, our customers,

and the community as a whole.

We have a rich and storied history. Colibri is one of the pioneers of online professional education,

introducing some of the first web-based professional education courses in 2001. Today, the company’s

family of brands are the leading online professional education platforms in their respective end-markets.

We proudly to serve >1 million customers annually and employ more than 1,000 mission-aligned

professionals. To learn more, please visit: www.colibrigroup.com

Position Overview

The Data Analytics Engineer will be a part of the Data Engineering team whose primary mission is to build trusted Data Ingestion Pipelines to seamlessly move and transform data from SaaS and in house transactional databases to a Data Warehouse to enable actionable business insights. The data in the Warehouse will serve as a foundation for Reporting, Machine Learning Analysis and Generative AI. The ideal candidate will be a strong technical resource with effective communication and leadership skills to be a member of data engineering team. The role requires collaboration with cross functional teams and ability to play multiple roles depending upon the situation. The ideal candidate should have strong bias for action, able to dig into details when requirements are fuzzy and have a passion to push forward to progress. If you are an ambitious technologist who is looking to grow, and who is passionate about building world-class products and technology, we encourage you to apply for this exciting opportunity.

What You'll Do

Implementation of data ingestion pipelines following the guidelines and best practices defined. 
Implement data mapping, data transformation, automated testing and data validation
Contribute and implement standards and best practices within the Data Engineering Team
Write detailed stories, sprint grooming and actively contribute in the Agile software development life cycle. 
Accountable for Delivery and 100% uptime of the Environments
Collaborate with cross-functional teams, including ecosystem leadership and data insights teams, engineering, sales, and marketing to ensure all stakeholders are aligned
Partner with scrum master daily to provide updates
Partner with Data Insights team on a daily basis to stay aligned on priorities
Stay up to date with the latest industry trends and emerging technologies to identify opportunities for innovation and growth
Establish and maintain effective communication channels with internal and external stakeholders


What You'll Need to Succeed

BS in Engineering, Business Administration, Statistics, Business Analytics, or applicable experience in related field 
Minimum of (3) three years of SDLC experience in ETL/ELT environments with tools/technologies such as Fivetran, dbt, SQL, AWS Redshift, Boomi, PowerBI , Git
Ability to work on-site at least two days a week in St. Louis, MO
Minimum of (2) years of experience in building out schemas for Analytical purposes
Minimum (1) years of hands on experience AWS Cloud and Services such as AWS Glue, AWS Lambda, Python
Familiar with ERP, CRM products such as Salesforce, NetSuite, Hubspot and Eloqua
Strong development background and expert level skills with modern SDLC best practices
Proven track record of delivering successful data initiatives
Excellent communication and interpersonal skills, with the ability to build strong relationships with internal and external stakeholders
Strong analytical and problem-solving skills, with the ability to make data-driven decisions
Experience working in a fast-paced, agile environment
Ability to prioritize and manage multiple projects simultaneously


Colibri Group welcomes applicants from all backgrounds and experiences, and we understand that not every candidate will meet every requirement listed in the job description. Research has shown that women and people of color may be less likely to apply to jobs unless they feel they meet every qualification, and we want to actively combat this bias in our hiring process. If you're excited about the role and believe you have the skills and experience to contribute to our team, we encourage you to apply, even if your background doesn't align perfectly with every qualification listed. We are committed to building a diverse and inclusive workplace, and we believe that diversity of perspectives and experiences is essential to our success. You may be just the right candidate for this role or another position within our organization. Don't hesitate to take the leap and apply today!

Colibri Group is an equal opportunity employer that is committed to diversity and inclusion in the workplace. Colibri Group prohibits discrimination and harassment of any kind based on race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, disability, genetic information, or any other status protected under federal, state, or local law.","Analítica de datos, Extraer, transformar y cargar (ETL), Git y Ingeniería de datos, Amazon Redshift, Build Strong Relationships, Comunicación, Dell Boomi, Resolución de problemas y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3833865530/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=THCoOqv5wTnjvRoB6wqRsg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Chandler, AZ","Acerca del empleo
Position - Data Engineer

 Type of Position- Contract

 Location - Remote, USA

Job Description

 Experience with big data technologies (e.g., Hadoop, Spark). 
 Knowledge of data streaming technologies (e.g., Apache Kafka, Apache Flink). 
 Familiarity with containerization and orchestration tools (e.g., Docker, Kubernetes). 
 Experience with version control systems (e.g., Git). 
 Relevant certifications in data engineering or cloud platforms are a plus. 

Qualifications

 Bachelor's or Master's degree in computer science, information technology, or a related field. 
 Proven experience as a Data Engineer or similar role, including experience with ETL processes, data integration, and data modeling. 
 Proficiency in programming languages like Python, Java, or Scala. 
 Strong SQL skills for data manipulation and querying. 
 Knowledge of database systems, data warehousing concepts, and data modeling techniques. 
 Experience with data integration tools and frameworks (e.g., Apache NiFi, Talend, Apache Beam). 
 Familiarity with cloud-based data platforms (e.g., AWS, Azure, Google Cloud). 
 Strong problem-solving and analytical skills. 
 Excellent communication and collaboration skills. 

About Us

 InterSources Inc, a Certified Diverse Supplier, was founded in 2007 and offers innovative solutions to help clients with Digital Transformations across various domains and industries. Our history spans over 16 years and today we are an Award-Winning Global Software Consultancy solving complex problems with technology. We recognize that our employees and our clients are our strengths as the diverse talents and opportunities they bring to the table enable us to grow as a global platform and they are causally linked with our success. We provide strategic and technical advice, and we have expertise in areas covering  Artificial Intelligence, Cloud Migration, Custom Software Development, Data Analytics Infrastructure & Cloud Solutions, Cyber Security Services, etc. We make reasonable accommodations for clients and employees and we do not discriminate based on any protected attribute including race, religion, color, national origin, gender sexual orientation, gender identity, age, or marital status. We also are a  Google Cloud partner company. We align strategy with execution and provide secure service solutions by developing and using the latest technologies that thrive our resources to deliver industry-leading capabilities to our clients and customers, making it convenient for our clients to do business with InterSources Inc. Our teams also drive growth by refining technology-driven client experiences that put the users first, providing an unparalleled experience. This results in strengthening the core technologies of clients, enabling them to scale with flexibility, create seamless digital experiences and build lifelong relationships.","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Integración de datos y Scala, Ciencias de la computación, Comunicación, Manipulación de datos, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3903976166/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=4frvBNOiKXxxz5WrHPuhXA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 3 días,Estados Unidos,"Acerca del empleo
About Paxos

Today’s financial infrastructure is archaic, expensive, inefficient and risky — supporting a system that leaves out more people than it lets in. So we’re rebuilding it.

We’re on a mission to open the world’s financial system to everyone by enabling the instant movement of any asset, any time, in a trustworthy way. For over a decade, we’ve built blockchain infrastructure that tokenizes, custodies, trades and settles assets for the world’s leading financial institutions, like PayPal, Venmo, Mastercard and Interactive Brokers.

About The Team

The Data team at Paxos includes Data Infrastructure, Data Engineering and Data Analytics. We focus on use cases ranging from product analytics to business intelligence and operational automation.

Data is the lifeblood of any organization based in financial services. We're building a data platform that is trusted and reliable, designed to deliver accurate measurement and insights about our products and business. This platform informs our strategy, provides reporting to clients and regulators, enables the automation of operational processes and drive cost savings and revenue opportunities.

Some of the people you would be working closely with are Patrick King, Joseph Eckert and MC Mundy.

About The Role

We're looking for a Data Engineer who will partner with the rest of the team, stakeholders and our broader engineering team to design, build and scale our quickly expanding analytics platform. You’ll lead the development of efficient and scalable data pipelines and extensible data models to make data at Paxos more accurate, accessible and actionable. You’ll work closely with partners to identify new use-cases for and sources of data to drive decision-making across the organization.Collaborating across disciplines, you'll identify internal and external data sources to design and implement new data models, table structures, data products, ELT strategies, automation frameworks and scalable data pipelines. You’ll have the opportunity to leverage modern technologies like Snowflake, DBT, Looker, S3, Terraform, Airbyte and Dagster. The team is at a pivotal moment, and this role will help shape its future.

What You’ll Do

Improve Paxos’s products and decision-making by crafting, growing and optimizing our data and data architecture.
Partner with engineering, the rest of the data organization and cross-functional partners to understand data needs.
Design, build and maintain efficient and reliable data pipelines to move data across a number of environments and platforms.
Ensure data access and segregation aligns with relevant data policies.
Work with data and cross-functional partners to automate manual ingestion processes and optimize data delivery.
Educate your partners: Use your data engineering and analytics experience to ""see what’s missing"", identifying and addressing gaps in their existing logging and processes.
Design and implement data unit tests on pipelines, ensuring accurate data reaches end users at all times.
Build data expertise and own data quality for your areas.
Become an advocate for data and data tooling within Paxos — make recommendations for new datasets, tools, analyzes to further the data team and Paxos’ missions.


About You

3+ years of experience with:

Data Modeling / Data Architecture.
Cloud-based Big Data/MPP analytics platforms like Snowflake, AWS Redshift, Google BigQuery, Azure Data Warehouse.
SQL.
One of Python, Java, C++, Scala or similar languages.
Workflow management engines like Dagster, Airflow, Google Cloud Composer, AWS Step Functions, Azure Data Factory or similar.
Custom ETL/ELT design, implementation and maintenance.
Data visualization tools like Looker, Metabase, Tableau or similar.


Pay And Benefits

Paxos offers a competitive total compensation and benefits package, including equity. Actual salary within that range is dependent upon the individual’s skills, experience and qualifications.

Expected range for the salary component for candidates located within the United States is:

$154,000—$181,000 USD","Analítica de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery y Ingeniería de datos, Amazon Redshift, Calidad de datos, Datasets y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3971720662/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=d0ekzQiFeYrOjePdieiaUg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"120 US$K/año - 160 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
CodePath is a national non-profit that is reprogramming higher education to create the most diverse generation of software engineers, CTOs, and founders. We deliver industry-vetted courses and career support centered on the needs of Black, Latino/a, Indigenous, and low-income students. Our students train with senior engineers, intern at top companies, and rise together to become the tech leaders of tomorrow. As of 2023, we've served 10,000 students a year nationwide, and we plan to scale up to ~100,000 students a year by 2028.

Founded in 2017, CodePath has taught over 26,000 students and delivered courses across over 110 universities. We are supported by some of the largest and most well-respected organizations, including Amazon, Andreessen Horowitz, Blue Meridian Partners, Comcast, Google, JP Morgan Chase, Knight Foundation, Meta, New Profit, and Salesforce, among others. In 2024, CodePath was recognized as one of the Most Innovative Companies in Education by Fast Company.

 We will never ask for bank information or for you to download any programs as part of our job application process and initial communications to applicants will be sent directly from our HR department.

About The Role

Location: Remote, US

Duration: FTE

Reports To: VP of Product Engineering

Compensation: $120,000-$160,000 per year

We’re at a critical moment in our growth as an organization. 2024 is a year of incredible change at CodePath and we’re looking for the right data engineer to lead the charge on our small, skilled, and growing team in the next phase of our exponential growth and impact as we transform computer science education. Our platform overview is a must watch in order to understand the significance that our Data Engineering team will play in the actualization of our mission.

As an early hire in data engineering, you will collaborate with stakeholders across the organization to help build and maintain our next generation data infrastructure. In order to build out our data warehousing and ETL/ELT infrastructure the right way, we are looking for somebody resourceful with a problem solving mindset who is self-directed and able to work closely with data scientists, software developers, and business stakeholders. We expect this infrastructure, once built out, to be self-serve, accurate and secure/compliant as it enables all sorts of data science projects, ML modeling, and BI analysis across everything we do from the delivery of our courses, student marketing, sales and fundraising — which makes this a challenging, creative, and exciting technical position for an experienced data engineer.

The ideal candidate is a true early stage startup Data Engineer and is inspired by CodePath’s ambitious vision to transform education starting with computer science, and to train engineers who will generate over $1.5T in wealth for low-income communities within the next 20 years.

Key Activities

Architect and develop data platforms including data warehouses, analytic products, data lakes, ETL/ELT workflows, or data pipelines with minimal oversight 
Design effective data models for analytics data use cases 
Profile data sources and develop ETL/ELT processes with knowledge of data modeling fundamentals, using both SQL and supporting ETL/ELT tools (dbt, Airbyte, and others) 
Develop and maintain data ingestion connectors to third-party services (Google Sheets, SurveyMonkey, HubSpot, Airtable) 
Develop and maintain reporting dashboards and self-serve analytics tools in Tableau or other BI tools 
Ensure architecture will support the requirements of the business by designing, implementing and maintaining a data quality assurance framework 
Develop and maintain documentation for data collection processes and standards 
Work with stakeholders across the organization to assist with data-related technical issues and support their data infrastructure needs. 

Qualifications

2 to 5 years of relevant professional experience 
Good understanding of data modeling principles including dimensional modeling, data normalization principles. 
Substantial experience with SQL and building efficient data pipelines in SQL and tuning query performance 
Experience managing cloud data warehouse such as Google BigQuery, Snowflake, Databricks, or Amazon Redshift 
Experience with the tools we use, including Postgres, Airbyte, dbt, and Tableau 
Thoughtful about making technical decisions and recommendations that serve the best interests of the organization, factoring in cost, complexity, maintenance burden, and outcomes 
Has strong presentation skills: takes analysis and makes it understandable (and interesting!) for stakeholders 

Additional Skills And Experience

Proven ability to drive projects from conception to completion. 
Aptitude for problem-solving and troubleshooting. 
Excellent communication and collaboration skills. 
Ability to work in a fast-paced, startup environment. 
Proactive, independent, responsible attitude with the ability to learn quickly. 

Pay range

$120,000—$160,000 USD

Benefits

We offer a comprehensive benefits package for full-time employees that includes:

Medical, dental, and vision premiums paid at 90% for FT positions and their dependents 
Flexible vacation and sick time policy with 12 company paid holidays plus a week long ""winter break"" office closure from Christmas to New Years. Employees take the time when they need it 
Flexible workplace and work schedule 
CodePath provides a laptop, monitor, and ergonomic office setup 
Annual professional development stipend 
Ability to voluntarily contribute pre and post-tax earnings to our 401k plan 
Employer Provided Short Term Disability and 10 weeks paid leave to support our employees in growing their family 
A commitment to developing leaders from within the organization 
Frequent opportunities to connect with students, universities, and communities we serve 
Opportunities to engage, collaborate and partner with top technology companies, venture capitalists, and engineering leaders 

About The Current Team

We are individuals from a multitude of backgrounds, experiences, and unlikely stories, all connected by a single dream: a world in which regardless of background, socioeconomic status, gender, or race all people have pathways to reach their full potential.

With a staff and board that cares deeply about diversity and equity, we believe that diverse perspectives and backgrounds create a richer work environment and enhance our ability to pursue our mission.

Note: Research suggests that women and BIPOC individuals may self-select out of opportunities if they don’t meet 100% of the job requirements. We encourage individuals who believe they have the skills necessary to thrive at CodePath to apply for this role.","Extraer, transformar y cargar (ETL) y Google BigQuery, Amazon Redshift, Aseguramiento de la calidad de los datos, Calidad de datos, Comunicación, Hojas de cálculo de Google, Lenguaje de consulta (query), Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3978121271/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=SnP%2FlaIbRRr%2BZ6ebESEopA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",Publicado de nuevo hace 1 semana,"Chicago, IL","Acerca del empleo
Horizontals client is in need of a Data Engineer to support end client GCP needs through the EOY.

Data Engineer

Requirements:

Experience as data engineer in digital marketing environment
Must have client facing experience
Experience with GCP and BigQuery are must haves
Experience with GA4 is a must
Experience with Google Marketing Platform and CDP would be a bonus


Role And Responsibilities

Oversee data engineering projects, including marketing data warehousing and ML modeling pipelines
Perform discovery and solutioning activities with business development, account & program management, and clients to define and scope projects. Attend client calls as the SME throughout the sales process
Create data infrastructure diagrams and develop ETL solutions leveraging various cloud technologies and third-party tools
Design Marketing Data Warehouses, ingesting data from web analytics, CRM, and media platforms, using appropriate cloud infrastructure for automation including Airflow, Apache Beam, microservices, and other solutions
Propose component solutions, when necessary, with sensitivity to MVP needs
Document data dictionaries and business glossaries for GMP data streams.
Model source data into data models for reporting and analysis.
Articulate complex ideas in a clear and concise manner, both in speech and writing; understand the audience and tailor messages accordingly
Drive collaboration by facilitating brainstorming, peer reviews, and trainings to increase technical capabilities of the team; drive the adoption of git for quality, scalable solutions
Monitor data engineering team bookings and maintain established utilization rates
Follow developments in the field by continuous learning and proactively champion promising new methods relevant to the problems at hand
Participate regularly in calls with Google’s GCP representative(s)
Gain trust with Google reps and leadership to earn new account opportunities
Represent GMP services to partners, clients, and industry
Retain a high-level of client satisfaction, ensuring the retention and expansion of the GMP client portfolio
Maintain a strong understanding of Google Marketing Platform offerings, staying updated on new features and capabilities
Achieve CGP certification and specialization with support from leaders of the Operating Unit and practice","Almacenamiento de datos, Buena práctica clínica, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery, Google Cloud y Ingeniería de datos, Bases de datos, Informes y análisis y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3830104488/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=pukELGisJHicItIPZBP1Qw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 meses,Estados Unidos,"Acerca del empleo
Position: Data Engineer

Location: Remote

Duration: Contract

Job Description

 Must Have Skills -

Building Data platform (Strong), Data Modeling (Strong), Project Leadership (Capable), Project Management methodologies (Capable)

3 - 7 years of experience as a Technical Program Manager, Technical Project Manager or Scrum Master, managing cross-functional projects in a data environment

3-7 years of experience working in a data related field -- Data Governance, Data Engineering, Data Quality, Data Profiling, Data Modelling, etc.
Extensive experience managing projects using a variety of SDLC methodologies - agile, scrum, waterfall, and combinations
Experienced in the toolsets for above methodologies
Experience managing multiple projects simultaneously and developing integrated program roadmaps that accelerate the work within project teams and communicate status to wider stakeholders.
Have strong technical proficiency, ex. coding skills, data engineering skills or similar.
Excellent communication skills and the ability to facilitate parties working collaboratively and efficiently.
Comfortability to work in ambiguous environments and use your management skills to drive progress
Experience creating KPIs, dashboards and metrics.
Working knowledge in release planning and management.
Experience in risk reporting, management, and mitigation.
Preferred certifications: Scrum Master Certification (such as CSM or PSM) or PMP and any other relevant certifications.
Must have legal right to work in the U.S","Almacenamiento de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Comunicación, Modelado de datos, Risk Reporting y Technical Proficiency",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979693883/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=mMQRI3zMA4GhMrlaAM7LAQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer Consultant,"66,50 US$/h - 77 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Tampa, FL","Acerca del empleo
Description

We are in search of a proficient Data Engineer to join our team in Tampa, Florida. The role focuses on the development of innovative data solutions and the application of advanced AI techniques. The function involves the use of AI in data configuration, the creation of AI chat bots, and the development of APIs. This role offers a short term contract employment opportunity.

Responsibilities

 Utilizing Apache Spark, Database, EO/IR systems, and Erwin Data for data engineering tasks.
 Implementing HDFS and AB Testing techniques to optimize data storage and experimentation.
 Applying Analytics and Business Intelligence (BI) tools to extract meaningful insights from data sets.
 Producing comprehensive Business Requirement Documents to communicate data requirements and strategies.
 Employing Data fabric and Databricks, including Azure Databricks, to streamline data processing tasks.
 Developing and managing AI chatbots as part of the data strategy.
 Applying AI algorithms in data configuration to enhance data quality and reliability.

Requirements

 Proficiency in Apache Spark, Database, EO/IR systems, Erwin Data, HDFS, AB Testing, Analytics, Business Intelligence (BI), Business Requirement Document, Data fabric, Databricks, Azure Databricks, Chatbot, AI strategy, and AI algorithms is required
 Demonstrated ability to manage and manipulate large datasets
 Experience with data modeling, data warehousing, and building ETL pipelines
 Familiarity with cloud platforms and services, with a preference for Azure Databricks
 Ability to develop and implement databases, data collection systems, data analytics, and other strategies that optimize statistical efficiency and quality
 Excellent understanding of machine-learning and operations research
 Knowledge of predictive modeling and experience with actual implementation
 Strong problem-solving skills with an emphasis on product development
 Experience using statistical computer languages to manipulate data and draw insights from large data sets
 Knowledge of a variety of machine learning techniques and their real-world advantages/drawbacks
 Strong written and verbal communication skills for coordinating across teams
 Drive to learn and master new technologies and techniques
 Ability to work independently and in a team.

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Ingeniería de datos y Modelos predictivos, Comunicación, Comunicación oral, Erwin, Estrategia de datos, Fiabilidad, Modelado de datos, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3984328316/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=66DO6sCujlyqLDI6xs7n8w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 3 días,"Nueva Jersey, Estados Unidos","Acerca del empleo
Data Engineer
New Jersey

Our client is a global leader in consumer products, specializing in the production, distribution, and provision of household, health care, and personal care products. With a rich history dating back over 200 years, they are committed to improving the quality of life for consumers worldwide. They are seeking a highly skilled Data Engineer to join their dynamic team and contribute to our mission of sustainability and innovation.

Responsibilities:
Design, architect, and implement a comprehensive data lake solution, integrating a variety of different types of data sources.
Develop and maintain Directed Acyclic Graphs (DAGs) using Apache Airflow for efficient data workflows.
Extract, transform, and load (ETL) data from various sources, including REST APIs, AWS S3 buckets, GCP BigQuery, and other on-prem/cloud databases.
Manage and optimize Snowflake databases to ensure high performance and reliability.
Solution data streams for both structured datasets (e.g., clinical measurements, lab results) and unstructured datasets (e.g., images, instrument data).
Collaborate with cross-functional teams to understand data requirements and deliver solutions that meet business needs.
Ensure data quality, integrity, and security across all data processes and platforms.
Utilize GitHub and GitHub Actions for version control and CI/CD pipelines.
Implement containerization and use container tools for efficient deployment and scaling of data solutions.

Required Skills:
Proven experience in designing and implementing data lake solutions in cloud (GCP/AWS).
Strong proficiency in Snowflake database management.
Hands-on experience with Apache Airflow and DAG development.
Ability to extract, transform, and load (ETL) data from diverse sources such as REST APIs, AWS S3, GCP BigQuery, and other databases.
Experience working with both structured and unstructured datasets.
Expertise in database management, ETL processes, and data architecture.
Proven track record of designing and implementing robust data solutions.
Proficiency with GitHub and GitHub Actions for version control and CI/CD.
Experience with containerization and container tools (e.g., Docker, Kubernetes).

Preferred Skills:
Clinical data background with experience in managing and analyzing clinical datasets.
Experience with Veeva Vault integration for managing clinical data and documents.
Experience with implementing or enhancing data sets for machine learning applications.
Professional Data Engineer certification or equivalent experience","Amazon Web Services (AWS) y Extraer, transformar y cargar (ETL), Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3968016044/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=loyGAFu1UuwH0c2HlTckMw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Virginia, Estados Unidos","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Job Description

The Data Engineer is responsible for designing, building, and maintaining the systems and processes that manage and store data. The Data Engineer is instrumental in ensuring data is available for use by other teams and systems, such as Data Scientists and Analysts.

Responsibilities

 Design and implement data storage solutions to meet business requirements
Develop data pipelines and workflows to extract, transform, and load data from various sources
Design and implement data security and privacy policies
Monitor and optimize data storage performance and scalability
Collaborate with Data Scientists and Analysts to understand their data requirements
Ensure data quality and integrity by implementing data validation processes
Maintain and update data documentation

Required Skills

Security Clearance: TS/SCI with an appropriate agency Polygraph is required.

 Strong knowledge of database technologies, such as SQL and NoSQL
Proficiency in programming languages such as Python and Scala
Experience with big data technologies, such as Hadoop, Spark, and Elasticsearch
Knowledge of cloud computing platforms, such as AWS, Azure, and Google Cloud
Ability to write efficient and scalable code
Strong problem-solving and critical thinking skills
Excellent communication and collaboration skills

Technologies

 SQL databases (MySQL, PostgreSQL, Microsoft SQL Server)
NoSQL databases (MongoDB, Cassandra, DynamoDB)
Big data technologies (Hadoop, Spark, Elasticsearch)
Cloud computing platforms (AWS, Azure, Google Cloud)
Programming languages (Python, Scala)

Benefits And Compensation

 Comprehensive health insurance coverage, including medical, dental, and vision 
Generous paid time off 
Company-supported training 
Relaxed work environment 
Very generous 401k plan
Predominantly remote work with occasional meetings

Company Description

Enterprize Software is a vibrant, energetic, and skilled software development company. We build our customers' desired solutions and exceed expectations. We're always looking for the best people, talent, and people who enjoy working in teams while creating some of the world's most innovative solutions. We pride ourselves on delivering great software and love having people on our team with the same drive. Please apply if you believe you would be a good fit for our team!","Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Pensamiento crítico y Scala, Comunicación, Necesidades empresariales, Razonamiento, Resolución de problemas y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3960295345/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=zx6Wg8%2BmBF7t%2FRDr5IZGLQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"70 US$K/año - 100 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",Publicado de nuevo hace 1 semana,"Nueva York, NY","Acerca del empleo
Department: Operations-Engineering

Employment Type: Full Time

Location: New York

Compensation: $70,000 - $100,000 / year

Description

Our Company

Private investments such as venture capital, hedge funds, and private equity, make up roughly 25% of the entire investments universe. Traditionally, investors, family offices, and managers track hundreds of investments in complicated spreadsheets, file folders, and busy email boxes. Not only is this tedious and time-consuming, but it is rife with opportunity for manual data entry errors, inconsistent reporting, and lost information. Enter, Arch.

Arch is a fintech startup that digitizes and automates portfolio management and document collection for private investments. With Arch, investors can rest assured knowing all of their investment information and data is organized, accurate, and secure.

We are a small company responsible for billions of dollars in investments, and we are looking to hire in all parts of our company as we are quickly growing. As a Data Engineer, you'll write much of the code for processing and organizing data from hundreds of sources in thousands of formats, keeping Arch's data organized as we continue to scale and grow our company.

The role

The Data Engineer role at Arch is a role whose responsibilities are tightly aligned with the efficiency and overall success of the Operations team.

Key Responsibilities

As a Data Engineer, you will be an integral part of our growing Operations team. In this role, you will be responsible for:

Diving into and mastering the many sources and formats of alternative investment data.
Maintaining, refining, and augmenting Arch data processing pipelines.
Both applying traditional data ingestion methods and making use of generative AI where relevant.
Tracking and improving Arch's data quality and processing efficiency.

Skills, Knowledge And Expertise

Reach out to us if you:

Are proficient in JavaScript or another mainstream programming language
Know SQL
Are a quick learner who enjoys partnering with others and solving difficult problems
Are interested in alternative investments and/or financial services

Bonus Points If You

Enjoy writing code to process and understand data
Are familiar with the terms capital calls, IRRs, recallable distributions, or K-1s
Have taken finance or accounting courses

Some Perks Of Working For Arch Include

 Your work is high impact - Being one of just a few people responsible for the company means you have real responsibility and impact from day one. You'll be involved in discussions that drive the growth and direction of our platform from the very beginning.
 You'll learn a lot about a lot - You get to learn about all aspects of the business and touch every system that's part of the product. Our team has helped bring previous companies from early stages to success and has intimate knowledge of the industry and we can quickly bring you up to speed.
 Low bureaucracy - We may be small, but that means we're nimble and can move quickly. New changes are being added to our platform every day, and we're always looking for ways to do things better. We move fast and if a feature or process doesn't make sense or could be more efficient, you always have the power to make it better.
 Team community and camaraderie - We are a small team, and we have enormous trust in each other and always do what we can do to support one another. We're always ready to step in to help.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Calidad de datos y JavaScript",Solicitar
https://www.linkedin.com/jobs/view/3969535226/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=U5AIFVVf%2FK7a0eCj1me4cQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"90 US$K/año - 165 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Newport Beach, CA","Acerca del empleo
We believe that a positive corporate culture is vital to our success. We place great value on collaboration, curiosity, responsibility, and authenticity. In our hiring process, we seek individuals who embrace these values. The result is a friendly, respectful, and collegial workplace where diversity is embraced, strong opinions are loosely held, and disagreements do not become personal. Our culture is well described in this article written by our former CEO, Katy Sherrerd.

What we’re looking for:

We seek data engineers passionate about creating efficient and robust data ingestion pipelines, as well as optimizing operational activities including quality control and process supervision. Expertise with financial data from major vendors is highly desirable. We work mostly in SQL and Python, but also with C#. We build infrastructure with PostgreSQL databases and AWS cloud resources and make heavy use of tools and libraries such as NumPy, Ansible, and Airflow. 
Our development approach employs modern language features and libraries, test-driven development, and extensive peer review. As well-rounded engineers, we participate in all facets of the development process from project planning to completion.

What you’ll be doing:

Collaborate with teams of software engineers and finance professionals to develop, optimize, and maintain code with a focus on test and quality.
Engage with data vendors, explore the market of financial data services, summarize financial data offerings to finance professionals.
Implement rigorous testing practices, adopt continuous integration and continuous deployment methods, and ensure code quality.
Participate in code reviews and pull-requests, as well as professional development activities to maintain expertise in relevant domains.
Develop software within Linux environments for deployment in AWS.
Embrace our culture and its core values to contribute positively to our teams and the firm.

Who we’re looking for:

A bachelor's degree or higher in Computer Science or a related field.
Demonstrated expertise in Python, SQL, database administration, and cloud infrastructure.
Familiarity with the libraries and technologies used by our team is beneficial but not required, as a willingness to learn and adapt is highly valued.

What we provide:

A flexible hybrid work model. We value collaboration and provide ample opportunities to foster a sense of community within your team and the company, while still understanding the need for flexibility. 
Generous comprehensive insurance plans.
Education assistance and tuition reimbursement program.
Company sponsored social and recreational activities. 
Flexible time off. 
Six-week sabbatical after 7 years of continuous employment.
Company sponsored daily in-office meals. 
Gym membership stipend. 

Salary Range: $90,000 - $165,000 + eligible for a discretionary bonus. Base pay will be determined on an individual basis considering such factors as location, qualifications, skills, and experience. The base pay range is subject to change and may be modified in the future.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Administración de bases de datos, Bases de datos, Ciencias de la computación y Revisión de código",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979662625/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=7FccmEbseIHfe3VeYLy10w%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Charlotte, NC","Acerca del empleo
This is a remote position.

Data Scientist Engineer - US Only

Experience: 1+ years

Employment Type: Full-time, Remote

Base Salary: $62K-$72K

Phoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They are specialized in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.

Skills and Abilities:

Strong knowledge of R or Python for data analysis and modeling. 
Proficiency in statistical programs such as R, SAS, MATLAB, or Python. 
Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). 
Basic understanding of SQL, Javascript, XML, JSON, and HTML. 
Ability to learn new methods quickly and work under deadlines. 
Excellent teamwork and communication skills. 
Strong analytical and problem-solving abilities. 
Basic understanding of SQL, Javascript, XML, JSON, and HTML. 

Preferred:

Knowledge of actuarial concepts and life, health, and/or annuity products. 
Experience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. 
Familiarity with Microsoft DeployR. 
Exposure to insurance risk analysis. 
Basic experience in computational finance, econometrics, statistics, and math. 
Knowledge of SQL and VBA. 
Familiarity with R or Python for predictive modeling 

Why Phoenix Recruitment LLC?

Phoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save you time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization.","Analítica de datos, Ciencia de datos, JSON, Modelos predictivos, SQL y XML, Análisis cuantitativo, Análisis financiero cuantitativo, Computational Finance y HTML",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3960514292/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=%2F%2BY8tlvgcU%2B6KV8DYKhNkw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Analyst,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",Publicado de nuevo hace 1 semana,"Chicago, IL","Acerca del empleo
Job Description

Epsilon Analytics partners with both internal and external clients, and data providers, leveraging various analytics to drive strategic thought and effective decision making. The Data Analyst is responsible for conducting data analyses using SQL and other tools in support of a variety of analytic solutions.

Roles & Responsibilities

Collaborate with internal/external stakeholders to manage data logistics – including data specifications, transfers, structures, and rules.
Access and extract data from a variety of sources of all sizes (including client marketing databases) via SAS Access, SQL, etc.
Master and perform all steps required to create analysis-ready data sets, including data integration/merging, variable preparation, and quality control (QA/QC)
Develop and execute SQL (or related) programs with detailed direction and supervision.
Provide problem solving and data analysis, derived from programming experience.
Demonstrate proficiency with desktop and UNIX toolsets (SAS, SAS ODS, SQL, MS Office) to create pivot tables and/or report content such as tables, reports, graphs, etc. (some positions require proficiency in digital analytic tools including Google and/or Adobe Analytics and familiarity with digital data, in addition to or in lieu of SAS/SQL)
Document and articulate steps taken in an analysis to project managers.
Answer questions about data sets and analyses
Follow all policies and procedures for programming, project documentation, and system management.
Become familiar with…
all offerings outlined in the Insider’s Guide to ACG
various statistical offerings and methods (CHAID, logistic/multiple regression, cluster analysis, factor analysis)
Epsilon data assets
the SAS macro library
Participate in the design, planning & execution of projects
Effectively manage time and resources in order to deliver on time / correctly on a limited number (1-4) of concurrent projects
Proactively communicate with supervisor regarding workload and the status of assignments
Prepare basic report content (Word, Excel, PowerPoint) in support of deliverables
Perform two tasks related to the role of Sr. Data Analyst during the year
Minimum Qualifications

Bachelor’s degree in a quantitative discipline (e.g., Statistics, Economics, Mathematics, Marketing Analytics) or significant relevant coursework
1-2 years of experience in the marketing analytics field
Demonstrated proficiency in SQL programming; minimum 2 years of experience
Strong analytic thought process and ability to interpret findings
Acute attention to detail (QA/QC)
Working knowledge of MS Office; including PowerPoint, Word, Excel and Outlook
Ability to work on multiple assignments concurrently
Excellent verbal and written communication skills
Highly motivated and collaborative team player with strong interpersonal skills
Effective organization and time management skills

Desirable Qualifications

Advanced degree (Master’s/PhD) in Statistics, Economics or other quantitative discipline
Database marketing experience/knowledge
Automotive industry knowledge
Ability to program in newer and emerging languages such as SAS, R, and Python

Additional Information

About Epsilon

Epsilon is a global advertising and marketing technology company positioned at the center of Publicis Groupe. Epsilon accelerates clients’ ability to harness the power of their first-party data to activate campaigns across channels and devices, with an unparalleled ability to prove outcomes. The company’s industry-leading technology connects advertisers with consumers to drive performance while respecting and protecting consumer privacy. Epsilon’s people-based identity graph allows brands, agencies and publishers to reach real people, not cookies or devices, across the open web. For more information, visit epsilon.com.

When you’re one of us, you get to run with the best. For decades, we’ve been helping marketers from the world’s top brands personalize experiences for millions of people with our cutting-edge technology, solutions and services. Epsilon’s best-in-class identity gives brands a clear, privacy-safe view of their customers, which they can use across our suite of digital media, messaging and loyalty solutions. We process 400+ billion consumer actions each day and hold many patents of proprietary technology, including real-time modeling languages and consumer privacy advancements. Thanks to the work of every employee, Epsilon has been consistently recognized as industry-leading by Forrester, Adweek and the MRC. Positioned at the core of Publicis Groupe, Epsilon is a global company with more than 8,000 employees around the world. Check out a few of these resources to learn more about what makes Epsilon so EPIC

Our Culture https //www.epsilon.com/us/about-us/our-culture-epsilon
Life at Epsilon https //www.epsilon.com/us/about-us/epic-blog
DE&I https //www.epsilon.com/us/about-us/diversity-equity-inclusion
CSR https //www.epsilon.com/us/about-us/corporate-social-responsibility

Great People Deserve Great Benefits

We know that we have some of the brightest and most talented associates in the world, and we believe in rewarding them accordingly. If you work here, expect competitive pay, comprehensive health coverage, and endless opportunities to advance your career.

Epsilon is an Equal Opportunity Employer. Epsilon’s policy is not to discriminate against any applicant or employee based on actual or perceived race, age, sex or gender (including pregnancy), marital status, national origin, ancestry, citizenship status, mental or physical disability, religion, creed, color, sexual orientation, gender identity or expression (including transgender status), veteran status, genetic information, or any other characteristic protected by applicable federal, state or local law. Epsilon also prohibits harassment of applicants and employees based on any of these protected categories. Epsilon will provide accommodations to applicants needing accommodations to complete the application process.

REF235548W","Analítica de datos, Análisis de marketing, Ingeniería de datos , Integración de datos y SQL, CHAID, Comunicación, Control de calidad, Documentación de proyectos y Habilidades sociales",Solicitar
https://www.linkedin.com/jobs/view/3982795537/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=%2BfCphdfcj77bMxuco4EQeA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, M9 Solutions, is seeking the following. Apply via Dice today!

M9 Solutions is dedicated to providing IT services and solutions to the Federal Government by mobilizing the right people, skills, clearance levels, and technologies to help organizations who desire improved performance and modern, sustainable change. M9 has provided quality IT services and support to 27 Federal Agencies and multiple commercial customers across 41 states nationwide. Our capabilities include digital transformation, software development, cloud migration, applications & infrastructure, cybersecurity, data delivery & analytics, and IT talent solutions.

M9 Solutions is seeking a Data Engineer to work remotely on a government contract for a client located in Arlington, VA.Candidates must have the ability to obtain a Public Trust level clearance. ship is required.

Responsibilities

Design and build data infrastructure and flows, ranging from data ingestion, transformation, and modeling for analytics and dashboarding features.
Conduct solution implementation strategies that enable iterative build of application's microservices architecture. 
Develop expertise in the data ingested by the application, helping to translate functional requirements into features with product team.
Consult with team and client subject matter experts to ensure analytics and dashboarding features are grounded in policy context. 
Safeguard data consistency, security, and reliability through technical design, build and process, working closely with DevOps and security colleagues. 
Collaborate with other developers and testers across the team to plan and sequence changes to data architecture and DataOps in cross-functional team structure.

Required Skills And Qualifications

Work Authorization: ship is required.
Ability to pass a background investigation for a Public Trust level clearance.
Bachelor's degree is required.
Experience designing and implementing data solutions with AWS serverless services.
Ability to design and implement data architecture in iterative approach that supports microservices architecture.
Experience with database administration and optimization.
Ability and desire to support team as data expert, acting as the go-to expert for data architecture and attribute definition knowledge.
Familiarity with event-driven application architecture.
Experience with tools such as AWS cloud-native services, Python, and SQL.

Full-Time Employee Compensation

M9 Solutions' pay range for this position is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include, but are not limited to, responsibilities of the position, education, experience, knowledge, skills, abilities, as well as internal equity, location, alignment with market data, applicable bargaining agreement (if any), or other law.
M9 Benefits - 

Salary Range

$60,000 - $180,000 USD

M9 Solutions LLC is an Equal Opportunity Employer - AA/EOE/W/M/Vet/Disabled.

M9 Solutions, LLC (M9) is a Federal contractor and an Equal Opportunity Employer. M9 is subject to Executive Order 11246, which requires government contractors to take affirmative action to ensure that equal opportunity is provided in all aspects of their employment. Please click here to complete M9's Voluntary Self-Identification Form and then email it to . Submission of this information is voluntary and refusal to provide it will not subject you to any adverse treatment. If you are an individual with disabilities who needs accommodation or you are having difficulty using our website to apply for employment, please contact M9's Human Resources Department at or at .

With 14+ years of proven delivery and steady growth, M9 Solutions is a unique small business with credible past performance and key capabilities offering project management services, solution architects, business analysts, program managers, technical architects, and technical consultants. M9 was recognized as an Inc. 5000 Fastest-Growing Private Companies in 2021, 2020, 2019, 2018, 2017, 2016, and 2012. M9 Solutions believes that work should be fun, rewarding, and something everyone can be excited about. We offer a competitive compensation package and value diversity in driving the vision of the company.

F 7.2-27 REV - 6 01/24/2023","Analítica de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Administración de bases de datos, Bases de datos, Confianza ciudadana, Modelado de datos y Optimización",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3974366250/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=YMwhi5B3Xii4ZhtC2Yxgfg%3D%3D&trk=flagship3_search_srp_jobs,Data Science & Visualization Enginee,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Cupertino, CA","Acerca del empleo
Job Title: Data Science & Visualization Engineer
Locations: Cupertino, CA
Type of hire: Fulltime 

Skill Required: Python | Tableau| iOS

Job Description:
Minimum 6+ years of experience working as a Data Engineer
Experience building data pipelines with Python and R
Willingness to learn new technologies and implement POCs in short time
Support SQL and python production jobs
Experience in data visualization using Tableau
Experience in Swift, iOS apps
Ability to follow triage playbook and contribute as new scenarios arise
Experience in writing and optimizing complex SQL queries
Excellent written and verbal communication skills
Excellent multi-tasking and organizational skills
Excellent communication and interpersonal skills","Analítica de datos, Canalizaciones de datos, Ciencia de datos, Python, R (Lenguaje de programación), Tableau, Visualización y Visualización de datos, Formación en comunicación y Swift (lenguaje de programación)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3961549731/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=AAx%2F6KTTHWIcwS3hH5%2FhVg%3D%3D&trk=flagship3_search_srp_jobs,Urgent: AWS Data engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 mes,"Municipio de Warren, NJ","Acerca del empleo
Title: AWS Data Engineer

Location: Warren, NJ

Duration: 6 months

Position type: W2 contract.

Required Skills & Experience

AWS (the dataset from the Cloud Engineer and Snowflake data lake), Snowflake, SQL, Python and PowerBI

Responsibilities

They will be designing, developing, and deploying a portfolio of Data & Analytics technology assets and platforms leveraging cloud-based tools and capabilities to capture, explore, transform, and deliver data. 
Additionally, they will analyze, document and develop solutions based on business needs and opportunities to deliver the intended outcomes in a timely manner. 
Key skills in AWS (the dataset from the Cloud Engineer and Snowflake data lake), Snowflake, SQL, Python and PowerBI willenable you to excel in this role. 
Put simply - We need a hands on Data Engineer that is really good with PowerBI and have experience working with AWS Data & Analytics Capabilities. 
They will like working with our business teams to rapidly build new datasets and dashboards.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Datasets, Lagos de datos, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3978648745/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=oCeFpc8ZS3%2FWTQbJeAowBw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Annapolis Junction, MD","Acerca del empleo
Description

Welcome to Interclypse, where innovation meets passion. Every team member is a vital piece of our success story. We are not just a company, but a dynamic community driven by the shared vision of redefining excellence. At Interclypse, you will find more than a career – you will discover a vibrant ecosystem where your talents are celebrated, your ideas are embraced, and your potential is achieved. Every Interclypse team member can benefit based on their efforts and collectively benefit through the overall company’s success. Join our mission to positively impact society, community, industry, and individuals by always “Doing What is Right”. Together, let's pioneer a future where greatness is achieved and exceeded.

To actualize this vision, Interclypse employs a growth mindset culture that empowers employees to rise in their careers by providing them with tools, mentorship, and a supportive environment to ensure long-term success.

Interclypse is supporting several Maryland state agencies in the modernization and sustainment of critical systems. This exciting opportunity provides candidates with the ability to contribute to the long-term health and success of the state while continuing to learn and grow professionally within Interclypse’s growth mindset culture.

All positions are required to be onsite at various locations in Maryland.

Make a difference. Join our team by applying today!

Responsibilities

Responsible for designing, building, and maintaining data pipelines and infrastructure to support data-driven decisions and analytics. The individual is responsible for the following tasks:

 Design, develop and maintain data pipelines, and extract, transform, load (ETL) processes to collect, process and store structured and unstructured data
 Build data architecture and storage solutions, including data lakehouses, data lakes, data warehouse, and data marts to support analytics and reporting
 Develop data reliability, efficiency, and qualify checks and processes
 Prepare data for data modeling
 Monitor and optimize data architecture and data processing systems
 Collaboration with multiple teams to understand requirements and objectives
 Administer testing and troubleshooting related to performance, reliability, and scalability
 Create and update documentation

Requirements

Required Qualifications

Bachelor’s or Master's degree from an accredited college or university with a major in computer science, statistics, mathematics, economics, or related field
Three (3) years of experience as a data engineer
Experience as data engineer or similar role with a strong understanding of data architecture and ETL processes
Proficient in programming languages for data processing and knowledgeable of distributed computing and parallel processing

Why You Will Love Interclypse

You want to work for an adaptive company that moves at your speed.
You want a healthy work-life balance.
You want to work with a passionate team on an important mission.
You want to work for an organization that values and appreciates you.
You want to work for an organization that invests in your growth.
You want the option for career mentorship, both in technology and in business.
You value a company with a strong culture of growth and support.

Employee Impact Program

Every employee has the opportunity to be rewarded for the contributions they can make toward the long-term health of the company, our customers, and employees. This program in combination with our comprehensive benefits, time off and leave programs allow you to design a career and compensation program that enables unmatched flexibility while ensuring company, customer, and employee health and prosperity.

Benefits

Personal Time Off (PTO) for vacations, holidays, illnesses
Parental Leave
Bereavement Leave
Jury Duty Leave
Retirement: Unlimited 401K match up to 8% of your salary up to the federal maximum
Financial education and planning support
Health Insurance (Medical, Dental, Vision)
Health Savings Account (HSA)
Medical and Dependent Care Flexible Spending Accounts (FSA)
Employee Assistance Program
Life Insurance
Accidental Death and Dismemberment Insurance
Disability: Short-term and long-term disability coverage
Educational support
Company apparel
Social events: Holiday Party, Spring Picnic, Fall Picnic, happy hours and more.
Access to group rates for voluntary benefits such as Accident, Hospital Indemnity, Critical Illness, Pet Insurance, and Identity Theft Protection

EOE AA M/F/Vet/Disability

Interclypse is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, disability, or protected Veteran status.","Almacenamiento de datos, Arquitectura de datos, Canalizaciones de datos, Ciencia de datos, Data Marts, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Ciencias de la computación y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3984566781/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=6PKixHzU7k2NZr3dc%2Fa9nA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer / Hybrid In Phoenix,"145 US$K/año - 175 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Phoenix, AZ","Acerca del empleo
A financial company is looking for a Data Engineer with experience with Python, Spark, AWS, and Airflow

Required Skills & Experience

3+ years of Data Engineering experience 
SQL 
NoSQL Database experience 
Python 
AWS 
Spark, Kafka, or Flink 

Desired Skills & Experience

Masters degree or higher in Computer Science or related field. 
Experience with Typescript, Javascript, or Nodejs 

What You Will Be Doing

Tech Breakdown

70% Data Engineering 
20% Data Analytics 
10% Machine Learning 

Daily Responsibilities

80% Hands On 
0% Management Duties 
20% Team Collaboration 

The Offer

10% Bonus 
Stock Options 

You Will Receive The Following Benefits

Medical Insurance 
Dental Benefits 
Vision Benefits 
Paid Time Off (PTO) 

Applicants must be currently authorized to work in the US on a full-time basis now and in the future.

Posted By: Julie Bennett","Almacenamiento de datos, Apache Kafka, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Ciencias de la computación, JavaScript, Node.js y TypeScript",Solicitar
https://www.linkedin.com/jobs/view/3982931412/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=UvVydoM59%2BBgfAr7sEG3%2Fw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (hybrid),"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"West Des Moines, IA","Acerca del empleo
Zirous is an accomplished tech firm based out of West Des Moines, IA. For over 35 years, Zirous has been committed to executing best practices and going above and beyond industry standards to provide viable solutions for customers in all things data & analytics, marketing technology, cloud adoption, enterprise integration, identity & access management, and custom application development. Zirous is seeking a Data Engineer who is eager to work with a variety of technologies and to continue to develop their expertise.

Zirous has been acclaimed for its outstanding workplace environment as a Top Workplace in the State of Iowa for several years. Additionally, Zirous was a finalist for the prestigious Best Technology Company Culture of the Year award by the Technology Association of Iowa. Join our fantastic team today!!

What you would be doing:

As a Data Engineer you will be responsible for designing and implementing data environments for Zirous' customers to meet their current and future reporting and analytics needs. This role is a combination of strategy & environment design, with hands-on implementation. The ideal leader is a database expert who has extensive experience in data modeling, SQL development, and architecting scalable cloud environments in Azure, AWS, or similar.

The position will:

Coordinate with the Zirous technical team and client teams on strategy and implementation details
Work with a project team to gather business and technical requirements
Lead a team in architecting cloud solutions for Zirous clients
Work closely with Project Managers to ensure successful project implementation
Provide expert and specialized hands-on technical support, consultation and coaching to clients
Responsible for development and management of technical strategies and initiatives that take advantage of existing technologies, as well as bring forth new technologies that address business needs in an efficient manner
Complete evaluation, selection, and management of technology partners to supplement Zirous' subject matter knowledge
Other duties as assigned


Location: This position will work in a hybrid model with both work from home (remote) days each Monday & Friday, as well as on-site days at the West Des Moines, IA office each Tuesday, Wednesday, Thursday. Zirous offers a flexible schedule so that you can work to meet clients needs and your personal needs. The ability to work on-site at the Zirous office weekly will be required.

What to expect:

To work independently on your individual tasks, but also work as part of a team on a variety of initiatives. Your expertise is wanted and needed - team collaboration is key
To work alongside some of the smartest people you'll ever know. You'll learn from them, and they'll learn from you
LOTS of perks: catered lunches, snacks, beverages, social hours, on-site gym & shower facility, small group activity options outside of work (think axe throwing, bowling, pickleball, go-karts, and more), and a flexible work schedule with the ability to work remotely and in the office
Recognition for a job well done! Employee of the Month and Employee of the Year awards with bonus amounts for each
Company-wide lunches on-site and remotely
Lunch and Learns (throughout the year we educate each other on different industry topics)
Casual dress - jeans, sweatshirts, and flip flops are all acceptable while in our office or while working from home. We do dress up when client-facing, but don't worry if you are just graduating and don't have business clothes - we offer a clothing allowance!
Professional development opportunities: certifications, continued education, speaking engagements, etc
Much, much, more!


Requirements

5+ years experience leading data warehouse implementations and managing complex system architectures
Bachelor's degree in Computer Science, Computer Engineering, Software Engineering, Data Analytics, Mathematics, Information Systems, or in a related field or equivalent work experience. A master's degree in a related field is a plus
Deep understanding and extensive experience with data structures, object oriented and declarative programming
Excellent communication skills, analytic ability, good judgment and the ability to work effectively with clients and staff
Solid organizational skills, ability to meet project deadlines with a focus on detail
5+ years of experience with the following technologies:
Microsoft Azure, Data Factory, Synapse, Data Lake Gen 2, Logic Apps, Fabric
Azure SQL Database, Microsoft SQL Server
SQL, Java, XML
AWS, Oracle, Google Cloud
Snowflake, Alteryx
R, Python
Tableau, Oracle Analytics Cloud, Qlik Sense, Google Data Studio, Looker, Quicksight, Microsoft Purview
Previous working experience with Power BI is a must
Jira, Confluence

Don't have all of the experience or skills? We encourage you to still apply today! We'll train you and teach you what you don't already know. We promise that you'll learn and grow within this role each day.


Benefits

Contributing to the success of a high caliber team
Competitive salary and benefits package including 401(k) match
An environment that fosters personal and professional growth
Hybrid model of in-office and work from home days each week
Flexible scheduling
Opportunities to work on exciting and varied projects
Flexible time off (FTO) - Unlimited time off - take what you need while still supporting your team members and Zirous
Paid holidays
Cell phone subsidy and discounts through Verizon
We value our employees' personal time, career desires and life goals

Zirous is an equal opportunity employer.","Almacenamiento de datos, Google Data Studio, Ingeniería de datos y Inteligencia empresarial, Comunicación, Declarative Programming, Estructuras de datos, Programación orientada a objetos (POO), Requisitos técnicos y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3959995603/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=IxpC%2Fj%2BDdgez8uPRWeU72w%3D%3D&trk=flagship3_search_srp_jobs,Expression of Interest - Senior Data Platform Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,Estados Unidos,"Acerca del empleo
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.
The Marlee Talent Pool is a pilot project designed to:
Help job seekers get discovered by our partners based on their anticipated hiring needs.
Provide optional support and resources for job seekers in their career endeavours.
Help individuals understand, and bring out the best in themselves and each other.
The Marlee Talent Pool process:
Once you express your interest, you will be asked to complete the Marlee work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.
About Marlee (Fingerprint For Success)
Backed by 20+ years of research, Marlee’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviours, and performance.
Ultimately, we help people find purpose and fulfillment at work, and help build and scale high-performing teams.
Keep in mind that joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your Marlee work style assessment to match the hiring needs of our partners.
Your feedback is a gift! Write to us via: hello@getmarlee.com to help co-create the future of recruitment, together.","Computación en la nube y DevOps, Bases de datos, Desarrollo de software, Hojas de estilos en cascada (CSS) y Microsoft Azure",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984238787/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=y2DE3tTzsgNAbY8MUeadFw%3D%3D&trackingId=b%2FX3%2Fsv0UYgAs09tXWccRg%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer - Python, SQL - fully paid Medical,Dental on Day 1","115 US$K/año - 140 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 4 días,"Houston, TX","Acerca del empleo
Position Overview

We are a strong, financial trading company seeking a highly skilled and motivated Data Engineer to join our global team. This is a growing department, you will be the first Data Engineer in this location reporting into our Global team!

This is a onsite position, 5 days a week.

Key Responsibilities

Collaborate with stakeholders to understand their data needs.
Produce solutions to ingest external data and produce pipelines.
Provide data distribution methods to allow the trading desks to integrate the data with their models.
Investigate Data Science or Machine Learning opportunities to provide insights to the business.

Qualifications

Bachelors or higher degree in Computer Science, Math, Engineering or related field
3+ years of Data Engineering experience
Strong knowledge of Python and SQL
Knowledge of workload automation tools (ActiveBatch) or workflow management tools (Apache Airflow.)
Working knowledge of MS Azure
Working knowledge of Databricks 
Experience with Apache Spark
Financial trading experience is a plus.
Ability to work independently and as part of a global team.
Excellent problem solving skills and attention to detail.
Strong communication skills with stakeholders.

We offer a base of $115,000 to $140,000 plus bonus and excellent benefits!

This is a onsite position, 5 days a week.

Benefits

FULLY PAID Medical and Dental premiums from Day 1! 

Bonus potentially 15-20%!

401K match, Tuition Reimbursement, vacation, holidays

Email Your Resume In Word To

Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:

rhona.kannon@cybercoders.com

Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : RK3-1812110 -- in the email subject line for your application to be considered.***

Rhona Kannon - Principal Director

Applicants must be authorized to work in the U.S.

CyberCoders is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity or expression, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, status as a crime victim, disability, protected veteran status, or any other characteristic protected by law. CyberCoders will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. CyberCoders is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please contact a member of our Human Resources team to make arrangements.","Airflow, Apache Spark, Ciencia de datos y Ingeniería de datos, ActiveBatch, Apache Airflow, Azure Databricks, Ciencias de la computación, Gestión de flujos de trabajo y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3936962756/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=HyVTiIRzoados%2Fvk1Ey0aw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 meses,Estados Unidos,"Acerca del empleo
Gen AI

 Data Engineering, ETL Jobs

 Snowflake

 Azure Cloud

Data Engineer Essential Job Functions

"" Design, develop, and maintain scalable data pipelines for ingesting, processing, and transforming large volumes of structured and unstructured data.

"" Implement efficient data processing workflows to support the training and evaluation of solutions using large language models, ensuring reliability, scalability, and performance.

"" Addressing issues related to data quality, pipeline failures, or resource contention, ensuring minimal disruption to systems.

"" Integrate Large Language Model into data pipeline for natural language processing tasks.

"" Working with Snowflake ecosystem

"" Deploying, scaling, and monitoring AI solutions on cloud platforms like Snowflake, Azure, AWS, GCP

"" Communicating technical and non-technical stakeholders and collaborate with cross-functional teams.

"" Cloud cost management and best practices to optimize cloud resource usage and minimize costs.

Data Engineer Preferred Qualifications

"" Experience working within the Azure ecosystem, including Azure AI Search, Azure Storage Blob, Azure Postgres and understanding how to leverage them for data processing, storage, and analytics tasks.

"" Experience with techniques such as data normalization, feature engineering, and data augmentation.

"" Ability to preprocess and clean large datasets efficiently using Azure Tools /Python and other data manipulation tools.

"" Expertise in working with healthcare data standards (ex. HIPAA and FHIR), sensitive data and data masking techniques to mask personally identifiable information (PII) and protected health information (PHI) is essential.

"" In-depth knowledge of search algorithms, indexing techniques, and retrieval models for effective information retrieval tasks. Familiarity with search platforms like Elasticsearch or Azure AI Search is a must.

"" Familiarity with chunking techniques and working with vectors and vector databases like Pinecone.

"" Experience working within the snowflake ecosystem.

"" Ability to design, develop, and maintain scalable data pipelines for ingesting, processing, and transforming large volumes of structured and unstructured data.

"" Experience with implementing best practices for data storage, retrieval, and access control to ensure data integrity, security, and compliance with regulatory requirements.

"" Be able to implement efficient data processing workflows to support the training and evaluation of solutions using large language models, ensuring reliability, scalability, and performance.

"" Ability to proactively identify and address issues related to data quality, pipeline failures, or resource contention, ensuring minimal disruption to systems.

"" Experience with large language model frameworks, such as Langchain and know how to integrate them into data pipelines for natural language processing tasks.","Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Calidad de datos, Datasets, Manipulación de datos, Microsoft Azure y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3983549464/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=0wZNc0NVl7zui767%2BRqijg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Teaneck, NJ","Acerca del empleo
Title: Data Engineer 
Full-time
We do not accept H1B/H1B Transfer/OPT
Hybrid role, mostly work from home; commute to the office as requested

Minimum Qualifications: 
The role is ideal for someone who thrives in a fast-paced, cutting-edge technology environment.
Proven Experience with data modeling and developing Enterprise Data Warehouse solutions - OLTP, OLAP, Dimensions, Facts, and Data modeling.
Experience in designing and implementing data pipelines for BI and Machine Learning solutions.
Experience with data security and privacy best practices.
Experience with on Azure Monitor, Load Balancer, Application Gateway, Azure Functions, Azure Data Factory Integrations
5+ years of hands-on experience in Data Management leveraging Microsoft Azure Cloud:
Data integration and ETL: Azure Data Factory, Power BI Dataflow, SSIS
Database Management: Azure SQL Database, Azure Synapse Analytics, Azure Data Lake Storage, MSSQL, Cosmos DB, Analysis Services
Compute: Azure Functions
Monitoring: Azure Monitor
Container orchestration: Kubernetes
Experience working and communicating cross functionally in a team environment.
Live within commuting distance to one of Behaviorally's offices

Preferred Qualifications: 
Certifications in data engineering AI/ML technologies and Azure, such as Azure AI Engineer, Azure Data Scientist, or Azure Solutions Architect

Responsibilities: 
In this role, you will play a key role in transforming structured, semi structured, and unstructured data into actionable insights.
Lead, designed, develop, and deliver large-scale data systems, data processing, and data transformation projects.
Design and develop scalable high performance data pipelines using Azure cloud platform technologies.
Build, test, and debug the software packages and pipelines required for optimal extraction, loading, and transformation (ELT) of data from a wide variety of data sources.
Implement best practices for data management, security, and governance.
Document data pipelines, data models, and reports.
 Summary: 
Behaviorally is a company specializing in data intelligence to influence consumer behavior at the point of purchase. We leverage the world’s largest database of behavioral consumer pack design metrics to help brands optimizing their packaging and drive sales. Our innovative tools are used by all major industry players to improve the in-store and online transactions in sectors such as Consumer Packaged Goods, Health and Well-Being, Technology, and Retail.
Behaviorally is seeking a Data Engineer to design and develop large-scale data systems and high-performance data pipelines using Azure technologies. Ideal candidates thrive in a fast-paced environment and have a minimum of 5 years of experience in data management and integration, with expertise in Azure Data Factory, Azure SQL Database, Azure Synapse Analytics, and Kubernetes. Responsibilities include transforming structured and unstructured data into actionable insights, implementing data security best practices, and documenting data models and pipelines. Preferred candidates will have relevant Azure certifications and experience with data modeling, ETL processes, and BI solutions. Candidates must live within commuting distance to a Behaviorally office.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Datos empresariales, Datos no estructurados, Modelado de datos, Modelo de datos, Procesamiento analítico en línea (OLAP) y Procesamiento de transacciones online",Solicitud sencilla
https://www.linkedin.com/jobs/view/3959939396/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=%2FfFFltiEz9lWujc4A%2B8hEg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"74,9 US$K/año - 90 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,"Pennsylvania, Estados Unidos","Acerca del empleo
Please be aware of recruiting scams! 

All legitimate communication from our recruitment team will come from an official calstart.org email address via email, we will not text you about a role you have not applied to or shown interest in. We will not perform any interviews via text or Zoom chat.

CALSTART does not ask for any fees or personal information such as social security numbers or bank details during the recruitment process.

About Us

CALSTART is a mission-driven industry organization focused on transportation decarbonization and clean air for all.

For over 30 years, it’s been CALSTART’s mission to develop, assess, and implement large-scale, zero-emission transportation solutions to mitigate climate change and support economic growth. CALSTART works with businesses, organizations, governments, and communities to create real-life impact toward clean air and equitable access to clean transportation for all. CALSTART provides scientific, technical and policy support for regulatory development and clean technology and infrastructure acceleration.

About The Role

As a Data Engineer, you'll design and refine advanced data pipelines using Apache Airflow, integrating complex datasets with Python, Pandas, and GeoPandas to support critical decision-making. You will implement Open-metadata for top-tier data governance and enhance data visualizations through Metabase, driving strategic insights. Collaborating with diverse stakeholders, you'll create a unified data ecosystem.

Additionally, you'll elevate our AWS security and infrastructure using tools like Docker. Your expertise in data engineering, cloud architectures, and data governance will drive process improvements and compliance. You'll tackle complex challenges, fostering an inclusive, collaborative environment, and advancing our mission for clean transportation through innovative solutions.

What You'll Do

Architect and refine state-of-the-art data pipelines utilizing Apache Airflow, integrating complex datasets with Python, Pandas, and GeoPandas into a seamless flow that powers decision-making. 
Champion the implementation of Open-metadata for elite data governance, crafting frameworks that ensure data excellence across the spectrum of our initiatives. 
Innovate and enhance data visualizations and dashboards through Metabase, enabling transformative insights that drive strategic initiatives and foster a culture of data-driven decision making. 
Collaborate with a spectrum of stakeholders from technical experts to strategic managers, weaving together diverse perspectives to craft a unified data ecosystem. 
Elevate our security posture and cloud infrastructure on AWS, deploying cutting-edge tools like Docker to safeguard our pioneering work in clean transportation. 

What You'll Bring To The Table

Deep technical expertise in data engineering, with a rich background in designing and deploying robust data systems using PostgreSQL, PostGIS, and advanced Python libraries. 
Demonstrable proficiency in cloud architectures, particularly AWS, with a mastery in leveraging cloud services to maximize efficiency and security. 
Adept at leading data governance and compliance efforts, with a proven track record of architecting data solutions that align with stringent standards and organizational goals. 
Exceptional problem-solving skills, with the ability to dissect and address complex challenges, turning obstacles into opportunities for growth and innovation. 
Exemplary communication abilities, capable of articulating complex technical details and engaging diverse audiences to foster an inclusive and collaborative environment. 

Preferred Qualifications

A passion for sustainability and technological innovation in the field of clean transportation. 
Agile project management prowess, adept at steering projects through agile frameworks to ensure adaptability and milestone achievement. 
Advanced expertise in security protocols, cloud DevOps practices, and proactive measures to ensure the integrity and privacy of sensitive data. 

$74,880 - $90,048 a year

CALSTART values transparency and strives to provide as much information regarding compensation as possible. The complete salary range for this role is $74,880 - $90,048 . We determine pay based on several factors, including but not limited to job-related skills, qualifications, experience, education, internal equity and other factors relevant to the job.

We understand that not everyone will match the above qualifications 100%. If your background isn't perfectly aligned but you feel you would be a great addition to the team, we'd love to hear from you.

We're a tight-knit team of world-class innovators, business minds, and change agents who believe passionately in our mission and put our team ahead of self. We are committed to the continued development and growth of our employees and invest in your success!

We care about your personal well being as much as your professional success and offer generous benefits to full time employees including: 100% company paid comprehensive health benefits for Medical, Dental, Vision, Short Term Disability, Long Term Disability and Life Insurance, Retirement plan with generous company contributions, FSA for Health and Dependent Care, 3 weeks of vacation time in the first year of employment, 11 paid company holidays, paid sick time, paid family leave, and more!

Our inclusive environment focuses on making decisions based on merit without regard to race, color, hair texture, gender, religion, age, nationality, social or ethnic origin, sexual orientation, gender identity, gender expression, LGBTQIA+ status, marital status, pregnancy, disability, genetics, veteran status, or any other characteristic protected by law.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Arquitectura técnica, Comunicación, Datasets y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3977356291/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=DiTNnRupbt5rYe3IUi6PgA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Description

Position at Berkeley Research Group, LLC

BRG’s Global Applied Technology team leverages our proven technologies and process, in conjunction with our broader firm’s industry and functional area experience, to unlock new value for organizations and provide the tools to enable learning, create knowledge, and solve problems.

We are a dynamic group of technologists within a global consulting firm whose dream is to turn the firm’s industry leading expertise into a cutting-edge product that will define a new category. We have studied the problem of how our clients consume data and business intelligence and believe that our solution will be a gamechanger. Join our small team to make this dream a reality!

Responsibilities

Quickly ramp up on new client data projects across diverse industries and domains, learning new systems and data models rapidly.
Design highly scalable and flexible data pipelines, leveraging cloud technologies like AWS, GCP and on-premises tools.
Develop reusable ETL modules and transformations to integrate data from a variety of structured and unstructured sources.
Build cloud-based and on-premises data infrastructure components like databases, data warehouses, data lakes, and data marts.
Create configurable data integrations between disparate systems using APIs, web services, service buses, and middleware tools.
Automate and orchestrate batch and real-time data collection processes using workflow schedulers and stream processing.
Identify and resolve data quality issues, inconsistencies, and integrity problems using profiling, validation, and data cleansing.
Continuously monitor and analyze performance, availability, and scalability of data solutions and optimize as needed.
Collaborate closely with data scientists, analysts, and engineers to gather requirements and deliver data for AI/ML initiatives.
Research, prototype, and evaluate new data technologies like containerization, machine learning ops, and low code ETL.
Develop and implement data security, privacy, compliance, and access control standards and processes.
Create adaptable data models, schemas, mappings, code, and technical design documents.
Work in fast-paced Agile environments - able to iterate rapidly and deliver incremental data solution value.

Basic Qualifications

Bachelor’s degree in MIS, systems engineer, mathematics, computer science, or another relevant technical field;
Minimum of two to four (2-4) years of professional experience in data engineering role;
Significant experience programming in Python with the desire to learn additional programming languages;
Hands-on experience with SQL and relational databases like PostgreSQL, MySQL; Snowflake, Databricks is highly desirable;
Experience with cloud data platforms like AWS, Azure, GCP;
Familiarity with data pipeline, ETL, ELT, and data warehousing concepts;
Experience or familiarity with Tableau or Power BI or similar visualization software;
Understanding of data structures, algorithms, distributed systems;
Passion for learning new technologies and techniques;
Strong analytical and problem-solving skills;
Excellent written and verbal communication skills;
Must be based on East Coast or Central US Time. May consider UK for the right person;
Ability to work independently as well as collaboratively in a team; and/or
Detail-oriented with good organizational skills.

Candidate must be able to submit verification of his/her legal right to work in the U.S., without company sponsorship.

Salary Range- $70,000 - $150,000

About BRG

Berkeley Research Group, LLC (BRG) is a global consulting firm that helps leading organizations advance in three key areas: disputes and investigations, corporate finance, and strategy and operations. Headquartered in California with over 40 offices around the world, we are an integrated group of experts, industry leaders, academics, data scientists, and professionals working beyond borders and disciplines. BRG strives to build and nurture a culture where inclusiveness is instinctive, not an initiative. We celebrate and value the diversity of our professionals and are dedicated to maintaining a truly inclusive work environment where all individuals feel respected and valued. We harness our collective expertise to deliver the inspired insights and practical strategies our clients need to stay ahead of what's next.

Berkeley Research Group is proud to be an Equal Opportunity Employer. Our hiring practices provide equal opportunity for employment without regard to race, religion, color, sex, gender, national origin, age, United States military veteran status, ancestry, sexual orientation, marital status, family structure, medical condition including genetic characteristics or information, veteran status, or mental or physical disability so long as the essential functions of the job can be performed with or without reasonable accommodation, or any other protected category under federal, state, or local law.

Know Your Rights

EEO is the Law Poster Supplement

Pay Transparency Nondiscrimination Provision

BRG is an E-Verify Employer

Right To Work Poster","Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Comunicación, Comunicación oral, Diseño técnico, Limpieza de datos, Modelo de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3871885324/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=lOIqrgkuDepnUAs4xs0Uig%3D%3D&trk=flagship3_search_srp_jobs,Machine Learning Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 meses,"Nueva York, NY","Acerca del empleo
Ideogram

Ideogram is a new generative AI startup looking for a dedicated Machine Learning Data Engineer to join the team. Our mission is to help people become more creative. We are developing state-of-the-art AI tools that will make creative expression more accessible, fun, and efficient. We are pushing the limits of what’s possible with AI, with a focus on creativity and a high standard for trust and safety. Our headquarters is in downtown Toronto, but we have a small presence in NYC too. Check out our text to image product at ideogram.ai to get a glimpse of what we're building.

Founding team

Our founding team consists of world-renowned AI experts including Jonathan Ho, William Chan, Chitwan Saharia, and Mohammad Norouzi. This team has previously led transformative AI projects at Google Brain, UC Berkeley, CMU, and the University of Toronto. Our fundamental work in AI includes: Imagen: Google’s text-to-image system, Imagen Video for video synthesis, Denoising Diffusion Models, WaveGrad for speech synthesis, neural speech recognition, neural machine translation, contrastive learning for learning visual representations, and generative adversarial imitation learning.

About The Role

As a ML Data Engineer, you will build state-of-the-art machine learning data infrastructure. You will build infrastructure to manage large scale datasets, build data processing tools, build data understanding tools, and most importantly research and innovate in the interplay between ML and data.

What We're Looking For

Experience with large scale data processing with MapReduce, Spark, Beam, Kubernetes. 
Experience with SQL and no-SQL databases. 
Python. 
Applied machine learning experience, prior experience in training ML models on large scale datasets. 
Image processing experience appreciated. 
HTML/Javascript/CSS experience appreciated. 

Engineering Culture

We're a small and committed technical team that oversees engineering, research, design, product, and operation. Everyone on the team is willing to do what's necessary to create a delightful product for our customers. We believe that a small dedicated team with a collaborative culture can move faster and build better and more coherent products than large hierarchical organizations.

As a member of the team, you’ll own projects end-to-end, directly engaging with our customers. At Ideogram, we provide mentorship and support to help our employees grow with the company and achieve their ambitious career goals.

Ideogram is committed to welcoming everyone, regardless of gender identity, orientation, or expression. Our mission is to remove exclusivity and barriers and encourage new thinking and perceptions, in a space of belonging. It is not about race, gender, or age, it is about the people.","Aprendizaje automático, Ciencia de datos, Inteligencia artificial, Reconocimiento de patrones y Visión por computador, Aprendizaje automático aplicado, Bases de datos, Ciencias de la computación, Datasets y Procesamiento de datos",Solicitar
https://www.linkedin.com/jobs/view/3902495594/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=K65tl2Y5y%2Bi1veiv3hlqZQ%3D%3D&trk=flagship3_search_srp_jobs,Remote Work - Need Data engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,Estados Unidos,"Acerca del empleo
must have

Data Engineer Experience

building data lakes

databricks

Medallion

delta file

pyspark and spark

seeking a talented Data Engineer with expertise in building data lakes in the Databricks platform using the Medallion architecture and delta file format to join her growing team. This role will contribute to the design, implementation, and optimization of data pipelines, transforming complex data into actionable insights.

Title: Data Engineer

Must Haves

Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
3+ years of experience working with Databricks, PySpark, SQL, Spark clusters, Jupyter Notebooks.
Experience building data lakes using the Medallion architecture.
Understanding of delta tables and the delta file format.
Familiarity with CI/CD pipelines and Agile methodologies and frameworks
Strong understanding of ETL processes, data modeling, and data warehousing principles.
Exceptional problem-solving skills, attention to detail, and ability to work in a fast-paced, collaborative environment.

Pluses

Experience with Power BI and other data visualization tools is a plus.
Knowledge of cybersecurity data, specifically vulnerability scan data is preferred.

Day To Day

Develop, construct, test, and maintain large-scale data processing systems using Databricks, PySpark, SQL, Spark clusters, delta tables, and Medallion architecture.
Collaborate with cybersecurity analysts to understand data requirements and deliver high-quality solutions.
Work closely with the data architecture and analytics team to ensure data quality and implement data governance standards.
Design and implement ETL processes, including data cleansing, transformation, and integration with an understanding of the delta file format.
Build and manage data lakes following the Medallion architecture principles.
Monitor and optimize data pipelines, employing CI/CD practices for efficient development and deployment. 
Collaborate with other team members to implement data analytics projects, utilizing tools like Jupyter Notebooks.
Adhere to Agile methodologies throughout the development lifecycle to promote iterative and collaborative development.","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Jupyter y PySpark, Azure Databricks, Calidad de datos, Ciencias de la computación, Limpieza de datos, Modelado de datos y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979166777/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=yT8oxnuKg2ee2Hn5%2FLpmxw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 1 semana,"Arlington, VA","Acerca del empleo
Description

Data Engineer

Come build the future as a Data Engineer at Amazon, where you will be inspired working along best-in-class inventors and innovators! You will have the opportunity to create meaningful experiences that deliver on the ever-evolving needs of our customers, and your work will impact millions of people around the world.

As an Amazon Data Engineer, you will solve unique and complex problems at a rapid pace, utilizing the latest technologies to create solutions that are highly scalable. You will find that there is an unlimited number of opportunities within Amazon, where developing your career across a wide range of teams is highly supported. We are committed to making your work experience as enjoyable as the experiences you’ll be creating for our customers.

Apply now and you will be eligible for Amazon Data Engineer positions that are based on your preferred location, team, and more. We’re hiring across Amazon Stores in the United States and Canada.

Teams with available positions include, but are not limited to:

 Consumer Technology: Build new generation features and products for amazon.com, constantly improving the Customer and Seller experience for billions around the globe. Whether building site wide features such as reviews and recommendations, category specific software for the likes of Pharmacy, Electronics, Digital Software and Video Games or seller infrastructure, there are a variety of complex problems to tackle using a range of technologies in the design of your technical solutions.
 Operations Technology: Shape the future of transportation planning and execution on a global scale, that impacts hundreds of fulfillment centers, thousands of Amazonians, and millions of customers across the world. Your technology will support thousands of operators worldwide to design, build and run the best-in-class Amazon transportation network. We are building intelligent software to make transportation more reliable, faster, and less costly, providing a better and less expensive experience for our customers.
 Human Resources Technology: Create a seamless experience for millions of Amazonians and/or candidates. Whether supporting technologies for onboarding, time and attendance, compensation, amazon.jobs, or recruiting, you’ll deliver robust feature sets, elegant designs, intuitive user interfaces and systems that make it easy for Amazonians to excel at performing critical business functions.

About Us

Work/Life Balance

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance be-tween your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. We care about your career growth and strive to assign opportunities based on what will help each team member develop into a better-rounded contributor.

Inclusive Team Culture

Here at Amazon, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Key job responsibilities

 Design, implement, and support a platform providing secured access to large datasets.
 Own the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions.
 Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
 Tune application and query performance using profiling tools and SQL.
 Analyze and solve problems at their root, stepping back to understand the broader context.
 Learn and understand a broad range of Amazon’s data resources and know when, how, and which to use and which not to use.
 Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with the increased data volume using AWS.
 Continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for datasets.
 Triage many possible courses of action in a high-ambiguity environment, making use of both quantitative analysis and business judgment.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com LLC - A03

Job ID: A2703397","AWS Lambda, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Datasets, Modelado de datos y Object Storage",Solicitar
https://www.linkedin.com/jobs/view/3945814516/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=re1Cyh7eYVnmrd70818u6A%3D%3D&trk=flagship3_search_srp_jobs,Data Warehouse Engineer,"120 US$K/año - 150 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Irvine, CA","Acerca del empleo
Job Description

We are seeking an experienced data engineer to join our remote team. This role involves transforming raw data into functional data systems, ensuring alignment with business goals for optimal efficiency.

Success in this BI engineering role requires strong analytical skills and the ability to seamlessly integrate data from diverse sources.

If you are detail-oriented, exceptionally organized, and experienced in the field, we want to hear from you.

Responsibilities

 Analyze and organize raw data
 Design and build robust data systems and pipelines
 Evaluate and understand business needs and objectives
 Develop data models in visualization tools for end-user consumption
 Consolidate raw data from multiple sources
 Enhance data quality and reliability through innovative solutions
 Collaborate with developers, data analysts, and business users
 Lead and manage projects
 Mentor and guide junior data engineers

Qualifications

 12+ years of experience as a BI engineer or in a similar role
 Expertise in data models and data warehouse design
 Proficiency in programming languages such as Java and Python
 Knowledge of Data Vault design for data warehouses
 Advanced SQL skills and familiarity with NoSQL databases like CosmosDB
 Comprehensive understanding of data warehousing concepts, including ETL processes and data modeling
 Ability to integrate data from various sources, such as web services, APIs, and file systems
 Knowledge of securing data platforms for Snowflake and Power BI
 Experience in building and maintaining data pipelines
 Proficiency in version control systems like Git, with a solid understanding of branching, merging, and tagging
 Excellent communication and problem-solving skills, with a commitment to continuous learning
 Bachelor’s degree in Computer Science, IT, or a related field, or equivalent experience

Experience With The Following Tools And Technologies

 Snowflake (required)
 QLIK Replicate and QLIK Enterprise Manager
 Matillion ETL
 Azure Cloud (Logic App, Azure Data Factory, Cloud Storage)
 Data Vault Warehouse Design (highly desired)
 Power BI","Almacenamiento de datos, Extraer, transformar y cargar (ETL) y Lenguajes de programación, Bases de datos, Java, Modelado de datos, Modelo de datos, Resolución de problemas, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3982033841/?eBP=BUDGET_EXHAUSTED_JOB&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=hjdGAr1zwMWT0bxxtU8ZJw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 días,"Milwaukee, WI","Acerca del empleo
Element6's client is looking for a Data Engineer to join their team in Milwaukee, WI. This is a key role on their IT Data Team requiring a broad range of skills and the ability to step into different roles depending on the size and scope of the business need. The self-motivated candidate will have proven experience architecting successful data solutions on key projects in a collaborative environment. Success will come from being able to prioritize, deliver value incrementally, problem solve, and manage changing priorities. You will work closely with the Company's business partners and interface with both technical and non-technical colleagues.

Responsibilities: 
Data Architecture
Specialize in data modeling, both 3NF and dimensional, with experience in conceptual, logical, physical, and industry data modeling. Strong knowledge and experience with data architecture methodologies.
Apply the appropriate level of modeling theory, pattern recognition, and abstractions to architect and design a pragmatic solution that functionally meets the business and technical requirements.
Partner with internal business units to define information requirements and translate them into appropriate data solutions.
Collaborate with IT and business partners to lead data discovery, profiling, analysis, and quality assessments in order to obtain clear information requirements.
Develop and validate source to target mappings and transformation logic required to support business needs. Understand the importance of capturing data lineage.
Architect, implement and verify end-to-end data solutions.
Develop test plans needed to ensure a quality deliverable. Participate in validation testing, coordinate user acceptance testing and training to ensure the final implementation enables the user to solve their business problem.
General Data Management
Play a critical role in architecting our data and analytics solution landscape
Demonstrate competence, experience, knowledge, understanding, and advocacy of data management concepts, data warehousing, BI, and analytics.
Demonstrate ability to perform appropriate level of strategic thinking by viewing initiatives both within the immediate project context as well as the overall architectural vision.
Participate and/or Lead in data architectural design and strategy discussions.
Data Delivery
Work with the business users to conduct data discovery engagements and can quickly identify, and prototype, a solution that brings together multiple data sources into one coherent concept and understanding. (data blending)
Leverage existing tools to create data visualizations and mentors the business to be self-sufficient.
Collaborate
Identify and communicate project risks and impediments and proactively work with other members of the Analytics team to complete high-value deliverables as identified by business partners and team leadership.
Partner with Analytics team members to translate business and functional requirements into technical designs
Strive to understand the data consumption needs of the business community, as well as the problems faced by business users involving the access and use of data
Help Analytics teams develop solutions that enable businesses to capitalize on business insights and drive toward gaining a competitive advantage

Requirements: 
5+ years of experience in Data Solution delivery in a complex environment working collaboratively in a team setting
Proficient in Data Solution tools and concepts such as:
Business Intelligence tools: Microsoft tools (SQL Server Management Studio, SSRS, SSAS, Power Pivot, Power Query, PowerBI), Alteryx
Database: SQL Server
Data Query tools: SQL, T-SQL
Data Management and Quality: data mapping, data profiling, metadata repository, relational data modeling, master data management
Data Modeling: ER/Studio Data Architect, 3NF and dimensional modeling
Data Warehousing concepts: Inmon, Kimball, Data Lake
Data Integration concepts and strategies: EII, ETL, EL-T and EAI","Almacenamiento de datos, Arquitectura de datos, Integración de datos, SQL y SQL Server Integration Services (SSIS), ER/Studio, Inmon, Integración de aplicaciones empresariales (EAI), Linaje de datos, Modelos dimensionales, Necesidades empresariales, Perfiles de datos y SQL Server Reporting Services (SSRS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3932052483/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=5MUnchtKcr1G0m%2BCjE2tKw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"140 US$K/año - 150 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Denver, CO","Acerca del empleo
We're hear.com, a tight-knit crew of tech enthusiasts, creators, and innovators dedicated to shaking things up in hearing care. We're not your average company—we thrive on creativity, collaboration, and a passion for making a real impact. Forget the corporate jargon and stiff office culture; we believe in flexibility, growth, and always having fun while doing what we love.

As a Data Engineer at hear.com you will play a pivotal role in architecting, building, and optimizing our data infrastructure. You will collaborate with a cross-functional team of experts to design and implement robust, scalable, and high-performance data solutions that empower our decision-making and contribute to our mission of providing better hearing care.

What You’ll Do

Design and implement scalable and reliable data pipelines, incorporating streaming data processing with Apache Pulsar to handle real-time data efficiently.
Optimize Data Systems: Work closely with peers, analysts, and product teams to improve data reliability, efficiency, and quality.
Innovate: Stay ahead of the curve by researching and implementing cutting-edge technologies that can enhance our data capabilities and leverage our business.
Data Governance and Strategy: Play a key role in defining and implementing data governance and security policies to ensure data integrity and compliance.

Technology Stack

Languages & Frameworks: Python, SQL
Data Warehousing & Streaming: Snowflake, Pulsar, Kinesis
Workflow Management: Airflow
Databases: PostgreSQL, MongoDB
Containers & Orchestration: Docker
Infrastructure as Code: Terraform
Cloud: AWS

What You’ll Need

Experience: 5+ years of experience in data engineering with a proven track record of building and optimizing data systems.
Technical Expertise: Strong programming skills in Python and SQL. Hands-on experience with our technology stack is highly desirable.
Problem-Solving Skills: Ability to tackle complex data challenges and deliver innovative solutions.
Team Player: Excellent communication skills and the ability to work effectively in a collaborative environment.
Continuous Learner: Passion for learning and adapting to new technologies and methodologies.

Why You’ll Love Working With Us

Global Vision and Growth: Be part of a company with a long-term vision and robust growth trajectory.
Customer Impact: Work with happy and grateful customers every single day.
Creative Environment: An open-minded and international working environment that fosters creativity.
Growth Mindset: Access to courses, conferences, and more to support your continual learning and development in an innovative, fast-moving company.
Autonomy and Responsibility: Enjoy a high degree of autonomy and responsibility from day one.
Unique Culture: Innovative, driven, and family-like work culture.
Hybrid Schedules: Enjoy the flexibility of working 2 days remotely and 3 days in our beautiful office in Coral Gables, Florida or Denver, Colorado.
Excellent Benefits & Compensation: Full medical, dental, vision, open PTO, paid company holidays and sick time, paid parental leave, and matching 401K program. Base salary range $140k-$150k, dependent on experience. 
Great Perks: Weekly lunch, a fully stocked kitchen, and a small, close-knit team that values ownership and collaboration.

At hear.com, we're not just shaping the future of hearing care; we're redefining it. Join us in our mission to help everyone hear well to live well.","Ciencia de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Comunicación, Resolución de problemas y Sistemas de datos",Solicitar
https://www.linkedin.com/jobs/view/3982679275/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=x%2BDx06OQT49OiYRLUkWZNg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 día,"Green Castle, MO","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
POSITION

Data Engineer

LOCATION

2x a week onsite in Castle Rock, MO (must be local)

DURATION

12+ Months

INTERVIEW TYPE

Video

VISA RESTRICTIONS

None

Required Skills

TOP SKILLS: Azure (Data Lake, Data Factory), SSIS, SQL Server, (Must have most if not all skills)
Proficient in using Azure DevOps to manage CI/CD pipelines and artifacts
Proficient in performing system and/or integration testing
Proficient in source code management using GitHub
Power BI
Skilled in scripting using Power Shell and Python

Cloud BC Labs Inc is a digital transformation organization aimed at creating seamless solutions for clients to effectively manage their business operations. The company specializes in Business and Management Consulting, AI/ML, Data Analytics & Visualization, Cloud Data Warehouse Migration, Snowflake Implementation, Informatica Implementation & Upgrade, Staffing Services and Data Management Solutions","Azure Data Factory, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , SQL y SQL Server Integration Services (SSIS), Bases de datos, Lagos de datos, Microsoft Azure y Pruebas de integración",Solicitar
https://www.linkedin.com/jobs/view/3946957381/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=BisK5F8p%2BIWvw3d%2BfKwf6g%3D%3D&trk=flagship3_search_srp_jobs,Finance Data Engineer,"73,9 US$K/año - 170,3 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 4 días,"Los Ángeles, CA","Acerca del empleo
Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

The Opportunity 

This is an exciting opportunity to join the Digital Media Finance team as we continue to propel the business through data-driven forecasts and provide influential insights. In this role, you will have the opportunity to help drive Adobe’s pivotal initiatives by data engineering solutions that connect systems to visualization platforms for management reporting. The position will report to the Group Manager, Finance Automation to drive our systems and automation effort. This position will provide an outstanding opportunity to drive timely insights across Digital Media (DMe) Business at Adobe and help drive the Finance Transformational efforts undertaken by the Finance Automation team.

The ideal candidate for this role is a well-rounded top performer with exceptional data engineering, analysis, and visualization skills, capable of driving growth in a fast-paced environment. You should possess a continuous learning mentality to explore and implement innovations in the team's reporting and transformational projects. Strong cross-functional collaboration within Adobe is essential. You will be responsible for automating complex financial models, developing engaging and interactive dashboards, streamlining processes, and enhancing efficiencies in management reporting

What you'll do:

Responsible for the strategy and execution of DMe lifecycle of financial and analytics dashboards and ensuring scalability of systems and processes. 
Manage data transitions and platform migrations between SAP HANA/DataBricks, Tableau/PowerBI, and related ETL processes. 
Develop data pipelines, connections, and infrastructure to better enable forecasting and data science modelling. 
Correct data errors via systematic, logical fixes and provide updates within the data pipelines, connections and infrastructures that are built to support the teams. 
Create documentation and support enablement for the teams who are interested and able to help themselves. 
Accountable for ad-hoc support & participation in critical business analytics and key project support as directed by management. 
Partner closely with IT, Finance Systems, Finance Transformation Office, and other Finance counterparts to continually improve, streamline & enhance planning and reporting processes. 

What you need to succeed:

BS/BA with preferred focuses in areas of Business, Finance, or Information Systems. Master’s in Data Science a plus. 
3+ years of demonstrated experience working with large and complex data structures and develop efficient queries to create calculated fields and data aggregates. 
2+ years of proven experience designing and developing dashboards using PowerBI or Tableau. 
3+ years of relevant experience in data science and analytics in creating/maintaining financial models for a subscription/SaaS business a plus. 
Proficient in data platforms/systems (such as SQL, Databricks), ETL tools (such as Python, SnapLogic), process automation, and standardization. 
Self-starter with high attention to details, excellent interpersonal skills, and ability to take charge, set objectives, and deliver results. 
Strong project management skills with ability to juggle multiple priorities. 
Strong team orientation and a learning mentality. 

Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $73,900 -- $170,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.

Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances.

Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.

Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.

Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, Automatización, Habilidades sociales, Panel de control y SAP HANA",Solicitar
https://www.linkedin.com/jobs/view/3971682950/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=D403k9Kkhb4pMRhv07%2ByAw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer Data Fabric,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Condado de Jefferson, CO","Acerca del empleo
En EY, tendrás la oportunidad de construir una carrera tan única como tú, con alcance global, apoyo, una cultura de inclusión y tecnología para convertirte en la mejor versión de ti. Contamos con tu talento y perspectiva para que EY también sea mucho mejor. Únete a nosotros y construye una experiencia excepcional para ti, y un mejor mundo de negocios para todos.

Aquí en EY centrarás tu inteligencia e imaginación en los temas más cruciales y complejos a los que se enfrentan las empresas, el gobierno y la sociedad actual. A través de retos podrás llenarte de conocimientos y experiencia que desarrollen tus habilidades para contribuir al desarrollo de tu carrera profesional.

La oportunidad

Únete a nuestro equipo y participa en proyectos en proyectos con uno de los clientes más importantes a nivel global, liderados por equipos de Estados Unidos pero interactuando con líderes y compañeros de varios países de América

Principales responsabilidades

Los servicios de Consultoría de EY tienen la amplitud y profundidad para proporcionar asesoría estratégica para ayudar a lograr resultados óptimos y sostenibles. Nuestro equipo global de consultores aporta una gran experiencia y un profundo rigor analítico a cada cliente. Desde el desarrollo de las primeras ideas hasta la implementación de estrategias con acciones concretas, estamos dispuestos a trabajar para definir estrategias que inspiren a sus clientes y empleados a alcanzar el gran potencial de su negocio.

Habilidades y atributos para postularte exitosamente

Para calificar al rol, debes contar con:

Estudiantes con disponibilidad de tiempo completo, Técnico, Tecnólogo o Profesional en Ingeniería de Sistemas, Electrónica o áreas afines a tecnología.
3 a 4 años de experiencia en
 Experiencia práctica demostrada trabajando con Synapse Pyspark 
Experiencia práctica demostrada trabajando con PowerBI (visualizaciones, modelado) ETLEngineering -Azure Stack
Experiencia en codificación con Spark y procesamiento distribuido es una plus
La experiencia de trabajo con Microsoft Fabric es un plus
Inglés avanzado indispensable. (EXCLUYENTE)

Lo que buscamos:

Colaborar en EY es un reto, necesitamos que adoptes nuevas tecnologías todos los días además de ser progresivo e innovador, sin perder de vista los estándares de calidad a los que nuestros clientes están acostumbrados. Además, debes visualizar siempre cómo contribuir a la sociedad, analizando y pensando siempre en cómo tu labor beneficia al mundo, siendo socialmente responsable y procurando el medio ambiente.

Queremos que en EY seas la mejor versión de ti mismo, por eso buscamos que veas por tu bienestar, seas curioso y abraces el cambio.

Te ofrecemos

Trabajar con tecnologías emergentes. Buscar nuevas oportunidades. Reinventarte cada día. Nuestra cultura de innovación en EY significa aceptar el cambio en todo lo que haces, desde la aplicación de nuevas tecnologías hasta la mejora de los procesos existentes. Tus ideas poderosas desbloquearán todo tu potencial, y el nuestro.

Además, podrás tener una carrera personalizada. Cuando tú prosperas, nosotros prosperamos. El paquete de beneficios de EY va más allá, enfocándose en tu bienestar físico, emocional, financiero y social.

Aprendizaje continuo: desarrollarás una mentalidad y habilidades que te lleven a navegar ante el futuro.
Éxito definido por ti: te daremos herramientas y flexibilidad, de esa manera tendrás un impacto significativo de la manera que lo deseas.
Liderazgo transformacional: te daremos las ideas, coaching y la confianza para ser el líder que el mundo necesita.
Una cultura diversa e incluyente: te valoraremos por lo que eres y te empoderaremos para que uses tu voz y así, ayudes a otros a alzar la suya.

Acerca de EY

Como líder global en servicios de aseguramiento, impuestos, transacciones y asesoría, estamos utilizando los productos financieros, experiencia y sistemas que hemos desarrollado para construir un mejor entorno de negocios. Esto comienza con una cultura que te ofrece la capacitación, las oportunidades y la libertad creativa que necesitas para mejorar las cosas. Sin importar el momento en que te unas a la firma, y sin importar cuánto tiempo te quedes, la experiencia excepcional de trabajar en EY dura para toda la vida. Y con nuestro compromiso por contratar y preparar a la gente más apasionada.

Si puedes demostrar que cumples con los criterios anteriores, contáctanos lo más pronto posible.

Envía tu solicitud ahora.","Almacenamiento de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos y PySpark, Bases de datos y Hojas de estilos en cascada (CSS)",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3976487790/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=31zp1mvFIFCOt3EQ7tn%2B%2FQ%3D%3D&trk=flagship3_search_srp_jobs,Program Data Analyst/BI Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 semana,"Boston, MA","Acerca del empleo
Description

Position at Zones LLC.

Position Details:

Term: 12 Month contract

Location: Hybrid - 4 days onsite per month

Hours: Day shift - Mon-Friday 8-5pm ET

Start date: 9/1/24

The Program Data Analyst/BI Engineer (DA/BI) will work with a Team comprised of Program staff, agency business and technical SMEs and the new solution system integrator (SI) and product vendor to design and implement the data information management and reporting strategies, tools, and outputs to provide a robust reporting and data analytics capacity for users of the new Financials solution. The implementation of the new Financials solution will provide users with the ability to run queries and reports on the live data in this application directly rather than waiting for data to be available through warehouse reports. Data will be integrated across modules permitting more robust, real-time reporting and analytics capabilities.

The Program Data Analyst/BI Engineer will work with the Financials Data Lead to identify reporting, dashboard and business analytics needs that will drive the configuration of standard reports and dashboards delivered through the new Financials solution. The DA/BI Lead will assist in leading requirements sessions with staff, agency staff and the system integrator and product vendor to review the reporting and analytics capabilities of the new solution and to identify available configuration options. The DA/BI Lead will develop and maintain a list of selected configurations for each report and dashboard and work with reports/dashboard developers during configuration to assist in translating business requirements to the configured reports. The DA/BI Lead will also participate in testing configured reports and dashboards and in organizing user testing of these items.

Whenever feasible, the Program intends to adopt delivered reports and dashboards, but in instances where reporting requirements cannot be met by the delivered solutions, the DA/BI Lead will work with business and the system integrator and product vendor to identify requirements for future report development and participate in testing of these items when they are developed. The DA/BI Lead will work with business and the system integrator and product vendor to identify opportunities for improving the availability of data for reporting and analytics by understanding the solution’s capacities in this area, sharing that information with business and documenting implementation options with owners of other Commonwealth business intelligence and data analytics resources.

Financials:

The DA/BI Lead will support data access for users in the following areas, working to improve data quality and access in the new solution:

Acquire to Retire (Capital Asset Management)
Cash Management
Cost Allocation
Debt Management
Grants Financial Management
Operating Budget Management
Order to Cash (Revenue/Accounts Receivable)
Procure to Pay (Accounts Payable)
Program Management
Record to Report (Financial Reporting)
Reporting/Analytics
Sub-fund Management
Vendor Management and Self Service

Operations:

Maintenance and Warranty Management
Incident Management
Capacity Management
Availability Management
Performance Management
Security Management
Nightly Batch Jobs Management
Software Change Management and Testing
Disaster Recovery Testing and Management

 What You'll Do As The Program Data Analyst/BI Engineer

The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions

Contribute to the development of data reporting and data access through database queries, reports, and dashboards.
Gather information from agency business users and technical staff regarding the types of data, frequency of reports or dashboards, distribution of reports or dashboards, and other information as needed to understand the current reporting needs of agencies.
Participate in developing an approach to identifying reporting and data analytics needs across the new solution user community. 
Document configuration decisions for each standard report and dashboard and provide guidance to report developers during the configuration stage.
Participate in testing of configured reports and dashboards and assist in coordinating user testing of these materials.
Support and manage enterprise architecture’s data operations (optimize data processing, query performance and data quality) and data pipelines.
Integrate data from diverse sources using ETL processes and transform raw data into datasets used for analysis.
Implement robust security measures, monitor data access and develop access policies.
Provide technological expertise to capture, store and employ data catalogue and lineage to support governance initiatives.
Understand business requirements in the BI context and design data models to convert raw data to meaningful insights.
Perform SQL, DAX queries and functions in Power BI.
Receive training and hands on guidance from the system integrator and product vendor in how to build queries, reports, and dashboards in order to assist reports developers in their work.
Assist in the creation of job aids and other training materials to support report writing by agency users.

Required Skills

Experience in building complex queries, reports and dashboards using a range of web-based tools.
Knowledge of structured data, such as entities, classes, hierarchies, relationships, and metadata.
Proven ability to collaborate with business owners, information architects, content architects and other stakeholders to support common goals and approaches including:
Interviewing managers and other stakeholders to understand their data needs.
Interviewing users to understand what they need from data systems to boost their performance.
Translating business goals, user needs and process improvements into data management functions and requirements.
Explaining the capabilities of data systems to business managers and users as the new solution is designed and configured.
Providing guidance on project scoping to meet stakeholders' requirements.
Assessing whether data is fit for use by performing initial validation of data delivered as part of the project.
Exhibiting an excellent understanding of how data are used within business processes and its impact.
In-depth exposure to data quality concepts, best practices, and tools.
Experience in designing, developing and deploying business analytics dashboards using PowerBI.
Experience in Cloud Data warehouses like Snowflake, Redshift.
Familiarity with Snowflake Data sharing, Snowpipe and Snowpark.
Experience in ETL tools such as Pentaho/Informatica and proficient in SQL.
Well-developed system analysis skills.
Understanding and knowledge of IT standards and controls.
Proficient written and verbal communication and interpersonal skills.

Preferred Qualifications

Experience with Software as a Service cloud implementations particularly those in which legacy on premise applications have been migrated to cloud delivery options.

Minimum Entrance Requirements

Bachelor's degree in computer science, system analysis or a related study, or equivalent experience. 
Minimum of 5 years of design and implementation experience in the areas of business intelligence/reporting and data warehousing.

Minimum Entrance Requirements

Bachelor's degree in computer science, system analysis or a related study, or equivalent experience. 
Minimum of 5 years of design and implementation experience in the areas of business intelligence/reporting and data warehousing.

Zones Offers a Comprehensive Benefits Package

While we’re committed to providing top-tier solutions, we’re just as committed to supporting our own team. Our employees enjoy a variety of comprehensive benefits, including medical/dental/vision coverage, life insurance, a 401(k) plan with matching provision, paid time off, and much more. And as a Minority Business Enterprise, a Corporate Plus member of the Northwest Minority Supplier Development Council, and an Equal Employment Opportunity Employer, our community is just as diverse.

At Zones, work is more than a job –with exciting careers with a global team who are client centric, have a passion for tech, who embrace change and lifelong learning in a collaborative culture. If you’re interested in working on the cutting edge of IT innovation, sales, engineering, operations, administration, and more, Zones is the place for you!

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status or on the basis of disability.","Almacenamiento de datos, Analítica de datos, Extraer, transformar y cargar (ETL) y Herramientas ETL, Administración del alcance, Amazon Redshift, Modelo de datos, Necesidades empresariales, Pentaho y Snowflake cloud",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3983981407/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=4YiKNsZ5iU45RKt%2BdO0C3w%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, TSI Science","Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 4 días,"Seattle, WA","Acerca del empleo
Description

As we strive to be Earth's most customer-centric company, Amazon has reinvented how hundreds of millions of people shop online – providing customers with the opportunity to find and discover virtually anything they want to buy and providing millions of sellers with a platform for growing successful businesses. We are looking for an exceptional data engineer to help us develop new ways to build trust and loyalty with sellers, a crucial component of our flywheel

Sellers’ trust in Amazon is our top priority and in this role, you will be tasked with building that trust over time by building and maintaining the data sources that power all of our decision making. Amazon’s growth requires leaders who move fast, have an entrepreneurial spirit to create new solutions, have an unrelenting tenacity to get things done, and are capable of breaking down and solving complex problems.

The successful candidate will be a self-starter, comfortable with ambiguity and be able to create and maintain efficient & automated processes. They know and love working with data engineering tools, can model multidimensional datasets, and can partner effectively with business leaders to build the right data pipelines to answer key business questions. They will build efficient, flexible, extensible, and scalable data models, ETL designs and data integration services. They will also be required to support and manage growth of these data solutions. They are analytical and creative, and don’t quit. This is a role with high visibility to senior leadership and with high opportunity for impact for those willing to roll up their sleeves and dive deep to achieve results.

Key job responsibilities

Develop data products, infrastructure and data pipelines leveraging AWS services (such as Redshift, Kinesis, EMR, Lambda etc.) and internal Amazon tools.

Develop new data models and end to data pipelines.

Collaborate with Senior Data Engineers, BI Analysts, and Software Development Engineers on data delivery best practices.

Contribute to infrastructure planning and operational excellence continued improvements.

Participate in design reviews and lead reviews for mid scale solutions.

About The Team

Here at Selling Partner Services, we embrace our differences. We are committed to furthering our culture of inclusion. We have 14 employee-led affinity groups, reaching 10,000+ employees in chapters globally. We have innovative benefit offerings, and we host annual and ongoing learning experiences, including our DEI Ambassador Program. Amazon’s culture of inclusion is reinforced within our 16 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com Services LLC

Job ID: A2711174","AWS Lambda, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Kinesis, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Modelado de datos, Modelo de datos y Object Storage",Solicitar
https://www.linkedin.com/jobs/view/3980495570/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=u%2FwGvz826tlSSPqo9UJ6bw%3D%3D&trk=flagship3_search_srp_jobs,Analytics Engineer/Data Engineer,"90 US$K/año - 120 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Madison, WI","Acerca del empleo
Education Analytics is a non-profit organization that strives to deliver sophisticated, research-informed analytics to educators and school administrators to support their work in improving student outcomes. The Data Engineering team enables this mission by ensuring that the data educators receive is accurate, up to date, secure, and easily accessible.

Learn about EA
Connect with EA! Learn more about EA on our blog and our website.

Position Description
We are seeking a full-time Data Engineer or Analytics Engineer to join our team. The person in this role will lead the design, build, and maintenance of automated data pipelines and analytic systems. An ideal candidate has strong SQL skills, experience with data warehousing concepts, and familiarity with complex data integration. The Data Engineering team is responsible for extract/load processes, data warehouse design, data transformations, and data modeling. While we want our team members to have familiarity with all aspects of ELT, we welcome specialists who may be more experienced on either the data engineering or analytics engineering side. Our current stack uses an ELT approach via Apache Airflow and dbt to create data warehouses in Snowflake.

Our team is working to change the education sector by building frameworks to enable collaboration across school systems. To do this, we use open data standards and open-source tools so that solutions we create can be used and re-used in different education agencies across the country. We build and maintain the Enable Data Union project (https://enabledataunion.org/) and open-source projects like Earthmover (https://github.com/edanalytics/earthmover). Our team’s work includes building tools and creating and managing data systems using those tools.

These posts illustrate some projects that members of our team might work on:
A Day in the Life of a Data Engineer at EA
https://www.edanalytics.org/blog/day-in-the-life-q-a-with-senior-data-engineer-tom-reitz
Introducing Enable Data Union, an open framework for analytics and data warehousing with Ed-Fi data
https://www.edanalytics.org/blog/introducing-enable-data-union-an-open-framework-for-analytics-and-data-warehousing-with-ed-fi-data
Turning public data into custom insights: How EA has built a public data platform for our partners
https://www.edanalytics.org/blog/turning-public-data-into-custom-insights-how-ea-has-built-a-public-data-platform-for-our-partners

Responsibilities
Lead the design and implementation of data warehousing structures for research, analytics, and reporting/dashboarding
Apply best practices from software engineering to the development of ELT data pipelines
Implement code testing, continuous integration, and deployment strategies to ensure system reliability
Structure data in organized, intuitive ways applying data modeling techniques and practices
Design and implement complex pipelines to integrate data coming from a mix of APIs, flat files, or other database sources
Develop and improve internal tools and systems to empower our team to work more efficiently
Collaborate within a team of analysts, school system leaders, and other engineers to create analytics solutions that are scalable and support research
Explore and apply new cutting-edge tools to drive innovation across a variety of projects
Proactively identify and defend against potential data quality & processing issues

Qualifications
Experience architecting data warehouse and data lake structures that are intuitive and performant
Knowledge of best design practices in cloud-based data warehouses
Knowledge of software engineering best practices, particularly in team-based development using Git
Experience designing, implementing, and maintaining modern ELT pipelines
Fluency in SQL
Experience with Python
Experience with Linux

Bonus Skills:
Experience with cloud-based columnar data warehouses and other cloud data technologies (Snowflake, RedShift, Databricks, MirageDB, Fabric, BigQuery)
Experience with Data Build Tool (dbt)
Experience with Apache Airflow or other modern data pipeline systems
Familiarity with AWS tooling and best practices

Hiring Process
Hiring team reviews resumes, cover letters, and application question responses.
Selected candidates are invited to a 30-minute interview with two Data Engineering team members to discuss skills, experience alignment, and interests.
Selected candidates are sent a technical skills exercise. Hiring team reviews exercise submissions.
Selected candidates are invited for a full day final interview (virtual or in person in our downtown Madison office). There will be another approximately 2-3 hours of interviews to meet other Data Engineering team members and key members of other teams and to help candidates learn more about Education Analytics & the role.

How you will successfully onboard in this role
First 30 days: Work through organizational and team onboarding. Get situated into Data Engineering team meetings and one on ones with your manager. Complete a training exercise our team has developed that familiarizes our new hires with our development setup and tooling.
First 60 days: Begin to get involved in your first projects, familiarizing yourself with the project history, context, and goals by joining project meetings and connecting with teammates. Continue familiarizing yourself with our team’s workflow processes and tools.
First 90 days: Now fully integrated into your first projects, making regular code and documentation contributions. You may start sharing your development at our team meetings. We will start planning for you to take a lead role on sub-tasks within a project.

Additional details
The weekly expectation is 45 hours per week, and nights and weekends are sometimes required. Our preference is for candidates to primarily work from EA’s office in Madison, WI. We are open to hiring a remote team member for the right candidate

Compensation and Benefits
The salary for the Data Engineer (Analytics Engineer) position is $90,000-$120,000, based on experience. EA also has a generous benefits package including:
A 12% employee salary contribution from EA to your 401k retirement plan
An additional 3% salary match by EA to your 401k
26.5 days of paid vacation annually + sick paid time off that accumulates per pay period
9 paid holidays of your choosing
93% of health insurance premium paid for by EA
Paid parental leave (if eligibility requirements are met)

EA’s primary location is in downtown Madison, WI, on the Capitol Square. Steps away from coffee shops, a weekly summer farmers’ market, restaurants, shops, and two lakes. Many staff walk, bike, or use public transportation to commute to a well-appointed office.

Equal Employment Opportunity
Education Analytics is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.",Ingeniería de datos,Solicitar
https://www.linkedin.com/jobs/view/3979446444/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=0%2BCSOPAvbhhOUm4HSzsKKw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II - ETL/SQL/Python/AWS,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Hartford, WI","Acerca del empleo
Who Are We?

Taking care of our customers, our communities and each other. That's the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.

Job Category

Data Analytics, Technology

Compensation Overview

The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.

Salary Range

$121,000.00 - $199,600.00

Target Openings

1

What Is the Opportunity?

Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate stories found in data by leveraging a variety of data programming techniques. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights. What Will You Do?

Build and operationalize complex data solutions, correct problems, apply transformations, and recommending data cleansing/quality solutions.
Design complex data solutions
Perform analysis of complex sources to determine value and use and recommend data to include in analytical processes.
Incorporate core data management competencies including data governance, data security and data quality.
Collaborate within and across teams to support delivery and educate end users on complex data products/analytic environment.
Perform data and system analysis, assessment and resolution for complex defects and incidents and correct as appropriate.
Test data movement, transformation code, and data components.
Perform other duties as assigned. 


What Will Our Ideal Candidate Have?

Bachelor's Degree in STEM related field or equivalent
Eight years of related experience
Highly proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and a full understanding of modern software engineering practices.
AWS Cloud Services, DataBricks, ETL, Python, PySpark experince.
The ability to deliver work at a steady, predictable pace to achieve commitments, deliver complete solutions but release them in small batches, and identify and negotiate important tradeoffs.
Demonstrated track record of domain expertise including understanding technical concepts necessary and industry trends, and possess in-depth knowledge of immediate systems worked on and some knowledge of adjacent systems.
Strong problem solver who ensures systems are built with longevity and creates innovate ways to resolve issues.
Strong written and verbal communication skills with the ability to work collaborate well with team members and business partners.
Ability to lead team members and help create a safe environment for others to learn and grow as engineers. and a proven track record of self-motivation in identifying opportunities and tracking team efforts.


What is a Must Have?

Bachelor's degree or equivalent training with data tools, techniques, and manipulation.
Four years of data engineering or equivalent experience.


What Is in It for You?

Health Insurance: Employees and their eligible family members - including spouses, domestic partners, and children - are eligible for coverage from the first day of employment.
Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.
Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.
Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs.
Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.


Employment Practices

Travelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.

In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions.

If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.

Travelers reserves the right to fill this position at a level above or below the level included in this posting.

To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.","Extraer, transformar y cargar (ETL), Gobierno de datos, Ingeniería de datos , PySpark y Python, Análisis de sistemas, Calidad de datos, Datos de prueba, Limpieza de datos y Prácticas recomendadas en ingeniería de software",Solicitar
https://www.linkedin.com/jobs/view/3984926607/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=AGsi68rCBFOp3fNgFveiow%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Media jornada
Coincide con tus preferencias de empleo. El tipo de empleo es Media jornada.
Intermedio",hace 2 días,"San Francisco, CA","Acerca del empleo
This is a remote position.

Staff Software Engineer

About Us: GrabaJobs helps engineers like you land job offers with maximum compensation and benefits by sending tailored job applications and cover letters to Fortune 500 and FAANG companies.

Job descriptions:

They are partnered with an incredibly exciting smart debit card scale-up to onboard a new CTO in a strategic position while working closely with the Engineering team.

Our client is on a mission to give users flexibility on every day's savings and investing to take advantage of market movements and to utilize the upside of stocks, cryptocurrencies, and ETFs quickly and easily. They partnered with companies such as Visa, Wyre, and Cardlytics in the Payments domain to accelerate their missions of revolutionizing consumer spending.

What they're looking for:

Someone with Strong FinTech (ideally Payments) background who can bring value to the team with their industry knowledge 
Be able to bridge the gap between CTO and COO, both operational and strategic focused, and marry that with KPIs, business metrics 
Strong technical background, having previous experience working with start-ups companies 
Proven previous experience in driving business growth and fundraising 
Excellent leadership skills; experience in a leadership team, growing teams, and driving high performance 
Payments start-ups experience would be a huge bonus! 

If you're looking for an exciting next step, and this sounds like something that would both challenge and excite you, then this is for you!

Benefits:

Competitive salary 
Bonus + Equity stock option 
Healthcare/vision/dental 100% coverage 
Learning and development resources 
401k 
Unlimited PTO 
Fully Remote and flexible working environment 
Care for employees' financial, physical, and mental health 

DISCLAIMER: This job posting is intended for the active pooling of candidates who will become part of our talent pool. Your qualifications will be assessed against both current and future opportunities. If your application aligns with a role that corresponds to your skills and experience, and an opportunity arises, our recruitment team will reach out to you immediately.","Almacenamiento de datos, Big data, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Indicadores clave de desempeño, Informática de alto rendimiento y Liderazgo",Solicitar
https://www.linkedin.com/jobs/view/3984544956/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=7ttoIOZg5EfxW6cG0g7dPg%3D%3D&trk=flagship3_search_srp_jobs,Sr. Data Engineer (Blockchain),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,Estados Unidos,"Acerca del empleo
Please see these samples taken from resumes so that you can see the type of candidate the client is looking for. This will help you get the interview. Please let me know if you have any questions.

SAMPLE #1 Resume

Digital Assets Fund, Blockchain Data 

Reduced annual data spend by $190k by scrapping websites and directly connecting to blockchains using Go RPCs and nodes hosted on cloud (AWS, Google BigQuery) then parsed data using Python and visualized on Streamlit. 
Automated dashboards and reports, which ran through manual intervention before, using AWS lambda saving :90 hours of monthly labor. 
Built a python library and API that abstracts complex blockchain queries for smart contract platforms (including Ethereum, Solana, Cardano, Cosmos etc.) 

SAMPLE #2 Resume

Data Analytics Engineer 

Deep Liquidity Analysis - Implemented real-time crypto exchange order book streaming system using Python, Airflow, Snowflake, Kafka. Created data pipeline and maintained the ETL pipeline for data dashboard. 
Data Platform Management - Built and maintained a centralized data platform on Azure SQL, Python, Airflow and Snowflake. Developed and maintained over 100+ ETL pipelines, powering >10 PetaByte data sources. 
Blockchain data analytics - Created data engineering workflow using Python, Snowflake, Azure, Zendesk API, and machine learning classification to predict wallet downtime, resulting in improved prioritization of customer support tickets, increased agent productivity and reduced operational costs for Wallet Operations team. Presented KPI's using a Streamlit dashboard. 
Client OPS for Order Spoofing - Assisted in the deployment of machine learning model for trading related fraud detection by implementing a scalable machine solution using technologies such as Spark, Docker and Kubernetes. 
Experimentation Platform : Built the data pipeline for experimentation platform using Python, Snowflake, Airflow and Streamlit to automate A/B testing for marketing team. 
Crypto Market research - Conducted weekly research and analysis on latest crypto protocols( AMM, consensus algorithms, Tokenomics etc.) to help wallet, Finance team. 

For this role someone that has been a data scientist before Data Engineer would be a successful fit for this role.

This is FTE but if you have a SOLID and close to perfect fit, they will consider a contract.

If you have any solid Data Engineers, not data developers with excellent Python and pulling large amounts of data (don't want bit data engineers) and excellent English with a desire to learn blockchain or have the experience, please send them to me.

They are looking for advanced skills in Python. R experience could be swapped out, but they would have to adapt very quickly. 
They are looking for a true Data Engineer, not a Data Scientist. 
Web3 experience - They do prefer Web3 experience. Since it's not common then they will talk with someone who meets the other criterias first (mentioned above) + blockchain interest. 
Hands on exp AWS cloud - google could work. 
Pandas exp desired
Any crypto/blockchain companies experience works

They are only open to US Citizens and Green card holders

The candidate must have EXCELLENT communication skills

Interview process: 

Interview process: 30 mins with Johnathan
1 hour video tech - review resume/experience, skills, topics of project
Presentation to team
30 mins with CTO then CEO - general convo to get to offer

Sr. Data Engineer (Blockchain) - (Remote in the US) - 1571

Location: 100% Remote (within US)

Position Type: FTE - no contracting

Work Authorization: No sponsorship offered. USC and GC only

Communication: Excellent in verbal, written and comprehension

The Role:

You'll work with a cross-functional team of research scientists and engineers to contribute to our software and data platforms. 
Drive the design and implementation of data models, ETL pipelines, and data processing logic while working closely with the other teams in order to translate operational data with microservices into business value. 
Collaborate in proof of concepts (POCs) from research concepts and then lead building those in a production environment. 
Work on complex problems ranging from mathematical proofs to big data processes to fueling research with blockchain, financial, and gaming metrics. 
We are looking for someone with a curiosity for data and the ability to iterate quickly in an ambiguous environment. 
Preferred experience with finance, gaming/crypto gaming, blockchain, or crypto protocols. 
Bonus Domain expertise in one or more areas: finance, gaming, DeFi, cryptocurrency, blockchain, and smart contracts. 

Required Experience

Advanced skills with python (or similar language) for data collection, analysis, and communication including familiarity with pandas package, object-oriented programming, and built-in functions. 
Experience in Web3 including smart contract development, blockchain data, and DeFi protocols such as Uniswap. 
Hands-on experience in AWS Cloud services including building and implementing server and serverless data engineering, data science, and analytics infrastructure. Bonus for experience integrating services such as Airflow. 
Experience communicating and delivering results with data scientists, machine learning, and modeling valuable to implementing use cases. 
Understanding of Test-Driven Development. 
Good knowledge of ETL, data warehousing, and pipelines design. 
Preferred 5+ years of experience in a data focused role including ETL, querying data with SQL, data modeling, data collection, data analysis, and data science. 
Curiosity, empathy, and an interest in working with a team with a high degree of collaboration. 
An ability to adapt and work under large amounts of uncertainty and ambiguity.","Airflow, Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Pandas (Software) y Python, Comunicación, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3848279824/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=U%2FqlvzcxdWsP91PW86CSQw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 meses,"Atlanta, GA","Acerca del empleo


Business Unit: ITS/Enterprise Data Services
Department: 874
Location: ATL/Admin
Number of approved openings: x

Job Title & Code: Data Analyst (Data Engineer)

Job Description:

The Opportunity
The Data Analyst will be a key part of the Enterprise Data Services team, which is responsible for transforming data from disparate systems to provide insights and analytics for business stakeholders. Specifically, this role will support a multi-year project to migrate legacy data jobs and reports owned by business areas across the company to cloud-based technology solutions and modern data tools. You will collaborate with Data Engineers, Data Analysts, DBAs, cross-functional teams, and business teams. You will analyze, classify, migrate or redesign data jobs/reports into modern data tools, using Agile methodology, that empower users to make informed business decisions. Progress will be documented and validated against current report outputs. [Note: Delta IT works in a rotating hybrid schedule with 2-3 days in the office per week - full remote work is not authorized].

Candidate
You are self-motivated, work independently, and have direct experience with data tools, data analysis, data engineering, data consulting, etc. You have a deep understanding of the full life data lifecycle and the role that high-quality data plays across applications, business analytics, and reporting. Strong candidates will exhibit solid critical thinking skills, the ability to resolve technical problems, and a talent for transforming data to create solutions that add value to business requirements.

You have the demonstrated ability to take ownership of assigned technical projects in a fast-paced environment. You have solid written and speaking communication skills as we work in a collaborative cross-functional environment and interact with many business divisions. You demonstrate curiosity and proficient interpersonal “soft” skills. Ideal candidates have more than just knowledge or skill set, as they also have a “can do” mindset to find solutions and learn what is needed.

Quals--
Position title : Data Analyst

Qualifications
• Bachelor of Science degree in Computer Science or equivalent
• At least 5+ years of post-degree professional experience
• Required experience in SQL and other querying languages to convert into modern data tools or jobs
• Strongly preferred experience in data analysis to create, publish, and/or manage reports and data visualizations in Tableau, PowerBI, AWS Quicksight, or AWS DataZone (i.e., our target solutions)
• Highly desirable experience implementing performance tuning in Tableau dashboards
• Highly desirable experience working with TOAD data tool
• Demonstrated experience building and preparing data for analytics
• Proficiency working with database technologies and data development such as Python, PLSQL, etc.
• Experience working with queries/applications, including performance tuning, utilizing indexes, and materialized views to improve query performance
• Identify necessary business rules for extracting data along with functional or technical risks related to data sources (e.g. data latency, frequency, etc.)
• Strong understanding of performing test cases for profiling data, validating analysis, testing assumptions, driving data quality assessment specifications, and define a path to deployment
• Familiar with best practices for data ingestion and data design

Equal opportunity employer including disability/veterans.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Pensamiento crítico, Calidad de datos, Ciencias de la computación, Comunicación y Lenguaje de consulta (query)",Solicitar
https://www.linkedin.com/jobs/view/3981359082/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=tCXUKcHVibMLkS80ZozlCw%3D%3D&trk=flagship3_search_srp_jobs,BigQuery Data engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,"Dallas, TX","Acerca del empleo
Dallas, TX (Hybrid)

6 months

Summary

We are seeking a highly skilled BigQuery Consultant (BI Consultant) to join our team, on-site in Dallas. This role involves designing and implementing data solutions with a strong focus on BigQuery and leveraging Google Cloud Platform (GCP) tools. The ideal candidate will provide consulting on best practices, be hands-on in executing solutions, and automate team deliverables. This position is a senior-level role requiring expertise in teaching and training team members.

Responsibilities

 Solution Design: Design data solutions and architectures in BigQuery, focusing on best practices and optimal performance
 Consulting and Best Practices: Provide consulting on BI best practices, particularly with BigQuery, to improve team efficiency and effectiveness
 Hands-On Implementation: Actively participate in the development and implementation of data solutions, including automation of existing processes
 Technology Stack: Work with Google Cloud services, including Google Fusion, Google Platform, and Google Stack
 SQL to Python Transition: Guide the team in transitioning from SQL to Python for data processing tasks
 Project Management: Manage projects and efforts, ensuring timely delivery and high-quality outputs
 4-5 years of hands-on experience working with BigQuery
 Extensive experience and ability to teach and train team members
 Experience with Google Analytics (GA) data stored and analyzed in BigQuery
 Proficiency with Power BI or other data visualization tools
 Strong skills in SQL and Python for data processing and analysis

Nice To Have

 Experience in eCommerce environments is a plus
 Experience with Business-to-Business (B2B) or Business-to-Consumer (B2C) environments
 Familiarity with Google Cloud services, including Google Fusion, Google Platform, and Google Stack","Analítica, Analítica de datos, Ciencia de datos, Google BigQuery, Ingeniería de datos , Minería de datos y Python, Diseño de soluciones empresariales, Diseño de soluciones técnicas y Procesamiento de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984549223/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=qc7eyUGZDPahFxjOd6Vk3A%3D%3D&trk=flagship3_search_srp_jobs,Senior Big Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 semanas,"Dallas, TX","Acerca del empleo
Position: Senior Big Data Engineer

Location: Dallas, TX

Duration: 12+ Months

Job Description:

 Must be highly hands-on lead, writing code, leading team, reviewing other developers code, working closely with the architect to implement the proposed design
 5+ years of experience writing enterprise grade Java or Scala or Python code (should be highly proficient)
 Solid understanding of data structures and fundamental algorithms (sort, select, search, queue)
 Solid understanding of distributed computing and/or massively parallel processing concepts and frameworks (at least one): Spark, Kafka, MapReduce, Impala
 2+ years of experience in some of the Big Data technologies: Hadoop (HDFS, Hive, Impala) or Kafka or Spark
 2+ years of experience building enterprise data platforms: Data Ingestion, Data Lake, ETL, Data Warehouse, Data Access Patterns/APIs, Reporting
 Decent data warehousing and data modeling skills
 Experience working in Linux
 Spring Boot/APIs implementation experience is a nice to have
 Azure experience is a nice to have
 Databricks experience is a nice to have

Skills: Hadoop, HDFS, Kafka, Spark, Datalake.

Full Name Mobile or Google Number Email ID: Address with Zip Code: Work Authorization Hourly Rate LinkedIn Profile: Education: US Work Experience DOB Last 4 digits of SSN (Optional) Visa Copy & DL Copy Interview Availability Vendor Details","Apache Kafka, Extraer, transformar y cargar (ETL), Hadoop, Hive, MapReduce, Python y Scala, Apache Impala, Ingesta de datos y Java",Solicitar
https://www.linkedin.com/jobs/view/3984545028/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=dsKLUgDR8MOrAi94wkZBGA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"San Antonio, TX","Acerca del empleo
Summary

This position is a(n) Exempt position, supporting the ARNG 16 AF SPPFS. The Data Engineer supports missions and functions for 16 AF Service needs and Air Forces Cyber (AFCYBER).

Responsibilities And Duties

Supports/integrates enterprise database systems, using sound database management practices to organize and store data.
Interacts with development and end-user personnel to determine application data access requirements, transaction rates, volume analysis, and other pertinent data required to develop and maintain integrated databases.
Provides support and reviews web-based applications including on-line and internal applications to support client operations. The Web Provides support in developing the site concept, interface design, and architecture of the website.
Supports the implementation of interfaces to applications, reviews both client side and server-side code to allow web-based applications to deliver the correct content.

Position Requirements

Qualifications

Be able to translate applications requirements into the design of complex web sites, including integrating web pages and applications.

Must be able to apply new and emerging technologies to the site development process.

Education Requirements

Bachelor's Degree in Computer Science, Information Technology, Database Management or related fields

Proficiency in programming languages such as Python, Scala, and SQL preferred

Additional certifications such as CCP, ABDE, or AWS Certified Data Analytics preferred

Additional Requirements

Must be able to obtain and maintain a TS-SCI clearance and meet other eligibility requirements for access to classified information.

EEO is The Law - click here for more information

Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled

Valiant Integrated Services is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law.

NOTICE: Valiant Integrated Services NEVER asks job applicants to issue any payment or service fees to Valiant or its recruiters as part of our application process. Before providing any personal information to outside parties, verify that the job you are applying for appears on our Careers site.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Administración de bases de datos, Bases de datos, Ciencias de la computación, Páginas web y TS",Solicitar
https://www.linkedin.com/jobs/view/3833864755/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=jz9hQEZhRyFICjn1H%2BrcTQ%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Programmer/Coder/Developer/Data Scientist/Analyst/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Scottsdale, AZ","Acerca del empleo
At SynergisticIT, we aim to bring aboard IT professionals to help them build a rewarding career in cutting-edge technologies. Being in the industry for more than 10 years, we provide a splendid range of lucrative opportunities to sustain a position in our top tech clients like Google, Apple, Cognizant, Client, PayPal, to name a few.

Our seasoned team firmly believes that the new tech talent can scale any business if given the right opportunity. We value your integrity, hard work, and commitment to make a difference in the technical sphere. For this reason, we focus on providing end-to-end career assistance and enhancing your already existing IT skills and knowledge.

Currently, we are looking for qualified entry-level Data Scientists who can apply Data Science principles to design, test, implement, and develop data-based solutions, including reporting, auditing, and preparing large databases for statistical analysis.

Qualifications

 Minimum Background and Qualifications Requirement 

 Bachelor's degree or Master's degree in Computer Engineering, Computer Science, Mathematics, Electrical Engineering, Information Systems, or IT 
 Must have Mathematics or Statistics background 

Required

 Technical and Soft Skills Required 

 Experience in Python programming and understanding of the software development life cycle. 
 Knowledge of Linear Algebra, Statistics, and Mathematics concepts. 
 Excellent written and verbal communication skills. 
 Highly motivated, self-learner, team player, and technically inquisitive. 
 Strong work ethics and creative problem-solving abilities. 

 Preferred skills 

 Deep Learning 
 Data visualization 
 NLP 
 Scala 
 Django 

 Roles and Responsibilities 

 Collaborate with dynamic teams of engineers, developers, and scientists who research and integrate algorithms to develop an application, software, and computer system solutions to address complex data problems. 
 Assess project requirements and develop data analysis algorithms. 
 Engage developers to share their opinions, knowledge, and recommendations to meet the deliverables. 
 Contribute to technical solutions and implement software analyses to unlock the secrets held by big data sets. 
 Integrate components like web-based UI, commercial indexing products, and access control mechanisms to create operational information and knowledge discovery systems. 

Benefits

 Competitive salary 
 Flexible work schedule & part-time off 
 E-verified 
 No relocation 
 H1B filing 
 On job technical support 
 Skill Enhancement 
 Opportunity to work with Fortune 500 Companies 

 Who Should Apply? 

Recent IT graduates looking to build a solid career in the tech industry. If you're lured by the endless possibilities presented by AI, Machine Learning, IoT, and Data Science, this job opportunity can be the right career path for you.

 Candidate's Outcome  : Best Programmers in USA | Best Coding Bootcamp - SynergisticIT

 No third-party candidates or c2c candidates 

 If you are interested, please apply to the posting. 

 No phone calls please, Shortlisted candidates would be reached out.","Analítica de datos, Análisis de datos, Programación y Python, Ciencias de la computación, Competencias transversales, Comunicación, Desarrollo de software, Matemáticas y Resolución creativa de problemas",Solicitar
https://www.linkedin.com/jobs/view/3965815700/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=Oy%2B7zqC4wa5RldF%2FKbiGRg%3D%3D&trackingId=P4NDH2egWBezQdve9y0QyA%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Developer/Coder/Programmer/Data Scientist/Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 semanas,"Huntsville, AL","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech Job market by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries.

 please check the below links to see success outcomes, salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please visit the below videos exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

 https://synergisticit.wistia.com/medias/tmwjwchxz5

 https://synergisticit.wistia.com/medias/n8487768di

 https://synergisticit.wistia.com/medias/o5gmv7i9eu

 https://synergisticit.wistia.com/medias/k6t6a1n4kb

 https://synergisticit.wistia.com/medias/pgrvq4fgni

 https://synergisticit.wistia.com/medias/ce4syhm853

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciencias de la computación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3872342937/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=yz90vCZ%2BpBcvJNHZnIgVXQ%3D%3D&trk=flagship3_search_srp_jobs,Data/ETL Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 4 meses,Estados Unidos,"Acerca del empleo
Title: Data/ETL Engineer (Pharmacy Data Focused)

Location: Remote (Anywhere in the USA)

Duration: 9-12 Months Contract

Implementation Partner: Sierra Solutions Group

End Client: To be disclosed

Jd

Required Experience:

10 years of Data/ETL experience
Pharmacy data experience required (anyone from a healthcare company that offers pharmacy benefits etc.)
ETL - Python/AWS GLUE is core dev skills experience.
Data aggregation, developing ETL from disparate DWH within a healthcare environment is critical. 
Redshift/CDK/Terraform Cloud experience.","Almacenamiento de datos, Extraer, transformar y cargar (ETL), Python y SQL Server Integration Services (SSIS), Bases de datos, DWH, DataStage, Hojas de estilos en cascada (CSS), Informatica (empresa) y Programación orientada a objetos (POO)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3941629529/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=l2osV%2BJNXeNYZJn8WdXU%2Bg%3D%3D&trk=flagship3_search_srp_jobs,ETL/Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,Estados Unidos,"Acerca del empleo
Title: Data/ETL Engineer

Mode of interview: Video Interview

Location: Remote

Duration-12+ months

Required Skills

Must be well versed with Data warehousing concepts including design patterns (Star schemas, Snowflake design schemas). Must be aware of data modeling concepts including the data modeling Normal forms 
Knowledge of AWS Infrastructure including S3, SNS, Ec2, CloudWatch and RDS 
7+ years working in with one or more related products such as Informatica or Talend or Microsoft SSIS ETL/Data transformation projects
7+ years working in , data warehousing initiatives business intelligence
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases like Oracle or Ms Sql Server or Vertica. 
Good to have at least 1+ years working with Matillion ELT tool and Snowflake Database
Good to have experience in building AWS Data Pipelines using Python or Spark, SparkSQL in any of Cloud Environments (AWS / Azure / Google). Good to have - Experience with any of the NoSQL datastores such as ElasticSearch, MongoDB, DynamoDB, Cassandra

Subham Kumar

Email: subham.kumar@stiorg.com

Contact: -6094473345","Almacenamiento de datos y Extraer, transformar y cargar (ETL), Almacenamiento, Inglés como lengua extranjera, Matillion ETL, Modelado de datos, Patrones de diseño, Schemas, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3929087956/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=16vPAK5DfwGRpGlbu6m9vw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer 3 (REMOTE),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Temporal
Coincide con tus preferencias de empleo. El tipo de empleo es Temporal.
Intermedio",hace 2 meses,Estados Unidos,"Acerca del empleo
DivIHN (pronounced “divine”) is a CMMI ML3-certified Technology and Talent solutions firm. Driven by a unique Purpose, Culture, and Value Delivery Model, we enable meaningful connections between talented professionals and forward-thinking organizations. Since our formation in 2002, organizations across commercial and public sectors have been trusting us to help build their teams with exceptional temporary and permanent talent.

Visit us at https://divihn.com/find-a-job/ to learn more and view our open positions.

Please apply or call one of us to learn more

For further inquiries regarding the following opportunity, please contact one of our Talent Specialists

Sivanesan at 224 369 0756

Title: Data Engineer 3 (REMOTE)

Location: Remote

Duration: 8 Months

Description

 Maintain, build, and optimize data warehouse, developing ETL solutions, automated data flow, and analytics “as-a-service” environment.
 Design and develop scalable, high-performance data processing solutions on AWS using tools like AWS Lambda, Step Functions, Amazon Redshift, and AWS Glue.
 Collaborate with stakeholders across the organization to understand business requirements and design data solutions that meet their needs
 Monitor actively the health of the environment in terms of data integrity, data pipeline performance, and overall security, to ensure resolutions are developed, implemented, and communicated efficiently
 Implement and maintain data governance policies and procedures to ensure data quality, security, and compliance
 Develop and maintain data architecture and infrastructure to support the growth and expansion of the data platform
 Design and develop tools to constantly monitor the health of the environment and ensure incidents communicated efficiently and resolved quickly.
 Mentor and guide junior data engineers in the team
 Drive and implement automation to empower the organization to scale efficiently and with flexibility
 Keep up to date with new technologies and methodologies related to data engineering and apply them to improve the data platform

Summary: The main function of a data engineer is to coordinate changes to computer databases, test, and implement the database applying knowledge of database management systems. A typical data engineer is responsible for planning, coordinating and implementing security measures to safeguard the computer database.

Job Responsibilities: Test programs or databases, correct errors and make necessary modifications. Modify existing databases and database management systems or direct programmers and analysts to make changes. Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions.

Skills: Verbal and written communication skills, problem solving skills, customer service and interpersonal skills. Basic ability to work independently and manage one�s time. Basic knowledge of database management software.

Education/Experience: Associate's degree in computer programming or equivalent training required. 5-7 years experience required.

About Us

DivIHN, the 'IT Asset Performance Services' organization, provides Professional Consulting, Custom Projects, and Professional Resource Augmentation services to clients in the Mid-West and beyond. The strategic characteristics of the organization are Standardization, Specialization, and Collaboration.

DivIHN is an equal opportunity employer. DivIHN does not and shall not discriminate against any employee or qualified applicant on the basis of race, color, religion (creed), gender, gender expression, age, national origin (ancestry), disability, marital status, sexual orientation, or military status.","Extraer, transformar y cargar (ETL), Ingeniería de datos y Programación, Administración de bases de datos, Amazon Redshift, Bases de datos, Comunicación escrita, Habilidades sociales, Programación informática y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982410687/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=OSBRX2qRg4mv%2BnfETacqZA%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer Plano , TX - Full Time","Presencial Media jornada
Coincide con tus preferencias de empleo. El tipo de empleo es Media jornada.
Sin experiencia",hace 5 días,"Plano, TX","Acerca del empleo
""ALL our jobs are US based and candidates must be in the US with valid US Work Authorization. Please apply on our website directly."" Interview Process: 2 in -person interviews and an online assessment

Define and implement a Data Lake reference architecture on AWS.

Define the data pipeline and ingestion workflow, incorporating AI modeling from different data sources and types, including voice recordings.

Design and build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies.

Experience with AWS cloud services: EC2, Lake Formation, Athena, Glue, Redshift Spectrum, S3 Storage, IAM, and KMS.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Arquitectura de referencia, Autorización, Bases de datos, KMS y Lagos de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3953548629/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=peUcuysrgz1CNCm%2FDm1tKQ%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, Product Analytics","109 US$K/año - 171 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 2 semanas,Estados Unidos,"Acerca del empleo
As a highly collaborative organization, our data engineers work cross-functionally with software engineering, data science, and product management to optimize growth, strategy, and experience for our 3 billion plus users, as well as our internal employee community. In this role, you will see a direct correlation between your work, company growth, and user satisfaction. Beyond this, you will work with some of the brightest minds in the industry, and you'll have a unique opportunity to solve some of the most interesting data challenges with efficiency and integrity, at a scale few companies can match.

Data Engineer, Product Analytics Responsibilities:

Manage and execute data warehouse plans for a product or a group of products to solve well-scoped problems
Identify the data needed for a business problem and implement logging required to ensure availability of data, while working with data infrastructure to triage issues and resolve
Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights in a meaningful way
Build data expertise and leverage data controls to ensure privacy, security, compliance, data quality, and operations for allocated areas of ownership
Design, build and launch new data models and visualizations in production, leveraging common development toolkits
Independently design, build and launch new data extraction, transformation and loading processes in production, mentoring others around efficient queries
Support existing processes running in production and implement optimized solutions with limited guidance
Define and manage SLA for data sets in allocated areas of ownership

Minimum Qualifications:

Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience.
2+ years of work experience in data engineering
Experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala, etc.)

Preferred Qualifications:

Experience with one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization, data privacy
Experience working with terabyte to petabyte scale data

About Meta:

Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today—beyond the constraints of screens, the limits of distance, and even the rules of physics.

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

$109,000/year to $171,000/year + bonus + equity + benefits

Individual compensation is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable. In addition to base compensation, Meta offers benefits. Learn more about  benefits  at Meta.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3837028543/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=hHitP6NzcwgY39QmF11MmA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer || Remote Role,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 meses,Estados Unidos,"Acerca del empleo
Job Description

Must-have skills: 3+ y as a Data engineer, with large scale data management exp; Python, Pandas; SQL

 PostgreSQL, AWS, Power BI data flow pipelines, Excel ETL; energy exp

PLEASE SEND LOCAL CANDIDATES ONLY

Requirements

 Experience working as Data Engineer (3+ years);
 Experience in Python, Pandas, SQL, and large scale data management and cleanup;
 Agile work experience.

Preferred

 Experience in PostgreSQL, AWS, PowerBI data flow pipelines, Excel ETL, API.

Responsibilities Include But Are Not Limited To The Following

 Work on creation, maintenance, and upgrades to a data pipeline.
 Creating datasets from multiple sources, working on debugging and polishing a real time data pipeline, python notebook dev time.","Ciencia de datos, Extraer, transformar y cargar (ETL), Gestión de datos, Ingeniería de datos , Pandas (Software), Python y SQL, Datasets, Flujo de datos y PostgreSQL",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984625795/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=WrnQM%2BCmeZEfhoAEeLSRtg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"70 US$/h Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 días,"Seattle, WA","Acerca del empleo
Job Title – Data Engineer III
 Location – Hybrid (Seattle, WA)
 Duration – 6+ Months
This is a W-2 role
  Job description:
As a Data Engineer on the company's Marketing Insights team, your pivotal role will revolve around designing, building, and maintaining robust data pipelines and infrastructure to drive data-driven insights. Core responsibilities include architecting scalable and fault-tolerant pipelines to extract, transform, and load data from various sources into data warehouses or lakes. Leveraging cutting-edge tools and technologies, you'll implement ingestion, transformation, and integration processes, ensuring stringent data quality through validation, cleansing, and deduplication.
You'll design and implement high-performance, scalable, and cost-efficient data storage solutions. Utilizing CI/CD principles, you'll automate infrastructure deployment and maintenance processes, ensuring a streamlined data ecosystem. Data modeling and engineering will be crucial, involving designing and implementing optimized data models, developing and maintaining ETL processes, and collaborating with analysts and functional managers to understand data needs and provide tailored data structures.
Ensuring data governance, security, and compliance will be paramount. You'll implement robust policies and processes, enforce access controls, encryption, and auditing mechanisms, and collaborate cross-functionally to establish and maintain data governance standards.
Continuous improvement and automation will be woven into your daily activities, continuously evaluating and optimizing processes, pipelines, and infrastructure, implementing automation and monitoring tools, and staying ahead of emerging data engineering technologies and trends.
 Responsibilities:
5+ years of data engineering, database engineering, business intelligence or business analytics experience
Bachelor's degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent
Knowledge of distributed systems as it pertains to data storage and computing
Knowledge of professional software engineering & best practices for full software development life cycle, including coding standards, software architectures, code reviews, source control management, continuous deployments, testing, and operational excellence
Proficient in programming languages such as Python, Scala, or Java, and familiarity with SQL.
Experience with data engineering tools and frameworks like Apache Spark, Apache Kafka, Apache Airflow, or similar.
Knowledge of data warehousing concepts, dimensional modeling, data modeling techniques and building ETL pipelines.
Strong problem-solving and analytical skills, with the ability to translate business requirements into technical solutions.
Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams. 
Preferred:
Master's degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent
Experience with business intelligence and visualization software (e.g., Quicksight.) to develop dashboards those are used by senior leadership.
Experience working with technical and non-technical, internal customers to drive their own analytics and reporting (self-serve reporting) and support ad-hoc reporting when needed.
Experience building world-class data environments with a high emphasis on data quality and data security","Almacenamiento de datos, Amazon Web Services (AWS), Apache Spark, Extraer, transformar y cargar (ETL), Python y SQL, Modelado de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3934810962/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=juqqZchxgSszjZDKjdYCPA%3D%3D&trk=flagship3_search_srp_jobs,GCP Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
Job Title: GCP Data Engineer

Location: Remote

Duration: 6+ Months

Implementation Partner: Infosys

End Client: To be disclosed

Jd

6+ years of experience
GCP services experience is MUST
Hands-on experience with Python, SQL","Almacenamiento de datos, Buena práctica clínica, Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud , Ingeniería de datos , Python y SQL, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984703636/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=PJv3oJy1O%2BnfjW8VcHyOuA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"San Francisco, CA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Hunt Hive Talent Recruiting Agency About Us: We connect amazing talent with fantastic opportunities. Join our team and be part of something extraordinary! About the Role: Be a data champion, ensuring accuracy, consistency, and reliability across systems. If you're passionate about data and problem-solving, we want you! Responsibilities: Profile, analyze, and assess data quality across various systems Test and validate data quality requirements Continuously measure and monitor data quality Deliver against data quality service level agreements Manage data quality issues and lead data cleansing activities Determine user accessibility and modify access rights Interpret company and regulatory policies on data Educate others on data governance processes Qualifications: Bachelor's Degree in Computer Science, Information Systems, or related field In-depth knowledge of data quality management techniques Experience with data profiling, cleansing, integration, and issue management tools Strong understanding of data modeling, storage, integration, and warehousing Familiarity with enterprise data architecture Ability to interpret and apply data policies Bonus Points: Experience with data visualization tools (e.g., Tableau, Power BI) Knowledge of data privacy regulations (e.g., GDPR, CCPA) Certification in data management or data quality Strong communication and interpersonal skills Benefits: Competitive salary and comprehensive benefits Professional development opportunities Fun, inclusive, and collaborative work environment Extra perks like wellness programs and team-building activities To Apply: Visit our website at

Hunt Hive Talent and submit your application.","Almacenamiento de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Modelado de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3983712520/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=zpV%2BjTgtRPcPSajZUrCWIg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Atlanta, GA","Acerca del empleo
Metas Solutions is a professional services firm headquartered in Atlanta, GA. We are an 8(a)-certified, Woman Owned Small Business (WOSB/EDWOSB) headquartered in Atlanta, GA. Metas is a full-service consulting firm focused on implementing public health and information technology (IT) solutions for our clients. We specialize in Health Informatics, Data Analytics, SharePoint Design and Transformation, Capacity Building and Technical Assistance, Evaluation, Strategy, and Population Health Strategies. See www.metassolutions.com for further details about us and careers with Metas.Job Description:Metas Solutions has an ""immediate"" opening for an experienced Data Engineer to work Remotely. The client is a one stop shop for supporting the acquisition, evaluation, management, and analytics of large healthcare data assets for their agency.*If you are interested in this position, we must speak with you before 11:00 am on Friday, July 26, 2024.Responsibilities:

Provides operational, data management, technical/engineering, and analytic support along with tooling that enables analytics, visualization, and machine learning on large volumes of complex healthcare data for multiple public health use cases. 
A dedicated engineering team is needed to develop, enhance, and scale technical capabilities to make effective use of the large healthcare data investments managed by client Data Hub in an Enterprise Data Analytics and Visualization (EDAV) platform.
These activities are critical to supporting client programmatic and response activities. 

Qualifications:

Bachelor's degree and 3+ years of experience with data engineering, developing, and implementing data pipelines.
3+ years of experience developing with Python and SQL in a professional environment.
Familiarity or experience with software delivery lifecycle practices, including continuous integration, configuration management, unit testing, and Agile development.
Experience working with and optimizing large-scale data systems. 

Preferred Skills:

3+ years of experience working with big data technologies and distributed computing processing. 
3+ years of experience working in Azure or AWS cloud in development capacity. 
Experience with data ingest and analytics tools, including Data Factory, Databricks, Spark, Airflow, Power BI, Tableau.
Ability to implement ITIL methodologies. 
Ability to quickly learn technical concepts. 
Ability to communicate with multiple functional groups and leadership. 
Ability to display a positive, can-do attitude to solve challenges and collaborate effectively.

Security, Salary, and Benefits:

Must be US Citizen or with the ability to obtain a US Government security clearance (Public Trust 5) within a reasonable period.
Market competitive salary, commensurate with experience and education.
Comprehensive benefits package available, Medical, Dental, Vision and Life Insurance, Paid Time Off (PTO), 401K with company match, growth, and promotion opportunities.

We are an Equal Opportunity Employer/Veterans/Disabled","Airflow, Analítica de datos, Apache Spark, Azure Data Factory, Big data, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Comunicación y Implantación de software",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3948542730/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=1qcHWO1KYjpoBcVdrlvtPw%3D%3D&trk=flagship3_search_srp_jobs,Big Data Engineer - OAG 6/17/2024,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
Remote. 

Client requires the services of 3 Data Scientist (Big Data Engineer) 2, hereafter referred to as Candidate(s), who meets the general qualifications of Data Scientist (Big Data Engineer) 2, Data/Database Administration and the specifications outlined in this document for the Client .

All work products resulting from the project shall be considered ""works made for hire"" and are the property of the Client and may include pre-selection requirements that potential Vendors (and their Candidates) submit to and satisfy criminal background checks as authorized by Texas law. Client  will pay no fees for interviews or discussions, which occur during the process of selecting a Candidate(s).

Client  is seeking a dynamic and visionary Data Scientist to join our team and support our System Modernization efforts for the Child Support Division. This individual must be able to work in a heavily technical environment, preparing and optimizing data for Snowflake and utilizing SAS Viya to build comprehensive federal and state reports. You will play a pivotal role in transforming, analyzing, and leveraging our data assets to drive strategic initiatives and business outcomes. You must be comfortable analyzing data, developing predictive modeling algorithms, and can communicate your findings through visualizations enabling the discovery of solutions to business problems. As a Data Scientist your role will be impactful, visible, and rewarding.

Responsibilities Will Include

Data Conversion and Reporting Support for System Modernization Efforts
Data Transformation and Integration: Prepare and optimize data for migration to Snowflake and SAS Viya platforms, ensuring seamless integration and functionality by creating data transformation processes using ETL, SQL, Python, and R.
Develop Federal and State Reports: Build comprehensive reports that meet federal and state requirements using Snowflake and SAS Viya, ensuring accuracy and compliance.
Scrum Team Collaboration: Work as a member of an agile team to deliver new features and functions, delivering best-in-class value-based technology solutions.
Data Quality Management: Develop and implement databases, ETL processes, data collection systems, and data quality strategies that optimize statistical efficiency, accuracy, and quality.
Problem Examination and Resolution: Examine problems within the Data Intelligence space using ETL, Lambda, and Glue, and implement necessary changes to ensure data quality improvement.
Data Analytics and Insights: Utilize advanced data analytics techniques to support strategic decision-making, ensuring data integrity, quality, and timeliness of results.

The above job description and requirements are general in nature and may be subject to change based on the specific needs and requirements of the organization and project.

Minimum Requirements

II. CANDIDATE SKILLS AND QUALIFICATIONS

Candidates that do not meet or exceed the minimum stated requirements (skills/experience) will be displayed to customers but may not be chosen for this opportunity. Years Required/Preferred Experience 4 Required Proven experience in data conversion and report building using Snowflake and SAS Viya. 4 Required Demonstrated experience in data transformation processes using ETL, SQL, Python, and R. 4 Required Experience working with data analytics and business intelligence tools. 4 Required Experience working in a Scrum or Agile development environment. 4 Required Proficiency in ETL processes and tools such as AWS Glue and Lambda. 4 Required Strong knowledge of database management and data warehousing concepts. 4 Required Expertise in SQL for data querying and manipulation. 4 Required Proven experience in data conversion and report building using Snowflake and SAS Viya. 4 Required Demonstrated experience in data transformation processes using ETL, SQL, Python, and R. 4 Required Experience working with data analytics and business intelligence tools. 4 Required Experience working in a Scrum or Agile development environment. 4 Required Proficiency in ETL processes and tools such as AWS Glue and Lambda.","AWS Lambda, Almacenamiento de datos, Ciencia de datos y Extraer, transformar y cargar (ETL), AWS Glue, Calidad de datos, Conversión de datos, Report Building, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3953120651/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=Mp8%2F6wQErBsSHI8QvZZiaQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,"Hoffman Estates, IL","Acerca del empleo
Work at OMRON!
OMRON is a global leader in the field of automation, an $8 billion global technology company celebrating more than 80 years of success. OMRON’s business fields cover a broad spectrum, ranging from industrial automation and electronic components to social systems and healthcare. OMRON Management Center of America, Inc. is the regional headquarters for OMRON in the Americas.

Data Engineer
As a Data Engineer at OMRON, you will play a crucial role in designing, implementing, and maintaining robust data pipelines and infrastructure on Snowflake, leveraging your expertise in data integration tools and AWS cloud services. You will work closely with cross-functional teams to ensure the efficient flow of data across various systems and platforms, while also validating data integrity and adherence to best practices. Additionally, you will contribute to data modeling efforts and collaborate with data Analysts/scientists to support advanced analytics initiatives. Experience with Power BI visualizations will be considered a plus.

Our Commitment to Employees:
Training and Career Development Program to give employees a learning path with the necessary tools and resources they need to help build their career at Omron.
Great financial opportunities with competitive compensation, immediate 401k match with 100% vesting, profit sharing, and Blue Cross Blue Shield for medical, dental, vision and prescription drug benefits.
Community Awareness that includes activities with local non-profit organizations and a Matching Gift Program.
Work-Life Balance with Flexible Work Arrangements, Flexible Work Hours, and Sick/Vacation/Holiday Pay.
Wellness Activities such as Walking Contests, Nutritional Learning Sessions, On Site Flu shots and Health Screenings.

Responsibilities:
Design, develop, and maintain data pipelines on Snowflake using Appflow, IICS, or other data integration tools.
Ensure data quality and integrity by implementing rigorous validation processes and recommending best practices for data management.
Collaborate with stakeholders to understand data requirements and translate them into scalable and efficient solutions.
Implement and optimize data models to support analytics and reporting needs, considering performance and scalability requirements.
Work closely with AWS cloud services, leveraging various tools and services for data storage, processing, and analysis.
Collaborate with data experts to deploy machine learning models and support data science projects.
Develop Power BI visualizations and dashboards to provide insights to stakeholders and drive decision-making.
Stay abreast of emerging technologies and industry trends, continuously improving processes and solutions.
Other duties and miscellaneous projects as needed

Job Requirements:
Bachelor’s degree in business or technical field; master’s degree preferred.
Proven experience as a Data Engineer, with a focus on Snowflake, data integration tools, and AWS cloud services.
Strong proficiency in SQL
Experience with programming languages such as Python is a big plus
Experience with data modeling concepts and tools.
Familiarity with data science techniques and tools is a plus.
Experience with Power BI or other data visualization tools is preferred.
Excellent problem-solving skills and attention to detail.
Strong communication and collaboration skills, with the ability to work effectively in a team environment

Don't meet every single requirement? Studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. At Omron we are dedicated to building a diverse, inclusive and authentic workplace, so if you're excited about this role but your past experience doesn't align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.

Omron is an Equal Opportunity Employer. We provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, we comply with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.","Analítica, Analítica de datos, Ciencia de datos y Ingeniería de datos, Calidad de datos, Comunicación, Modelado de datos, Modelo de datos, Resolución de problemas y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3909338144/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=OUx%2FygA8L583nGYNovynyA%3D%3D&trk=flagship3_search_srp_jobs,Snowflake Informatica Data Engineer,"110 US$K/año - 140 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 mes,Estados Unidos,"Acerca del empleo
About Us:
LTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700+ clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by nearly 90,000 talented and entrepreneurial professionals across more than 30 countries, LTIMindtree — a Larsen & Toubro Group company — combines the industry-acclaimed strengths of erstwhile Larsen and Toubro Infotech and Mindtree in solving the most complex business challenges and delivering transformation at scale. For more information, please visit www.ltimindtree.com.

Job Title: 
Snowflake Informatica Data Engineer

Work Location
Remote

Job Description:
Primary Skill :
Design develop test deploy and maintain enterprise level applications using the Snowflake platform
Work with a variety of stakeholders to understand requirements and deliver solutions
Take ownership of a project and see it through to completion
Experience with the Snowflake platform
Experience with enterprise level applications
Ability to work independently as well as part of a team
Ability to take ownership of a project and see it through to completion
Strong analytical and problem solving skills
Working experience on Snowflake cloud data warehouse
Able to write advanced SQL queries
ETL IICS knowledge
Have working experience on DATA Warehousing Projects
Understanding of Dimensions Facts and Datamart Terminologies""

Benefits and Perks:
Comprehensive Medical Plan Covering Medical, Dental, Vision
Short Term and Long-Term Disability Coverage
401(k) Plan with Company match
Life Insurance
Vacation Time, Sick Leave, Paid Holidays
Paid Paternity and Maternity Leave

The range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.

Disclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting.

LTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.
 Safe return to office:
In order to comply with LTIMindtree’ s company COVID-19 vaccine mandate, candidates must be able to provide proof of full vaccination against COVID-19 before or by the date of hire. Alternatively, one may submit a request for reasonable accommodation from LTIMindtree’s COVID-19 vaccination mandate for approval, in accordance with applicable state and federal law, by the date of hire. Any request is subject to review through LTIMindtree’s applicable processes.","Amazon Web Services (AWS), Capacidad de análisis, Extraer, transformar y cargar (ETL), Herramientas ETL, Integración de datos y Tableau, Ciencias de la computación, Informatica (empresa), Matemáticas, Panel de control y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3985556387/?eBP=BUDGET_EXHAUSTED_JOB&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=WaMur8XptqYBhbpMnTc3FA%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"90 US$/h - 100 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 días,Estados Unidos,"Acerca del empleo
Job Description Summary
The role requires a Senior-level Designer with strong infographic and data visualization skills. The candidate will work closely with SEO and Creative teams to create visually appealing and informative content. The focus is on distilling complex information into clear and concise graphics while adhering to the company's brand guidelines.
Ideal Candidate Profile
Highly Skilled: Proven experience in designing effective infographics and data visualizations.
Autonomous: Able to work independently with minimal supervision.
Strong Communication: Excellent at translating complex information into visual formats.
Collaborative: Works well with SEO and Creative teams.
Detail-Oriented: Ensures high-quality output with a keen eye for design.
Tech-Savvy: Proficient in design tools like Figma and project management tools like Asana.
Flexible: Willing to adapt to changing requirements and timelines.
Key Requirements and Preferences
Portfolio Emphasis: The hiring manager strongly prefers candidates with a strong portfolio showcasing infographic and data visualization work.
Senior Level Experience: Mid to Senior level designer with substantial experience.
Autonomy: The candidate should be self-motivated and require minimal hand-holding.
Technical Skills: Proficiency in Figma and Asana is essential.
Time Zone Flexibility: Willing to accommodate meeting times across different time zones.","Analítica, Analítica de datos, Ciencia de datos, Minería de datos, Visualización y Visualización de datos, Asana, Destreza tecnológica, Figma y Infografías",Solicitud sencilla
https://www.linkedin.com/jobs/view/3921571071/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=HlbnQYJd9lNJwGWAwqJIIg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer 2,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Boise, ID","Acerca del empleo
Our vision is to transform how the world uses information to enrich life for all. 

Micron Technology is a world leader in innovating memory and storage solutions that accelerate the transformation of information into intelligence, inspiring the world to learn, communicate and advance faster than ever.

Develop and grow Micron’s methods and systems for extracting new insight for our expanding data streams. Collaborate with data scientists, engineers, technicians and data mining teams to design and implement systems to extract data from Micron’s business systems, transforming it into an actionable format, and as needed, creating dynamic presentation layers for use by high-level engineers and managers throughout the company. Create new solutions, as well as, supporting, configuring, and improving existing solutions. Work in a technical team through development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics using machine learning and artificial intelligence. Design and optimize data structures in data management systems (Snowflake and Cloud platforms) to enable AI/ML solutions. Build custom software components and analytics applications. Determine transformation requirements and develop processes to bring structured and unstructured data from the source to a new physical Data Model. Work with Data Scientists to implement strategies for cleaning and preparing data for analysis, to develop data imputation algorithms, and optimize performance of big data and machine learning systems.

Employer will accept a Master's degree in Machine Learning, Data Science, Computer Science, Statistics, Mathematics or related field.

As a world leader in the semiconductor industry, Micron is dedicated to your personal wellbeing and professional growth. Micron benefits are designed to help you stay well, provide peace of mind and help you prepare for the future. We offer a choice of medical, dental and vision plans in all locations enabling team members to select the plans that best meet their family healthcare needs and budget. Micron also provides benefit programs that help protect your income if you are unable to work due to illness or injury, and paid family leave. Additionally, Micron benefits include a robust paid time-off program and paid holidays. For additional information regarding the Benefit programs available, please see the Benefits Guide posted on micron.com/careers/benefits.

Micron is proud to be an equal opportunity workplace and is an affirmative action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, age, national origin, citizenship status, disability, protected veteran status, gender identity or any other factor protected by applicable federal, state, or local laws.

To learn about your right to work click here.

To learn more about Micron, please visit micron.com/careers

For US Sites Only: To request assistance with the application process and/or for reasonable accommodations, please contact Micron’s People Organization at hrsupport_na@micron.com or 1-800-336-8918 (select option #3)

Micron Prohibits the use of child labor and complies with all applicable laws, rules, regulations, and other international and industry labor standards.

Micron does not charge candidates any recruitment fees or unlawfully collect any other payment from candidates as consideration for their employment with Micron.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Ciencias de la computación, Comunicación y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3966609309/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=thRZ7MSv6I%2BEef9d%2BJVRzw%3D%3D&trk=flagship3_search_srp_jobs,GCP Data Engineer,"Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 semanas,"Toronto, OH","Acerca del empleo
Title: GCP Data Engineer 

Location: remote -Canada (preference to Calgary or Vancouver)

Duration: 6+ month contract to extend or convert

Project Details

 They need to build a customer database (data asset) to help them identity: who are our customers, prospects, what RGUs/revenue insights do we have and proper segmentation of the customers - creating a unified version of this data view for the marketing team
 They will leverage this data for marketing campaign team (outreach, scorecards, etc)
 Individual will work with the business, take the business requirements convert them to technical specifications leveraging SQL/Data mapping/Python. Acting as a liaison between technical and business.
 Also trying to migrate their processes from on-prem to GCP
 They will leverage this data for marketing campaign team

Skills

3+ years of Experience
Experience building pipelines from data source to GCP for consumption
Experience working with SQL database
Python experience for Automation
Moving data from on prem to GCP
ETL pipeline development and Process experience
Datamapping

Plus Skills: neither are required

GCP Certification
Previous Telecom Exp

Responsibilities

There will be a ramp up period initially is 1-2 months, Team Lead will guide the resource for the first 1-3-ish months, then they are expected to be self sufficient, working with the business requirements, building pipelines into existing source data to pull into GCP, automation with python (for pulling the data sets), participating in daily stand ups, etc

TSG is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

69515","Base de datos SQL, Extraer, transformar y cargar (ETL), Google Cloud , Ingeniería de datos , Python y SQL, Asignación de datos, Bases de datos, Especificaciones técnicas y Necesidades empresariales",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3822848627/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=DGjbFO2cvsu7B0QoIOiIig%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 meses,"Nueva York, NY","Acerca del empleo
Remote

Data Engineer

12+ Months

Experience in COINS/SAP/Oracle/NetSuite

5+ Years of experience working in an IT environment independently and in teams (including remote and global delivery models)

Organize and streamline operating companies and data collection to meet reporting needs from COINS ERP and business systems

Creating and maintaining data models, defining data structures, relationships, and data storage requirements, using techniques like entity-relationship diagrams and data flow diagrams

Developing data transformation processes, including data cleansing, normalization, aggregation, and enrichment, to prepare data for analytics and reporting

Identifying and resolving performance bottlenecks in data processing and storage systems, optimizing query performance, and improving overall data pipeline efficiency

Implementing data quality assurance processes, validating data pipelines, and resolving data quality issues

Monitoring data pipelines, diagnosing and troubleshooting issues, performing system upgrades and maintenance tasks to ensure data reliability and availability

Ensures data services solutions, configurations, and processes are sustainable and scalableProficient in Microsoft Excel and other data cleansing/migration tools","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Calidad de datos, Diagramas de flujo de datos (DFD), Flow Diagrams, Modelo de datos y NetSuite",Solicitud sencilla
https://www.linkedin.com/jobs/view/3978328833/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=qluD5%2Fg7fU9gJzDLLEAwZw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Chicago, IL","Acerca del empleo
Who We Are: It all starts with purpose.

We are a purpose-driven nonprofit with a dynamic staff culture.

With a meaningful purpose, motivated staff, and excellent benefits, working here will definitely have you smiling! The ADA’s headquarters is located just steps from Chicago’s Magnificent Mile and close to public transportation. With more than 400 colleagues, the ADA Staff are some of the most talented people in the Chicago, Washington D.C., and Maryland area.

We were named a Top Workplace by the Chicago Tribune in 2019 and 2021! Come join our team!

Job Responsibilities:

Data Engineer will oversee the integration and management of data in our Salesforce and Azure-based SQL data warehouse environment. They will be responsible for designing, implementing, and maintaining robust ETL processes, ensuring high data quality, and optimizing data integration strategies to support our business intelligence and analytics needs.

Must Have:

Applicants must be legally authorized to work in the United States and should not now or in the future require sponsorship for employment. 
Bachelor's degree in Computer Science, Information Technology or related 
Minimum 3 years experience in data engineering or a similar role, specifically in Azure environment; or 7 years of experience in lieu of degree
Previously worked in a salesforce environment 
Excellent problem-solving, analytical, and communication skills
Knowledge of ETL concepts, data warehousing methodologies, and cloud computing
Strong understanding of SQL and data modeling
Ability to lead projects and work collaboratively in a team environment
Proficiency in Microsoft Office Suite (Word, Excel, and PowerPoint), Azure Data Factory, Azure SQL Database, and other Azure data services

Nice to Have:

Certifications in Salesforce and Azure cloud technologies 
Experience with BI tools like Power BI, Tableau, or similar 
Knowledge of Python, Java, or other scripting languages 
Knowledge of Purview 

Just a few of the benefits offered to employees:

Promotes Work/Life Balance
Hybrid Work Schedule (2-3 days from home)
Health insurance/ dental reimbursement plan
Ample Paid time off
401(k) 
Pension
Flexible Spending Account
Life insurance
Tuition reimbursement
Paid Parental Leave
Pet Insurance
Student Loan Refinance
2 days off to work at a charity event of your choice

The American Dental Association is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. The American Dental Association is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at recruiting@ada.org.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities.","Almacenamiento de datos, Azure Data Factory, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Comunicación, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3978290056/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=ZyV9TJbx1MRNvXRI%2BAMNdw%3D%3D&trk=flagship3_search_srp_jobs,Data Center Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Las Vegas, NV","Acerca del empleo
Nesco Resource is seeking multiple Data Center Technicians and Engineers who are local to Las Vegas a for data center installation project! Earn some extra cash while you work your full-time job or get some work in while you look for a full-time role!

Project Expectations

 Techs will be onsite and can anticipate up to 8- 12 hours shifts for projects 
 Project Managers and leads will be onsite to provide direction and process expectations 
 If interested and qualified, there are opportunities to travel domestically. 
 If interested and qualified, there are opportunities to support on-call SLA work. 

 Hands-on details: 

 Racking/stacking, and install and de-install of Server Farm/Data Center hardware 
 Cabling 
 Reverse Wire Pulls and Snips 
 Staging of Rails for future installs 
 Build outs 

 Qualifications: 

 General Data Center/Server Farm expertise is a must 
 Minimal of 1-2 years of data center technician experienced 
 Break Fix Skills or Systems Admin Skills are preferred 
 Valid government ID 
 High school degree or equivalent 

 Pay: (Depends on tech/eng experience) 

Rate is dependent on location and experience

 Nesco Resource offers a comprehensive benefits package for our associates, which includes a MEC (Minimum Essential Coverage) plan that encompasses Medical, Vision, Dental, 401K, and EAP (Employee Assistance Program) services.

Nesco Resource provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws. com171",Cableado y Infaestructura del centro de datos,Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3980875792/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=j1KU8bMJiysTKDl%2BU7PliA%3D%3D&trk=flagship3_search_srp_jobs,SQL Server Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Nueva York, NY","Acerca del empleo
Location: NY/NJ

Job Description

5+ years' experience required

Strong knowledge of Microsoft SQL Server, including designing, developing, and implementing complex SQL queries, stored procedures, and functions.

Expertise in Python programming, including experience with data analysis libraries such as NumPy, and Pandas.

Proficiency in SSIS (SQL Server Integration Services) for ETL (Extract, Transform, and Load) processes.

Proficient in C# programming, including experience with .NET framework.

Strong analytical and problem-solving skills, with the ability to analyze complex data and identify trends, patterns, and insights.

Key Skills

SQL Server, SSIS, C#, Python (Numpy, Pandas), financial experience (Investment Banking)

SQL Server Data Engineer,

Skills

Microsoft SQL Server, including designing, developing, and implementing complex SQL queries, stored procedures, and functions.

Education

BS/BA in computer/data science/engineering or related fields

Education: Bachelors Degree","Analítica de datos, NumPy, Pandas (Software), Programación y SQL Server Integration Services (SSIS), .NET Framework, C#, Procedimientos de almacenado, Programación en C y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3981029253/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=BhoixEvxq8UKtwPGqSk9Gw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer [Insurance],"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 días,"Boston, MA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Boston , MA - Onsite

Long Term

Job Opportunity

As a Data Engineer, you will be part of a cross-functional, largely autonomous data engineering team. You should have

excellent problem-solving skills, strong attention to detail, and the technical know-how to independently build solutions from

start to finish. We are looking for a true expert on ETL development / data modeling, who is comfortable working with

datasets of varying latencies and size sourcing data from various disparate data providers and platforms.

Duties & Responsibilities

Collaborate closely with application engineers, data analysts, data engineers, and stakeholders in building and

maintaining Berxi data warehouse to support the operations and analysis of a direct-to-consumer small commercial

insurance business.

Contribute to the development, maintenance, and evolution of our platform.

Deeply involved with working on SQL Server, developing Kimball dimensional modeling-based data marts & build

new models or data marts and supporting data analyst teams in further assessing available data.

Play a significant part in augmenting the data platform as we implement a new policy administration system data

into Azure cloud-based data platform leveraging all the latest tools available anchoring on Azure resources.

Qualifications, Skills, And Experience

2+ years of hands-on data engineering experience with ETL processing, data modeling and Business Intelligence.

Relevant Work Experience with Microsoft SQL Server (2016+), SQL Server Integration Services (SSIS), ETL and

Transact-SQL.

Experience with Microsoft Azure cloud platform and Azure resources.

Familiarity with data engineering concepts, languages, and tools; ability to develop on multiple platforms.

Experience using sophisticated data transformation tools and cleansing methods, such as windowing functions in

Transact-SQL.

Experience with relational modeling, star schemas and Kimball Data Warehousing.

Experience with contemporary data file formats such as JSON and unstructured data.

Familiarity modeling for BI visualization tools such as Power BI / SSRS.

Familiarity with Spark SQL / C# / Python is a plus.

Experience with NoSQL/Non-relational databases are a plus, especially blob storage and Azure file tables.

Experience with Agile data engineering concepts and processes, such as CICD, pipelines, backlog tracking,

burndown metrics, and incremental delivery a plus

Solid Collaboration, Prioritization, And Adaptability Skills Required.

Excellent interpersonal and communication skills.

Proactive and self-driven, demonstrated initiative and be a logical thinker.","Almacenamiento de datos, Analítica de datos, Data Marts, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL Server Integration Services (SSIS), Comunicación interpersonal, Kimball, Modelado de datos y Schemas",Solicitar
https://www.linkedin.com/jobs/view/3981215592/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=eac54JsiXfSrs6ZFkUIaqg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer(Machine Learning/Clinical Development) Contractor -BB530,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 6 días,"Cambridge, MA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Job Description

Data Engineer (Machine Learning/Clinical Development) Contractor –Remote – BB530

6+ Months

Part-time or Full-time

We are seeking a Data Engineer Contractor position with a strong background in Machine Learning and software development to enhance our innovative applications of Generative AI in clinical development. This position involves close collaboration with biostatisticians to deploy automated solutions and fine-tune large language models using frameworks such as RAG, PyTorch and various foundation models

Key Responsibilities:

Work alongside biostatisticians to automate and refine data processes within clinical development databases. 
Develop and maintain automated ETL systems using LangChain and ensure seamless data integration. 
Utilize Python for object-oriented programming to develop scalable and efficient data solutions. 
Fine-tune large language foundation models using PyTorch and Hugging Face, applying techniques like direct preference optimization to improve model accuracy and reliability. 
Test and validate automated systems to comply with industry standards and regulatory guidelines. 
Continuously learn and apply the latest advancements in AI, machine learning, and data management. 

Qualifications:

Bachelor Degree or students currently enrolled in Master/PhD Program in Data Science, Statistics, Bioinformatics, Computer Science, or related field. 
Extensive experience in Python/R, with a strong understanding of object-oriented programming. 
Experience with PyTorch and Hugging Face for model development and fine-tuning. 
Demonstrated expertise in designing automated data systems, preferably within a clinical or healthcare setting. 
Ability to collaborate effectively with biostatisticians and other technical teams. 
Strong problem-solving skills and a proven track record of managing complex data projects. 
Excellent communication skills, able to articulate technical concepts to non-technical stakeholders. 

Company Description

www.techdataservice.com

www.techdataservice.com","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Programación, Python y R (Lenguaje de programación), Bases de datos, Comunicación, Programación orientada a objetos (POO) y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3980244246/?eBP=BUDGET_EXHAUSTED_JOB&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=fPfb%2BUeZumDPbXSlAqD9XQ%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, Analytics","Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 días,"Burlingame, CA","Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Meta Platforms, Inc. (f/k/a Facebook, Inc.), is seeking the following. Apply via Dice today!

Meta Platforms, Inc. (f/k/a Facebook, Inc.) has the following position in Burlingame, CA: 

Data Engineer, Analytics: Design, build, and launch data pipelines to move data across systems and build the next generation of data tools that generate business insights for a product. (ref. code REQ-2405-138290: $212109/year - $235400/year).

Individual pay is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base salary only, and do not include bonus or equity or sales incentives, if applicable. In addition to base salary, Meta offers benefits. Learn more about benefits at Meta at this link: https://www.metacareers.com/facebook-life/benefits. For full information & to apply online, visit us at the following website http://www.metacareers.com/jobs & search using the ref code(s) above.","Almacenamiento de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Solicitar
https://www.linkedin.com/jobs/view/3982514322/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=5sZ%2B2i8fWxvp5TaHb9IraQ%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Madison, WI","Acerca del empleo
Why Jerry?

Join a pre-IPO startup with capital, traction and runway ($240M funded | 40X revenue growth in 4 years | $2T market size)
Work closely with brilliant leaders and team members who have scaled companies like Nvidia, Better, Nerdwallet, Wayfair, Sofi, etc. 
Disrupt a massive market and take us to a $5B business in the next few years
Be immersed in a talent-dense environment and greatly accelerate your career growth

About the opportunity: 

Jerry is building the first AllCar™ super app to help people optimize all aspects of owning a car – insurance, buy/sell, registration, loans, safety, repairs, parking, etc – a $2T market in the U.S. We started with insurance in 2019, and since then we’ve launched loan refinancing, real-time driving insights, a repair marketplace, car diagnostics, and a GenAI-powered chatbot. We have amassed over 5M customers, raised $240MM in funding, scaled our revenue 40X and our team to 250 across 7 countries.

The Data team at Jerry is responsible for supporting intelligent business decision making through analytical insights and advanced models. As a senior data engineer, you bring your previous industry experience in infrastructure scaling to Jerry and lead the development of a new infrastructure solution that will continue to perform with great stability and efficiency even when the company grows 10x bigger. You will join a team of data engineers, data analysts, and data scientists who operate with urgency and passion for excellence in pursuit of our mission to build the first AllCar™ super app.

How you will make an impact:

Partner with data scientists, software engineers and cross-functional stakeholders (Marketing, Operations, Finance) to build data pipelines that will provide critical data and insights to drive growth for the business, while ensuring data availability and accuracy
Partner with data scientists and machine learning engineers to evolve, optimize, and integrate critical predictive models
Partner with software engineers and data scientists to build data pipelines to consume and refine application data
Define data engineering standards and best practices and drive operational excellence
Perform regular maintenance and optimization of our data infrastructure

Ideal profile:

Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related technical discipline. Master’s degree in Computer Science or Engineering preferred
5+ years of experience in data engineer or similar role
Proven success in communication with database users, software engineers and senior management to collect requirements, discuss data modeling decisions and define data engineering strategies
Outstanding communication and problem solving skills, eager to work on the most pressing business problems and drive impact for the company
Experience with data modeling, data warehousing, and ETL pipeline development
Experience with column-based databases (Amazon Redshift, ClickHouse, etc.) and infrastructure integration with analytical platforms (Tableau, Qlik, Power BI, etc.)
Hands-on experience and advanced knowledge of SQL
Experience with big data technologies (Spark, Clickhouse, Redshift, Snowflake etc.)
Experience in process automation and deployment of advanced machine learning models

Jerry is proud to be an Equal Employment Opportunity employer. We prohibit discrimination based on race, religion, color, national origin, sex, pregnancy, reproductive health decisions or related medical conditions, sexual orientation, gender identity, gender expression, age, veteran status, disability, genetic information, or other characteristics protected by applicable local, state or federal laws. 

Jerry is committed to providing reasonable accommodations for individuals with disabilities in our job application process. If you need assistance or an accommodation due to a disability, please contact us at recruiting@getjerry.com

About Jerry:

Jerry is America’s first and only AllCar™ app. We are redefining and radically improving how people manage owning a car, one of their most expensive and time-consuming assets.

Backed by artificial intelligence and machine learning, Jerry simplifies and automates owning and maintaining a car while providing personalized services for all car owners' needs. We spend every day innovating and improving our AI-powered app to provide the best possible experience for our customers. From car insurance and financing to maintenance and safety, Jerry does it all.

We are the #1 rated and most downloaded app in our category with a 4.7 star rating in the App Store. We have more than 6 million customers — and we’re just getting started.

Jerry was founded in 2017 by serial entrepreneurs and has raised more than $242 million in financing.

Join our team and work with passionate, curious and egoless people who love solving real-world problems. Help us build a revolutionary product that’s disrupting a massive market.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Automatización de procesos, Bases de datos, Comunicación, Modelado de datos, Oil Pipeline Development y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3981019017/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=BlviIhC2MXb9Te8wt3qjrg%3D%3D&trackingId=aHJlgPuAvPd3f9Fj5VD3Rw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Intermediate level,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 4 días,"Phoenix, AZ","Acerca del empleo
Why USAA?

At USAA, we have an important mission: facilitating the financial security of millions of U.S. military members and their families. Not all of our employees served in our nation’s military, but we all share in the mission to give back to those who did. We’re working as one to build a great experience and make a real impact for our members.

We believe in our core values of honesty, integrity, loyalty and service. They’re what guides everything we do – from how we treat our members to how we treat each other. Come be a part of what makes us so special!

The Opportunity

As a dedicated Data Engineer, you are engaged in all phases of the data management lifecycle. This includes gathering and analyzing requirements, collecting, processing, storing, securing, and archiving data. You will develop and maintain technical systems for data reporting and technical solutions utilizing emerging technologies. You will also partner with the business to ensure data management solutions align to business objectives.

To fully deliver these imperatives, we have developed technical training curriculum to establish base-level knowledge essential to data engineering standards and practices used within data engineering teams at USAA. After successfully completing the training curriculum, you will be assigned to an engineering team.

We offer a flexible work environment that requires an individual to be in the office 4 days per week. This position can be based in one of the following locations: San Antonio, TX, or Phoenix, AZ. Relocation assistance is not available for this position.

What You'll Do

Participates in the full life cycle of data engineering to include analysis, solution design, data pipeline engineering, testing, deployment, scheduling, and production support with guidance from senior team members.
Assists in the implementation of technical solutions for data reporting and analytic systems.
Assists with designing and writing test scripts to verify data integrity and application of functionality. Reviews functionality of existing test scripts for understanding.
Demonstrates familiarity with IT Change and Release Management best practices. Deploys data pipeline code with assistance from senior team members.
Participate in design and code review sessions.
Actively participates in Agile ceremonies such as daily standup, iteration planning, backlog grooming, and retrospective sessions.
Develops intermediate familiarity of data management best practices by participating in trainings, reviewing documentation, and reading code from existing solutions.
Demonstrates knowledge and understanding of business products and processes.
Assists senior team members in breaking down business features into technical stories and approaches.
Actively learns about new and emerging technologies in the data engineering space. Seeks to apply learnings in current and future projects.
Ensures risks associated with business activities are effectively identified, measured, monitored, and controlled in accordance with risk and compliance policies and procedures.

What You Have

Bachelor’s degree; OR 4 years of related experience (in addition to the minimum years of experience required) may be substituted in lieu of degree; OR Approved certification from CodeUp, Galvanize, VetFIT (Veterans for IT) or eFIT (Employees for IT)
2 years of data engineering, data analysis or software development experience implementing data solutions.
Working Experience in SQL and Relational Databases.
Basic understanding of Agile methodology practices.
Strong analytical and problem-solving skills.
Working experience in Cloud technologies and tools.

What Sets You Apart

Experience with cloud modernization efforts.
Experience with Snaplogic.
Experience with DBT.
Experience with Snowflake.

The above description reflects the details considered necessary to describe the principal functions of the job and should not be construed as a detailed description of all the work requirements that may be performed in the job.

What We Offer

Compensation: USAA has an effective process for assessing market data and establishing ranges to ensure we remain competitive. You are paid within the salary range based on your experience and market data of the position. The actual salary for this role may vary by location. The salary range for this position is: $74,210.00 - $141,830.00.

Employees may be eligible for pay incentives based on overall corporate and individual performance and at the discretion of the USAA Board of Directors.

Benefits: At USAA our employees enjoy best-in-class benefits to support their physical, financial, and emotional wellness. These benefits include comprehensive medical, dental and vision plans, 401(k), pension, life insurance, parental benefits, adoption assistance, paid time off program with paid holidays plus 16 paid volunteer hours, and various wellness programs. Additionally, our career path planning and continuing education assists employees with their professional goals.

For more details on our outstanding benefits, please visit our benefits page on USAAjobs.com.

Applications for this position are accepted on an ongoing basis, this posting will remain open until the position is filled. Thus, interested candidates are encouraged to apply the same day they view this posting. 

USAA is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.","Analítica de datos, Base de datos relacional y Ingeniería de datos, Bases de datos, Elaboración de informes de datos, Integridad de la información, Resolución de problemas, Revisión de código, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3963972040/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=F4LIZ4L3eybw0Pn67l9RFg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,"Florida, Estados Unidos","Acerca del empleo
LHH Recruitment Solutions is seeking a Data Engineer (AWS), for a direct hire remote opportunity with a rapidly growing industry leader.

**must be located on the East Coast**

Responsibilities:
Improving internal workflows by upgrading infrastructure, optimizing data delivery, and automating tasks.
Setting up roles, databases, schemas, and ETL tools using Snowflake and cloud technologies.
Measuring and tuning SQL performance.
Utilizing SQL and various cloud technologies.
Implementing access control models for systems and data.
Handling data masking, encryption, tokenization, and managing data pipelines.
Configuring AWS S3, EC2, external stages, and SQS/SNS.
Integrating data with MSK Kafka connect and Delta Lake (Databricks).
Developing systems for data extraction, transformation, and loading using AWS and SQL.
Creating tools to analyze data and provide business insights.

Qualifications:
3+ years of experience as a Data Engineer
SQL: For database and query management.
Snowflake: For data modeling and cloud data warehousing.
Python and Java
AWS: Including S3, EC2, SQS, and SNS for cloud services.
ETL Tools: For data extraction, transformation, and loading.
Data Integration Tools: Such as MSK Kafka connect and Delta Lake (Databricks).

Education:
Bachelor's Degree in Computer Science and/or related field

Compensation:
Salary will be commensurate with experience and qualifications.
Benefits: 401(k) matching, Dental insurance, Health insurance, Health savings account, Life insurance, Paid time off, Vision insurance.
Bonus of up to 10%.

Type of Hire:
Full-time, permanent position.","Herramientas ETL, Amazon Simple Notification Service (SNS), Amazon Simple Queue Service (SQS), Data Masking, Snowflake, Snowflake cloud y Tokenización",Solicitud sencilla
https://www.linkedin.com/jobs/view/3964429835/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=ZAaDOF4eQXkr0rHvJ4o2WQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,Estados Unidos,"Acerca del empleo
DataOps Engineer - GCP
Location:- Remote
We are seeking a highly motivated DataOps Engineer with expertise in Google Cloud Platform (GCP or Azure or AWS) to join our growing team. In this role, you will be responsible for designing, developing, and maintaining data platform, ML models, MLOps, DBT Transformations that deliver high-quality data for our machine learning models and business intelligence needs.
Responsibilities:
Design, develop, and implement data machine learning models through entire data lifecycle.
Utilize DBT for efficient Extract, Transform, Load (ETL) or Extract, Load, Transform (ELT) processes within the GCP platform.
Collaborate with data scientists and analysts to understand data requirements and develop solutions for data transformation and cleansing.
Work with Vertex AI or other Machine Learning platforms to understand model use cases and perform model testing for performance optimization.
Develop and maintain automated workflows to streamline data processing tasks.
Implement best practices for data governance and security to ensure data integrity and compliance.
Monitor and troubleshoot data pipelines to identify and resolve issues proactively.
Continuously optimize data pipelines for performance and cost efficiency.
Qualifications:
5+ Years of experience in Data Ops.
Proven experience designing, developing, and deploying data pipelines using cloud technologies (preferably GCP).
Hands-on experience with DBT for data transformation.
Familiarity with Machine Learning concepts, tools like Vertex AI, and Python scripting for model testing.
Strong programming skills in SQL and Python.
Experience working with data warehousing and data modeling concepts (a plus).
Excellent communication and collaboration skills.
Experience in platform engineering is a plus.
A mindset focused on cost reduction and data platform governance.
Telecom background experience is a plus.
Tech Mahindra is an Equal Employment Opportunity employer. We promote and support a diverse workforce at all levels of the company. All qualified applicants will receive consideration for employment without regard to race, religion, color, sex, age, national origin or disability. All applicants will be evaluated solely on the basis of their ability, competence, and performance of the essential functions of their positions","Google Cloud , Herramientas ETL, Python y SQL, Data Operations",Solicitud sencilla
https://www.linkedin.com/jobs/view/3977000842/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=wbdIQbx3uYZ2CDZBYZr9PA%3D%3D&trk=flagship3_search_srp_jobs,CONTRACT: Data Engineer (REMOTE),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 1 mes,"Beaverton, OR","Acerca del empleo
Who We Are

Discogs is the largest crowd-sourced, community-driven database of recorded music information in the world. Every day, millions of people use the Discogs Marketplace to connect, learn about music, and buy and sell vinyl records, CDs, and cassettes. As Discogs continues to grow, we are looking for bright, dedicated, creative, and highly motivated people to help us realize our mission to serve the music fan in everyone. We are relatively small, so individual contributions can have a large impact. High value is placed on quality, critical thinking, and continuous improvement. Our teams work collaboratively but are distributed geographically and open-source tools are important to who we are and how we work. We value the experiences and skills each team member contributes to helping us serve our music community.

Who We're Looking For

We are seeking an experienced Data Engineer. The ideal candidate will have a strong background in managing and integrating data feeds from physical distributors to our online platform, ensuring the timely and accurate vinyl record listings. You will play a crucial role in maintaining data integrity, optimizing data workflows, and supporting our analytics capabilities. This is an individual contributor role and not seeking 3rd party agency assistance at this time.

Location: This is a fully remote position. The ideal candidate will reside in one of the following states but is not required: OR, WA, CA, TX, CO, IL - We will be seeking candidates from MST and PST time zones preferred

Eligibility: This is a 1099 contract role and must be available for minimum 6 months. At this time we cannot support Visa Sponsorship

Compensation Range: $70 - $80 per hour DOE

What You'll Accomplish

Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

Design, develop, and maintain data pipelines to ingest, process, and store data from various physical distributors
Collaborate with distributors to understand their data formats, delivery methods, and schedules
Ensure the timely and accurate integration of new vinyl record information into our online marketplace
Implement data validation and quality checks to maintain the integrity of incoming data
Optimize and automate data workflows to improve efficiency and reduce manual intervention
Work closely with the product, engineering, and analytics teams to support data-driven decision-making
Develop and maintain documentation related to data processes, workflows, and system architecture
Troubleshoot and resolve data-related issues promptly to minimize disruptions
Monitor and enhance the performance of data infrastructure, ensuring scalability and reliability
Stay updated with industry trends and best practices in data engineering and apply them to improve our systems


Requirements

What You'll Contribute

Bachelor's degree in Computer Science, Information Technology, Data Science, or a related field. Master's degree preferred
3+ years of experience as a Data Engineer or in a similar role, preferably within an e-commerce or marketplace environment
Demonstrated understanding of common music metadata standards (e.g., DDEX, MusicBrainz) and familiarity with metadata structures used by music DSPs (Spotify, Apple Music, Deezer, Tidal, YouTube Music) and music distribution companies
Proficiency in data integration and ETL processes
Strong experience with SQL and database management (e.g., MySQL, PostgreSQL)
Experience with data pipeline tools and technologies (e.g., Apache Kafka, Apache Nifi, Apache Airflow)
Knowledge of programming languages such as Python, Java, or Scala
Familiarity with cloud platforms and services (e.g., AWS, GCP, Azure)
Understanding of data warehousing concepts and technologies (e.g., Redshift, BigQuery, Snowflake)
Excellent problem-solving skills and attention to detail
Strong communication and collaboration skills to work effectively with cross-functional teams
Preferred Qualifications:
Experience in the music industry or a strong personal interest in vinyl records
Knowledge of API integration and data exchange formats such as JSON, XML, and CSV
Experience with data visualization tools (e.g., Tableau, Looker)

Benefits

This is contract position and does not qualify for benefits with the company.

What We Believe In

Discogs' mission is to serve the music fan in everyone. We represent a diverse and inclusive community, and we are committed to serving our community with innovative and creative solutions. We know that innovation happens best when varying perspectives are embraced and integrated. Our global team reflects our global community.

Discogs is an Equal Opportunity Employer.

Applicants needing accommodation to apply should contact us at 503-597-6340

If you apply for this role, you will be required to upload a resume, cover letter, and fill out a few questions regarding your application. Once submitted, our hiring team will review your application and contact you if you are selected for an interview. Whether you are successful or not, we will store your application and data in our system for a maximum period of one year from the application date in case another role becomes available that you are suitable for. If you have any questions or concerns about us storing this data and/or the period of time, please contact us at legal@discogsinc.com and we will respond to you within 30 days.","Extraer, transformar y cargar (ETL), Ingeniería de datos , Integración de datos, Scala y Validación del sistemas informáticos (CSV), Automatización, Industria de la música, Resolución de problemas, Validación de datos y Vinilo",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3980071639/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=jwE%2Bci0tPTKJpD5%2FHKplyQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 6 días,Estados Unidos,"Acerca del empleo
About The Position

Velosio is looking for a Data Engineer to be an integral member of the DPS Team.

As a Data Engineer at Velosio, you will be responsible for providing guidance to customers in the assessment, structure, storage, integration, management, and use of data as a corporate asset. You will also support the deployment of our Microsoft-based business applications and migrations/integrations with other additional business apps in the enterprise market space as well as our SMB customers of size. You will provide guidance around the development of the intellectual property as it relates to industry vertical solutions through the use of the Power Platform, Customer Insights, Azure Synapse, and other related tools and technologies. The ideal candidate must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. Excellent written and verbal skills a requirement with comfort presenting and defending assessments, designs, approaches, and technology choices.

Your Day Might Look Like

Deploying Microsoft Fabric in client environments
Developing and maintaining scalable data pipelines and building out / deploying new API’s integrations to support continuing increases in data volume and complexity.
Collaborating with analytics and business teams to assess and improve data models that feed operational, financial, and business intelligence tools that drive increasing data accessibility and fostering data-driven decision-making across the organization.
Implementing Data Governance processes and systems to monitoring data quality, master data management, enhanced security, and redundancy by ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
Deployed and conversant in all technologies related to data warehousing, operational data stores, data lakes, etc.
Performing data analysis required to troubleshoot data-related issues and assist in the resolution of data issues.
Working closely with a team of frontend and backend engineers, product managers, and analysts.
Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.
Designing data integrations and data quality framework.
Designing and evaluating open source and vendor tools for data lineage.
Working closely with all business units and engineering teams to develop strategies for long-term data platform architecture.

What You'll Bring

BS or MS degree in Computer Science or a related technical field
6+ years of increased focus on data
6+ years of SQL/Azure experience
6+ years of experience with schema design and data modeling
6+ years of integration/analytics experience
Microsoft technical certifications.
Experience with or knowledge of Agile Software Development methodologies.
Excellent problem solving and troubleshooting skills.
Process-oriented with great documentation skills.
Excellent oral and written communication skills with a keen sense of customer service.
Conversancy and expertise in: 
Microsoft Fabric
Azure Synapse Analytics
Azure Data Lake
Azure Data Factory
Azure Data Bricks
Azure SQL
Logic Apps / SSIS
Power Automate
Dataverse (Common Data Services)
Web Services / JSON
Microsoft Customer Insights
AWS cloud services: EC2, EMR, RDS, Redshift
Hadoop, Spark, Kafka, etc.
Experience sourcing passive candidates across a variety of departments and various platforms. 

Who We Are

At Velosio, we focus on what matters most - our people. We are a values-driven organization committed to delivering an outstanding employee experience to all our team members.

Velosio’s years of experience with business applications translate to a deep bench of tenured team members with competency across enterprise resource management (ERP), customer relationship management (CRM), and cloud services.

We have earned recognition from Microsoft as a top 1% performing partner worldwide and an emerging NetSuite leader. We have strategic independent software vendor (ISV) partnerships and a portfolio of solution and service offerings to accelerate what’s next for business.

Our mission is to enable clients to realize business value faster, simplify the process of deploying technology, acquire deeper data-driven insights, and explore ongoing innovation to drive business forward. We support the entire Microsoft Dynamics portfolio, Office 365 family and Azure services. Velosio is the only Microsoft Cloud Distributor that specializes in Dynamics 365 and is a prominent Microsoft Master VAR. Headquartered in Columbus, Ohio, Velosio’s 400 employees serve over 4,000 clients.

Beyond our expertise, as a Velosio team member, you can leverage the Velosio award winning culture - a network of top-notch peers, day-to-day flexibility, career development resources and the largest incentive opportunities in the industry.

Some Reasons You Might Like Working With Us

At Velosio, YOU MATTER. 

Due to our proven commitment to delivering an exemplary employee experience, Velosio was awarded Best Company Culture and Best Company for Women by Comparably in 2023, 2022 and 2021, in addition to Best Company for Career Growth, Best Perks & Benefits, and Best Leadership Team by Comparably in 2022 and 2023!

About

Access the following link to see why 100% of current Velosio team members feel their company is invested in their career growth, 99% of current Velosio employees feel their manager cares about them as a person, and 99% of current Velosio team members look forward to interacting with their coworkers every day:

https://www.comparably.com/companies/velosio

At Velosio, YOUR WELLNESS MATTERS.

Benefits

We know one size doesn't fit all, which is why we offer a comprehensive benefits package that allows our team members to create a personalized plan best suited for their unique needs, including:

3 Medical Insurance options with a company contribution to HSA
3 Dental Insurance options including adult orthodontics
3 Vision Insurance options
PTO with added time with each year of service
Remote working environment 
401k Match 50% of the first 6%
StayWell Program - a cash reimbursement up to $600 a year toward Wellness 
Quarterly Incentive Program","Analítica de datos, Análisis de datos, Gobierno de datos y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Comunicación, Modelado de datos, Modelo de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3985504331/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=ZnnkkrRxEzSxM0Q4xavzsg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer III (US) (Business Data Analyst),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Charlotte, NC","Acerca del empleo
This inclusive employer is a member of myGwork – the largest global platform for the LGBTQ+ business community.  

Work Location:

United States of America

Hours

40

Pay Details

$86,840 - $139,360 USD

TD is committed to providing fair and equitable compensation opportunities to all colleagues. The included salary range for this role takes into account multiple factors that are considered in making compensation decisions. The base pay actually offered may vary based upon candidate's skills and experience, job-related knowledge, licensure and certifications, geographic location, and other specific business and organizational needs. As TD puts career development at the forefront of our colleague experience, it is not typical for an individual to be hired at or near the top of the range for their role.

As a candidate, you are encouraged to ask compensation related questions and have an open dialogue with your recruiter who can provide you more specific details for this role.

Line Of Business

Technology Solutions

Job Description

Provide a broad range of data engineering functions including data modeling, data quality, data profiling, data acquisition and ingestion, extract transform load (ETL), metadata enrichment and management, data provenance and lineage, and other specialized data management functions.

Depth & Scope

Expert knowledge of data engineering frameworks, technologies, tools, processes, patterns, and procedures, including how they impact and integrate with other technology areas such as Architecture or Infrastructure
Performs highly complex technical tasks independently
Expert knowledge of TD applications, systems, networks, innovation, design activities, business, organization, best practices, and standards
Designs and develops to meet business and technical requirements; analyzes, adapts, integrates, codes, tests, debugs, and executes
Uses and evolves established patterns to solve complex problems; leads the development of new patterns where necessary
Recognized as primary subject matter expert in multiple areas directly and indirectly related to key accountabilities
Consults with peers and partners across multiple teams on all aspects of research, analysis, design, and support

Education & Experience

Degree, Postgraduate Degree, or Technical Certificate in Data Management or related discipline (e.g. Computer Science, Engineering), or equivalent practical experience
Graduate Degree preferred
5+ years of relevant experience

Customer Accountabilities

Performs data analysis and assesses data management requirements for a specific Platform or Journey, including complex analysis involving multiple pods or products
Maintains expert knowledge of upstream data, including knowledge provided through data profiling, data quality reporting, and via the production of metadata
Supports the acquisition and ingestion of data
Articulates complex, large scale, and high impact technical design and development details to non-technical business partners
Elicits, analyzes, and understands business and data requirements to develop complete business solutions, including data models (entity relationship diagrams, dimensional data models), ETL and business rules, data life-cycle management, governance, lineage, and metadata
Ensures data is maintained in compliance with enterprise data standards, policies, and guidelines
Develops and maintains complex data models using industry standard modeling tools
Develops and maintains complex ETL jobs and frameworks using the Bank's standard tools
Provides support to the development and testing teams to resolve data issues, including escalation support on complex issues
Supports partners and stakeholders in interpreting and analyzing data
Builds effective working relationships within own pod and across partner teams to encourage collaboration on all pod deliverables

Shareholder Accountabilities

Coordinates with technology work teams such as ITS, ARE, Architecture, Enterprise Protect etc. to ensure overall delivery success
Supports the QA team with data analysis/investigations of complex issues/ test cases as part of SIT/UAT/PAT testing
Provides oversight on post implementation activities during the warranty period
Executes & approves code check-in/ check-out into source code repository as part of source code management
Works closely with ITS/ ARE teams to support code packaging & deployment (CI & CD) into higher environments
Is the lead participant in the design & architecture reviews or the application
Raises service-now requests and works with the change management team to support release management activities
Leads data engineering initiatives and capabilities, data governance principles and how they apply across the organization
Ensures metadata and data lineage is captured and compatible with enterprise metadata and data management tools and processes
Adheres & contributes towards standard security coding practices to ensure application is free of most common coding vulnerabilities
Ensures technical decisions, technical risks and lessons learned are identified, clearly documented and enhancements are accordingly implemented
Protects the interests of the organization - identifies and manages risks, and escalates non-standard, high-risk activities as necessary
Adheres to internal policies/procedures and applicable regulatory guidelines
Keeps current on emerging trends/ developments and grow knowledge of the business, related tools, and techniques
Enables team members by sharing knowledge and leveraging engineering best practices

Employee/Team Accountabilities

Participates fully as a member of the team, supports a positive work environment that promotes service to the business, quality, innovation and teamwork and ensures timely communication of issues/ points of interest
Provides thought leadership and/ or industry knowledge for Data engineering best practices and participates in knowledge transfer within the team and business unit
Keeps current on emerging trends/ developments and grows knowledge of the business, related tools and techniques
Participates in personal performance management and development activities, including cross training within own team
Keeps others informed and up-to-date about the status / progress of projects and / or all relevant or useful information related to day-to-day activities
Actively mentors and enables team members by sharing knowledge and leveraging engineering best practices
Supports the team by providing guidance and proactively identifying and resolving issues
Leads, motivates and develops relationships with internal and external business partners / stakeholders to develop productive working relationships
Contributes to a fair, positive and equitable environment that supports a diverse workforce
Acts as a brand ambassador for your business area/function and the bank, both internally and/or externally

Preferred Qualifications

Risk and Regulatory Knowledge

Banking Domain Knowledge

Extensive end user and stakeholder engagement experience.

Who We Are

TD is one of the world's leading global financial institutions and is the fifth largest bank in North America by branches/stores. Every day, we deliver legendary customer experiences to over 27 million households and businesses in Canada, the United States and around the world. More than 95,000 TD colleagues bring their skills, talent, and creativity to the Bank, those we serve, and the economies we support. We are guided by our vision to Be the Better Bank and our purpose to enrich the lives of our customers, communities and colleagues.

TD is deeply committed to being a leader in customer experience, that is why we believe that all colleagues, no matter where they work, are customer facing. As we build our business and deliver on our strategy, we are innovating to enhance the customer experience and build capabilities to shape the future of banking. Whether you've got years of banking experience or are just starting your career in financial services, we can help you realize your potential. Through regular leadership and development conversations to mentorship and training programs, we're here to support you towards your goals. As an organization, we keep growing - and so will you.

Our Total Rewards Package

Our Total Rewards package reflects the investments we make in our colleagues to help them and their families achieve their financial, physical and mental well-being goals. Total Rewards at TD includes base salary and variable compensation/incentive awards (e.g., eligibility for cash and/or equity incentive awards, generally through participation in an incentive plan) and several other key plans such as health and well-being benefits, savings and retirement programs, paid time off (including Vacation PTO, Flex PTO, and Holiday PTO), banking benefits and discounts, career development, and reward and recognition. Learn more

Additional Information

We're delighted that you're considering building a career with TD. Through regular development conversations, training programs, and a competitive benefits plan, we're committed to providing the support our colleagues need to thrive both at work and at home.

Colleague Development 

If you're interested in a specific career path or are looking to build certain skills, we want to help you succeed. You'll have regular career, development, and performance conversations with your manager, as well as access to an online learning platform and a variety of mentoring programs to help you unlock future opportunities. Whether you have a passion for helping customers and want to expand your experience, or you want to coach and inspire your colleagues, there are many different career paths within our organization at TD - and we're committed to helping you identify opportunities that support your goals.

Training & Onboarding

We will provide training and onboarding sessions to ensure that you've got everything you need to succeed in your new role.

Interview Process 

We'll reach out to candidates of interest to schedule an interview. We do our best to communicate outcomes to all applicants by email or phone call.

Accommodation

If you are an applicant with a disability and need accommodations to complete the application process, email the TD Bank US Workplace Accommodations Program at USWAPTDO@td.com. Include your full name, best way to reach you, and the accommodation needed to assist you with the application process.

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.","Arquitectura de datos y Extraer, transformar y cargar (ETL), ERD, Life-cycle Management, Metadatos, Modelo de datos, PAT Testing, Perfiles de datos, Reglas de empresa y Resolución de cuestiones",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3960764344/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=Wq82zC7HLSvV4qAfaHhplw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 semana,"Fort Wayne, IN","Acerca del empleo
Job Requirements

Sweetwater, the nation’s #1 online retailer of pro audio equipment and music instruments, is seeking to hire a Data Engineer to add to our growing Data & Analytics team! In this role, you will work closely with other Data Engineers and Database Administrators to grow and maintain our evolving Data Lake using Google Cloud Platform services and open-source tools. A positive attitude, and the ability to be flexible and collaborate with both those who generate and steward operational data and those who consume data to drive innovation will be essential.

Job Responsibilities

Provide thought-leadership on architecture choices and efficient design for data and data pipelines in Google Cloud.
Work closely with web and application domain experts to identify and extract key data from transactional systems for inclusion in the Data Lake
Collaborate with business leaders to determine data reporting needs that may be shared across business functions
Help drive decision-making by working with the Business Intelligence and Data Science teams in transforming and aggregating core data for ingestion into other systems

Qualifications

Expertise working with Big Data in a cloud environment is required, Google Cloud Platform is preferred.
Working knowledge is required for building and managing ETL/ELT Data Pipelines in the cloud at scale using tools such as Airflow, Kafka, or Spark
Experience designing and populating datasets in NoSQL Datastores, BigQuery is preferred
Proficiency in a scripting language is required, Python preferred
Knowledge or experience with Kafka would be a big plus
Ability to create and maintain shell/bash scripts
Familiarity working in a source-controlled environment is desired
Knowledge of PostgreSQL would be a big plus
A proactive problem solver who takes initiative and can keep up with Sweetwater’s ever-changing, fast-paced environment

Sweetwater’s “Ideal” Team Player

WOWs the Customer – Deep passion and desire for creating amazing customer and colleague experiences
Get Things Done – Great work ethic and moves with a sense of urgency
Obsesses over the Details – Committed to paying attention to the details
Drives Continuous Improvement – Always focusing on effective and efficient work and ways to get better
Develops the Future – Committed to the pursuit of growing personally and professionally and has a focus on bringing your colleagues along with you on that journey

We function best as a unified team, so relocation to our state-of-the-art campus in Fort Wayne, Indiana is required for this role. Interested and qualified candidates, please apply. We look forward to exploring your #FullTimeDream at Sweetwater!

Sweetwater’s culture is one that is built on the creation and celebration of music, which unifies people from all walks of life. Our individual differences make Sweetwater stronger as a company and a better place to work. Having a truly diverse workforce allows us the opportunity to collectively respect, learn, and grow from each unique perspective and experience. We have welcomed thousands of employees and families who have joined the Sweetwater team from all over the nation and the world, and we welcome you, too, to find out why we consider a career at Sweetwater the full-time dream.","Airflow, Analítica de datos, Apache Kafka, Apache Spark, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Lagos de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3984382525/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=Df9IFfsWpd3Cd5Kywc45uQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Golang),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Cedar Park, TX","Acerca del empleo
Job Title: Data Engineer (Golang)

Location: Creve Coeur, MO (Remote)

 Duration: Long term contract

The mission of Client is centered on developing agricultural solutions for a sustainable future that will include a global population projected to eclipse 9.6 billion by 2050. We approach agriculture holistically, looking across a broad range of solutions from using biotechnology and plant breeding to produce the best possible seeds, to advanced predictive and prescriptive analytics designed to select the best possible crop system for every acre.

To make this possible, Bayer collects terabytes of data across all aspects of its operations, from genome sequencing, crop field trials, manufacturing, supply chain, financial transactions, and everything in between. There is an enormous need and potential here to do something that has never been done before. We need great people to help transform these complex scientific datasets into innovative software that is deployed across the pipeline, accelerating the pace and quality of all crop system development decisions to unbelievable levels.

What you will do is why you should join us:

 Be a critical senior member of a data engineering team focused on creating distributed analysis capabilities around a large variety of datasets
 Take pride in software craftsmanship, apply a deep knowledge of algorithms and data structures to continuously improve and innovate
 Work with other top-level talent solving a wide range of complex and unique challenges that have real world impact
 Explore relevant technology stacks to find the best fit for each dataset
 Pursue opportunities to present our work at relevant technical conferences
Google Cloud Next 2019: https://www.youtube.com/watch?v=fqvuyOID6v4
GraphConnect 2015: https://www.youtube.com/watch?v=6KEvLURBenM
Google Cloud Blog: https://cloud.google.com/blog/products/containers-kubernetes/google-kubernetes-engine-clusters-can-have-up-to-15000-nodes
 Project your talent into relevant projects. Strength of ideas trumps position on an org chart


If you share our values, you should have:

 At least 7 years’ experience in software engineering
 At least 2 years’ experience with Go
 Proven experience (2 years) building and maintaining data-intensive APIs using a RESTful approach
 Experience with stream processing using Apache Kafka
 A level of comfort with Unit Testing and Test-Driven Development methodologies
 Familiarity with creating and maintaining containerized application deployments with a platform like Docker
 A proven ability to build and maintain cloud-based infrastructure on a major cloud provider like AWS, Azure or Google Cloud Platform
 Experience data modeling for large scale databases, either relational or NoSQL


Bonus points for:

 Experience with protocol buffers and gRPC
 Experience with: Google Cloud Platform, Apache Beam and or Google Cloud Dataflow, Google Kubernetes Engine or Kubernetes
 Experience working with scientific datasets, or a background in the application of quantitative science to business problems
 Bioinformatics experience, especially large-scale storage and data mining of variant data, variant annotation, and genotype to phenotype correlation


This is a remote position.","Apache Kafka y Ingeniería de datos, Anotación, Datasets, Desarrollo de software, Google Cloud Dataflow, Modelado de datos, Procesamiento de streaming, Protocol Buffers y gRPC",Solicitar
https://www.linkedin.com/jobs/view/3952254337/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=HPJgXA7w93XKVFEVuSQBQA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"90 US$K/año - 110 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",Publicado de nuevo hace 1 semana,"New Haven, CT","Acerca del empleo
Are you ready to join Connecticut Innovation’s vibrant community of innovators? Connecticut Innovations (“CI”) is Connecticut’s strategic venture capital arm, and we are passionate about serving our portfolio of 220+ companies across various industries, with strengths in life sciences, technology, and climate tech.

Come join a quickly growing CT-based startup founded by a Yale MBA, EVident Battery!

EVident Battery develops a comprehensive and non-destructive inspection and scanning solution for EV battery packs. Our technology combines inspection hardware and analytics software to improve transparency and certainty in the EV market.

Data Engineer 
EVident Battery is hiring a Data Engineer to join a collaborative team of experts in various fields, including engineering, machine learning, and EV battery reliability. The Junior Data Engineer will gain invaluable professional development experience in a company setting new industry standards for EV battery services.

What You’ll Do:
The Data Engineer will develop robust data pipelines, integrating state-of-the-art machine learning models, and ensure efficient data management and analysis, leveraging modern technologies like Terraform, HTMX, and Ansible to ensure a seamless, scalable, and interactive experience.
Data Pipeline Management
Develop and maintain data pipelines and ETL processes to ensure seamless data flow and integrity.
Data Management and Storage
Design and implement scalable, high-performance data storage solutions using SQL and NoSQL databases.
Implement automated, reproducible cloud infrastructure provisioning, ensuring a scalable and reliable foundation for data management and application deployment.
Data Analysis and Visualization
Develop APIs and backend services to support data-driven applications and real-time data visualization.
Cross-Collaboration 
Collaborate with data scientists and machine learning engineers to deploy machine learning models into production environments.
Develop new features based on feedback from stakeholders and users to continuously improve our technology platform.

About You:
Current enrollment or completion of a bachelor's degree in computer science, Data Science, or a related degree program.
2+ years of previous work experience as a Data Engineer.
Proficiency in at least one programming language (e.g., Python, Java, C++) and experience with data manipulation libraries (e.g., pandas, NumPy).
Understanding of database management (SQL and NoSQL databases) and principles of data modeling and ETL processes.
Previous experience with cloud platforms (AWS, GCP, Azure) and Infrastructure as Code (IAC) technologies like Terraform (Optional).
Familiarity with backend frameworks (e.g., Flask, Django, Express.js) and API development.
Experience with containerization technologies (Docker, Kubernetes) and CI/CD pipelines is a plus.
Previous internship or project experience in software development, data engineering, or data science is preferred.
Desire to contribute to a quickly growing startup.
Able to work hybrid at the ClimateHaven office in New Haven, CT.

Why Work at EVident Battery?
EVident Battery has award winning recognition. In 2024 EVident Battery was awarded the Sobotka Seed Prize at Yale’s Center for Business and the Environment (CBEY) and secured first place in the startup pitch competition at the Harvard College China Forum.
EVident Battery’s innovative technology solutions are patented, strengthening their market position for future ventures and collaborations in a TAM of $85 billion.
EVident Battery’s team is built by a team of distinguished advisors with entrepreneurial success in battery technology and the automotive sector, including Dr. Yan Wang, Marc Bronzetti, and Chen Chen.
EVident Battery’s non-destructive and cost-effective solution improves transparency and repairability in the post-sale EV market, facilitating the transition to a more sustainable world.

Perks & Growth Opportunities:
Flexible work hours and hybrid working to accommodate your schedule.
Mentorship from experienced professionals in electric vehicle technology and software development.
Opportunity to work on cutting-edge technology that contributes to the advancement of sustainable transportation.
Access to a dynamic, supportive startup culture that values innovation, teamwork, and the personal growth of its employees.

EVident Battery is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.","Amazon Web Services (AWS), Docker, Extraer, transformar y cargar (ETL), Integración continua y entrega continua (CI/CD), Kubernetes, NumPy, Pandas (Software) y Python, Express.js y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3976365280/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=%2Bi9iHQJdAyYX3id5VbsWuQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"70 US$/h - 74 US$/h Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Boston, MA","Acerca del empleo
Job Title: Blockchain Data Engineer
Must have
Crypto knowledge
Strong Python experience
Experience with Data analysis/Database design (ideally Oracle)
Skills
B.S. degree or higher in Computer Science or equivalent experience
Conceptual understanding of Blockchain and crypto based analytics gained in a financial services enterprise environment
Experience with data analysis and database design
2+ years of recent experience with relational database technology, data Engineering and ETL/ELT skills
Working experience with Linux shell scripts and job scheduling tools like Control-M/Autosys
Ability to develop Data APIs to support varied application requirements
Experience with AWS services, Snowflake and Python
Exposure to GitHub, and Jenkins, is desirable.
Knowledge of Finance and Investing domains is a plus
Experience crafting, implementing, and supporting distributed blockchain-based enterprise solutions for the financial/banking/asset management domain with a good understanding of smart contracts for Ethereum based applications/token standard like ERC20, ERC721, and ERC1400.
Experience with diverse blockchain and distributed ledger implementations like Corda, Hyper ledger Fabric, Polygon, Solana, Stellar etc.
Expertise in SQL, identifying patterns and trends in Blockchain data, recommend and define data requirements, mastery in implementing data quality checks to ensure accuracy and completeness.
Experience using data from Blockchain data vendors to generate insights and analytics.
Dexian is a leading provider of staffing, IT, and workforce solutions with over 12,000 employees and 70 locations worldwide. As one of the largest IT staffing companies and the 2nd largest minority-owned staffing company in the U.S., Dexian was formed in 2023 through the merger of DISYS and Signature Consultants. Combining the best elements of its core companies, Dexian's platform connects talent, technology, and organizations to produce game-changing results that help everyone achieve their ambitions and goals.

Dexian's brands include Dexian DISYS, Dexian Signature Consultants, Dexian Government Solutions, Dexian Talent Development and Dexian IT Solutions. Visit https://dexian.com/ to learn more.

Dexian is an Equal Opportunity Employer that recruits and hires qualified candidates without regard to race, religion, sex, sexual orientation, gender identity, age, national origin, ancestry, citizenship, disability, or veteran status.","Analítica de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Autosys, Bases de datos, Calidad de datos, Modelado de datos y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3970463848/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=WMMy%2Bl0%2BVuxMeNCh98cMgw%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer I, Professional Services, Google Cloud","118 US$K/año - 174 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 semanas,"Chicago, IL","Acerca del empleo
The application window will be open until at least July 18, 2024. This opportunity will remain online based on business needs which may be before or after the specified date.

Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Austin, TX, USA; Atlanta, GA, USA; Boulder, CO, USA; Chicago, IL, USA.Minimum qualifications:

Bachelor's degree in Computer Science, Mathematics, a related field, or equivalent practical experience.
3 years of experience with data processing software (e.g., Hadoop, Spark, Pig, Hive) and algorithms (e.g., MapReduce, Flume).
3 years of experience in Google Cloud.
Experience managing client-facing projects, troubleshooting technical issues, and working with Engineering and Sales Services teams.
Experience programming in Python and SQL.

Preferred qualifications:

Experience in technical consulting.
Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT, and reporting/analytic tools and environments.
Experience working with Big Data, information retrieval, data mining, or machine learning.
Experience in building multi-tier high availability applications with modern web technologies (e.g., NoSQL, MongoDB, SparkML, TensorFlow).
Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments.

About The Job

The Google Cloud Consulting Professional Services team guides customers through the moments that matter most in their cloud journey to help businesses thrive. We help customers transform and evolve their business through the use of Google’s global network, web-scale data centers, and software infrastructure. As part of an innovative team in this rapidly growing business, you will help shape the future of businesses of all sizes and use technology to connect with customers, employees, and partners.

As a Data Engineer, you will guide customers on how to ingest, store, process, analyze, explore, and visualize data on Google Cloud Platform. You will lead data migrations and transformations, partner with clients to architect scalable data processing systems, build efficient data pipelines, and resolve platform challenges.

In this role, you will collaborate with Google's strategic cloud customers and our team to successfully implement Google Cloud products.

Google Cloud accelerates every organization’s ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google’s cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.

The US base salary range for this full-time position is $118,000-$174,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.

Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.

Responsibilities

 Act as a trusted technical advisor to customers and solve complex Big Data challenges. 
 Create and deliver best practice recommendations, tutorials, blog articles, sample code, and technical presentations, tailoring approach and messaging to varied levels of business and technical stakeholders. 
Analyze on-premises and cloud database environments and consult on the optimal design for performance and deployment on Google Cloud Platform.
Travel regularly up to 30% of the time, in-region for meetings, technical reviews, and onsite delivery activities.
Communicate effectively via video conferencing for meetings, technical reviews, and onsite delivery activities.


Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .","Ciencia de datos, Computación en la nube, Extraer, transformar y cargar (ETL) y Google Cloud, Ciencias de la computación, Comunicación, Evaluaciones técnicas, Presentaciones técnicas, Resolución de incidencias y Tutoriales",Solicitar
https://www.linkedin.com/jobs/view/3975877473/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=WwwhDIAFZJLPDZ8b8cNvVA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Cupertino, CA","Acerca del empleo
Job Role: Data Engineer

Location: Cupertino, CA (Hybrid)

Duration: 2 Months (High poss. of Extn.)

Key Responsibilities

 Develop and maintain Python scripts for data analysis and automation.
 Design, implement, and manage ETL pipelines using tools such as Apache Airflow.
 Analyze large datasets to identify trends, patterns, and insights.
 Collaborate with cross-functional teams to understand data requirements and deliver actionable insights.
 Conduct quality assurance and validation of data to ensure accuracy and reliability.
 Prepare reports and visualizations to communicate findings effectively.

Desired Skills And Qualifications

 Proficiency in Python programming.
 Hands-on experience with data ETL processes, especially using Airflow.
 Strong analytical and problem-solving skills.
 Experience with human evaluation, prompt engineering, machine learning, statistical analysis is a plus.
 Excellent communication and teamwork abilities.
 Bachelor’s degree in Computer Science, Data Science, or a related field.

Note: The Company is committed to complying with the California Privacy Rights Act (“CPRA”) effective January 1, 2023; and all data privacy laws in the jurisdictions in which it recruits and hires employees. A Notice to California Job Applicants Regarding the Collection of Personal Information can be located on our website. Applicants with disabilities may access this notice in an alternative format by contacting NAhr@spectraforce.com . About Us: Established in 2004, SPECTRAFORCE® is one of the largest and fastest-growing diversity-owned staffing firms in the US. The growth of our company is a direct result of our global client service delivery model that is powered by our state-of-the-art A.I. proprietary talent acquisition platform, robust ISO 9001:2015/ISO 27001 certified processes, and strong and passionate client engaged teams. We have built our business by providing talent and project-based solutions, including Contingent, Permanent, and Statement of Work (SOW) services to over 140 clients in the US, Canada, Puerto Rico, Costa Rica, and India. Key industries that we service include Technology, Financial Services, Life Sciences, Healthcare, Telecom, Retail, Utilities and Transportation. SPECTRAFORCE is built on a concept of “human connection,” defined by our branding attitude of NEWJOBPHORIA®, which is the excitement of bringing joy and freedom to the work lifestyle so our people and clients can reach their highest potential. Learn more at: http://www.spectraforce.com

Benefits: SPECTRAFORCE offers ACA compliant health benefits as well as dental, vision, accident, critical illness, voluntary life, and hospital indemnity insurances to eligible employees. Additional benefits offered to eligible employees include commuter benefits, 401K plan with matching, and a referral bonus program. SPECTRAFORCE provides unpaid leave as well as paid sick leave when required by law.

Equal Opportunity Employer: SPECTRAFORCE is an equal opportunity employer and does not discriminate against any employee or applicant for employment because of race, religion, color, sex, national origin, age, sexual orientation, gender identity, genetic information, disability or veteran status, or any other category protected by applicable federal, state, or local laws. Please contact Human Resources at LOA@spectraforce.com if you require reasonable accommodation.","Airflow, Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Comunicación, Datasets y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3888450280/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=8snjjnKv0Z63lf90dezJfA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,"Irvine, CA","Acerca del empleo
Position: Data Engineer

Location: Irvine/Los Angeles, CA

Duration: Long Term Contract

Qualifications

Experience building data pipelines in a cloud-based environment
Prior experience working on Data Science projects with a focus on Data Analysis and Data Quality
3+ years of experience in development with a focus on backend programming, ETL, and building data pipelines
2+ years of experience engineering complex, high-volume data pipelines using SQL and Python
2+ years of experience in Python (Java is fine as well if we can't find resources with Python experience)
Prior experience with AWS, Jupyter/SageMaker, PostgreSQL (or MS SQL Server), Amazon S3
Mandatory skills: Python(or Java), Postgre (or MS SQL), AWS, Data Analysis
Desired Skills: Jupyter, SageMaker, ETL, Amazon S3

Thanks & Regards

Krishna | IT Minds LLC |

Phone:(949)534-3939 Ext 406 Direct: 949-200-7533| Email: krishna@itminds.net |

: 9070 Irvine Centre DR, Suite 220 | Irvine, CA 92618 |

44075 Pipeline Plaza, Suite 305 | Ashburn, VA 20147|

102, Manjeera Trinity Corporate, Kukatpally, Hyderabad 500072|

www.itminds.net","Almacenamiento de datos, Analítica de datos, Canalizaciones de datos, Ciencia de datos, Desarrollo web back end, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Calidad de datos",Solicitar
https://www.linkedin.com/jobs/view/3965100323/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=kvNf7mOCQ3Rxr6EwcrElgw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 6 días,"San Francisco, CA","Acerca del empleo
Crusoe Energy is on a mission to unlock value in stranded energy resources through the power of computation.

Take a look at what we do! - https://www.youtube.com/watch?v=Rlt8k71Quqw

We aim to align the long term interests of the climate with the future of global computing infrastructure. As data centers consume an exponentially growing power footprint to deliver technology to all connected devices, we are inspired by making sure that the energy meeting that demand is sourced in an environmentally responsible fashion. Crusoe co-locates mobile data centers with stranded energy resources, like flare gas and underloaded renewables, to deliver low-cost, carbon-negative distributed computing solutions. Crusoe Cloud is a managed cloud services platform powered by stranded energy that enables climate-friendly innovation in computationally intensive fields including artificial intelligence, graphics rendering and computational biology.

About This Role:

Crusoe is looking for an ambitious, motivated and experienced engineer to join our Data Engineering team. The Data Engineering team at Crusoe is responsible for building distributed data pipelines that serve multiple teams. You will communicate with the stakeholders to gather requirements and share status on the progress of the projects. Own technical initiatives and deliver the data solutions to provide efficient analytical and reporting capabilities. You will be the data subject matter expert for the initiatives that you own and will make your own decisions with the available context in the best interest of Crusoe.

As a Data Engineer, you will be responsible for serving all data and data visualization needs of the company.

A Day In The Life:

You will build, maintain and support innovating data engineering solutions supporting multiple business areas within Crusoe.
Develop data solutions that process events and aggregate them to report bi-weekly, monthly, quarterly and yearly metrics.
Maintain source code in git and build UDF libraries that enable engineering efficiency and reduce redundancy.
Build Dashboards that are used by colleagues who have varying degrees of technical expertise.
Collaborate with Software Engineering teams to give feedback on data model and data format for events being logged
Communicate on current status of the work and plan the next engineering projects
Enable self service by building intuitive and easy to use systems
Define and implement data engineering standards and practices at Crusoe


You Will Thrive In This Role If:

Expertise in designing and implementing data solutions in a dynamic environment
5+ years experience as a data engineer 
Experience with a data visualization tool
Experience with SQL, ETL, data modeling and a programming language
Knowledge of data warehouse system design and how data pipelines work
Comfortable with git, CI/CD and working on a command line
Experience with ticketing software and issue tracking
Experience in scripting languages a big plus
Exceptional analytical, organizational and communication skills
Proven ability to work independently and easily switch context
Demonstrated ability to foster teamwork and influence peers


Benefits

Hybrid work schedule 
Industry competitive pay
Restricted Stock Units in a fast growing, well-funded technology company
Health insurance package options that include HDHP and PPO, vision, and dental for you and your dependents
Employer contributions to HSA accounts 
Paid Parental Leave 
Paid life insurance, short-term and long-term disability 
Teladoc 
401(k) with a 100% match up to 4% of salary
Generous paid time off and holiday schedule
Cell phone reimbursement
Tuition reimbursement
Subscription to the Calm app
MetLife Legal
Company paid commuter benefit; $50 per pay period


Compensation Range

Compensation will be paid in the range of $125,000 - $185,000. Restricted Stock Units are included in all offers. Compensation to be determined by the applicant’s education, experience, knowledge, skills, and abilities, as well as internal equity and alignment with market data.

Crusoe Energy is an Equal Opportunity Employer. Employment decisions are made without regard to race, color, religion, disability, genetic information, pregnancy, citizenship, marital status, sex/gender, sexual preference/ orientation, gender identity, age, veteran status, national origin, or any other status protected by law or regulation.","Almacenamiento de datos, Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Comunicación, Modelado de datos y Sistema de seguimiento de incidentes",Solicitar
https://www.linkedin.com/jobs/view/3984190296/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=vv9lfmOc1kWTaFgg9vUC0Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Developer- Hybrid,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Fort Mill, SC","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Design, develop, and optimize data pipelines using Snowflake, dbt and Apache Airflow.
Optimize Airflow DAGS for performance, reliability, and scalability.
Implement data transformations and modeling in dbt to support analytics and reporting needs.
Strong proficiency in dbt for data transformations and modeling.
Ensure data quality, integrity and consistency across the data warehouse.
Monitor and troubleshoot performance issues in the data pipelines and data warehouse.
Leverage AWS services like S3 for data storage, processing, or streaming as needed.
Write and optimize complex SQL queries to transform and cleanse data.
Utilize AWS Lambda serverless functions for specific data processing tasks or event-driven architecture.
Continuously explore and learn new technologies, tools, and methodologies relevant to our industry.
Solid SQL programming skills with experience in writing complex queries.
Understanding of data quality best practices.
Excellent communication and collaboration, and problem-solving skills.
Ability to work independently and as part of a team.","Airflow, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Programación y SQL, Calidad de datos, Comunicación, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3963466494/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=Kj9iuZK6fRcKK7dH0a6g2A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 6 días,"Brookfield, WI","Acerca del empleo
Codeworks is an IT Services firm headquartered in SE Wisconsin, known for our strong commitment to quality and for our direct client relationships.

Who We’re Looking For: A Data Engineer with 5+ years of experience, 6 month contract to hire on site in the greater Milwaukee WI Area.

Responsibilities:

Develop and implement data solutions using relational and non-relational databases and business intelligence tools.

Collaborate with Business Analysts and Data Engineers to validate requirements and technical capabilities.

Design and build databases, ensuring they meet project goals and minimize risks.

Create comprehensive documentation according to project and organizational standards.

Estimate project timelines and participate in defining data development processes.

Research and recommend new data technologies aligned with organizational goals.

Participate actively in solution design activities.

Communicate effectively with development teams and stakeholders regarding issues, risks, and changes.

Perform other duties as assigned.

Qualifications:

Bachelor’s degree with at least 1 year of professional experience, or 3+ years of experience in developing data solutions (ETL, reports, data warehouse).

Strong proficiency in SQL and SSIS

Hands-on experience with ETL, data integration, and reporting solutions.

Ability to design, model, and build databases and data structures.

Familiarity with Visual Studio and SSDT.

Proficiency in scripting languages such as C# or Python for data manipulation.

Solid analytical, problem-solving, and troubleshooting skills.

Understanding of source control and CI/CD pipelines.

Knowledge of database administration tasks (backup, restore, logging).

Excellent written and verbal communication skills.

Experience working in iterative delivery processes.

About Codeworks: Codeworks has over 25 years of experience serving Fortune 1000 companies in Wisconsin as well as our client's national locations. Our recruiting team excels at evaluating, advising, and connecting IT professionals with new opportunities that will satisfy their expectations regarding income and opportunity for growth. At Codeworks, we're committed to diversity, equity, and inclusion in our workforce and beyond. We believe in equal opportunities and value the unique perspectives that every individual brings to our team. Join us in creating an inclusive, innovative, and collaborative workplace where your talents can thrive.

Codeworks is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, disability, or national origin.","Almacenamiento de datos, Analítica de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Minería de datos y SQL Server Integration Services (SSIS), Bases de datos, Comunicación, Manipulación de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3965919943/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=oa8WkDRA991f59cvHwPITQ%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, Technical Infrastructure","Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 4 días,"Austin, TX","Acerca del empleo
Minimum qualifications:

Bachelor's degree in Electrical, Process, or Manufacturing Engineering, or equivalent practical experience.
5 years of experience in manufacturing engineering.
2 years of experience in development of databases, ETL, SQL, Analytics, and Machine Learning.

Preferred qualifications:

3 years of experience in full-stack enterprise application development.
Experience in hardware quality and reliability domains.
Excellent communication skills.

About the jobBehind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible.

The US base salary range for this full-time position is $111,000-$163,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.

Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about the benefits at Google .

Responsibilities

Develop analytics applications to deliver on-demand and predictive insights to improve Google's data center quality.
Own data engineering workstreams, including data acquisition, data pipelining, data quality, data prep, and data warehousing.
Lead the technical delivery, implementation, and business adoption of new scalable and reliable data analytics and business intelligence solutions for cross-functional teams.
Design and implement advanced analytics, machine learning models, and rich data visualizations.
Engage stakeholders and partner teams to gain a deep understanding of business needs, and to develop innovative solutions to challenging business problems.


Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Desarrollo de aplicaciones, Ingeniería de fabricación, Integración de aplicaciones empresariales (EAI) y Stack",Solicitar
https://www.linkedin.com/jobs/view/3983054850/?eBP=BUDGET_EXHAUSTED_JOB&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=rM8kVtEMbRAe%2B%2FSodZtOPw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 2 días,Houston y alrededores,"Acerca del empleo
Data Engineer 
Type: Full-Time

Overview:
We are looking for an experienced Data Engineer to join our team. This role is crucial for implementing our data strategy, ensuring that our data is timely, accurate, and actionable. As a key member of our data management team, you will work closely with technical experts to drive operational excellence and support informed decision-making.
Key Responsibilities:
Data Architecture Development and Optimization:
Design and continually refine scalable and resilient data models to meet evolving business needs.
Enhance data architecture to boost efficiency and performance, ensuring high availability and reliability of essential data systems.
Develop solutions for large-scale data collection, storage, processing, and analysis across diverse platforms and environments.
Pipeline Construction and Maintenance:
Build and maintain robust, reusable, and efficient data pipelines to automate the movement and transformation of substantial data sets.
Implement ETL/ELT processes for integrating data from various sources, ensuring accuracy, completeness, and timeliness.
Apply best practices in continuous integration and deployment to optimize data operations and updates.
Data Integration and System Interoperability:
Facilitate seamless data integration across multiple systems to enhance data consistency and accessibility organization wide.
Work with IT and business units to develop and execute data integration strategies that support business objectives and technological frameworks.
Resolve complex data integration challenges, including those involving legacy systems and emerging technologies.
Collaboration and Technical Leadership:
Partner with analytics, IT, and business teams to establish data requirements and system design specifications.
Offer expert guidance and technical leadership in data engineering, mentoring team members on best practices and cutting-edge technologies.
Engage with stakeholders to ensure data engineering projects align with business goals and strategic priorities.
Security Compliance and Data Governance:
Develop and implement policies for data security, quality, and compliance, adhering to regulatory standards and internal guidelines.
Establish and maintain robust data governance frameworks to manage data access, conduct audits, and uphold ethical data practices.
Monitor and address potential security risks related to data handling and storage.
Documentation and Continuous Improvement:
Document all data engineering processes, architectures, and decisions to ensure transparency and future reference.
Continuously assess and enhance data systems and processes, staying updated with industry trends and technological advancements.
Drive the adoption of innovative data technologies and practices to improve overall data management effectiveness and efficiency.
Stakeholder Engagement:
Regularly engage with various business units to understand their data needs and address challenges.
Act as a liaison between technical and non-technical departments, ensuring clear communication and alignment of objectives.
Provide actionable insights and recommendations based on a deep understanding of data structures and system capabilities.
Qualifications:
At least 5 years of experience in a similar role.
Proficient in SQL and relational databases.
Skilled in designing, building, and maintaining data pipelines and workflows using Azure Data Factory and SSIS.
Experience with modern data warehouse technologies such as Fabric, Databricks, or Snowflake.
Experience with data integrations involving APIs.
Familiarity with working in an Azure cloud environment.
Strong analytical and problem-solving abilities.
Excellent communication skills, with the ability to convey complex technical concepts to both technical and non-technical audiences.
Self-driven and capable of working independently in a dynamic business and IT environment.
Benefits:
Opportunities for community involvement and outreach programs
Comprehensive Medical, Dental, and Vision insurance
Paid Vacation, Holidays, and PTO
401(k) with discretionary 8% match
Profit Sharing
New Home Discounts for Team Members & Families
Product Discounts for Team Members
College Scholarship Program
Sabbaticals
And more!
If you’re passionate about data engineering and eager to contribute to a leading organization, we invite you to apply and join our dynamic team.","Almacenamiento de datos, Azure Databricks y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3921566123/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=96Pllvgjk5mKLIUIRLcvyA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer / Architect,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",Publicado de nuevo hace 2 semanas,"Mineápolis, MN","Acerca del empleo
Do you have Data Engineering experience, and are you seeking a new job in Minneapolis? Horizontal Talent is helping a collaborative company recruit a contract Data Engineer, and the hybrid role comes with an attractive pay rate.

As a Data Engineer, you will collaborate directly with the Digital Analytics team to design, document, build, and refine a scalable data framework to support digital reporting and analytics. You will also design and build data pipelines to move decentralized data into a common data warehouse.

During your first few weeks in this Data Engineer role, you can expect to begin work on some of the following:

Define and document data requirements to meet business needs
Develop logical and physical database models
Write ETL scripts and commands
Analyze and organize raw data to be translated into usable data tables for consumption
Review & resolve database performance, capacity, data integrity, and replication issues


To apply for this Data Engineer/Architect position, your soft skills, expertise, and experience should include:

Experience in scripting for automation using SQL, including experience writing DML and DDL statements- 5+ years
Knowledge of data warehousing and data modeling
Experience designing Power BI semantic models
Comfortable working with GitLab or GitHub


In return for your passion, collaborative approach, and commitment, you'll receive a generous pay rate, joining a friendly and inclusive culture.

If this contract hybrid Data Engineer/Architect job motivates and inspires you, please don't hesitate to apply with Horizontal Talent today. We would love to help you get your next role.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Lenguaje de definición de datos (DDL), Modelado de datos, Necesidades empresariales y Secuencia de comandos",Solicitar
https://www.linkedin.com/jobs/view/3956582194/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=zAg54J8XjKYJej%2FpdY45qA%3D%3D&trk=flagship3_search_srp_jobs,Junior/Entry Level Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Filadelfia, PA","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación y Programación, Comunicación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3978469414/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=ZeQinWoOaBm%2Fh5hFZu2oXA%3D%3D&trk=flagship3_search_srp_jobs,"Data Science / Analytics Developer _ Tucker, GA. 30084/ Onsite - MUST be Local","Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Tucker, GA","Acerca del empleo
Job Title:Data Science / Analytics Developer

Location: Tucker, GA. 30084/ Onsite - MUST be Local Metro Area Candidates

Duration: 6+ months of contract with strong possibility of extension

Interview: Either Web Cam or In Person

Onsite from Day 1

1st interview can be virtual (video required), 2nd interview in person

Job Description

Job Summary:

Under general supervision, designs, codes, tests, modifies and debugs computer software. Researches and analyzes program or systems problems and develops program documentation. Translates business requirements into development activities in secure and maintainable code.

The ideal candidate is an enthusiastic, creative, highly motivated, and goal-oriented team player with at least three years of data visualization (SAS strongly preferred) experience with strong knowledge of SQL and python for ETL and database programming.

Essential Duties And Responsibilities

Develop / maintain current and future SAS data reports and analyses.
Develop / maintain current and future ETL scripts and code.
Ad hoc reporting and analysis using data stored in Oracle and Microsoft SQL Server databases
Accurately and effectively communicate with the Project Manager and/or Team Lead the level of effort to implement proposed solutions, the status of on-going work, and any potential issues. 
Work well in both a team environment as well as a sole contributor with little or no supervision.
Possess excellent time management skills in handling multiple priorities with individual deadlines.
Adapt to a rapidly changing environment and work creatively with minimal supervision.
Ability to work from written specifications and pre-established guidelines.
Possess excellent written and oral communication skills.

Education Qualifications

Bachelor’s degree in computer science, Data Science, or a related scientific or technical degree; or equivalent experience.

Technical Skills/Experience

Minimum three years' experience in data science applications. Very strong preference for SAS programming experience.
Minimum three years' experience in python coding.
Minimum three years' experience in database coding.
Minimum three years' experience in data cleansing/ preparation for reporting and visualization.
Strong knowledge of and experience with Oracle and Microsoft SQL Server databases.
Strong understanding of object-oriented programming principles.
Strong knowledge of and experience with python.
Strong knowledge of and experience with SAS, SAS Enterprise Guide, SAS Visual Analytics, and SAS Viya strongly preferred.
Understanding of the Unix shell for tasks in Linux environment.","Ciencia de datos, Oracle Database, Programación, Python y Visualización, Bases de datos, Cleansing, Informes ad hoc, Limpieza de datos y Programación SAS",Solicitud sencilla
https://www.linkedin.com/jobs/view/3963385945/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=V106i9pauJFkeY3ZS8dBcg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Dallas, TX","Acerca del empleo
Data Engineer - SSIS/SSRS & SQL

Dallas, TX

Fulltime 

Job Description

Build and maintain ETL processes in SSIS and Azure Data Factory
Engineer scalable, reliable and performant systems to manage data
Develop, implement and optimize stored procedures and functions
Research and analyze data issues and provide automated solutions
Implement new technologies to enhance the optimization of current practices
Provide valuable suggestions regarding new ideas and technologies

Location

Dallas, TX

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL Server Integration Services (SSIS), Bases de datos, Optimización, Procedimientos de almacenado y SQL Server Reporting Services (SSRS)",Solicitar
https://www.linkedin.com/jobs/view/3971244413/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=cluoVhXmtyXpnBoDUDeezg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Tier 2 Support,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Dallas, TX","Acerca del empleo
Job Title: Data Engineer - Tier 2 Support

Work location: Los Angeles, CA / Dallas, TX.

Job Summary: We are seeking a Data Engineer with expertise in Teradata and Vantage Cloud Lake to join our Tier 2 support team. The ideal candidate will be familiar with Goldengate tools and experienced in data synchronization processes. This role requires excellent problem-solving skills and the ability to analyze and resolve issues within ETL jobs effectively.

Key Responsibilities

Manage and optimize data synchronization processes using Teradata and Vantage Cloud Lake.
Utilize Goldengate tools to ensure data accuracy and consistency across distributed systems.
Troubleshoot and resolve issues in ETL jobs, implementing fixes to prevent future occurrences.
Collaborate with cross-functional teams to improve data flow and quality.
Monitor data systems performance and adjust configurations to enhance efficiency.
Document technical procedures and configurations related to data management and recovery.

Qualifications

Proven experience in data engineering with a strong focus on Teradata and Vantage Cloud Lake.
Familiarity with Goldengate or similar data integration tools.
Strong problem-solving skills and experience in troubleshooting ETL job failures.
Ability to work collaboratively in a fast-paced, team-oriented environment.
Excellent communication skills and the ability to document and explain technical details clearly.
Bachelor’s degree in computer science, Information Systems, or a related field.

Preferred Skills

Familiarity with SQL and script writing.
Experience in a Tier 2 support role or similar.
Knowledge of data warehousing best practices and principles.","Almacenamiento de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Comunicación, Escritura de guiones, GoldenGate, Resolución de incidencias, Resolución de problemas, Sincronización de datos y Teradata",Solicitud sencilla
https://www.linkedin.com/jobs/view/3973505200/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=bIFp%2BnKC0XGyT452T3sc8A%3D%3D&trk=flagship3_search_srp_jobs,Database Administrator/Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Dubuque, IA","Acerca del empleo
We are seeking a skilled Database Administrator/Data Engineer with a strong emphasis on reporting and data analytics. The ideal candidate will be responsible for ensuring the performance, integrity, and security of databases while also developing and maintaining complex reports and dashboards that provide actionable business insights.

Responsibilities

Managing all aspects of Microsoft SQL Server, including installation, patching and upgrading; securing access; performance tuning and troubleshooting; assisting with the creation and optimization of T-SQL queries and stored procedures.
Developing and scheduling SSIS packages and SSRS reports.
Developing reports using PowerBI.
Designing data marts, warehouses, and lakes.

Requirements

Good oral and written communication.
Curious.
Analytical mindset.
Excited by opportunities to develop new skills.
Gets along well with others.
Strong experience with Microsoft SQL Server, ETL processes, reporting tools, and data organization.
Bachelor degree or higher in a computer related discipline.

Join our team and take your career to the next level as a valued member of our information technology team!

About Cottingham & Butler

At Cottingham & Butler, we sell a promise to help our clients through life’s toughest moments. To deliver on that promise, we aim to hire, train, and grow the best professionals in the industry. We look for people with an insatiable desire to succeed, are committed to growing, and thrive on challenges. Our culture is guided by the theme of “better every day” constantly pushing ourselves to be better than yesterday – that’s who we are and what we believe in.

As an organization, we are tremendously optimistic about the future and have incredibly high expectations for our people and our performance. Our ability to grow as a company, fuels investments in new resources to better serve our clients and provide the amazing career opportunities our employees want and deserve. This is why we are a growth company and why we are committed to being better every day.

Want to learn more? Follow us on www.CottinghamButler.com | LinkedIn | Facebook","Data Marts, Extraer, transformar y cargar (ETL), Microsoft SQL Server y SQL Server Integration Services (SSIS), Administración de bases de datos, Bases de datos, Comunicación, Comunicación escrita, Diseño de bases de datos y Procedimientos de almacenado",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982796248/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=dPbxu7cRrL2ryI2FS6F7NA%3D%3D&trk=flagship3_search_srp_jobs,Tableau Power BI Developer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Navtech, LLC, is seeking the following. Apply via Dice today!

Role Tableau Power BI Developer

Location Austin TX

(CANDIDATES NEED TO BE in TX ONLY)

Duration 12 months

Our Client is looking to add an Sr Tableau Power BI Developer With 15 Years of Experience.

Understands business objectives and problems, identifies alternative solutions, performs studies and cost/benefit analysis of alternatives. Analyzes user requirements and , procedures, and problems to automate processing or to improve existing computer system

Contractor will be creating dashboards and reports using Tableau or PowerBI as determined by the business requirements. Contractor will translate business requirements into Tableau/PowerBI reports, and work with the program staff to ensure it meets the needs of the customer. Contractor will participate in testing or reports and dashboards to ensure proper function, and work with coworkers to publish and promote the reports as needed.

Experience Creating reports or data analysis, using Tableau, PowerBI, SQL, Cognos, Business Objects, QlikView or similar

Experience working with very large scale data sets in Oracle, SQL Server, MySQL, Snowflake and working with Tableau creating dashboards and reports with Power BI creating dashboards and reports

sshyam at navtechconsulting com","Analítica de datos, Análisis de datos, SAP BusinessObjects, SQL y Tableau, Expresiones de análisis de datos (DAX), Panel de control, QlikView, Snowflake y Snowflake cloud",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3981399538/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AblVH0TmLg7tfzL1RtSmFQ%3D%3D&trackingId=ocfWlmztK%2B5vI9V5pB2AwA%3D%3D&trk=flagship3_search_srp_jobs,Data Governance Engieer/ Analyst,"50 US$/h - 60 US$/h Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 días,"Pittsburgh, PA","Acerca del empleo
Innova Solutions is immediately hiring for a Data Governance Analyst
 Position type: Contract
Duration: 12 Months
Location: Pittsburgh, PA
 As a Data Governance Analyst:
 You will Review auto-generated data quality issues and self-identified data issues from line of businesses (e.g., review issues reported, understand problem statement/root cause, perform quality assurance, survey for downstream impact and risk assessment) by coordinating and driving meetings with appropriate Subject Matter Expert.
 The ideal candidate will have:
  5+ years of experience as Data Engineering/Analyst
Should be good with Data Governance profiles - Data Profiling and Quality are the top skills needed
Collibra - Will need to have experience with data profiling and issue remediation (Would prefer experience with Collibra but open to Informatica as well)
Should understand physical data element, critical data element, data quality rules/dimensions, etc.
Must have strong communication - will be working with stakeholders
Should have good basic experience with SQL
Should have some understanding of ServiceNow
  Qualified candidates should APPLY NOW for immediate consideration! Please hit APPLY to provide the required information, and we will be back in touch as soon as possible.
 PAY RANGE AND BENEFITS:
Pay Range*: $50.00 - $60.00 per hour
*Pay range offered to a successful candidate will be based on several factors, including the candidate's education, work experience, work location, specific job duties, certifications, etc.
 Benefits: Innova Solutions offers benefits( based on eligibility) that include the following: Medical & pharmacy coverage, Dental/vision insurance, 401(k), Health saving account (HSA) and Flexible spending account (FSA), Life Insurance, Pet Insurance, Short term and Long term Disability, Accident & Critical illness coverage, Pre-paid legal & ID theft protection, Sick time, and other types of paid leaves (as required by law), Employee Assistance Program (EAP).
 ABOUT INNOVA SOLUTIONS: Founded in 1998 and headquartered in Atlanta, Georgia, Innova Solutions employs approximately 50,000 professionals worldwide and reports an annual revenue approaching $3 Billion. Through our global delivery centers across North America, Asia, and Europe, we deliver strategic technology and business transformation solutions to our clients, enabling them to operate as leaders within their fields.
 Recent Recognitions: 
One of Largest IT Consulting Staffing firms in the USA - Recognized as #4 by Staffing Industry Analysts (SIA 2022)
ClearlyRated® Client Diamond Award Winner (2020)
One of the Largest Certified MBE Companies in the NMSDC Network (2022)
Advanced Tier Services partner with AWS and Gold with MS
Website: https://www.innovasolutions.com/
 Innova Solutions is an Equal Opportunity Employer and prohibits any kind of unlawful discrimination and harassment. Innova Solutions is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment on the basis of race, color, religion or belief, national origin, citizenship, social or ethnic origin, sex, age, physical or mental disability, veteran status, marital status, domestic partner status, sexual orientation, or any other status protected by the statutes, rules, and regulations in the locations where it operates. If you are an individual with a disability and need a reasonable accommodation to assist with your job search or application for employment, please contact us at or (770) 493-5588. Please indicate the specifics of the assistance needed. Innova Solutions encourages all interested and qualified candidates to apply for employment opportunities. Innova Solutions (HireGenics/Volt) does not discriminate against applicants based on citizenship status, immigration status, or national origin, in accordance with 8 U.S.C. § 1324b.
The company will consider for employment qualified applicants with arrest and conviction records in a manner that complies with the San Francisco Fair Chance Ordinance, the Los Angeles Fair Chance Initiative for Hiring Ordinance, and other applicable laws.
 
Desired Skills and Experience

Collibra OR Informatica AND ServiceNow, SQL

American Cybersystems, Inc is acting as an Employment Agency in relation to this vacancy.","Analítica de datos, Arquitectura de datos, Gobierno de datos, Ingeniería de datos , SQL y Visualización de datos, Calidad de datos, Comunicación, Perfiles de datos y ServiceNow",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3969077996/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=4f6doyicFnynPVVXooRw%2BA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"125 US$K/año - 155 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
We’re a cognitive science company on a mission to optimize the measurement of brain health to advance the development of new medicines and to enable earlier clinical insights in healthcare.

That’s why we’re seeking a Data Engineer responsible for building and maintaining data pipelines in Azure Databricks to enable Cogstate’s data strategy. The candidate will have a central critical role for establishing and maintaining the Azure Databricks infrastructure, provide best practices and recommendations for using these tools, and will working closely with data scientists, data managers, clinical scientists, and other business stakeholders as a point of contact for the data platform infrastructure.

*Salary Range: $125,000 - $155,000 per year (commensurate with experience). Benefits listed below.


Core Responsibilities
Design, develop, and maintain scalable data pipelines using Azure Data Factory, and integrate data from various sources and systems into a coherent data ecosystem.
Utilize Azure Databricks for complex data transformation and processing tasks, ensuring high performance and alignment with business requirements.
Manage Azure Functions for efficient data ingestion and processing, optimizing for performance and cost.
Implement and maintain data storage solutions in Azure Data Lake Storage Gen2 (ADLS2), applying best practices for data lifecycle management, security, and access control.
Collaborate with cross-functional teams to define data requirements, deliver comprehensive data solutions, and support data-driven decision-making processes.
Employ GitHub for source control management, adhering to best practices for code reviews, branching, and versioning.
Facilitate CI/CD pipelines using GitHub Actions to automate testing, integration, and deployment processes, enhancing productivity and ensuring high-quality deployments.
Monitor, troubleshoot, and optimize data systems and pipelines for performance, reliability, and scalability.
Stay abreast of industry trends and advancements in cloud and data engineering technologies, continuously seeking opportunities for innovation and improvement.
Maintain Data Governance infrastructure to ensure appropriate data access rights and restrictions for internal and external users.

Qualifications
BS/BA in Computer Science, Data Science, or a related field or relevant experience
5+ years’ experience as a data engineer. 
Must have 2+ years experience in implementing data engineering solutions with Apache Spark, preferably Databricks.
Must have deep expertise in one of the programming languages for data processes (Python, Scala). Experience with Python, PySpark, Hadoop, Hive and/or Spark to write data pipelines and data processing layers.
Direct experience with relational databases like SQL Server. Excellent SQL experience for writing complex SQL transformations.
Strong knowledge on Databricks configuration, troubleshooting and performance tuning.
Experience with development tools for CI/CD, unit and integration testing, automation and orchestration, including GitHub, Azure Data Factory and Azure functions.
Familiarity with Agile workflow methodologies, demonstrating an ability to work effectively in a dynamic, iterative development environment and manage tasks with varying priorities.

Benefits
Remote Work Practices: Cogstate is a virtual first company. Cogstate employees can work from anywhere where Cogstate is registered to business within the United States, Australia, or the United Kingdom!
Generous Paid Time-off: Cogstate employees receive 20 days of vacation leave, 10 days of personal leave and 10 paid public holidays, unused days roll over year after year.
401(k) Matching: As you invest in yourself and your future, Cogstate invests in you too: we match up to 3% of your yearly salary in Cogstate’s 401k program
Competitive Salary: We offer competitive base salaries plus additional earning opportunities based on the position.
Health, Dental & Vision Coverage: We've invested in comprehensive health & dental insurance options with competitive company contributions to help when you need it most. We also offer free vision insurance for all full-time employees.
Short-Term & Long Term Disability Life Insurance: 100% employer sponsored
Pre-Tax Benefits: Healthcare and Dependent Care Flexible Spending Accounts
Learning & Development Opportunities: Cogstate offers a robust learning program from mentorships to scholarships to Coursera to improve knowledge or obtain certifications in applicable areas of interest.


Why We Do What We Do
Our mission at Cogstate is to optimize the measurement of brain health to advance the development of new medicines and to enable earlier clinical insights in healthcare. We are driven by the notion that through our work we impact the health of communities by delivering solutions that combine breakthrough science with advanced technologies. We are inspired by the dedication of researchers and the resilience of patients, and we are strengthened by our 20-year heritage supporting them. We are always working to simplify the complex with solutions that offer insights and hope for the future of healthcare, particularly dementia-related diseases and rare and pediatric disorders.

Commitment to Diversity & Inclusion
Cogstate is committed to building and maintaining a fair, diverse and inclusive workplace where the personal worth of each employee is recognized, and all are respected and valued for their differences. Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. If you need assistance in applying please email PeopleandCulture@cogstate.com.","Apache Spark, GitHub, Ingeniería de datos , Metodologías ágiles, Microsoft SQL Server, Python y Scala, Azure Databricks, Pruebas de integración y Transformación de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3894436196/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=vS33GOozhp9dRSRTCJ3pzw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 meses,"San Francisco, CA","Acerca del empleo
Intro description:

Legalist is an institutional alternative asset management firm. Founded in 2016 and incubated at Y Combinator, the firm uses data-driven technology to invest in credit assets at scale. We are always looking for talented people to join our team.

As a highly collaborative organization, our data engineers work cross-functionally with software engineering, data science, and product management to optimize growth and strategy of our data pipeline. In this position, you will be joining the data engineering team in an effort to take our data pipeline to the next level.

Where you come in:

Design and develop scalable data pipelines to collect, process, and analyze large volumes of data efficiently.
Collaborate with cross-functional teams including data scientists, software engineers, and product managers to understand data requirements and deliver solutions that meet business needs.
Develop ELT processes to transform raw data into actionable insights, leveraging tools and frameworks such as Airbyte, BigQuery, Dagster, DBT or similar technologies.
Participate in agile development processes, including sprint planning, daily stand-ups, and retrospective meetings, to deliver iterative improvements and drive continuous innovation.
Apply best practices in data modeling and schema design to ensure data integrity, consistency, and efficiency.
Continuously monitor and optimize data pipelines and systems for performance, availability, scalability, and cost-effectiveness.

What you'll be bringing to the team:

Bachelor's degree (BA or BS) or equivalent.
A minimum of 2 years of work experience in data engineering or similar role.
Advanced SQL knowledge and experience working with a variety of databases (SQL, NoSQL, Graph, Multi-model).
A minimum of 2 years professional experience with ETL//ELT, data modeling and Python.
Familiarity with cloud environments like GCP, AWS, as well as cloud solutions like Kubernetes, Docker, BigQuery, etc.
You have a pragmatic, data-driven mindset and are not dogmatic or overly idealistic about technology choices and trade-offs.
You have an aptitude for learning new things quickly and have the confidence and humility to ask clarifying questions.

Even better if you have, but not necessary:

Experience with one or more of the following: data processing automation, data quality, data warehousing, data governance, business intelligence, data visualization.
Experience working with TB scale data.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y Visualización de datos, Bases de datos, Calidad de datos y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3982425961/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=D9%2Blf%2BTiNvu7XUMIG%2FRApg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer I,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Frisco, TX","Acerca del empleo
A Data Engineer I works alongside senior Data Engineers, Information Architects, Data Scientists, and other stakeholders to design and maintain small to moderate data models. The Data Engineer I is responsible for day-to-day monitoring and maintaining small to medium-scale databases that support web applications and other digital services. The Data Engineer I is responsible for supporting reports that provide accurate and timely data for internal and external clients. The incumbent will assist with advancing a data infrastructure that powers our ability to make timely and data-driven decision. This role requires an entry-level familiarity of data architecture and extraction & manipulation of data sets of small to medium sizes.

Position Responsibilities

Design and Methodology

Perform day-to-day monitoring of critical business flows and take ownership of triaging incidents.
Create and maintain data tables in data analysis tools (i.e., Informatica, Snowflake).
Utilize data from multiple cloud sources and develop/maintain dashboards.
Review existing code to ensure performance and reliability of data extraction and processing.
Review data products for analytics and Data Scientist team members to improve their productivity.
Update and optimize local and metadata models.
Document and test small to moderate data systems that bring together data from disparate sources.
Manage existing pipelines from a variety of sources (relational, XML, etc.)
Review and maintain solutions to track data quality, stabilize data pipeline, etc. to ensure reliable operations.
Test data processes including performance of through data validation and verification.

Technical Consulting

Work with development teams to create conceptual data models and data flows.
Review modifications of existing systems for cross-compatibility.
Evaluate implemented data systems for variances, discrepancies, and efficiency.
Participate in data quality initiatives and troubleshooting.
Participate in sprint planning meetings as needed.
Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions.
Optimize support for ad-hod analysis across various data sources.

Planning & Organizing

Ensure best practices are followed across architecture, codebase, and configuration.
Review and make recommendations for ongoing reporting and analysis processes, automating or simplifying self-service modeling and production support for customers.
In collaboration with the senior level Data Engineers, contribute to the exploration and understanding of new tools and techniques for improvements to the data pipeline.

Risk Management

Review and are familiar with automated processes for performance and fault tolerance.
Review functional and technical designs to build familiarity of risk and any missing requirements.
Review security measures to protect data from unauthorized access or misuse.
In collaboration with senior level Data Engineers, design backup and recovery procedures to ensure data integrity is maintained.

Administration/Support

Keeps management informed of status of on activities through accurate, timely, and appropriate reporting.
Contribute to Data Governance, system documentation and sharing of data asset knowledge.
Actively participates in committees representing the department and/or planning unit.
Keeps abreast of leading-edge technologies in the Data Engineering space.

Position Qualifications

Bachelor's Degree from an accredited university in Computer Science, Engineering or in a Technology related field OR equivalent through a combination of education and/or technology experience OR 12 years of technology experience
4 years of experience in Data Engineering, BI Engineering or related field in architecting and developing end-to-end scalable data applications and data pipelines
4 years of development experience in extracting, transforming/manipulating, and loading data sets of various sizes using Informatica IICS cloud or Snowflake and experience using CI/CD automation, Jenkins etc.
4 years of experience using programming languages Python and querying languages (SQL)
4 years of working knowledge of different databases (e.g., SQL & NoSQL)
3 years of experience developing strong collaborative relationships with key partners in data engineering, business intelligence, software development, finance, modelling, and product teams
2 years of experience working with software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes and testing
2 years of experience using AWS or Azure cloud technologies leveraging S3, Lambda, EC2 resources

Licenses/Certificates

Preferred, Informatica CDI or Snowflake SnowPro or Amazon Web Services (AWS) Certified Solutions Architect

17 Cowboys Way

8:00am - 5:00pm Monday - Friday","Canalizaciones de datos, Ingeniería de datos y SQL, Arquitectura técnica, Calidad de datos, Ciencias de la computación, Diseño técnico, Modelo de datos, Snowflake y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3955921373/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=RIMLrn9h0rreTO1xaMWE3g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Nueva York, NY","Acerca del empleo
About The Company

Our client is a private equity real estate firm and is hiring a Data Engineer who can sit fully remote.

Responsibilities

Develop, construct, test and maintain architectures
Develop data set processes
Build data systems and pipelines
Build algorithms and prototypes
Evaluate business needs and objectives
Data acquisition and ingestion – Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and Alteryx
Assist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference data
Create and optimize data models to support various business applications
Automate and support workflows to ensure timely delivery

Requirements

5+ years data engineering experience
Proficient programming capability, Python preferred
Experience designing data models and data warehouses and using SQL database management systems along with data processing using traditional and distributed systems
Experience with data modeling, data warehousing, and building ETL pipelines
Experience working within financial services industry and/or familiarity of different asset classes is preferred

Salary Range

$150,000-$250,000","Almacenamiento de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Automatización, Bases de datos, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3872028750/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=nCl0ZSZyToxQvzpLFpxBLg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 4 meses,Estados Unidos,"Acerca del empleo
Remote Opportunity

Healthcare Experience Required

Please find below JD: Need Profiles ASAP

Strong experience with HL7 FHIR concepts and terminology
Strong experience with Google Cloud Healthcare Data Engine, Whistle, and BigQuery
Strong experience with common data engineering tools (Spark, Python, shell scripting)
This is the 2nd category – these are engineers with strong experience mapping Hl7 to FHIR and strong google native stacks experience,
This common Ingestion Framework spits out Hl7 messages – this is where 2nd block of work start. HL7 messages need to be converted to FHIR using GCP native stack ( big query + whistle) –
Folks who have hands on experience of mapping hl7 to fhir will be required along with hands on experience with GCP native stack.

**We are an Equal Opportunity Employer**

https://primevcs.com/jobs","Apache Spark, Buena práctica clínica, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery, Google Cloud y Ingeniería de datos, Fast Healthcare Interoperability Resources (FHIR), HL7 y Stack",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982279117/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=M45WUxPhmy342cRzaIPS2w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Remote),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Hoffman Estates, IL","Acerca del empleo
Job Description

JOB SUMMARY:

The Data Engineer will sift through structured and unstructured data from diverse sources to design and build solutions to support BI, data science and operational needs. Will work closely with business leaders, product managers and data scientists to understand the potential business value of data sets and ultimately build data processing pipelines around those data sources. In addition to development around new data sources, the candidate will deliver provisioning capabilities to end users as well as core frameworks and infrastructure that supports a rapidly growing number of use cases.

Reports To

Director

Responsibilities/Skills/Experience Requirements

JOB DUTIES/RESPONSIBILITIES: 

 Inform, influence, support, and execute our product decisions and product launches 
 Manage data warehouse plans for a product or a group of products. 
 Interface with engineers, product managers and product analysts to understand data needs. 
 Partner with Product and Engineering teams to solve problems and identify trends and opportunities. 
 Build data expertise and own data quality for allocated areas of ownership. 
 Design, build and launch new data extraction, transformation and loading processes in production. Support existing processes running in production. 
 Define and manage SLA for all data sets in allocated areas of ownership. 
 Work with data infrastructure to triage infra issues and drive to resolution. 
 Build platform infrastructure and interfaces between and within GCP, AWS, and other big data platforms in support of BI and operational needs 
 Provide guidance and support to big data ETL engineer on performance and facilitate ETL performance tuning 
 Performs other duties as assigned 

JOB REQUIREMENTS:

 Bachelors Degree 
 5-10 years of related experience 
 Valid Driver License for the State of employment 
 18 years of age or older (except some locations which may allow for 16 and 17 year old individuals) 

REQUIRED SKILLS:

 BS/BA in Technical Field, Computer Science or Mathematics. 
 Experience with DBT (data build tools) required 
 Experience and focus on operations data stores to build out fundamental data models 
 4+ years’ experience in the data warehouse space. 
 4+ years’ experience in custom ETL design, implementation and maintenance. 
 4+ years’ experience working with either a Map Reduce or an MPP system. 
 4+ years’ experience with schema design and dimensional data modeling. 
 4+ years’ experience in writing SQL/ NoSQL statements. 
 4+ years’ experience using Python or Java 
 Ability to analyze data to identify deliverables, gaps and inconsistencies. 
 Communication skills including the ability to identify and communicate data driven insights. 
 Ability in managing and communicating data warehouse plans to internal clients 
 Experience or demonstrated interest in big data technologies 
 Experience with Hadoop or other big data platforms 
 Experience in coding 
 Strong communication and presentation skills 

Years Experience

5 - 10 Years Experience

Travel Requirements

None

Country

United States

Work-In City

Remote

Work-In State

Remote

Work-In Postal Code

Remote

Business

Transformco Corporate

Job Function

Engineering/Quality

Employment Category

Regular, Full-time

Compensation Range

90k-100k

Additional Compensation Explanation

N/A

EEO/EOE Footer

Equal Opportunity Employer / Disability / Vet.

Posting Tags

#Remote, #Technology, #HSCorporate

Company Brand

Transformco

Location City

HOFFMAN ESTATES","Almacenamiento de datos, Big data, Extraer, transformar y cargar (ETL), Hadoop, NoSQL y SQL, Build Tools, Ciencias de la computación, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3912534519/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=vAA3eeDQtSti22AlRD5KAg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer 3 (REMOTE),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Temporal
Coincide con tus preferencias de empleo. El tipo de empleo es Temporal.
Intermedio",hace 3 meses,Estados Unidos,"Acerca del empleo
DivIHN (pronounced “divine”) is a CMMI ML3-certified Technology and Talent solutions firm. Driven by a unique Purpose, Culture, and Value Delivery Model, we enable meaningful connections between talented professionals and forward-thinking organizations. Since our formation in 2002, organizations across commercial and public sectors have been trusting us to help build their teams with exceptional temporary and permanent talent.

Visit us at https://divihn.com/find-a-job/ to learn more and view our open positions.

Please apply or call one of us to learn more

For further inquiries regarding the following opportunity, please contact one of our Talent Specialists

Lavanya | 224 369 0873 

Title: Data Engineer 3 (REMOTE)

Location: Remote

Duration: 9 Months

Description

 Maintain, build, and optimize data warehouse, developing ETL solutions, automated data flow, and analytics “as-a-service” environment.
 Design and develop scalable, high-performance data processing solutions on AWS using tools like AWS Lambda, Step Functions, Amazon Redshift, and AWS Glue.
 Collaborate with stakeholders across the organization to understand business requirements and design data solutions that meet their needs
 Monitor actively the health of the environment in terms of data integrity, data pipeline performance, and overall security, to ensure resolutions are developed, implemented, and communicated efficiently
 Implement and maintain data governance policies and procedures to ensure data quality, security, and compliance
 Develop and maintain data architecture and infrastructure to support the growth and expansion of the data platform
 Design and develop tools to constantly monitor the health of the environment and ensure incidents communicated efficiently and resolved quickly.
 Mentor and guide junior data engineers in the team
 Drive and implement automation to empower the organization to scale efficiently and with flexibility
 Keep up to date with new technologies and methodologies related to data engineering and apply them to improve the data platform

Summary

The main function of a data engineer is to coordinate changes to computer databases, test, and implement the database applying knowledge of database management systems. A typical data engineer is responsible for planning, coordinating and implementing security measures to safeguard the computer database.

Job Responsibilities

Test programs or databases, correct errors and make necessary modifications.

Modify existing databases and database management systems or direct programmers and analysts to make changes.

Write and code logical and physical database descriptions and specify identifiers of database to management system or direct others in coding descriptions.

Skills

Verbal and written communication skills, problem solving skills, customer service and interpersonal skills.

Basic ability to work independently and manage ones time.

Basic knowledge of database management software.

Education/Experience

Associate's degree in computer programming or equivalent training required. 5-7 years experience required.

Need someone strong is Python, AWS, sql and certified ideally

About Us

DivIHN, the 'IT Asset Performance Services' organization, provides Professional Consulting, Custom Projects, and Professional Resource Augmentation services to clients in the Mid-West and beyond. The strategic characteristics of the organization are Standardization, Specialization, and Collaboration.

DivIHN is an equal opportunity employer. DivIHN does not and shall not discriminate against any employee or qualified applicant on the basis of race, color, religion (creed), gender, gender expression, age, national origin (ancestry), disability, marital status, sexual orientation, or military status.","Extraer, transformar y cargar (ETL), Ingeniería de datos y Programación, Administración de bases de datos, Bases de datos, Comunicación, Comunicación escrita, Habilidades sociales, Programación informática y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982884248/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=K3UPrjnND2n1T2QQCUoi6w%3D%3D&trk=flagship3_search_srp_jobs,Databricks Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,Estados Unidos,"Acerca del empleo
Harmonia Holdings Group, LLC is an award-winning minority and female owned federal government contractor committed to providing innovative, high-performing solutions to our government clients and focused on fostering a workplace that encourages growth, initiative, creativity, and employee satisfaction.

We are seeking an experienced Databricks Data Engineer to join our dynamic team. The successful candidate will play a crucial role in building and managing data pipelines, processing large datasets, and ensuring efficient and secure data management practices. They will also be responsible for inspiring the adoption of advanced analytics and data science across the organization.

Required Skills

Previous experience in a data engineering role or similar
Experience with data processing and pipeline management
Familiarity with industry-standard data engineering platforms and tools
Strong data manipulation and processing skills
Proficiency in data engineering tools such as Databricks, Apache Spark, Delta Lake, MLflow, and SQL
Understanding of the Databricks Lakehouse Platform, its workspace, architecture, and capabilities
Ability to perform multi-hop architecture ETL tasks using Apache Spark SQL and Python
Knowledge of incremental data processing in batch and streaming mode
Familiarity with open-source tools, cloud computing, machine learning, and data visualization
Strong interpersonal skills and a collaborative work style
Understanding of security and governance best practices
Strong problem-solving and analytical skills
Excellent written and verbal communication abilities Major Duties/Tasks:
Build and manage data pipelines for data engineering applications
Process and manipulate large datasets efficiently and securely
Work with data engineering platforms and tools such as Databricks, Apache Spark, Delta Lake, MLflow, and SQL
Maintain best practices around security and governance
 Model data management solutions
Manage, test, and deploy code
Provide expertise on data concepts to the advanced analytics group
Install continuous pipelines of filtered information for data analysts and scientists to access relevant datasets
Collaborate with cross-functional teams to ensure effective data utilization


Minimum Required Qualifications

US citizenship is required.
Public Trust clearance.
Bachelor's degree in a relevant field or equivalent combination of education and experience
7+ years of experience in the data engineering field
3+ years of experience in a data analytics environment or the intelligence community 
Master's degree in a relevant field (may substitute for 3 years of general experience)


Certifications: Databricks Certified Data Engineer Associate certification

As per the Executive Order on Ensuring Adequate Covid Safety Protocols for Federal Contractors and regulations as detailed by www.saferfederalworkforce.gov , it is recommended that all federal government contractors be vaccinated against Covid-19, unless approved for an exemption/ accommodation on the basis of a sincerely held religious belief or medical circumstance.

Here At Harmonia We Are Pleased To Have Been Repeatedly Recognized For Our Outstanding Work Culture, The Innovative Work We Do, And The Employees On Our Team Who Make a Difference Each Day. Some Of These Recognitions Include

Recognized as a Top 20 ""Best Place to Work in Virginia""
Recipient of Department of Labor's HireVets Gold Medallion
Great Place to Work Certification for five years running
A Virginia Chamber of Commerce Fantastic 50 company
A Northern Virginia Technology Council Tech 100 company 
Inc. 5000 list of fastest growing companies for eleven years
Two-time SBA SBIR Tibbett's Award winner
Virginia Values Veterans (V3) Certification


We recognize that every bit of our success is the result of our teams of hard-working, motivated, and innovative professionals who are proud to call themselves part of the Harmonia family! In addition to competitive compensation, a family-focused culture, and a dynamic, productive work environment, we offer all full-time employees a variety of benefits including, but not limited to

Traditional and HSA- eligible medical insurance plans w/ Wellness Incentives for employees and family
100% employer-paid dental and vision insurance options 
100% employer-sponsored STD, LTD, and life insurance
Veterans Cohort
Gym membership reimbursement
401(k) matching
Dollar-for-dollar 501(c)(3) donation matching
Flexible-schedules and teleworking options
Paid holidays and Flexible Paid Time Off
Adoption Expense Reimbursement
Paid Parental Leave
Professional development and career growth opportunities and paid training days
Employer-sponsored Employee Assistance Program for employee and family
Team and company-wide events, recognition, and appreciation-- and so much more! 


Check out our LinkedIn, Facebook, and Instagram to find out a little more about who we are and if we are the right next step for your career! 

Harmonia is an Equal Opportunity Employer providing equal employment opportunity to all employees and applicants for employment without regard to race, color, religion, national origin, age, gender, gender identity, sexual orientation, disability, or genetics. Harmonia does and will take affirmative action to employ and advance in employment individuals with disabilities and protected veterans. To perform the above job successfully, an individual must possess the knowledge, skills, and abilities listed; meet the education and work experience required; and must be able to perform each essential duty and responsibility satisfactorily. Other duties in addition to those listed may be assigned as necessary to meet business needs. Reasonable accommodation will be made to enable an applicant with a disability to successfully apply for and/or perform the essential duties of the job. If you are in need of an accommodation, please contact HR@harmonia.com.","Analítica de datos, Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Comunicación, Datasets, Habilidades sociales, Manipulación de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982637519/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=N%2FuduCbjCg2UNfF3nBBxAQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,Estados Unidos,"Acerca del empleo
On this journey for over 12 years, Qventus is leading the transformation of healthcare. We enable hospitals to focus on what matters most: patient care. Our innovative solutions harness the power of machine learning, generative AI, and behavioral science to deliver exceptional outcomes and empower care teams to anticipate and resolve issues before they arise.

Our success in rapid scale across the globe is backed by some of the world's leading investors. At Qventus, you will have the opportunity to work with an exceptional, mission-driven team across the globe, and the ability to directly impact the lives of patients. We’re inspired to work with healthcare leaders on our founding vision and unlock world-class medicine through world-class operations.

As a Data Engineer you will join our team and contribute to the next evolution of Qventus's products. This role involves enhancing our data processing and analytics capabilities with a strong focus on healthcare operational efficiency. You will work closely with our Founders and engineering team to develop innovative solutions that impact patient care.

Key Responsibilities:

Translate product / analytical vision into data pipelines and functional updates to support high-quality and highly trusted data products and the overall end-to-end workflow of data users at Qventus (including Leading targeted development of data structures, data transformation pipelines, data acquisition, etc.).
Improve the data quality and transparency of the pipelines (defining data requirements, identifying and implementing data observability tooling - lineage, sources, transformations).
Monitor, maintain, and iterate on key elements of the data platform infrastructure to proactively maintain stable and highly trusted services.
Work closely with core team members to develop, test, deploy, and operate high-quality, scalable software and raise engineering standards.


What We’re Looking For: 

Demonstrated experience in building, debugging, and enhancing production transformation pipelines in close collaboration with data stakeholders
Aptitude for interpreting complex datasets, including the ability to discern underlying patterns, identify anomalies, and extract meaningful insights, demonstrating data intuition and analytical skills.
Experience developing & coordinating execution in a fast-paced, dynamic environment across multiple technologies & platforms (AWS experience preferred)
Strong cross-functional communication - ability to break down complex technical components for technical and non-technical partners alike
3+ years of professional experience working with modern programming languages such as Java, C/C++, and Python with a dedication to high code quality.


It’s a Plus if You Have… 

Degree in Computer Science, Engineering, or related field, or equivalent training/experience
Competence in participating in technical architecture discussions to help drive high-quality technical development within your team
Practical hands-on experience with:
Building large-scale, high-complexity metrics and monitoring
ELK, DBT, Snowflake, AWS, Terraform, Looker, Ansible experience
Experience building and maintaining robust and efficient backend data systems with functional proficiency with AWS cloud services & modern data warehouse services (Snowflake)
The salary range for this role is $95,000 to $135,000. Qventus salary bands represent market data across different geographies. We consider several factors when determining compensation, including location, skills and qualifications, and prior relevant experience. Salary is just one component of Qventus’ total package. Some of our key benefits and perks* include but are not limited to: Open Paid Time Off, paid parental leave, professional development, wellness, and technology stipends, generous employee referral bonus, and employee stock option awards.

We believe that diversity, equity, inclusion, and belonging are fundamental to improving healthcare and society, and that’s why we’re building a company that leads the way. We hold ourselves accountable to using fair hiring processes that mitigate the negative impacts of unconscious bias. We also work to ensure that people from underrepresented groups play meaningful roles on both sides of the interview table. We are an equal opportunity employer and give all qualified applicants consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.

Candidate information will be treated in accordance with our candidate privacy notice which can be found here: https://qventus.com/ccpa-privacy-notice/

Employment is contingent upon the satisfactory completion of our pre-employment background investigation and drug test.

Benefits and perks are subject to plan documents and may change at the company's discretion.","Almacenamiento de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Arquitectura técnica, Bases de datos, Calidad de datos, Ciencias de la computación, Comunicación, Datasets y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3955026059/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=Uksrx5I7RRhvyegwPnHaZw%3D%3D&trk=flagship3_search_srp_jobs,Data Science Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,América del Norte,"Acerca del empleo
Applicants must reside in one of the following states to be eligible for remote work: AL, AZ, CT, FL, GA, ID, IN, KS, KY, LA, MD, MO, NV, NC, OH, OK, OR, PA, TX, WA, WI

Who We're Looking For

We're seeking out an enthusiastic and motivated Data Science Engineer to join our Analytics and Data Science team.
You are a self-motivated teammate with a curiosity for all things data-related and love to solve problems.
You have worked with large structured and unstructured datasets to help reveal patterns and solve business problems.
You have contributed to models, algorithms, and orchestration to solve business problems, drive growth, and increase marketing effectiveness.
You are excited by the challenge of building something from the ground up.
You ask lots of questions and are continually learning.
You take pride in your hard work and are committed to advancing your knowledge and expertise.
You thrive on collaboration and drive to advance team-based projects and processes.


What We'll Expect From You

As a Data Science Engineer, you'll work with a team of Data Scientists on subjects like Customer Lifetime Value, churn, customer experience, and call center activity.
Enrich data assets to extract valuable business insights and improve the predictive power of ML models.
Focus on feature engineering, analytics, and the presentation of findings that help drive business results.
Identify and understand valuable data assets and automate ingestion/ETL processes.
Clean, prep and verify the integrity of data for use in ML or reporting.
Develop sophisticated and automated pipelines for ML and Data processing.
Demonstrate a high degree of critical thinking and problem-solving skills.
Undertake processing of structured and unstructured data. Advanced skills in Spark is preferred.
Analyze large amounts of information to discover trends and patterns.
Present information using data visualization techniques.
Propose solutions and strategies to business challenges.
Analytical with a knack for data manipulation, problem-solving, and storytelling.
You have a passion for creating end-to-end data pipelines along with a solid interest and understanding of ML and AI.


What You'll Bring To Us

5+ years of relevant professional experience.
Experience in data mining, processing, and modeling.
Ability to solve complex analytical problems.
Experience in Microsoft tools including Azure Data Factory, Azure Synapse, Data Bricks, and Spark.
Some experience in data engineering, data science, or operations research/analysis is preferred.
Demonstrable knowledge of common coding languages (Python, SQL, R) with the ability to learn new skills and apply skills in new environments.
Experience using business intelligence tools (e.g. Power BI) and data frameworks (e.g. relational models).
Ability to collaborate with colleagues (IT, Business, and Data Science) and work well with vendors/partners.
Earned at least a Bachelor’s Degree in Computer Science, Mathematics, Statistics, Applied Mathematics, Data Science, or quantitative field.
Excellent communication and presentation skills.
Analytical mind and business acumen, specific experience in Marketing and customer analytics is preferred.


About Consumer Cellular

Consumer Cellular is the top-rated wireless provider that provides cellphones and no-contract service plans primarily to those 50+. Founded over 25 years ago on the belief that everyone should have affordable access to the safety and convenience of cellular service, they have become well known for their 100% U.S. based, award-winning customer support. Consumer Cellular has been honored by J.D. Power as #1 in Customer Service among Value MVNOs, 15 Times in a Row, making it the Most Awarded Brand for Wireless Customer Support among Value MVNOs. Additionally, the company has been ranked on the Inc. 5000 list 12 times. The company has been an approved AARP Provider for over 14 years and offers AARP members exclusive discounts on service. The Scottsdale, AZ. based company is privately held with 2300 employees and utilizes the nation's largest voice and data networks, which cover 99 percent of the U.S. population. Consumer Cellular's wireless phones and plans are sold nationwide at leading retailers such as Target and Walmart, as well as directly to consumers at ConsumerCellular.com or (888) 345-5509. For J.D. Power 2022 Wireless Customer Care Mobile Virtual Network Operator Study award information, visit jdpower.com/awards. For cellphone tutorials, features, applications, and company news, connect with Consumer Cellular on Facebook and Youtube.

Pay & Benefits Data (in accordance with the Equal Pay and Opportunities Act)

 Minimum Salary: $100,000
 Maximum Salary: $140,000


This information reflects the anticipated base salary range for this position based on current national data. Minimums and maximums may vary based on location. Individual pay is based on skills, experience and other relevant factors. Our Talent Acquisition team are able to answer any additional questions you may have as you move through the selection process. As part of our Total Rewards package, Consumer Cellular, Inc. offers a broad range of Health, Life, Voluntary Lifestyle and other benefits and perks that enhance your physical, mental, and emotional wellbeing.

Competitive base pay with potential for shift differential, overtime and bonus pay
Medical insurance (98% company-paid for full-time employee only coverage)
Dental and Vision insurance (100% company-paid for full-time employee only coverage)
401(k) company match of 100% up to 6% of your pay
Discounted Consumer Cellular wireless phone plan for employees
Paid Time Off (PTO) available following a 30-day waiting period*
6 company-paid holidays plus 16 hours of floating holiday accrual per year
Flexible Spending Accounts (FSA) for health care and dependent care expenses
Life and AD&D insurance equal to 1x your annual earnings (100% company-paid)
Long-Term Disability insurance (100% company-paid)
Short-Term Disability insurance (100% company-paid)
Employee Assistance Program (100% company-paid)
Education reimbursement
Employee rewards program
Accrue up to 40 hours in 1st year for hourly positions and up to 120 hours for salaried positions.


Pre-employment Background Check And Drug Screen Is Required.

Primary Location

United States-Work from Home (Remote)

Job

Information Technology

Schedule

Full-time

Travel

Yes, 5 % of the Time

Job Posting

Jul 26, 2024

Unposting Date

Aug 1, 2024","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Pensamiento crítico, Ciencias de la computación, Comunicación, Manipulación de datos, Matemáticas aplicadas y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3976471540/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=6ZNeYEUJxNN7tgY8B%2BPErQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"San Diego, CA","Acerca del empleo
We are looking for talented and motivated professionals to join our team! We invite you to apply for our open position(s) below.

OPEN POSITION ANNOUNCEMENT!

Data Engineer

Open until filled.

Posted 05/14/2024 San Diego County – hybrid work schedule set by SDCP

About the role: The San Diego Community Power (SDCP) is seeking a seasoned Data Engineer to join our growing team of analytics experts who will be responsible for designing, maintaining, expanding, and optimizing our data infrastructure for data collection, management, transformation, and access. A key priority of this role will be to assist in SDCP's development of centralizing its data eco-system to allow for creation of pipelines that convert raw data into usable formats for data analysts and other data consumers to utilize. The Data Engineer will handle the core data aspects of software engineering and data science and utilize software engineering principles to develop algorithms that automate the data flow process. They will collaborate with data and system analysts to build machine learning and analytics infrastructure from testing to deployment.

WHO IS SAN DIEGO COMMUNITY POWER?

San Diego Community Power is a community-driven, not-for-profit public agency providing cleaner energy to the San Diego region. Formed in 2019, Community Power is the second largest Community Choice Aggregator (CCA) in California, serving nearly 1 million customer accounts across its member agencies: the Cities of San Diego, Chula Vista, Encinitas, Imperial Beach, La Mesa and National City, as well as the unincorporated areas of San Diego County.

OUR HISTORY

San Diego Community Power was formed in 2019 as a public, not-for-profit community choice aggregator (CCA) in the San Diego region. We began electric service in 2021 and serve five member agencies: San Diego, Chula Vista, Encinitas, La Mesa and Imperial Beach, County of San Diego, and National City. SDCP was formed to bring local control and customer choice to San Diego while also providing clean and renewable energy at competitive rates. By the end of 2023, SDCP will provide electricity for nearly half the electric load in San Diego Gas and Electric's service territory and will be the second largest CCA in California. For more information, please visit SDCommunityPower.org.

COMMITMENT TO DIVERSITY

At SDCP, we value diversity and are committed to creating an inclusive environment for all employees. We represent a diverse customer base and hope to hire employees that reflect our communities. We provide equal employment opportunities to all applicants for employment and prohibit discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.

ESSENTIAL DUTIES AND PRIMARY RESPONSIBILITES

Building and maintaining a centralized cloud-based data infrastructure for optimal extraction, transformation, and loading of data from a wide variety of sources. 
Assemble large, complex data sets that meet functional/non-functional business requirements. 
Developing data tools and APIs for data analysis. 
Deploying and monitoring machine learning algorithms and statistical methods in production environments to solve organizational needs. 
Ensuring data accessibility and security and implementing company data policies regarding data privacy and confidentiality. 
Improving data systems reliability, speed, and performance. 
Build analytics tools that utilize the data pipeline to provide actionable insights into customer trends, operational efficiency, and other key business performance metrics. 
Collaborating with other internal teams, data analysts, and other stakeholders to understand and optimize how data can be leveraged to meet business needs. 
Assist in drafting data and analytics' solicitations and in the selection of consultants. 
Assist with drafting staff reports and presentations for Board and Committee meetings. 
Performs other related duties and responsibilities as required. 

MINIMUM KNOWLEDGE, SKILLS, AND ABILITES

Advance working knowledge of SQL, experience working with relational databases, query authoring (SQL) and working familiarity with various databases. 
Demonstrated experience in developing API and integrations to support analytics in cloud environment. 
Experience building and optimizing 'big data' data pipelines, architectures, and data sets. 
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. 
Strong analytic skills related to working with unstructured datasets. 
Build processes supporting data transformation, data structures, metadata, dependency, and workload management. 
A successful history of manipulating, processing, and extracting value from large, disconnected datasets. 
Working knowledge of message queuing, stream processing, and highly scalable 'big data' data stores. 
Good understanding of data architecture, data design, data warehousing and data modeling concepts. 
Strong project management and organizational skills. 
Experience supporting and working with cross-functional teams in a dynamic environment. 

PREFERRED KNOWLEDGE, SKILLS, AND ABILITES

Experience in the energy sector and/or supporting the implementation of programs funded by California state agencies (e.g., California Public Utilities Commission, California Energy Commission, California Air Resources Board).
Ability to strongly represent the organization in various professional engagement settings from local to national convenings (conferences, workshops, executive meetings, etc.).

PREFERRED QUALIFICATIONS, EDUCATION AND EXPERIENCE

Minimum of five (5) years of professional experience in a Data Engineer role, preferably in the energy industry with a graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. The candidate should also have experience using the following software/tools:

Experience with big data tools: Hadoop, Spark, PowerBI, Kafka, etc.
Experience with AWS, Azure and Google Cloud services.
Experience with relational SQL and NoSQL databases.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with open-source technologies: Python, Pytorch, Flask, Tensorflow or Keras
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Java, C++ etc.
Experience with batch, micro-batching and real-time data ingestion methodologies.
Experience with GitHub or similar code repositories

WORK ENVIROMENT & CONDITIONS

Prolonged periods sitting at a desk and working on a computer. The position requires occasional carrying, lifting and/or moving objects up to 25 pounds. Occasional local travel required and reliable transportation to be able to attend SDCP events, meetings, and workshops as needed is expected. At SDCP we work in the communities we serve and in the office. SDCP works to ensure a safe and healthy workplace for employees and in our communities. SDCP requires employees to be fully vaccinated for COVID-19. SDCP is an agency required to adopt and promulgate a Conflict-of-Interest Code (""COI""). The COI code requires employees in designated positions, including those identified under the interim disclosure process to file a Statement of Economic Interests (Form 700) on an annual basis. A Successful candidate accepting this position may be required to file Conflict of Interest forms subject to the regulations of the Fair Political Practices Commission.

COMPENSATION: 

Salary Range: The position salary range is: $110,400 to $135,800; with exact compensation to be determined by SDCP, depending upon experience.

Benefits: Standard benefits package including but not limited to:

Insurance: SDCP offers group health benefits, including medical, vision, and dental insurance, for eligible FT employees. Also provided is a $100,000 Life & AD&D policy, STD and LTD coverage that is 100% paid by SDCP.

Retirement: SDCP offers a 457(b) plan for employee contributions and contributes 10% of eligible compensation to the employee's Money Purchase Plan.

Paid Time Off: 11 holidays per year + paid winter holiday (between 12/24-12/31), 160 hours of accrued paid time off per year (increases with time in service), and 96 hours per year of accrued paid sick leave.

This job description may not be inclusive of all assigned duties, responsibilities, or aspects of the job described, and may be amended at the discretion of SDCP as needed. 

California Pay Range

$110,400—$135,800 USD","Almacenamiento de datos, Analítica de datos, Arquitectura de datos, Canalizaciones de datos y Ciencia de datos, Bases de datos, Datasets, Ingesta de datos, Modelado de datos y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3904669639/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=IVz31bUkUg%2BCxBm5xT%2BK7g%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/ Engineer.,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 3 meses,"Richmond, VA","Acerca del empleo
Immediate need for a talented Data Analyst/ Engineer. This is a 12+ months Contract opportunity with long-term potential and is located in Richmond, VA/McLean, VA/Plano, TX (Hybrid). Please review the job description below and contact me ASAP if you are interested.

Job ID: 24-19393

Pay Range: $45 - $50/hour. Employee benefits include, but are not limited to, health insurance (medical, dental, vision), 401(k) plan, and paid sick leave (depending on work location).

Key Responsibilities:
  Communicating with stakeholders to understand data content and business requirements.
Build well-managed data solutions, tools, and capabilities to enable self-service frameworks for data consumers.
Partner with the business to provide consultancy and translate the business needs to design and develop tools, techniques, metrics, and dashboards for insights and data visualization.
Troubleshooting, debugging, maintaining and improving existing reporting solutions.
Demonstrate ability to explore and quickly grasp new technologies to progress varied initiatives.
Drive analysis that provides meaningful insights on business strategies.
Drive an understanding and adherence to the principles of data quality management. including metadata, lineage, and business definitions.
Build and execute tools to monitor and report on data quality.
Key Requirements and Technology Experience: 
Skills: Data Analyst/ Engineer, Python, SQL, AWS - basics, Spark, Tableau/Quicksight.
Experience working in AWS environment (S3, Snowflake, EC2, APIs).
Skilled in coding languages (Python,SQL,Spark).
Ability to thrive in a fast paced, evolving work environment Experience with BI tools like Tableau, Quicksight.
Previous experience building and executing tools to monitor and report on data quality
Excellent communication skills (written and verbal).
Having a sense of ownership and craftsmanship around the code base.
Open to learning about new technologies and sharing your knowledge with others.
Our client is a leading Financial Industry, and we are currently interviewing to fill this and other similar contract positions. If you are interested in this position, please apply online for immediate consideration. 

Pyramid Consulting, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.

#was3","Amazon QuickSight, Analítica de datos, Análisis de datos, Ciencia de datos, Tableau y Visualización de datos, Calidad de datos, Comunicación, Metadatos y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3894831897/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=gSgqgoRzGclc02R8k%2Becjg%3D%3D&trk=flagship3_search_srp_jobs,Snowflake Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 meses,"Virginia, Estados Unidos","Acerca del empleo
Job Title: Snowflake Data Engineer 

Location: VA

Duration: 2 Years

Job Type: C2C 

Work Type: Remote

Job Description

Are you a Data Engineer working at a Large Financial Institution and being told by your leadership that you are too hands-on or detail-oriented or think and work like a start-up?

We are looking forward to you joining our Platform Engineering Team.

Our Platform Engineering Team is working to solve the Multiplicity Problem. We are trusted by some of the most reputable and established FinTech Firms. Recently, our team has spearheaded the Conversion & Go Live of apps that support the backbone of the Financial Trading Industry.

We Are Looking For Engineers Who Can

Design, develop, and maintain data pipelines to ingest, transform, and load data from various sources into Snowflake
Implement ETL (Extract, Transform, Load) processes using Snowflake's features such as Snowpipe, Streams, and Tasks
Design and implement efficient data models and schemas within Snowflake to support reporting, analytics, and business intelligence needs
Optimize data warehouse performance and scalability using Snowflake features like clustering, partitioning, and materialized views
Integrate Snowflake with external systems and data sources, including on-premises databases, cloud storage, and third-party APIs
Implement data synchronization processes to ensure consistency and accuracy of data across different systems
Monitor and optimize query performance and resource utilization within Snowflake using query profiling, query optimization techniques, and workload management features
Identify and resolve performance bottlenecks and optimize data warehouse configurations for maximum efficiency
Work on Snowflake modeling - roles, databases, schemas, ETL tools with cloud-driven skills
Work on SQL performance measuring, query tuning, and database tuning
Handle SQL language and cloud-based technologies
Set up the RBAC model at the infra and data level
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks)
Setup AWS S3/EC2, Configure External stages, and SQS/SNS
Perform Data Integration e.g. MSK Kafka connect and other partners like Delta Lake (data bricks)


We work closely with

★ Data Wrangling

★ ETL

★ Talend

★ Jasper

★ Java

★ Python

★ Unix

★ AWS

★ Data Warehousing

★ Data Modeling

★ Database Migration

★ ECreLT

★ RBAC model

★ Data migration

Kindly please share your resumes with srikar@isoftteckinc.com or 707-435-3471
Aptitudes y experiencia deseables
AWS
Data Modeling
Data Warehousing
Data migration
Database Migration
ETL
EcreLT
Jasper
Java
Python
RBAC model
Talend
Unix
★ Data Wrangling","Extraer, transformar y cargar (ETL) y Herramientas ETL, Ajuste de consultas, Bases de datos, Data Masking, Disputas de datos , Materialized Views, Modelo de datos, Optimización de bases de datos y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984757649/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=3hthpDRoG3OufuCBsgxvlw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Alpharetta, GA","Acerca del empleo
Syensqo is all about chemistry. We’re not just referring to chemical reactions here, but also to the magic that occurs when the brightest minds get to work together. This is where our true strength lies. In you. In your future colleagues and in all your differences. And of course, in your ideas to improve lives while preserving our planet’s beauty for the generations to come.

We are looking for:
We are currently looking for a Data Engineer who will participate in projects within the CM Excellence Group in North America and support digital transformation in North America.

We count on you for: 
To support several manufacturing sites performing their excellence improvement projects by participating in the assessment of their industrial improvement potential covering End-to-End Transformation of CM Sites.
Support / challenge industrial management of these sites to define challenging targets and objectives and achieved the expected results
Deploy within those sites the change management methodology and toolbox, training and supporting local Change Agents
Ensure the exchange of best practices and lessons learned between sites, GBUs and between Manufacturing & Energy Excellence Experts.
Support deployment of improvement plans in various areas of manufacturing (production, quality, reliability) with strong components of change management from the preparation until delivery of sustainable results in collaboration with change teams in the manufacturing sites
Transfer of change management expertise
Inspire a mindset of continuous improvement, and establish the sustainability of improvement
Keep track of change project results (in cooperation with ME corporate team and change leaders on the site)
Deploy and improve the Production Management System.
Train and coach using the ME methodologies and toolbox
Regular alignment with the ME Leaders in the GBUs


You can count on us for: 
We offer the opportunity to join an exciting growth company
A full range of benefits as expected of a successful company
Exciting career opportunities in a fast moving environment
An extensive training package


You will bring:
Strong technical and process expertise with experience in production (process and/or manufacturing) for instance in plastics, chemicals or oil & gas industries.
Familiar with demanding (GMP, automotive, …) quality standards and their impact on product lifecycle.
Education: B.S. Industrial Engineering
Experience: 5 years + in an Industrial Engineering role
Familiar with the industrial environment, results-oriented and culture of continuous improvement.
Leadership personality and leadership skills
Coaching, Influencing and motivating capabilities
Highly flexible and mobile
Used to collaborating in a global multicultural environment
Language: English, second language is a plus (Spanish, French, German)


You will get:
Competitive salary and benefits package
The U.S. base salary range reasonably expected to be paid for this position is $90,880 to $113,600 per year 
Since actual compensation packages are based on a variety of factors unique to each candidate we may ultimately pay more or less than the posted base salary range. Total compensation for this role also includes bonus and/or other incentives.
16, or more weeks, of maternity/paternity and co-parenting leave, according to local regulations
Training platform for all employees
Free language courses (24 languages available)
Free well-being sessions (physical and psychological)


Additional Information:
Location: Alpharetta (or a site), base location can be flexible
80% travel


About Us:
Syensqo is a science company developing groundbreaking solutions that enhance the way we live, work, travel and play. Inspired by the scientific councils which Ernest Solvay initiated in 1911, we bring great minds together to push the limits of science and innovation for the benefit of our customers, with a diverse, global team of more than 13,000 associates. Our solutions contribute to safer, cleaner, and more sustainable products found in homes, food and consumer goods, planes, cars, batteries, smart devices and health care applications. Our innovation power enables us to deliver on the ambition of a circular economy and explore breakthrough technologies that advance humanity.
At Syensqo, we seek to promote unity and not uniformity. We value the diversity that individuals bring and we invite you to consider a future with us, regardless of background, age, gender, national origin, ethnicity, religion, sexual orientation, ability or identity. We encourage individuals who may require any assistance or accommodations to let us know to ensure a seamless application experience. We are here to support you throughout the application journey and want to ensure all candidates are treated equally. If you are unsure whether you meet all the criteria or qualifications listed in the job description, we still encourage you to apply.

#Senior
 

Important Important EEO information related to opening in the US 

Solvay is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other legally protected characteristics. Click on the links to read ""The EEO is the Law"" poster and the “EEO is the Law” poster Supplement.

 We will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants. For more information, please click here.

 Solvay is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please send an e-mail to hrstaffingoffice.na-us@solvay.com or call 800-311-3555 and let us know the nature of your request and your contact information.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive y Ingeniería de datos, Bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3946954721/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=YuQ87CHUgD6hgR1FszRbxQ%3D%3D&trk=flagship3_search_srp_jobs,Finance Data Engineer,"73,9 US$K/año - 170,3 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 4 días,"San José, CA","Acerca del empleo
Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

The Opportunity 

This is an exciting opportunity to join the Digital Media Finance team as we continue to propel the business through data-driven forecasts and provide influential insights. In this role, you will have the opportunity to help drive Adobe’s pivotal initiatives by data engineering solutions that connect systems to visualization platforms for management reporting. The position will report to the Group Manager, Finance Automation to drive our systems and automation effort. This position will provide an outstanding opportunity to drive timely insights across Digital Media (DMe) Business at Adobe and help drive the Finance Transformational efforts undertaken by the Finance Automation team.

The ideal candidate for this role is a well-rounded top performer with exceptional data engineering, analysis, and visualization skills, capable of driving growth in a fast-paced environment. You should possess a continuous learning mentality to explore and implement innovations in the team's reporting and transformational projects. Strong cross-functional collaboration within Adobe is essential. You will be responsible for automating complex financial models, developing engaging and interactive dashboards, streamlining processes, and enhancing efficiencies in management reporting

What you'll do:

Responsible for the strategy and execution of DMe lifecycle of financial and analytics dashboards and ensuring scalability of systems and processes. 
Manage data transitions and platform migrations between SAP HANA/DataBricks, Tableau/PowerBI, and related ETL processes. 
Develop data pipelines, connections, and infrastructure to better enable forecasting and data science modelling. 
Correct data errors via systematic, logical fixes and provide updates within the data pipelines, connections and infrastructures that are built to support the teams. 
Create documentation and support enablement for the teams who are interested and able to help themselves. 
Accountable for ad-hoc support & participation in critical business analytics and key project support as directed by management. 
Partner closely with IT, Finance Systems, Finance Transformation Office, and other Finance counterparts to continually improve, streamline & enhance planning and reporting processes. 

What you need to succeed:

BS/BA with preferred focuses in areas of Business, Finance, or Information Systems. Master’s in Data Science a plus. 
3+ years of demonstrated experience working with large and complex data structures and develop efficient queries to create calculated fields and data aggregates. 
2+ years of proven experience designing and developing dashboards using PowerBI or Tableau. 
3+ years of relevant experience in data science and analytics in creating/maintaining financial models for a subscription/SaaS business a plus. 
Proficient in data platforms/systems (such as SQL, Databricks), ETL tools (such as Python, SnapLogic), process automation, and standardization. 
Self-starter with high attention to details, excellent interpersonal skills, and ability to take charge, set objectives, and deliver results. 
Strong project management skills with ability to juggle multiple priorities. 
Strong team orientation and a learning mentality. 

Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $73,900 -- $170,300 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.

Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances.

Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.

Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.

Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, Automatización, Habilidades sociales, Panel de control y SAP HANA",Solicitar
https://www.linkedin.com/jobs/view/3964407986/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=h3cdguudemBTkyLU%2BEuoOw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"60 US$K/año - 70 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Columbus y alrededores, Ohio","Acerca del empleo
Data Engineer
12 Month CTH
Columbus, OH 
$28/hr

As a Data Engineer, you will be supporting our energy client! This is a large project initiative team that is providing a ton of growth opportunities. You will be working with multiple systems, providing valuable analysis and solutions. If you are looking for a team with exposure to senior leaders, career growth, and an opportunity to learn business operations, this is the role for you!

Requirements
Degree in Data Analytics, Management Information Systems, Computer Science, Supply Chain, or similar area of study
SQL experience
Cloud experience such as GCP (Google Cloud Platform), AWS Azure
Tableau experience
Process mapping experience
Agile experience
Excellent written and verbal communication

Responsibilities
Work with the team to support internal projects
Participate in Scrum stand up meetings and the Agile process
Partner with business product owners on project initiatives
Work along side Tableau engineers 
Work to build machine learning models, analyze large data sets, and work between multiple data sets
Work with the data lake and write SQL queries, big queries, and additional data tools

Why Should You Apply?
Be mentored by senior team members
Participate in technical training and professional development through the Elevate Program

Eight Eleven Group provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.","Analítica de datos, Aprendizaje automático, Capacidad de análisis, Ingeniería de datos , Metodologías ágiles, Microsoft Excel, SQL y Tableau, Mapeo de procesos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3957772338/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=dSrcJ60vdwBJFUXDhx0I5Q%3D%3D&trk=flagship3_search_srp_jobs,Data & Analytics Engineer (Contract-to-Hire),"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",Publicado de nuevo hace 6 días,"Tampa, FL","Acerca del empleo
Data & Analytics Engineer
Tasked with working collaboratively to define and develop custom reports and dashboards that deliver information and insights to support all areas of the business. As an Analytics Engineer, you will play a crucial role in designing, developing, and maintaining our advanced analytics solutions within the Microsoft Azure ecosystem. As a member of the Data Management team, there is an expectation that you will support application development teams as a subject matter specialist in the areas of SQL and report development. The ideal candidate will have the ability to support multiple simultaneous projects, possess a passion for operational excellence and possess excellent interpersonal and soft skills. 

Responsibilities: Design and develop end-to-end analytics solutions using Microsoft Azure technologies, including SQL Server Management Studio (SSMS)/ Azure Data Studio, Azure Data Factory (ADF), Databricks, and Power BI. 
Collaborate with business users, data engineers, and other stakeholders to gather requirements, understand data sources, and define data models. 
Extract, transform, and load (ETL) data from various sources into the Azure data platform, ensuring data quality and integrity. 
Develop and maintain data pipelines and workflows using Azure Data Factory, Databricks, and other relevant tools to ensure efficient and timely data processing and availability. 
Create visually compelling and interactive dashboards, reports, and data visualizations using Power BI, enabling stakeholders to gain actionable insights from the data. 
Monitor and optimize the performance of Azure-based data solutions, identifying and resolving issues to ensure smooth and efficient operation. 
Stay up to date with the latest trends and advancements in the Microsoft Azure ecosystem and business intelligence domain, identifying opportunities for improvement and innovation. 
Assist in troubleshooting production code integration issues. 

Qualifications: Bachelor's degree in Computer Science, Management Information Systems, or a related field. Certifications with strong portfolio exemplifying expertise. 
Proven experience as an Analytics Engineer, Business Intelligence Developer, or similar role, with expertise in the Microsoft Azure stack. 
Strong proficiency in Azure Data Factory, SSMS, Databricks, and Power BI. 
Strong proficiency in SQL and experience with relational databases and data modeling concepts. Understanding Python is a plus. 
3 years or more hands-on technical experience with T-SQL development. 
Experience in designing and implementing ETL processes and data pipelines. 
Familiarity with data warehousing principles, dimensional modeling, and data integration techniques. 
Excellent analytical and problem-solving skills, with the ability to work with large and complex datasets. 
Strong understanding of data visualization principles and best practices. 
Ability to collaborate effectively with cross-functional teams and communicate complex technical concepts to non-technical stakeholders. 


Benefits: Medical, dental, and vision insurance 
401(k) with company match 
Associate discounts including furniture 
Company paid life and disability insurance 
Paid time off 
Employee Assistance Program 
Wellness Programs 
And more! 

We do not discriminate in hiring or employment against any individual on the basis of race, color, gender, national origin, ancestry, religion, physical or mental disability, age, veteran status, sexual orientation, gender identity or expression, marital status, pregnancy, citizenship, or any other factor protected by anti-discrimination laws 

Applicants must be authorized to work in the U.S.","Azure Data Factory, Microsoft Power BI, Python y SQL, Azure Databricks, Consultas en bases de datos y Expresiones de análisis de datos (DAX)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979305508/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=Z%2BmDjNaqNv4ZAueR%2BstAaA%3D%3D&trk=flagship3_search_srp_jobs,Azure Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
100% Remote

Need valid LinkedIn

Must be W2 or 1099

Hands on experience with Azure Data Services – Azure Data Factory, Databricks, Data Streaming. 
Experience in handling big data sets. 
NEEDS to have health insurance/health plan experience.

At Least Seven Years Of Azure Data Engineering Experience.

 Experience must include requirement gathering, development, testing and maintenance.
 Data analysis, data modeling, and data integration using azure technologies like Azure Data Factory (ADF).
 At least three years of hands-on experience with Azure Synapse, Azure Stream Analytics, Azure Cosmos, Azure SQL, Azure Data Factory, Azure Event Hubs, Azure Event Grid, Databricks, ADLS Gen 2 preferred.
 Experience working with event-based/streaming technologies to ingest and process data; building ETL / data warehouse transformation processes; software troubleshooting and agile software development methodologies.
 Expert or professional-level Azure certifications are a plus.
 Experience in extracting and transforming data from multiple sources and integrating in centralized platform.","Analítica, Analítica de datos, Azure Data Factory, Big data, Extraer, transformar y cargar (ETL), Ingeniería de datos y Minería de datos, Microsoft Azure, Modelado de datos y Obtención de requisitos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3920533530/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=TkAR2yJJaLpxZGfLOwHJBQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II: 24-01229,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 meses,"Bellevue, WA","Acerca del empleo
Primary Skills: AWS, SQL, ETL, Redshift,

Contract Type: W2 Only

Duration: 11 months contract with possible conversion

Location: Bellevue, WA

Pay range:$60 - $64 per hour on W2

Work at the world’s Largest E-commerce company

Job Responsibilities

Design and support analytical data infrastructure to provide ad hoc access to large datasets.
Manage a broad array of AWS resources including EC2, RDS, Redshift, etc.
Utilize SQL and AWS big data technologies to extract, transform, and load data from diverse sources.
Explore, learn, and employ the latest AWS technologies to introduce new capabilities and enhance efficiency.

Job Requirements

Proficiency in AWS technologies and management.
Solid understanding and experience with SQL and data modeling.
Strong collaboration skills to work effectively with BIEs and DSs.

ABOUT AKRAYA

""Akraya is an award-winning IT staffing firm consistently recognized for our commitment to excellence and a positive work environment. Voted the #1 Best Place to Work in Silicon Valley (2023) and a Glassdoor Best Places to Work (2023 & 2022), Akraya prioritizes a culture of inclusivity and fosters a sense of belonging for all team members. We are staffing solutions providers for Fortune 100 companies, and our industry recognitions solidify our leadership position in the IT staffing space. Let us lead you to your dream career, join Akraya today! ""","Amazon Web Services (AWS), Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Amazon Redshift, Datasets, Infraestructura de datos y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3945672732/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=BdAcRfMpWk%2BKktNDwCBRAw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Chicago, IL","Acerca del empleo
Who We Are: It all starts with purpose.

We are a purpose-driven nonprofit with a dynamic staff culture.

With a meaningful purpose, motivated staff, and excellent benefits, working here will definitely have you smiling! The ADA’s headquarters is located just steps from Chicago’s Magnificent Mile and close to public transportation. With more than 400 colleagues, the ADA Staff are some of the most talented people in the Chicago, Washington D.C., and Maryland area.

We were named a Top Workplace by the Chicago Tribune in 2019 and 2021! Come join our team!

Job Responsibilities:

Data Engineer will oversee the integration and management of data in our Salesforce and Azure-based SQL data warehouse environment. They will be responsible for designing, implementing, and maintaining robust ETL processes, ensuring high data quality, and optimizing data integration strategies to support our business intelligence and analytics needs.

Must Have:

Applicants must be legally authorized to work in the United States and should not now or in the future require sponsorship for employment. 
Bachelor's degree in Computer Science, Information Technology or related 
Minimum 3 years experience in data engineering or a similar role, specifically in Azure environment; or 7 years of experience in lieu of degree
Previously worked in a salesforce environment 
Excellent problem-solving, analytical, and communication skills
Knowledge of ETL concepts, data warehousing methodologies, and cloud computing
Strong understanding of SQL and data modeling
Ability to lead projects and work collaboratively in a team environment
Proficiency in Microsoft Office Suite (Word, Excel, and PowerPoint), Azure Data Factory, Azure SQL Database, and other Azure data services

Nice to Have:

Certifications in Salesforce and Azure cloud technologies 
Experience with BI tools like Power BI, Tableau, or similar 
Knowledge of Python, Java, or other scripting languages 
Knowledge of Purview 

Just a few of the benefits offered to employees:

Promotes Work/Life Balance
Hybrid Work Schedule (2-3 days from home)
Health insurance/ dental reimbursement plan
Ample Paid time off
401(k) 
Pension
Flexible Spending Account
Life insurance
Tuition reimbursement
Paid Parental Leave
Pet Insurance
Student Loan Refinance
2 days off to work at a charity event of your choice

The American Dental Association is proud to be an Equal Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. The American Dental Association is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at recruiting@ada.org.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities.","Almacenamiento de datos, Azure Data Factory, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Comunicación, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3954541152/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=22oU473i0sR2Ost5C0ZBLw%3D%3D&trk=flagship3_search_srp_jobs,Hybrid Work - Need Data Engineer :: HANA in San Jose CA,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,"San José, CA","Acerca del empleo
Domain: Software Development and Digital media products & Services

Client: Adobe

Job title: Data Engineer - HANA

Workplace type: Hybrid (1-2 days onsite) - Locals Only

Worksite location: 345 Park Ave, San Jose, California, 95110-2704, United States

Duration: 1-year contract

Top Skills: HANA, SQL, Data modeling, HANA modeling

Top Skills' Details

**Please note that the position is hybrid onsite in San Jose, CA. Candidates must be already local, or be onsite on day 1.**

 Strong demonstrated skill working with SQL programming
 Experience in HANA database
 Performance Tuning of Database Schemas, Databases, SQL, ETL Jobs, and related scripts
 Demonstrated skills in data modeling (HANA data modeling), SQL Stored Procedures
 Functional knowledge of SAP S/4 and/or ECC (specifically SD) a big plus
 Experience with troubleshooting/ production support
 Experience with Big Data (Azure, Databricks) desired","Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Ajuste de rendimiento, Apoyo a la producción, Bases de datos, Modelado de datos, Procedimientos de almacenado, Resolución de incidencias y SAP HANA",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984136551/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=t%2BNOw4F%2Fbstc4kxp06lIyg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Irving, TX","Acerca del empleo
Role : Data Engineer/Developer

Duration: 12 plus months

Location: Remote if needed - Dallas Preferred

 Must have Schwab Experience (Must have been out of Schwab for at least 6 month to be considered)

Required Skills

Informatica Powercenter

Control M

Strong batch Processing

PowerShell Scripting

Python Scripting

IICS Experience

Investment Experience (Asset Management)","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Procesamiento por lotes",Solicitar
https://www.linkedin.com/jobs/view/3976442350/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=jDkwEigfS6AzRQNAW2Gaaw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Independence, OH","Acerca del empleo
Independence, OH

Direct Hire

$110k/year

Position Description

The Data Engineer will play a crucial role in designing, implementing, and maintaining our organization's data infrastructure. This role will contribute to the efficient storage, retrieval, and analysis of data, enabling informed decision-making across our organization.

Responsibilities

 Collaborate with cross-functional teams to understand data requirements to design efficient data warehouse solutions 
 Develop and maintain a robust data warehouse architecture that ensures scalability, performance, and data integrity 
 Implement pipelines to move raw data in Azure Synapse using Azure Data Factory, SQL, Python, C# in line with well-established architectural standard related to Data Lakehouse and 
 Data Warehouse modeling standards 
 Data Quality Assurance - ensure data accuracy, consistency, and integrity throughout all processes, and implement data governance best practices. 
 Monitor and tune the data warehouse performance to ensure optimal query execution and data retrieval times 
 Identify and resolve bottlenecks in the ETL pipelines and data warehouse infrastructure 
 Develop and maintain data models, including dimensional and star schemas, to support efficient querying and reporting 
 Collaborate with reporting and analytics teams to understand requirements and translate them into effective data structures 
 Implement security measures to protect sensitive data within the data warehouse 
 Work closely with developers, analysts, and other stakeholders to understand their data needs and provide necessary support 
 Document data warehouse processes, data dictionaries, and ETL workflows for knowledge sharing and future reference 
 Design, implement, and document data architecture and data modeling solutions 
 Participate with Data Analyst(s) as needed to define minimal viable data assets in support of the visual needs. 
 Review and provide architectural guidance for analytic solutions 
 Triage and troubleshoot data anomalies submitted by the user community 

Welcome to ConsultNet and the family of companies, Tekne, SaltClick, TechBridge, and OmniMedia. As a premier national provider of technology talent and solutions, our expertise spans across project services, contract-to-hire, direct placement, and managed services both onshore and nearshore.

Celebrating more than 25 years of partnership with a diverse client base, we've crafted rewarding opportunities for our consultants, fostering high-performing teams that deliver impactful results.

Over the last few years thousands of consultants have found their calling with us in roles that have made a meaningful impact on their lives, enhanced their career, challenged them, and propelled them towards achieving their personal and professional goals. At the ConsultNet family of companies, we believe effective communication is crucial in aligning the right job with your unique skills and professional aspirations. To us, it's all about the personal approach we take and the values we uphold.

Our comprehensive service offerings cover a wide range of technology positions across key markets nationwide. Client more at  www.consultnet.com .

We champion equality and inclusivity, proudly supporting an Equal Opportunity Employer policy. We welcome applicants regardless of Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other status protected by law.","Almacenamiento de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Calidad de datos, Lenguaje de consulta (query), Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3954225072/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=nVBKAYwsar7i0DjDCgBTJA%3D%3D&trk=flagship3_search_srp_jobs,"AWS Python Data Engineer - H1B, GC , USC, H4-EAD (C2C role) Remote","En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Stuart, FL","Acerca del empleo
Position: AWS Python Data Engineer

Client - Fannimae

Location: Remote

Exp: 10+ Years

Job Description

Must Have Strong working experience on Python, PySpark and SQL.

AWS (S3, Hive, Glue, AWS Batch, DynamoDB, Redshift, EMR, CloudWatch, RDS, Lambda, SNS, SQS etc.)

Strong experience on Glue.

Very Strong coding experience on SQL, Scala & PySpark","Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , PySpark, Python, SQL y Scala, Bases de datos, Experiencia laboral y Pegamento",Solicitar
https://www.linkedin.com/jobs/view/3982993129/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=IV7Znx6uWeJ3iVfxfa%2B3mg%3D%3D&trackingId=zYDqFHqGvi5sg%2F2jxFZQ%2Fg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Rockville, MD","Acerca del empleo
Employment Type: Full-Time, Mid-level

Department: Business Intelligence

CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges.

CGS brings motivated, highly skilled, and creative people together to solve the government’s most dynamic problems with cutting-edge technology. To carry out our mission, we are seeking candidates who are excited to contribute to government innovation, appreciate collaboration, and can anticipate the needs of others. Here at CGS, we offer an environment in which our employees feel supported, and we encourage professional growth through various learning opportunities.

Skills and attributes for success:

Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists


Qualifications:

Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency in developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories


Ideally, you will also have:

Experience creating and consuming APIs
Experience with DHS and knowledge of DHS standards a plus
Candidates will be given special consideration for extensive experience with Python
Ability to develop visualizations utilizing Tableau or PowerBI
Experience in developing Shell scripts on Linux
Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions
Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences


Our Commitment:

Contact Government Services (CGS) strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting-edge technology with world-class personnel to deliver customized solutions that fit our client’s specific needs. We are committed to solving the most challenging and dynamic problems.

For the past seven years, we’ve been growing our government-contracting portfolio, and along the way, we’ve created valuable partnerships by demonstrating a commitment to honesty, professionalism, and quality work.

Here at CGS we value honesty through hard work and self-awareness, professionalism in all we do, and to deliver the best quality to our consumers mending those relations for years to come.

We care about our employees. Therefore, we offer a comprehensive benefits package:

Health, Dental, and Vision
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off and Observance of State/Federal Holidays


Contact Government Services, LLC is an Equal Opportunity Employer. Applicants will be considered without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Join our team and become part of government innovation!

Explore additional job opportunities with CGS on our Job Board:

https://cgsfederal.com/join-our-team/

For more information about CGS please visit: https://www.cgsfederal.com or contact:

Email: info@cgsfederal.com","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Datos no estructurados, Estrategia de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3891475109/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=Qo0LxAXc3U1pm4WyPJ%2BWbQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 meses,Estados Unidos,"Acerca del empleo
candidates are muat be local to Columbus

Data Engineer

Fully remote.

Our direct client is looking for a Data Engineer to help support the IT organization’s data strategy in support of their business customers, as well as internal initiatives.

If you are a “Data Person”, who knows how to build data pipelines, aggregate data from multiple sources into a common data store and have experience with industry-leading cloud data platforms and scripting, we’d love to talk with you.

The Data Engineer will work closely with Client’s IT Data Architect, Integration Engineers, Business Operations customers, and Services Partners to help advance their data and analytics ambitions.

This role will serve as a hands-on contributor and be a thought leader in the areas of data engineering, cloud data strategy, Business Intelligence, Data Modeling and ETL/ELT.

Responsibilities

with the IT Data Engineering Team and assist in developing the next generation data and analytics infrastructure.
have strong SQL modeling skills (dbt is a plus)
high quality SQL code to retrieve and analyze data from database tables (primarily Databricks)
high quality SQL models for ad-hoc requests, as well as ongoing reporting / dashboarding.
directly with business stakeholders to translate between data and business needs.
improve SQL models through automating or simplifying self-service support for datasets

Requirements

Bachelor's degree in Computer Science, Information Systems, Engineering, Data Science, or other similarly technical related field Mathematics, or specialized training/certification. Or equivalent work experience.

1- 3-year minimum professional experience in cloud data engineering and science and associated technologies utilizing cloud data platforms such as Databricks, AWS RedShift, and Snowflake.
1 - 3-year minimum experience with advanced SQL data modeling and query optimization
Proficiency in using visualization tools such as Tableau, Domo, or Power BI.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong verbal, written & presentation skills with the ability to effectively communicate complex technical information to personnel at all levels of the organization.

Nice To Have

Specific experience with Data Warehouse/Data Lake configuration and development using Databricks platform.
Experience with Tableau / Sigma Computing
operating in an Agile development environment.
with usage of Agile tools (JIRA / Confluence)
of CI/CD deployment models and release strategy as well as SCM tools (Git preferred) and code management best practices.
in AWS environment.
with cloud ELT platforms such as AWS Glue, Talend Stitch, or FiveTran","Almacenamiento de datos, Ingeniería de datos y SQL, Amazon Redshift, Lagos de datos, Lenguaje de consulta (query), Modelado de datos, Optimización, Optimización de consultas y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3971932759/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=WboB7J93%2BKzsCOiTKkZzcQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Nueva York, Estados Unidos","Acerca del empleo
This inclusive employer is a member of myGwork – the largest global platform for the LGBTQ+ business community.  

At Dataiku, we're not just adapting to the AI revolution, we're leading it. Since our beginning in Paris in 2013, we've been pioneering the future of AI with a platform that makes data actionable and accessible. With over 1,000 teammates across 25 countries and backed by a renowned set of investors, we're the architects of Everyday AI, enabling data experts and domain experts to work together to build AI into their daily operations, from advanced analytics to Generative AI.

Dataiku is looking for a Data Engineer to join our Enterprise Data and Analytics (EDA) team. As a member of the EDA Team, you will play a central role in delivering data to fuel analytics and data-driven insights to various stakeholders and teams within the company. You will also be a key technical member contributing to the Data Platform that fuels centralized analytics, embedded analytics teams, and self service users across the organization. At this level of seniority, non-technical skills, learning & teaching, and business acumen are critical, along with the expectation of technical excellence as you will help drive technical decision making and solutioning of key technical challenges both within the team and across the Analytics Community.

The Data Engineering day-to-day will primarily be within the Data Platform built using Snowflake, Dataiku & Github. The primary development will focus on python & SQL (among other languages), DataOps processes built within Github Actions & Dataiku, and data platform processes built within Snowflake & Dataiku. Strong software development lifecycle knowledge and DataOps skills are a must! The ideal candidate is naturally curious, has excellent verbal and written communication skills, a sharp analytical mind, a positive attitude towards work and thrives when collaborating towards a shared goal. This is an internal and non-client facing role.

How You’ll Make An Impact

Dataiku is unique in that every Dataiker is encouraged to use our own product within our Enterprise Data Platform. That means this is a unique opportunity to deliver a scalable platform with governed data to fuel an entire company of current or potential Data Analysts! Your responsibilities within the team include but are not limited to:

Be an expert level engineer within the Dataiku Platform 
Be an expert level engineer within Snowflake for data engineering and security/governance features
Build & maintain python & SQL data replication & data pipelines on large & often complex data sets 
Build & maintain data quality metrics & observability to help drive data quality standards
Design data models for both short term and long term use cases to support data warehouse scalability
Build & maintain administration systems and applications for monitoring, alerting, data observability, access management, platform metrics, and end user transparency
Identify opportunities for improvements & optimization for greater scalability & delivery velocity
Collaborate closely with Analytics Engineers to provide data & data models for analytical deliverables
Perform root cause analysis on often complex errors to help ensure data pipeline availability
Help drive technical & architectural decisions on the data platform including decisions on data architecture, data engineering processes, data quality frameworks, data access security & governance frameworks, DataOps processes & data consumption models.
Help test new features in Dataiku and partner tools to both provide feedback internally as well as determine value towards internal analytics & data platform integration
Work closely with key stakeholders across the organization including Infra, embedded analytics teams, Product and Engineering to help foster both technical implementations & requirements gathering
Promote the Data Platform internally, help train & mentor others within the organization with technical excellence, & provide support on data & platform related inquiries
Proactively drive innovation internally with dedicated innovation time & projects that aim to be transformational for either the platform, team or company as a whole.
Actively contribute to the expertise level and competencies of the EDA Team and participate in the creation and support of data development standards and best practices. 

What You’ll Need To Be Successful

3-4 years' of relevant experience in Data Engineering / Data Platform Engineering / Data Architecture
Expertise in SQL & Python is a must. Experience in Dataiku DSS is a big plus. 
Prior experience with Snowflake strongly desired
Prior experience with DevOps technologies such as Github Actions, Azure DevOps or Jenkins
Strong understanding of data architecture & data modeling concepts
Location: Must be currently located within the EST or CST regions of the United States. Open for candidates based in the UK, if they have the right skillset.

What Will Make You Stand Out

Prior experience building and maintaining replication & data pipelines in a cloud data warehouse or data lake environment
Excellent analytical and creative problem-solving skills - exhibit confidence to ask questions to bring clarity, share ideas and challenge the norm.
Passion for continuous learning and teaching to help learn & teach new technologies & implementation strategies
Experience working with complex stakeholders; dissecting vague asks and helping to define tangible requirements
Ability to manage multiple projects and time constraints simultaneously in a high trust remote environment
Ability to wear multiple hats depending on the project with the focus on accomplishing end goals while inspiring colleagues to do the same
Excellent written and verbal communication skills (especially with senior level stakeholders) with the ability to speak to both the business value, data products, & technical capabilities of a platform. Ability to create clear and concise documentations with a high degree of precision

Compensation And Benefits

The final compensation package for this role will be determined during the interview process and is based on a variety of factors, including, but not limited to, geographic location, internal equity, education, skill set, experience and training. Eligible roles may also be entitled to receive commission or other variable compensation through Dataiku's incentive compensation program.

Dataiku also offers comprehensive benefits, including stock options, medical, dental, and vision plans, flexible spending accounts, pre-tax commuter benefits, a 401k company match, paid vacations and sick leave, paid parental leave, employer paid disability coverage, and additional health and wellbeing perks and benefits. Dataiku reserves the right to amend or modify employee perks and benefits at any time.

US only national base pay ranges$140,000—$165,000 USD What are you waiting for! At Dataiku, you'll be part of a journey to shape the ever-evolving world of AI. We're not just building a product; we're crafting the future of AI. If you're ready to make a significant impact in a company that values innovation, collaboration, and your personal growth, we can't wait to welcome you to Dataiku! And if you’d like to learn even more about working here, you can visit our Dataiku LinkedIn page. Our practices are rooted in the idea that everyone should be treated with dignity, decency and fairness. Dataiku also believes that a diverse identity is a source of strength and allows us to optimize across the many dimensions that are needed for our success. Therefore, we are proud to be an equal opportunity employer. All employment practices are based on business needs, without regard to race, ethnicity, gender identity or expression, sexual orientation, religion, age, neurodiversity, disability status, citizenship, veteran status or any other aspect which makes an individual unique or protected by laws and regulations in the locations where we operate. This applies to all policies and procedures related to recruitment and hiring, compensation, benefits, performance, promotion and termination and all other conditions and terms of employment. If you need assistance or an accommodation, please contact us at: reasonable-accommodations@dataiku.com","Arquitectura de datos y Ingeniería de datos, Calidad de datos, Comunicación, Comunicación oral, Dataiku DSS, Modelado de datos, Modelo de datos, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3954469254/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=UDpFLVsUZuR3MytC950BxQ%3D%3D&trk=flagship3_search_srp_jobs,Data Science Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 mes,"Arizona, Estados Unidos","Acerca del empleo
As a Data Science Engineer, you'll work with a team of Data Scientists on subjects like Customer Lifetime Value, churn, customer experience, and call center activity.

WHAT YOU'LL DO:
Enrich data assets to extract valuable business insights and improve the predictive power of ML models.
Focus on feature engineering, analytics, and the presentation of findings that help drive business results. Identify and understand valuable data assets and automate ingestion/ETL processes.
Clean, prep and verify the integrity of data for use in ML or reporting.
Develop sophisticated and automated pipelines for ML and Data processing.
Demonstrate a high degree of critical thinking and problem-solving skills.
Undertake processing of structured and unstructured data. Advanced skills in Spark is preferred.
Analyze large amounts of information to discover trends and patterns.
Present information using data visualization techniques.
Propose solutions and strategies to business challenges.
Analytical with a knack for data manipulation, problem-solving, and storytelling.
You have a passion for creating end-to-end data pipelines along with a solid interest and understanding of ML and AI.

WHAT YOU'LL BRING:
5+ years of relevant professional experience.
Experience in data mining, processing, and modeling.
Ability to solve complex analytical problems.
Experience in Microsoft tools including Azure Data Factory, Azure Synapse, Data Bricks, and Spark.
Some experience in data engineering, data science, or operations research/analysis is preferred.
Demonstrable knowledge of common coding languages (Python, SQL, R) with the ability to learn new skills and apply skills in new environments.
Experience using business intelligence tools (e.g. Power BI) and data frameworks (e.g. relational models).
Ability to collaborate with colleagues (IT, Business, and Data Science) and work well with vendors/partners.
Earned at least a Bachelor’s Degree in Computer Science, Mathematics, Statistics, Applied Mathematics, Data Science, or quantitative field.
Excellent communication and presentation skills.
Analytical mind and business acumen, specific experience in Marketing and customer analytics is preferred.","Ciencia de datos, Inteligencia artificial, Inteligencia empresarial, Pensamiento crítico, Python y SQL, Análisis de clientes, Aptitudes para hacer presentaciones y Comunicación",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982692421/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=s%2FQU3ZEtk5GjymLpR99UFQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Mountain View, CA","Acerca del empleo
On this journey for over 12 years, Qventus is leading the transformation of healthcare. We enable hospitals to focus on what matters most: patient care. Our innovative solutions harness the power of machine learning, generative AI, and behavioral science to deliver exceptional outcomes and empower care teams to anticipate and resolve issues before they arise.

Our success in rapid scale across the globe is backed by some of the world's leading investors. At Qventus, you will have the opportunity to work with an exceptional, mission-driven team across the globe, and the ability to directly impact the lives of patients. We’re inspired to work with healthcare leaders on our founding vision and unlock world-class medicine through world-class operations.

As a Data Engineer you will join our team and contribute to the next evolution of Qventus's products. This role involves enhancing our data processing and analytics capabilities with a strong focus on healthcare operational efficiency. You will work closely with our Founders and engineering team to develop innovative solutions that impact patient care.

Key Responsibilities:

Translate product / analytical vision into data pipelines and functional updates to support high-quality and highly trusted data products and the overall end-to-end workflow of data users at Qventus (including Leading targeted development of data structures, data transformation pipelines, data acquisition, etc.). 
Improve the data quality and transparency of the pipelines (defining data requirements, identifying and implementing data observability tooling - lineage, sources, transformations). 
Monitor, maintain, and iterate on key elements of the data platform infrastructure to proactively maintain stable and highly trusted services. 
Work closely with core team members to develop, test, deploy, and operate high-quality, scalable software and raise engineering standards. 

What We’re Looking For: 

Demonstrated experience in building, debugging, and enhancing production transformation pipelines in close collaboration with data stakeholders
Aptitude for interpreting complex datasets, including the ability to discern underlying patterns, identify anomalies, and extract meaningful insights, demonstrating data intuition and analytical skills. 
Experience developing & coordinating execution in a fast-paced, dynamic environment across multiple technologies & platforms (AWS experience preferred)
Strong cross-functional communication - ability to break down complex technical components for technical and non-technical partners alike
3+ years of professional experience working with modern programming languages such as Java, C/C++, and Python with a dedication to high code quality. 

It’s a Plus if You Have… 

Degree in Computer Science, Engineering, or related field, or equivalent training/experience
Competence in participating in technical architecture discussions to help drive high-quality technical development within your team
Practical hands-on experience with:
Building large-scale, high-complexity metrics and monitoring
ELK, DBT, Snowflake, AWS, Terraform, Looker, Ansible experience
Experience building and maintaining robust and efficient backend data systems with functional proficiency with AWS cloud services & modern data warehouse services (Snowflake)
The salary range for this role is $95,000 to $135,000. Qventus salary bands represent market data across different geographies. We consider several factors when determining compensation, including location, skills and qualifications, and prior relevant experience. Salary is just one component of Qventus’ total package. Some of our key benefits and perks* include but are not limited to: Open Paid Time Off, paid parental leave, professional development, wellness, and technology stipends, generous employee referral bonus, and employee stock option awards.

We believe that diversity, equity, inclusion, and belonging are fundamental to improving healthcare and society, and that’s why we’re building a company that leads the way. We hold ourselves accountable to using fair hiring processes that mitigate the negative impacts of unconscious bias. We also work to ensure that people from underrepresented groups play meaningful roles on both sides of the interview table. We are an equal opportunity employer and give all qualified applicants consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.

Candidate information will be treated in accordance with our candidate privacy notice which can be found here: https://qventus.com/ccpa-privacy-notice/

Employment is contingent upon the satisfactory completion of our pre-employment background investigation and drug test.

Benefits and perks are subject to plan documents and may change at the company's discretion.","Almacenamiento de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Arquitectura técnica, Bases de datos, Calidad de datos, Ciencias de la computación, Comunicación, Datasets y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3984724497/?eBP=BUDGET_EXHAUSTED_JOB&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=%2FVoB2sHBmo5ZRv2nrMxZ0A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"105 US$K/año - 160 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Seattle, WA","Acerca del empleo
Costco IT is responsible for the technical future of Costco Wholesale, the third largest retailer in the world with wholesale operations in fourteen countries. Despite our size and explosive international expansion, we continue to provide a family, employee centric atmosphere in which our employees thrive and succeed. As proof, Costco ranks  eighth in Forbes “World’s Best Employers”  .

This is an environment unlike anything in the high-tech world and the secret of Costco’s success is its culture. The value Costco puts on its employees is well documented in articles from a variety of publishers including Bloomberg and Forbes. Our employees and our members come FIRST. Costco is well known for its generosity and community service and has won many awards for its philanthropy. The company joins with its employees to take an active role in volunteering by sponsoring many opportunities to help others.

Come join the Costco Wholesale IT family. Costco IT is a dynamic, fast-paced environment, working through exciting transformation efforts. We are building the next generation retail environment where you will be surrounded by dedicated and highly professional employees.

Data Engineers are responsible for developing and operationalizing data pipelines/integrations to make data available for consumption (i.e. Reporting, Data Science/Machine Learning, Data APIs, etc.). This includes data ingestion, data transformation, data validation/quality, data pipeline optimization, orchestration; and deploying code to production via CI/CD. The Data Engineer role requires knowledge of software development/programming methodologies, various data sources (Relational Databases, flat files (csv, delimited), APIs, XML, JSON, etc.), data access (SQL, Python, etc.), followed by expertise in data modeling, cloud architectures/platforms, data warehousing, and data lakes. This role also will partner closely with Product Owners, Data Architects, Platform/DevOps Engineers, etc. to design, build, test, implement, and maintain data pipelines.

The Data Engineer is responsible for developing data pipelines and/or data integrations of test data for Costco’s enterprise certified data sets that are used for business-critical data consumption use cases (i.e. Reporting, Data Science/Machine Learning, Data APIs, etc.). At Costco, we are on a mission to significantly leverage data to provide better products and services for our members. This role is focused on data engineering to build and deliver data pipelines that will deliver data securely for use by Costco business groups. The Data Engineer will partner with product owners, data architects, and data platform teams to design, build, test, and automate data pipelines that are relied upon across the company as the single source of truth.

If you want to be a part of one of the worldwide BEST companies “to work for”, simply apply and let your career be reimagined.

 ROLE 

Implements streaming data pipelines using event/message-based architectures.
Works in tandem with Data Architects to align on data architecture requirements provided by the requestor.
Defines and maintains optimal data pipeline architecture.
Identifies, designs, and implements internal process improvements: automating manual processes, optimizing data delivery/orchestration.
Analyzes data to spot anomalies, trends, and correlate data to ensure Data Quality.
Performs peer review for another Data Engineer’s work.
Develops and operationalizes data pipelines to bring data into the various test environments used for the development of our certified data sets.
Works in tandem with Data Architects, Data Stewards, and Data Quality Engineers to design data pipelines and recommends ongoing optimization of data storage, data ingestion, data quality, and orchestration.
Designs, develops, and implements ETL/ELT/CDC processes using Informatica Intelligent Cloud Services (IICS), Azure Data Factory, AWS Glue, dbt, and other ETL products.
Uses Azure services such as Azure SQL DW (Synapse), ADLS, Azure Event Hub, Cosmos, Databricks, and Delta-Lake to improve and speed delivery of our data products and services.
Implements big data and NoSQL solutions by developing scalable data processing platforms to drive high-value insights to the organization.
Identifies ways to improve data reliability, efficiency, and quality of data management.
Communicates technical concepts to non-technical audiences both in written and verbal form.

Required

5+ years’ experience engineering and operationalizing data pipelines with large and complex datasets.
2+ years’ hands-on experience with Informatica IICS, Azure Data Factory, AWS Glue, or other ETL tools.
3+ years’ experience working with Cloud technologies such as ADLS, Azure Databricks, Spark, Azure Synapse, Cosmos DB, and other big data technologies.
5+ years’ experience with Data Pipeline, ETL, and Data Warehousing.
Extensive experience working with various data sources (DB2, SQL,Oracle, flat files (csv, delimited), APIs, XML, JSON.
Experience implementing data integration techniques such as event/message-based integration (Kafka, Azure Event Hub), ETL.
Advanced SQL skills; solid understanding of relational databases and business data; ability to write complex SQL queries against a variety of data sources.
Strong understanding of database storage concepts (data lake, relational databases, NoSQL, Graph, data warehousing).
Experience with Git/Azure DevOps.

Required Documents

Cover Letter
Resume

California applicants, please click to review the Costco Applicant Privacy Notice.

Pay Ranges

Level 2 - $105,000 - $135,000

Level 3 - $130,000 - $160,000

We offer a comprehensive package of benefits including paid time off, health benefits - medical/dental/vision/hearing aid/pharmacy/behavioral health/employee assistance, health care reimbursement account, dependent care assistance plan, short-term disability and long-term disability insurance, AD&D insurance, life insurance, 401(k), stock purchase plan to eligible employees.

Costco is committed to a diverse and inclusive workplace. Costco is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or any other legally protected status. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to IT-Recruiting@costco.com

If hired, you will be required to provide proof of authorization to work in the United States. In some cases, applicants and employees for selected positions will not be sponsored for work authorization, including, but not limited to H1-B visas.","Almacenamiento de datos, Azure Data Factory, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Validación del sistemas informáticos (CSV), Bases de datos, Calidad de datos, Datasets y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3981817758/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=D20LrT2xkEtCGcWQizNnsw%3D%3D&trk=flagship3_search_srp_jobs,Data Warehouse Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,Estados Unidos,"Acerca del empleo
Data Warehouse Developer/Data Engineer
Remote (EST / CST preferred candidates) 
Direct Hire 

Position Summary:
We are seeking a skilled Data Warehouse Developer/Data Engineer to join our team. This role involves analyzing, designing, and developing solutions for Data Warehouse and Business Intelligence projects to address business challenges through data. Responsibilities include modeling, constructing, and populating the Data Warehouse, along with delivering reporting capabilities. The developer will support both on-premises and cloud-based data warehouses, as well as legacy data systems.

General Duties and Responsibilities:
Participate in all phases of Data Warehouse implementation: requirements gathering, design, development, testing, implementation, and support, ensuring best practices for quality and scalability.
Create and maintain logical and physical dimensional data models.
Evaluate business rules; perform source-to-target data mappings; design, review, implement, and optimize ETL processes; automate ETL and batch jobs.
Implement data integration services between source systems and the Data Warehouse.
Productionize data solutions and maintain source control.
Implement various levels of data security.
Participate in peer code reviews.
Create and maintain documentation for work products.
Gain knowledge of source systems and their data structures to support the design, development, testing, implementation, and support of data solutions.
Provide data analysis and identify data-related issues within the Data Warehouse environment and source systems as needed.
Support and maintain existing legacy data solutions.
Migrate existing legacy data solutions to the Data Warehouse.
Develop and execute queries upon request for business users.
Work independently to complete assigned tasks.

Qualifications and Experience:
Bachelor’s degree in Computer Science, Information Technology, or a related field, or equivalent experience.
6+ years of experience with relational databases, SQL, stored procedures, and Data Warehouse systems, utilizing sound Data Warehouse development methodologies.
4+ years of experience with ETL and data engineering on the Microsoft Azure Cloud platform, including SQL Server on Azure, Azure Data Factory, and Azure Synapse.
Experience with notebooks using PySpark and Python.
Proficiency in data cleansing, profiling, validation, and reporting.
Experience with SQL Server databases, both on-premises and in Azure Cloud, for building Data Warehouses.
Familiarity with data integration tools and techniques.
Experience with Power BI.
Software development experience is a plus.
Strong communication, analytical, and reasoning skills.
Proficient with Microsoft Office products.","Almacenamiento de datos, Azure Data Factory, Azure Data Lake, Base de datos relacional, Extraer, transformar y cargar (ETL), Microsoft SQL Server y PySpark, Bases de datos, Microsoft Azure, Modelo de datos, Procedimientos de almacenado y SQL de Azure",Solicitud sencilla
https://www.linkedin.com/jobs/view/3955837235/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=0E74q68QAgbxSo5rKH0lbg%3D%3D&trk=flagship3_search_srp_jobs,"Junior Data Engineer -- 6+ Months contract -- Phoenix, AZ (Onsite)","Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 mes,"Phoenix, AZ","Acerca del empleo
Job Title: Junior Data Engineer

Location: Phoenix, AZ (Onsite)

Duration: 6+ Months contract

Key Responsibilities

4 - 6 years of relevant work experience.
Good debugging skill in Hive and SQL.
Designing, constructing, installing, testing, and maintaining highly scalable data management systems
Responsible for conceptualizing and designing detailed solutions.
Works with product team to prioritize features for ongoing sprints and managing a list of data requirements based on industry trends, new technologies and project requirements.
Communicating project progress to stakeholders and capable of handling risks and issues affecting the project.","Almacenamiento de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos y SQL, Bases de datos, Depuración de programas y Generación de conceptos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3921398421/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=bcxUv2LxSfuJetLxLNQ7JQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"97,3 US$K/año - 128,1 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 2 semanas,"Tampa, FL","Acerca del empleo
HOW MIGHT YOU DEFY IMAGINATION?

You’ve worked hard to become the professional you are today and are now ready to take the next step in your career. How will you put your skills, experience and passion to work toward your goals? At Amgen, our shared mission—to serve patients—drives all that we do. It is key to our becoming one of the world’s leading biotechnology companies, reaching over 10 million patients worldwide. Come do your best work alongside other innovative, driven professionals in this meaningful role.

Data Engineer

Live

What You Will Do

Let’s do this. Let’s change the world. In this vital role you will be part of the established technical/engineering team, develop web UI interface, plus data flow pipelines to extract, transform, and load data from various data sources in various data format to enterprise data lake and data warehouse system in three regions in AWS. Provide data analytics and predictive analysis to business users.

Be a key team member assisting in design and development of the data pipeline for Global Data and Analytics team
Collaborate with Data Architects, Business SME’s, and Data Scientists to design and develop end-to-end data pipeline to meet fast paced business need across geographic regions
Serve as system admin to manage AWS and Databricks platform;
Adhere to best practices for coding, testing and designing reusable code/component
Able to explore new tools, technologies that will help to improve ETL platform performance and machine learning operations
Participate in sprint planning meetings and provide estimations on technical implementation; Collaborate and communicate effectively with the product teams

Win

What We Expect Of You

We are all different, yet we all use our unique contributions to serve patients. The professional we seek will have these qualifications.

Basic Qualifications:

Master’s Degree

OR

Bachelor’s degree with 2 years Data Engineering and/or and Software Engineering experience

Or

Associate’s degree 6 years of Data Engineering and/or Software Engineering experience

Or

High school diploma and 8 years of Data Engineering and/or Software Engineering experience

Preferred Qualifications:

Familiar with PySpark dataframe and data processing libraries, machine learning frameworks (like Tensorflow, Keras or PyTorch), and other machine learning libraries
Familiar with machine learning operations
Experience with software development (Java, Python preferred), end-to-end system design
Experience with data modeling for both OLAP and OLTP databases, hands-on experience with SQL, preferred Oracle, PostgreSQL, and Hive SQL; SQL performance tuning
Experience with web development, java script, html, CSS, any web framework or microservice architecture
Experience with software DevOps CI/CD tools, such Git, Jenkins
Experience on AWS, familiar with EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, DynamoDB, and API gateway
Experience with docker container, Kubernetes container orchestration
Experience with Apache Airflow and Apache Spark; Spark performance turning
Experience with Tableau Dashboard and Tableau Server
Experience with Pharmaceutical industry, commercial operations
Ability to learn quickly, be organized and detail oriented

Thrive

What You Can Expect Of Us

As we work to develop treatments that take care of others, we also work to care for our teammates’ professional and personal growth and well-being.

The expected annual salary range for this role in the U.S. (excluding Puerto Rico) is posted. Actual salary will vary based on several factors including but not limited to, relevant skills, experience, and qualifications.

Amgen offers a Total Rewards Plan comprising health and welfare plans for staff and eligible dependents, financial plans with opportunities to save towards retirement or other goals, work/life balance, and career development opportunities including:

Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts.
A discretionary annual bonus program, or for field sales representatives, a sales-based incentive plan
Stock-based long-term incentives
Award-winning time-off plans and bi-annual company-wide shutdowns
Flexible work models, including remote work arrangements, where possible

Apply now

for a career that defies imagination

Objects in your future are closer than they appear. Join us.

careers.amgen.com

Application deadline

Amgen does not have an application deadline for this position; we will continue accepting applications until we receive a sufficient number or select a candidate for the position.

Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.

We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.","Airflow, Apache Spark, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Redshift, Comunicación, Microservicios, Modelado de datos y Procesamiento de transacciones online",Solicitar
https://www.linkedin.com/jobs/view/3979362136/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=2MKlXcfwiQF251IlPuUzeQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (PySpark data frame)- AWS,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Tampa, FL","Acerca del empleo
Job Title: Data Engineer

Job Duration: 12 Months contract on W2

Job Location: Tampa Florida 33608 (Hybrid)

Note: US Citizenship or Permanent Resident is required.

Job Description

In this vital role you will be part of the established technical/engineering team, develop web UI interface, plus data flow pipelines to extract, transform, and load data from various data sources in various data format to enterprise data lake and data warehouse system in three regions in AWS. Provide data analytics and predictive analysis to business users.
Be a key team member assisting in design and development of the data pipeline for Global Data and Analytics team
Collaborate with Data Architects, Business SME’s, and Data Scientists to design and develop end-to-end data pipeline to meet fast paced business need across geographic regions
Serve as system admin to manage AWS and Data bricks platform;
Adhere to best practices for coding, testing and designing reusable code/component
Able to explore new tools, technologies that will help to improve ETL platform performance and machine learning operations
Participate in sprint planning meetings and provide estimations on technical implementation; Collaborate and communicate effectively with the product teams.

What We Expect Of You

We are all different, yet we all use our unique contributions to serve patients. The professional we seek will have these qualifications.

Basic Qualifications

Master’s Degree OR Bachelor’s degree with 2 years Data Engineering and/or and Software Engineering experience Or Associate’s degree 6 years of Data Engineering and/or Software Engineering experience Or High school diploma and 8 years of Data Engineering and/or Software Engineering experience.

Preferred Qualifications

Familiar with PySpark dataframe and data processing libraries, machine learning frameworks (like Tensorflow, Keras or PyTorch), and other machine learning libraries
Familiar with machine learning operations
Experience with software development (Java, Python preferred), end-to-end system design
Experience with data modeling for both OLAP and OLTP databases, hands-on experience with SQL, preferred Oracle, PostgreSQL, and Hive SQL; SQL performance tuning
Experience with web development, java script, html, CSS, any web framework or microservice architecture
Experience with software DevOps CI/CD tools, such Git, Jenkins
Experience on AWS, familiar with EC2, S3, Redshift/Spectrum, Glue, Athena, RDS, Lambda, DynamoDB, and API gateway
Experience with docker container, Kubernetes container orchestration
Experience with Apache Airflow and Apache Spark; Spark performance turning
Experience with Tableau Dashboard and Tableau Server
Experience with Pharmaceutical industry, commercial operations
Ability to learn quickly, be organized and detail oriented.

https://www.linkedin.com/company/trispoke-managed-services-pvt-ltd/jobs

For more jobs kindly click on above link.

#Hiring #JobSearch #NowHiring #Recruitment #JobOpening #Career #JobOpportunities #JobHunt #JobSeeker #Employment #W2jobs #Hybridjobs #CareerOpportunities #Job #DataEngineer #DataJobs #TampaJobs #FloridaJobs #BigData #DataScience","Airflow, Apache Spark, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Apache Airflow, Framework web, Hojas de estilos en cascada (CSS), Java y Microservicios",Solicitud sencilla
https://www.linkedin.com/jobs/view/3942855423/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=maBwpPwuo4wTZZbK75TpsA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"60 US$K/año - 65 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 3 semanas,"Redmond, WA","Acerca del empleo
About Centific:
Centific expertly engineers platforms and curates multimodal, multilingual data to empower the ‘Magnificent Seven’ and enterprise clients with safe, scalable Artificial intelligence (AI) deployment. Our team includes over 150 PhDs and data scientists, along with more than 4,000 AI practitioners and engineers. We leverage an integrated ecosystem comprised of industry-leading partnerships, and 1.8 million vertical domain experts across 230 locales, to create high-quality pre-trained datasets, fine-tuned industry-specific Large Language Models (LLMs), and Retrieval-Augmented Generation (RAG) pipelines supported by vector databases. Our innovations can reduce Generative Artificial Intelligence (Gen AI) costs by up to 80% and bring Gen AI solutions to market 50% faster.

Our mission is to bridge the gap between AI creators and industry leaders by bringing best practices in Generative AI to unicorn innovators and enterprise customers. We aim to help these organizations unlock significant business value by leveraging Generative AI at scale, ensuring they stay at the forefront of technological advancement and maintain a competitive edge in their respective markets.

Website - https://www.centific.com/

Job Title: Data Engineer

Job Description:
Proven experience (3+ years) as a Data Engineer or in a similar role.
Experience with big data tools and technologies (e.g., Databricks, Hadoop, Spark, Kafka).
Proficiency in SQL and experience with relational databases (e.g., MySQL).
Experience with cloud platforms (e.g., Azure, AWS , Google Cloud) and their data services.
Strong programming skills in languages such as Python, C#, Java, or Scala.
Experience with ETL tools (e.g., Azure Data Factory, Azure Batch, Databricks, etc.).
Knowledge of data modeling, data architecture, and schema design.
Excellent problem-solving skills and attention to detail.
Strong communication and collaboration skills.
Ability to work in a fast-paced, agile environment.

Benefits offered - comprehensive healthcare, dental & vision, 401k plan, PTO, etc.

Centific is an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, ancestry, citizenship status, age, mental or physical disability, medical condition, sex (including pregnancy), gender identity or expression, sexual orientation, marital status, familial status, veteran status, or any other characteristic protected by applicable law. We consider qualified applicants regardless of criminal histories, consistent with legal requirements.","Análisis de datos, Base de datos relacional y MySQL, Adquisición de conocimientos, Bases de datos, Herramientas de colaboración y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982906105/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=Sig%2B6mTpLA1x8G1t%2F5RsNQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"140 US$K/año - 170 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Nueva York, NY","Acerca del empleo
Data Developer / Engineer | Hybrid NYC | $140,000 - $170,000

Role: Full time, permanent position based in Manhattan, NYC 3 days per week. This role does not offer sponsorship.

Client: Leading international law firm covering a range of practices including real estate, cybersecurity, renewable energy, capital markets and more.

Responsibilities:
Develop PowerBI dashboards for visualization
Create SQL scripts within Azure
Manage & maintain the MS Power Platforms inc Power Automate, Power Apps and PowerBI

What you need:
Experience working within an MS Azure environment.
Excellent developing SQL scripts.
Experience developing PowerBI dashboards, reports and visualization.

What you will get:
Annual starting salary up to $170k
Full benefits package
Company push on work-life balance
Training, professional development and career progression

If this interests you, click apply or reach out to me with your latest resume as soon as possible. 📩","Analítica de datos, Microsoft Power BI, Microsoft PowerApps, SQL, Visualización y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Microsoft Azure, Microsoft Power Automate y Panel de control",Solicitud sencilla
https://www.linkedin.com/jobs/view/3862431923/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=LtY%2Bp4OcBEg%2BHXeP%2B69jfQ%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, Sr Level 3 - Remote","200 US$K/año - 215 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,"California, Estados Unidos","Acerca del empleo
At Globant, we dream and build Digital Journeys that matter to millions of users. We do that by leveraging engineering, design and innovation with our own industry-leading practices, like our Agile PODs and specialized Studios. We want you to join us in creating these journeys for the biggest clients in financial services, banking, retail, travel, e-commerce and media, revolutionizing and growing their core businesses while helping them (and you!) stay ahead of the curve.

WHAT ARE WE LOOKING FOR?

If you are a Data Engineer, we want to meet you!

YOU WILL GET A CHANCE TO:

Mentoring and Gatekeeping
Presales
Engage with project stakeholders to gather requirements and understand project objectives.
Communicate technical concepts effectively to both technical and non-technical team members.
Collaborate with cross-functional teams including designers, product managers, and quality assurance to deliver exceptional products.
Participate in team meetings, stand-ups, and brainstorming sessions to foster collaboration and innovation.


WHAT WILL MAKE YOU SUCCEED:

Programming Language (OOP, Functional) Python, Java, SQL
Software Engineer Practices (SOLID, Code Reviews, Quality Code & Refactoring, Desing Patterns, Data Structures)
Software Architecture Types, Architecture Design
RDBMS (ACID, Integrity, Indexes, Scalability, Partitioning, Tuning, Normalization, OLTP, vs OLAP)
Distributed FileSystems (Resiliency, Replication, Parquet, Data Serialization)
Data Modeling (DW, Modeling and Tools like Snowflake, Redshift)
NoSQL (Architecture, Sharding, BASE, CAP, Graph databases, Columnar DBs, Document Stores)Knowlegde at least in one of the most popular Cloud Providers (AWS; GCP, Azure)Containerization
Infrastructure as Code (How IaC works, Tools like Terraform / CloudFormation)
CI/CD (Deployment Strategies Canary / Blue Green, Tools like Jenkins/Harness/Gitlab)Soft Skills (Critial thinking, Problem Solving, Teamwork, leadership, career management amoung others)


Globant reasonably expects the annual base compensation for this role to be between $200,000 to $215,000 CAD. Specific offer details are determined by carefully considering a variety of factors, including the candidate’s primary work location, skills, experience, education, market demands, and internal parity.

Job Segment:  Data Modeler, Cloud, Software Engineer, QA, Database, Data, Technology, Engineering, Quality","Buena práctica clínica, Google Cloud , NoSQL y Python, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Java, Modelado de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3904500008/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=3yLpfclKwXS9Uen5YdvGUA%3D%3D&trk=flagship3_search_srp_jobs,Remote Work - Need Data Engineer - W2 only,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 meses,Estados Unidos,"Acerca del empleo
100% Remote

Need LinkedIn

W2 candidates only

There will be a short tech assessment on this role which needs to be completed and cleared to be considered

Skills Requirement | Success Criteria for all:

Hyper communication and transparency
Drivers with high level of self-motivation
Extreme accountability and ownership
Hands-on executionists vs theorists
Critical thinking and problem-solving skills

Skills Requirement | Success Criteria for Data Engineer: 

Must be hands-on with development and build. Need team members who are self-motivated and driven.
Must have deep experience with SSIS, Python, SQL, and ideally Azure and Azure Data Factory. Primary relevant experience would be coding complex SQL, building the integration packages (including logic development), flat file transfers, etc. There is no XML work and no experience needed with Pyspark, Spark, or any other big data platforms. The standard requirement is Rest or Soap-based APIs but the majority are flat files. There may be some ETL work moving data from various source systems to data warehouse.","Almacenamiento de datos, Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos y Pensamiento crítico, Bases de datos, Comunicación y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3813098231/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=lXZw9apCNU16dL6MbhvahQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 6 meses,"Washington, DC","Acerca del empleo
 Data Engineer
Washington, D.C.
 
Requirements:
  Eight (8) years relevant experience in applied data science research or big data analytics. 
Bachelor's Degree in Computer Science, Engineering, Information Systems or related technical discipline. A Master's degree may be substituted for up to two (2) years of experience. A PhD may be substituted for up to five (5) years of experience. 
Ability to perform functional and data requirements analysis, and implementation of data engineering projects, analyze customer requirements and provide solution recommendations. 
Demonstrate knowledge of information engineering methodologies, process improvement, and performance measurement 
Ability to support the development of organization-wide data models for use in designing and building integrated, shared software and database systems
Be a US Citizen and pass a rigorous background check
 ","Almacenamiento de datos, Analítica, Analítica de datos, Análisis de Big Data, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Análisis de requisitos y Bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3964545306/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=Fk13acEGDMLc901yqForQQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer I,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,"Hartford, WI","Acerca del empleo
Who Are We?

Taking care of our customers, our communities and each other. That's the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.

Job Category

Data Analytics, Technology

Compensation Overview

The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.

Salary Range

$105,100.00 - $173,400.00

Target Openings

1

What Is the Opportunity?

Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights. What Will You Do?

Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.
Design data solutions.
Analyze sources to determine value and recommend data to include in analytical processes.
Incorporate core data management competencies including data governance, data security and data quality.
Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.
Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.
Test data movement, transformation code, and data components.
Perform other duties as assigned.


What Will Our Ideal Candidate Have?

Bachelor's Degree in STEM related field or equivalent
Six years of related experience
Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices.
The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.
Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.
Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.
Strong verbal and written communication skills with the ability to interact with team members and business partners.
Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.


What is a Must Have?

Bachelor's degree or equivalent training with data tools, techniques, and manipulation.
Four years of data engineering or equivalent experience.


What Is in It for You?

Health Insurance: Employees and their eligible family members - including spouses, domestic partners, and children - are eligible for coverage from the first day of employment.
Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.
Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.
Wellness Program: The Travelers wellness program is comprised of tools and resources that empower you to achieve your wellness goals. In addition, our Life Balance program provides access to professional counseling services, life coaching and other resources to support your daily life needs. Through Life Balance, you're eligible for five free counseling sessions with a licensed therapist.
Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.


What Is in It for You?

Health Insurance: Employees and their eligible family members - including spouses, domestic partners, and children - are eligible for coverage from the first day of employment.
Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.
Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.
Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs.
Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.


Employment Practices

Travelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.

In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions.

If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.

Travelers reserves the right to fill this position at a level above or below the level included in this posting.

To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.","Analítica de datos, Gobierno de datos y Ingeniería de datos, Análisis de sistemas, Calidad de datos, Comunicación, Depuración de programas, Limpieza de datos, Prácticas recomendadas en ingeniería de software y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982198072/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=Wu1dZCXzBTQPEyTs0Zgvlg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"63 US$/h - 68 US$/h Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",hace 6 días,"Englewood, CO","Acerca del empleo
Responsibilities

Kforce has a client that is seeking a Data Engineer in Greenwood Village, CO. Summary: We are seeking an experienced ETL Developer to join our dynamic team at a leading telecommunications company. The ideal candidate will be responsible for the full cycle of data processing, including designing, implementing, and maintaining ETL pipelines. This role requires a strong background in Python, Spark, and SQL, with a focus on batch processing and automation. Familiarity with tools such as Airflow and AWS is a plus, as we are continuously improving our data engineering practices. Responsibilities:

 Data Engineer will design, develop, and maintain ETL pipelines to process and transfer data efficiently
 Work with Spark and Python to create and optimize ETL workflows and data transformations
 Manage and maintain data processing templates, ensuring efficient batch processing of daily data transfers
 Utilize SQL for data manipulation and querying to support data-driven decision-making
 Identify opportunities to automate and streamline data processing tasks to improve efficiency
 As a Data Engineer, you will collaborate with other teams to ensure seamless data integration and processing
 Develop and maintain YAML configurations and CI/CD pipelines for code deployment and management
 Stay up to date with emerging technologies and incorporate them into existing processes where beneficial
 Prepare for the integration of Airflow and contribute to its implementation and usage

Requirements

 Proven experience with Python, Spark, and SQL in a data engineering or ETL development role
 Hands-on experience with batch processing and ETL pipeline development
 Familiarity with YAML and CI/CD practices for code deployment and management
 Strong problem-solving skills and the ability to think critically about improving and automating processes
 Experience with Oracle databases and data processing frameworks

Preferred Skills

 Experience with AWS and cloud-based data engineering solutions
 Knowledge of Airflow or similar workflow orchestration tools
 Ability to adapt to new technologies and tools as needed

The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.

We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.

Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.

This job is not eligible for bonuses, incentives or commissions.

Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.","Airflow, Apache Spark, Extraer, transformar y cargar (ETL), Ingeniería de datos y Pensamiento crítico, Bases de datos, Manipulación de datos, Oil Pipeline Development, Procesamiento por lotes y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984582301/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=lyx9HTKffYQ4o3JuRiHNTA%3D%3D&trk=flagship3_search_srp_jobs,Linguistics Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Somerville, MA","Acerca del empleo
Babel Street is the trusted technology partner for the world's most advanced identity intelligence and risk operations. We deliver advanced AI and data analytics solutions providing unmatched, analysis-ready data regardless of language, proactive risk identification, 360-degree insights, high-speed automation, and seamless integration into existing systems. Babel Street empowers government and commercial organizations to transform high-stakes identity and risk operations into a strategic advantage. The actionable insights we deliver safeguard lives and protect critical assets around the world. Babel Street is headquartered in Reston, Virginia, with regional offices in Boston, MA and Cleveland, OH, and international offices in Australia, Canada, Israel, Japan, and the U.K. For more information, visit www.babelstreet.com.

About the Role:

In this role you will have the opportunity to work with multiple, discrete engineering teams providing annotated, reliable data to train, develop, and evaluate natural language processing systems as well as consult on the language specific aspects of multilingual text. Join us and help us create the next wave of software for Natural Language Processing and Text Analytics. 

What you will do:

Manage large-scale text mining, data acquisition and annotation projects 
Train and supervise contractors as they perform manual annotation tasks 
Measure reliability of parallel, manual annotations 
Survey and catalogue new data releases and best practices in data maintenance, conversion, and analytics 
Educate and consult with engineering teams about linguistics,NLP, and computational linguistics 
Evaluate and advocate for linguistic quality in our products 

What you will bring:

Strong scripting abilities, especially Python 
Data cleaning, conversion, organization 
Parsing XML, JSON, tabular data sets 
Scraping and collecting text from online resources including web sites and APIs 
Ability to write and revise annotation guidelines 
Ability to translate product requirements into annotation guidelines 
Ability to synthesize clear instructions and instructive examples 
Knowledge of Linguistics and NLP applications including 
Language identification 
Tokenization 
Part of speech tagging 
Morphological analysis 
Entity extraction, disambiguation, and linking 
Syntactic parsing 
Sentiment analysis 
Experience working with manual annotation tools and platforms such as Inception, brat, WebAnno, Prodigy, Mechanical Turk, etc. 

Nice to have: 

Experience with databases such as SQL and Mongo  Experience with SPARQL query language 
Proficiency in at least one natural language in addition to English 
Experience with conversion, storage, version control and maintenance tasks for large multilingual text collections 
Familiarity with prominent linguistic annotation guidelines (e.g., Penn Treebank) 
Familiarity with linguistic community resources and data providers such as 
Universal Dependencies treebank project 
ClueWeb 
CommonCrawl 
Linguistic Data Consortium

Range For This Position Based On Qualifications And Experience

$125,000—$145,000 USD

Benefits at Babel Street (just to name a few...)

Health Benefits: Babel Street covers 90-100% monthly premium costs for Medical, Dental, Vision, Life & Disability insurances – for you and your family!
Retirement Plans: Babel Street offers both a Traditional and Roth 401(K) with a very competitive match.
Unlimited Flexible Leave: We trust our employees to manage their own time and balance their personal and work lives.
Holidays: Babel Street provides employees with 12 paid Federal Holidays
Tuition Reimbursement: We are committed to investing in our employees. One way we do that is with our Tuition Reimbursement Program for continuing education.

Babel Street is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law. Further, Babel Street will not discriminate against applicants for inquiring about, discussing or disclosing their pay or, in certain circumstances, the pay of their co‐worker, Pay Transparency Nondiscrimination. In addition, Babel Street's policy is to provide reasonable accommodation to qualified employees who have protected disabilities to the extent required by applicable laws, regulations and ordinances where a particular employee works. Upon request, we will provide you with more information about such accommodations.","Ciencia de datos, Ingeniería de datos y Query Languages, Anotación, Bases de datos, Lenguaje de consulta (query), Limpieza de datos, Mantenimiento de datos, Requisitos de productos y SPARQL",Solicitar
https://www.linkedin.com/jobs/view/3952737248/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=xD3frigfryqlO3WX%2Blf8Xg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Chicago, IL","Acerca del empleo
About Northern Trust

Northern Trust, a Fortune 500 company, is a globally recognized, award-winning financial institution that has been in continuous operation since 1889.

Northern Trust is proud to provide innovative financial services and guidance to the world’s most successful individuals, families, and institutions by remaining true to our enduring principles of service, expertise, and integrity. With more than 130 years of financial experience and over 22,000 partners, we serve the world’s most sophisticated clients using leading technology and exceptional service.

Major Duties :

 Uses existing procedures to solve standard problems; analyzes information and standard practices to make judgments
 Has limited impact on own work team
 Works within standardized procedures and practices to achieve objectives and meet deadlines

Knowledge :

 Requires conceptual knowledge of theories, practices and procedures within a job discipline
 Applies general knowledge of business developed through education or past experience
 Exchanges straightforward information, asks questions and checks for understanding

Experience :

 Accountable for own contributions

Working With Us

As a Northern Trust partner, greater achievements await. You will be part of a flexible and collaborative work culture in an organization where financial strength and stability is an asset that emboldens us to explore new ideas.

Movement within the organization is encouraged, senior leaders are accessible, and you can take pride in working for a company committed to assisting the communities we serve! Join a workplace with a greater purpose.

We’d love to learn more about how your interests and experience could be a fit with one of the world’s most admired and sustainable companies! Build your career with us and apply today. #MadeForGreater

Reasonable accommodation

Northern Trust is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation for any part of the employment process, please email our HR Service Center at MyHRHelp@ntrs.com.

We hope you’re excited about the role and the opportunity to work with us. We value an inclusive workplace and understand flexibility means different things to different people.

Apply today and talk to us about your flexible working requirements and together we can achieve greater.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3968008091/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=N7113jtCH9NDMKEQybvdiA%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/ Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Municipio de Warren, NJ","Acerca del empleo
Job Title: Data Analyst/ Data Engineer

Location: Warren, NJ (1 days a week)

Duration: Long Term (W2 only)

Experience level : 10+ yrs 

Data Engineer Specializing in ADF (Azure Data Factory), Databricks, Spark SQL, and Dashboard Development

We are currently seeking a skilled Data Analyst with proficiency in Azure Data Factory (ADF), Databricks, Spark SQL, and dashboard creation. This role involves:
Data Analysis with ADF and Databricks: Utilizing Azure Data Factory and Databricks for comprehensive data analysis. This includes extracting, transforming, and loading data (ETL) processes, and managing data workflows.
Expertise in Spark SQL: Leveraging Spark SQL for querying and analysing large datasets within Databricks environments. The candidate should possess the ability to write efficient, complex SQL queries.
Data Mapping Creation: Developing detailed data mappings to assist in the integration and transformation of data from various sources into structured formats for analysis.
Dashboard Development: Designing and building intuitive, informative dashboards that effectively communicate data insights. Experience with tools like Tableau, Power BI, or Databricks' native visualization capabilities is preferred.
Data Quality and Integrity: Ensuring the accuracy and consistency of data throughout the data processing lifecycle, from extraction to visualization.
Collaborative Project Involvement: Working collaboratively with cross-functional teams, including data engineers and business stakeholders, to understand requirements and deliver data-driven solutions.
Data-Driven Decision Making: Assisting in the interpretation of data and providing insights that support business decision-making.
Continuous Learning and Improvement: Staying updated with the latest trends and advancements in data analytics, ADF, Databricks, and Spark SQL to continuously improve processes and implementations.
Technical Documentation: Creating clear and comprehensive documentation regarding data processes, mappings, and dashboard functionalities.
Problem-Solving Skills: Utilizing strong analytical and problem-solving skills to address challenges in data analysis and dashboard development.
The ideal candidate will have a strong background in data analysis, with a demonstrated ability to handle large datasets and build effective visualizations. They should be comfortable working in a dynamic, fast-paced environment and possess excellent communication skills to articulate complex data concepts to non-technical stakeholders.

Thanks & Regards

Anoop Tiwari

Extend Information Systems

Cell: - 571 - 386 - 2431

Email: Anoop@extendinfosys.com","Analítica de datos, Análisis de datos, Capacidad de análisis, Visualización y Visualización de datos, Comunicación, Dashboard Building, Datasets, Oracle Application Development Framework y Panel de control",Solicitar
https://www.linkedin.com/jobs/view/3983541052/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=bnAo1Ii6IogZiKuky0MFbw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"66,50 US$/h - 77 US$/h Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Houston, TX","Acerca del empleo
Description

We are offering a contract employment opportunity for a Data Engineer in the Energy/Natural Resources industry, located in Houston, Texas. This role involves working closely with data architects and stakeholders to design, develop, and implement scalable and efficient data solutions on the Azure platform.

Responsibilities

 Collaborate with data architects and stakeholders to design and develop efficient data solutions on the Azure platform
 Develop and maintain Extract, Transform, Load (ETL) processes using Spark, Python, Scala, Databricks, Azure Data Factory, and other relevant tools
 Implement and configure Azure services such as Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL Database, and Azure Cosmos DB
 Design and implement data models to optimize data storage, querying, and retrieval
 Ensure compliance with data privacy regulations and industry standards by implementing data security measures such as encryption, access controls, and monitoring mechanisms
 Assist team members with technical challenges, employing problem-solving skills and knowledge of data engineering tools and techniques
 Oversee the implementation of data security measures and ensure compliance with data privacy regulations and industry standards. This includes implementing encryption, access controls, and monitoring mechanisms to protect sensitive data.

Requirements

 Demonstrated proficiency in ETL - Extract Transform Load
 Prior experience with Azure Data Factory
 Proficiency in Azure SQL Database
 Demonstrable skills in Python programming language
 Relevant experience in the Energy/Natural Resources industry
 Strong understanding of data engineering principles and methodologies
 Ability to work collaboratively with cross-functional teams
 Excellent analytical and problem-solving skills
 Strong verbal and written communication skills
 Ability to handle multiple tasks and meet deadlines
 Bachelor's degree in Computer Science, Information Systems, or a related field
 Continuous learner, staying updated with the latest industry trends and best practices.

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Azure Data Factory, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Ciencias de la computación, Comunicación, Comunicación escrita, Modelo de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3977887922/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=OJSrUX2HEf5u4JV6DNgS4Q%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer ( Hadoop/scala/databricks ) | inperson Must @ Boston, MA","Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,"Boston, MA","Acerca del empleo
Data Engineer

Boston, MA

Long term

Job Description

Skill: Data Engineer for Hadoop/scala/databricks

Senior developer position who has extensive experience with development and designing of bigdata platform with Hadoop and Scala experience.

The candidate should have extensive experience in the bigdata developer with data engineering/big data developer.

The candidate should have good experience in Scala programming language for developing bigdata or data bricks development.

Knowledge/experience on Databricks.

Greetings !!

For More Details & Immediate Response 

Contact - https://www.linkedin.com/in/saisreenca/","Analítica, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Hadoop, Ingeniería de datos , Lenguajes de programación, Minería de datos y Scala, Azure Databricks",Solicitud sencilla
https://www.linkedin.com/jobs/view/3959139809/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=CJib6m30wbvuDAmU6ytmNg%3D%3D&trk=flagship3_search_srp_jobs,Applied Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Meridian, ID","Acerca del empleo
Are you a skilled Data Engineer looking for a full-time role in Meridian? Horizontal Talent has an exciting opportunity for an Applied Data Engineer to join our client's team, and the role offers an attractive salary and benefits for your expertise.

In this Applied Data Engineer role, you will provide innovative modernization and ensure the availability, reliability, and performance of the company's Data Analytics ecosystem. You'll be assembling large, complex sets of data that meet non-functional and functional business requirements and identifying, designing, and implementing internal process improvements.

To give you an idea of how this Applied Data Engineer role would look and feel, here are some areas you can expect to work in:

Building the required infrastructure for optimal extraction, transformation, and loading of data from various data sources using AWS and SQL technologies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics, including operational efficiency and customer acquisition
Working with stakeholders, including the Executive, Product, Data, and Design teams, to support their data infrastructure needs while assisting with data-related technical issues
Working with the architects, including enterprise, application, and data architects, to design, implement, and support the company's data lake
Translating analytical program models, including but not limited to scripting, error handling, and documentation


We are looking for an applied data engineer with a graduate and/or bachelor’s degree in information systems, informatics, statistics, computer science, or another quantitative field, as well as 5 years of data engineering experience. You will also require:

Ability to build and optimize data sets, data pipelines, and architectures
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
Excellent analytical skills associated with working on unstructured datasets
Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata
Comfortable recomending data strategies, ETL processes, and procedures for getting data in and out of the data lake


If you’re looking to impact and create change positively, you'll be rewarded with an excellent salary and benefits package for your inclusive and committed approach.

To apply for this full-time Applied Data Engineer job in Meridian, please reach out to Horizontal Talent today. We would love to help you get your next role and fulfill your professional ambitions.","Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Datasets, Eficiencia operativa, Establecer prioridades del trabajo, Estrategia de datos y Métricas de rendimiento",Solicitar
https://www.linkedin.com/jobs/view/3964733824/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=yqKG63UHgjccSvvuNErmnA%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist/Engineer - Junior,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Austin, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Hello,

We appreciate you showing interest in our Job placement program. This program is a fee-based program wherein you pay partial fees before enrolling and the remainder fees (Payable in installments over 2 years) once you secure a Job which pays you a minimum salary of $75,000 P.A. or more.

I am attaching the Presentation with this email which explains everything about our program, includes some videos about SynergisticIT and events etc. which we attend for connecting with the tech industry, cost of the program and technologies covered in the program and the methodology.

Once you have gone through the presentation please reply and let me know your interest in pursuing the program via email, text at (phone number removed).

After that we will have a phone call to discuss and evaluate your suitability for our program.

If you feel the program is expensive you are 100% correct. However, it's because of the nature of our program.

I can go over the details over a call if you are interested in the program and are comfortable with the investment in the program.

Please do understand the final decision about the suitability of your candidature for our program will be made by my manager via a zoom call which will involve a discussion about your experience, skill set, personality, attitude and communication skills and ability to commit to the program.

Full-time Job Placement Program(remote/Full-time)

Under this program, we will make you stand out from the rest of the jobseekers by offering Skill enhancement training on the technologies in-demand including time management skills, study skills, test-taking skills, and learning strategies after which we can assist candidates in getting jobs as software programmers / Java Programmers / Data scientists / Machine learning engineers / Data analysts. Herein we first give real-time hands-on experience on the technologies by the experts working in the industry(6-7 trainers) , get candidates certified with IT Certifications given by Microsoft, Oracle, Amazon, IBM, Udemy like AWS/JAVA/SQL/TABLEAU/POWER BI/SAAS/EXCEL(Sessions to be attended separately) , gives real-time projects to practice the technologies and get professional-level experience in the real-world scenario, prepare candidates for all types of Coding/Interview assessments, behavioral questions , and finally market and arrange the interviews(unlimited) for the FULL-TIME positions until they get an offer with our Fortune-500 technology clients on their direct payroll(W2 NO CONTRACTS, NO SALARY CUTS).

We Offer 2 Tracks

Kindly go through the course contents in each of these technologies:-

 Java track

In the Java track, we will cover JAVA, AWS, MERN STACK, HADOOP .

JAVA PROGRAM teaches you from scratch and requires no coding experience from learners. You'll be given an introduction to JAVA and OOPS and its related technologies like SERVLET, JSP, JQUERY, HIBERNATE, SPRING, JAVASCRIPT, and MICROSERVICES.

Introduction to Java, Oops Concepts

Multi-threading,

Exception Handling, Java API's

JSP, Servlets

How to deploy a web application

jQuery , AJAX, JavaScript, JSON, Jenkins, GitHub,

Spring MVC, Spring Core, Spring Boot, Rest Webservices, Hibernate, Spring Security, Microservices.

JPA, AOP in Spring, Spring IOC , REST API's , etc.

MERN STACK will give you a better and in-depth understanding of CSS, HTML, JAVASCRIPT, MONGO DB, EXPRESS.JS, REACT, NODE.JS. Express. JS, React.JS, MongoDB using Mongoose, Bootstrap, Redux.

PL/SQ/ORACLE and Databases

Also Pl/SQL, stored procedures and triggers and databases like SQL, Oracle and Mysql and servers like Tomcat and J-boss.

In HADOOP , you'll be given an introduction to BIG DATA, APACHE HADOOP, HADOOP ECOSYSTEM, CLOUDERA QUICKSTART VM along with core concepts of the Hadoop framework including MapReduce, HIVE, PIG, SQOOP, FLUME, HBASE, OOZIE.

DATA STRUCTURES AND ALGORITHMS - Java programmers use data structures to store and organize data, and we use algorithms to manipulate the data in those structures.

Data structures ex: Arrays, Linked List, Stacks, Queues

Algorithms Examples

"" Breadth-First Search (BFS) Depth-First Search (Client)

"" Insertion of a node in Linked List (On the basis of some constraints)

"" Longest Common Subsequence

"" Binary Search

"" Find Minimum Depth of a Binary Tree

AWS training includes all the fundamental concepts, infrastructure security, services of AWS like Amazon route, Amazon VPC, Elastic IP, Amazon EC2, Amazon RDS, AWS Direct connect. As programmers are supposed to be able to deploy their code to the cloud

You learn AWS architecture, storage services, Content delivery, Compute services like AWS lambda, Beanstalk, AWS EC2, Auto scaling and load balancing.

Feel free to take a free 5-minute java test:-

 Data Science/Machine learning track

Data science will have an introduction with PYTHON, further into numerical python, PANDAS DATA ANALYSIS, DATA VISUALISATION.

Machine learning will cover everything from data science, Artificial intelligence, business analytics, deep learning, and computer science.

Python will be covered along with Django and Scala in depth.

PL/SQ/ORACLE and Databases

AWS training includes all the fundamental concepts, infrastructure security, services of AWS like Amazon route, Amazon VPC, Elastic IP, Amazon EC2, Amazon RDS, AWS Direct connect.

DATA STRUCTURES AND ALGORITHMS

In our Data science track we prepare you to get job as one of the following: Python developer, a data analyst, data visualization developer, a statistician, a machine learning engineer or a data scientist.

Our data science track covers most of the below: Topics and content might change based on the market requirements

Python

NumPy and Pandas

Matplotlib data visualization

Difference between Machine learning, Artificial Intelligence and Deep learning

Data Manipulation: Cleansing-Munging

Data Analysis. Statistics

Tableau and Power BI

PL/SQL and databases both SQL and NoSQL

Data structures and algorithms

Artificial Intelligence and Machine learning

Machine learning Algorithms

Decision Tree and Random Forest Algorithms

Nai ve Bayes and KNN Algorithm

Support Vector Machine (SVM)

Statistics

Random variables, Zscore, Hypothesis testing, Expected Value

Predictive Modeling : Different kind of Business problems and different phases of Predictive modeling and Popular Modeling Algorithms. Data exploration, Data preparation.

Web scraping using Python Beautiful soup . What is web scraping (Difference between web scraping software vs a web browser) , what is parser?

Time series Analysis

Different algorithms like Decision Tree and Random Forest algorithms, Support Vector Machine Algorithm

Deep Learning and Computer vision

Neural Networks, Tensor Flow and Keras

Natural language processing (NLP) and Text mining

Market Basket Analysis

NLP with Python, Sentiment analysis

Linear regression, Scikit Learn, Confusion matrix, Decision tree, Ensemble approach

Random forest, Cross validation.

XGboost, Hierarchical clustering, Polynomial Regression

Regards,","Ciencia de datos, Modelos predictivos, MongoDB, Pandas (Software) y Procesamiento de lenguaje natural, JavaServer Pages, Manipulación de datos, Market Basket Analysis, Máquinas de vectores de soporte y Preparación de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982372256/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=XCgo5Ors5MUHJ9Ch%2F%2FS70g%3D%3D&trk=flagship3_search_srp_jobs,Technology and Data - Database Engineer 4 - Contingent,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Chandler, AZ","Acerca del empleo
Job Description: In this contingent resource assignment, you may: Consult on complex initiatives with broad impact and large-scale planning for Database Engineering. Review and analyze complex multi-faceted, larger scale or longer-term Database Engineering challenges that require in-depth evaluation of multiple factors including intangibles or unprecedented factors. Contribute to the resolution of complex and multi-faceted situations requiring solid understanding of the function, policies, procedures, and compliance requirements that meet deliverables. Strategically collaborate and consult with client personnel. Required Qualifications: 5+ years of Database Engineering experience, or equivalent demonstrated through one or a combination of the following: work or consulting experience, training, military experience, education.

Comments for Suppliers (Use this area to provide any additional comments to all suppliers. For example, any specific skills to prioritize when looking for profiles.: We are currently looking for a Lead Data Engineer who can:

Lead complex/large data delivery initiatives with broad cybersecurity domain impacts and act as key participant in large scale application planning and implementation for high-value/cyber domain use cases. Review and analyze complex application enhancement initiatives for domains, including operational or technical improvements that require in depth evaluation of multiple data, functional and non-functional factors including non-functional intangibles or unprecedented factors. 

Design and develop feature enhancements or expedite maintenance / fixes on the Data Pipeline framework.

Drive the implementation and adoption of quality engineering practices and automated quality assurance solutions across the data engineering community.

Ensure key risk areas related to data management such as data classification, attribute level data access, data completeness/accuracy, and data use are addressed in the design phase.

**See additional skills**

EEO:

“Mindlance is an Equal Opportunity Employer and does not discriminate in employment on the basis of – Minority/Gender/Disability/Religion/LGBTQI/Age/Veterans.”
Aptitudes y experiencia deseables
DATA PIPELINE","Acceso a datos, Aseguramiento de la calidad, Clasificación de datos, Ingeniería de calidad, Investigación y Soporte técnico",Solicitar
https://www.linkedin.com/jobs/view/3972578994/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=%2FmMbHafB6EFsIDQuak2MqQ%3D%3D&trackingId=wVjnMr9DBEEWHecjeYQGuA%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer - Python, SQL - fully paid Medical,Dental on Day 1","115 US$K/año - 140 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,"Houston, TX","Acerca del empleo
Position Overview

We are a strong, financial trading company seeking a highly skilled and motivated Data Engineer to join our global team. This is a growing department, you will be the first Data Engineer in this location reporting into our Global team!

This is a onsite position, 5 days a week.

Key Responsibilities

Collaborate with stakeholders to understand their data needs.
Produce solutions to ingest external data and produce pipelines.
Provide data distribution methods to allow the trading desks to integrate the data with their models.
Investigate Data Science or Machine Learning opportunities to provide insights to the business.

Qualifications

Bachelors or higher degree in Computer Science, Math, Engineering or related field
3+ years of Data Engineering experience
Strong knowledge of Python and SQL
Knowledge of workload automation tools (ActiveBatch) or workflow management tools (Apache Airflow.)
Working knowledge of MS Azure or Databricks is a plus.
Financial trading experience is a plus.
Ability to work independently and as part of a global team.
Excellent problem solving skills and attention to detail.
Strong communication skills with stakeholders.

We offer a base of $115,000 to $140,000 plus bonus and excellent benefits!

This is a onsite position, 5 days a week.

Benefits

FULLY PAID Medical and Dental premiums from Day 1! 

Bonus potentially 15-20%!

401K match, Tuition Reimbursement, vacation, holidays

Email Your Resume In Word To

Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:

rhona.kannon@cybercoders.com

Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : RK3-1809834 -- in the email subject line for your application to be considered.***

Rhona Kannon - Principal Director

Applicants must be authorized to work in the U.S.

CyberCoders is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity or expression, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, status as a crime victim, disability, protected veteran status, or any other characteristic protected by law. CyberCoders will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. CyberCoders is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please contact a member of our Human Resources team to make arrangements.

CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.","Airflow, Ciencia de datos, Ingeniería de datos y Python, ActiveBatch, Apache Airflow, Automatización de cargas de trabajo, Ciencias de la computación, Gestión de flujos de trabajo y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3959340930/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=XV%2BkraRPQ5aF1Owhn%2BmNCw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,Estados Unidos,"Acerca del empleo
Expertise in working with healthcare data standards (ex. HIPAA and FHIR), sensitive data and data masking techniques to mask personally identifiable information (PII) and protected health information (PHI) is essential.
In-depth knowledge of search algorithms, indexing techniques, and retrieval models for effective information retrieval tasks. Familiarity with search platforms like Elasticsearch or Azure AI Search is a must.
Familiarity with chunking techniques and working with vectors and vector databases like Pinecone.
Ability to design, develop, and maintain scalable data pipelines for ingesting, processing, and transforming large volumes of structured and unstructured data.
Experience with implementing best practices for data storage, retrieval, and access control to ensure data integrity, security, and compliance with regulatory requirements.
Be able to implement efficient data processing workflows to support the training and evaluation of solutions using large language models, ensuring reliability, scalability, and performance.
Ability to proactively identify and address issues related to data quality, pipeline failures, or resource contention, ensuring minimal disruption to systems.
Experience with large language model frameworks, such as Langchain and know how to integrate them into data pipelines for natural language processing tasks.
Experience working within the snowflake ecosystem.
Knowledge of cloud computing principles and experience in deploying, scaling, and monitoring AI solutions on cloud platforms like Snowflake, Azure, AWS.
Ability to communicate complex technical concepts effectively to technical and non-technical stakeholders and collaborate with cross-functional teams.
Analytical mindset with a keen attention to detail, coupled with the ability to solve complex problems efficiently.
Knowledge of cloud cost management principles and best practices to optimize cloud resource usage and minimize costs.

Must Have:

Minimum of 10 years' experience as a data engineer
Hands-on experience with Azure Cloud eco-system.
Hands-on experience using Python for data manipulation.
Deep understanding of vectors and vector databases.
Hands-on experience scaling POC to production.
Hands-on experience using tools such as Document Intelligence (formerly Azure Form Recognizer), Snowflake, function app. Azure AI Search
Experience working with PII/PHI

Hands-on experience working with unstructured data.","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Calidad de datos, Comunicación, Manipulación de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3961463242/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=Ncqu0EBcac7A70n%2BPRNnGA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Nueva York, NY","Acerca del empleo
This inclusive employer is a member of myGwork – the largest global platform for the LGBTQ+ business community.  

Experience Level: Experienced Hire

Categories

Engineering & Technology

Location(s):

Remote - United States, US
Remote - United States, US
Remote - United States, US
7 World Trade Center, 250 Greenwich Street, New York, New York, 10007, US
7575 Gateway Blvd., Suite 300, Newark, California, 94560, US
Remote - United States, US
Remote - United States, US

At Moody's, we unite the brightest minds to turn today's risks into tomorrow's opportunities. We do this by striving to create an inclusive environment where everyone feels welcome to be who they are-with the freedom to exchange ideas, think innovatively, and listen to each other and customers in meaningful ways.

If you are excited about this opportunity but do not meet every single requirement, please apply! You still may be a great fit for this role or other open roles. We are seeking candidates who model our values: invest in every relationship, lead with curiosity, champion diverse perspectives, turn inputs into actions, and uphold trust through integrity.

The Compliance & 3rd-Party Risk unit at Moody's leverages data-driven insights and cutting-edge technologies to help businesses deliver prudent risk management in the KYC, Supply Chain, and Trade Credit space. We are currently seeking a Data Engineer to join our dynamic and motivated Data & Analytics team. The ideal candidate will have a strong foundation in data engineering concepts such as pipelines, orchestration and streaming; as well as a passion for working with cutting-edge technologies to ensure clean, quality, and usable data is democratized to the business.

In this role, you will be highly involved working on creating foundational data models and data products within our data warehouse which supports various business and product reporting needs. In your day-to-day you will be working in a squad consisting of data engineers, analysts, software engineers, and product managers.

The Work

Develop and maintain product-specific data pipelines to ensure the timely and accurate flow of data from various sources to our Databricks platform.
With the guidance of technical peers, learn to work with business and other cross-functional team members to identify, integrate, and distribute data data from sources to destinations.
Work on project squads adhering to project management best practices and participating in scrum ceremonies
Gain an understanding of data usage patterns and requirements within the organization to design and implement appropriate data models in the data warehouse, ensuring compliance with industry regulations.
Utilize Databricks and AWS/GCP services to manage, store, and process data, ensuring optimal performance, security, and scalability while adhering to data protection standards.
Leverage orchestration tools, such as Databricks Workflows or Airflow, to orchestrate and monitor data pipelines, ensuring seamless integration, data quality, and compliance with regulatory requirements.
Implement data transformation and validation processes using dbt and SQL to ensure data accuracy, consistency, and regulatory compliance across the organization.
Continuously monitor, troubleshoot, and optimize data pipelines to ensure data reliability, performance, and adherence to best practices and industry regulations
Stay current with industry trends, emerging technologies, and regulatory changes to ensure the company remains at the forefront of data engineering best practices in the compliance and AML space.

Qualifications

0-2+ years of experience in data, or backend engineering roles
Bachelor's degree in Computer Science, Engineering, or a related field.
Some understanding of data engineering concepts such as orchestration & pipelining, data modeling, streaming and warehousing
Excellent problem-solving skills and the ability to work independently or as part of a team.
Eagerness to learn new technologies and adapt to industry trends and changes

For US-based roles only: the anticipated hiring base salary range for this position is $106,600.00 - $154,600.00, depending on factors such as experience, education, level, skills, and location. This range is based on a full-time position. In addition to base salary, this role is eligible for incentive compensation. Moody's also offers a competitive benefits package, including not but limited to medical, dental, vision, parental leave, paid time off, a 401(k) plan with employee and company contribution opportunities, life, disability, and accident insurance, a discounted employee stock purchase plan, and tuition reimbursement.

Moody's is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody's also provides reasonable accommodation to qualified individuals with disabilities or based on a sincerely held religious belief in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications.

For San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance.

This position may be considered a promotional opportunity, pursuant to the Colorado Equal Pay for Equal Work Act.

Click here to view our full EEO policy statement. Click here for more information on your EEO rights under the law. Click here to view our Pay Transparency Nondiscrimination statement. Click here to view our Notice to New York City Applicants.

Candidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody's Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.","Airflow, Almacenamiento de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Modelado de datos, Modelo de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3968878268/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=%2BU4O4xWDrt1uvJ0vukzjPw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 semanas,"Chicago, IL","Acerca del empleo
Job Description for Data Engineer
Use programming language and tools (SQL, Python) to automate data intake and cleansing processes
Create and maintain data visualization dashboards and reports for internal and external use
Identify ways to improve data reliability, efficiency and quality
Develop, construct, test and maintain cloud architectures
Interface with research team to incorporate latest industry and business intelligence
Use large data sets to continually address business issues
Document best practices and create state of data documentation
Deploy sophisticated analytics programs, machine learning and statistical methods
Deliver updates to stakeholders based on analytics
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs
Keep data separated and secure across national boundaries through multiple data centers within AWS and Google Cloud

Qualifications for Data Engineer
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience with object-oriented/object function scripting languages, particularly Python and Pandas
Data visualization expertise with proven examples in Power BI or other visualization tools
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
Strong project management and organizational skills.
We are looking for a candidate with 3+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:

Required
Experience with relational SQL databases, including Postgres and MySQL
Experience with object-oriented/object function scripting languages, particularly Python and Pandas
Familiarity with data visualization tools, particularly PowerBI or equivalent such as Tableau

Nice To Have
Familiarity with AWS cloud services: EC2, RDS, Data Pipeline
Familiarity with GCE cloud services: BigQuery, Composer, CloudSQL
Knowledge of the food industry and/or supply chain","Amazon Web Services (AWS), Analítica de datos, Base de datos SQL, Base de datos relacional, Canalizaciones de datos, Google BigQuery, Microsoft Power BI y SQL, Secuencia de comandos y Servicios en la nube",Solicitud sencilla
https://www.linkedin.com/jobs/view/3971729704/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=0EkLJIraWwx2CSx5KwiFLQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Detroit, MI","Acerca del empleo
Rocket Mortgage, backed by Rocket Companies®, means more opportunities for you to carve your own career path forward. From our desire to revolutionize the way people get mortgages to addressing challenges big or small with outside-the-box solutions, we’re not your typical employer. We’ll provide you with everything you need to make sure you’re successful here.

As a Technology team member, you’re empowered to make an impact, employ your entrepreneurial spirit and build a career customized by you because at Rocket, you can. We are creating digital products that solve life’s most complex moments. You’ll get the chance to shape the future of tech, have your voice heard, get ahead in your career and develop your skills. With a tech career here, there's no limit to what you can achieve.

Apply today to join a team that offers career growth, amazing benefits and the chance to work with leading industry professionals.

Minimum Qualifications

Bachelor's degree in computer science, information technology, or a related field or equivalent experience

Preferred Qualifications

3 years of experience working with database tools
3 years of programming experience using Python
3 years of experience working with SQL server integration services or ETL tools
3 years of experience working with Pyspark
3 years of experience working with data integration tools
2 years of experience working with AWS tools
Proficiency in the Microsoft Office suite
Experience working with ETL tools
Knowledge of data integration tools
Software programming languages, such as Python

Job Summary

As a Data Engineer, you'll work with database engineers to design, develop and maintain the infrastructure of data within the data warehouse, including setting the ETL (extract, transform, load) processes, bringing in new data sources, modifying existing data sources, making sure data is clean, complete and consumable, as well as designing data models within the data warehouse. You'll work as part of one or more project teams and will be responsible for designing and building mechanisms to move, integrate, cleanse and publish large volume datasets. This is a developer role with a specialty in data and requires deep knowledge of a variety of programming languages and design patterns.

Responsibilities

Design and support the new and evolving sources of data being brought into the data warehouse
Work closely with data architects and follow best practices for data management consumption
Work closely with business analysts to work through business requirements and develop processes to provide the needed data visibility via the data warehouse and reporting platform
Model application layer and metadata design
Design and create automated applications and reporting solutions
Work closely with front-end developers to ensure data is being brought in and data integrity is being maintained
Monitor and troubleshoot performance issues on the data warehouse servers

Benefits And Perks

Our team members fuel our strategy, innovation and growth, so we ensure the health and well-being of not just you, but your family, too! We go above and beyond to give you the support you need on an individual level and offer all sorts of ways to help you live your best life. We are proud to offer eligible team members perks and health benefits that will help you have peace of mind. Simply put: We’ve got your back. Check out our full list of Benefits and Perks.

Who We Are

Rocket Companies® is a Detroit-based company made up of businesses that provide simple, fast and trusted digital solutions for complex transactions. The name comes from our flagship business, now known as Rocket Mortgage®, which was founded in 1985. Today, we’re a publicly traded company involved in many different industries, including mortgages, fintech, real estate and more. We’re insistently different in how we look at the world and are committed to an inclusive workplace where every voice is heard. We’re passionate about the work we do, and it shows. We’ve been ranked #1 for Fortune’s Best Large Workplaces in Financial Services and Insurance List in 2022, named #5 on People Magazine’s Companies That Care List in 2022 and recognized as #7 on Fortune’s list of the 100 Best Companies to Work For in 2022.

Disclaimer

This is an outline of the primary responsibilities of this position. As with everything in life, things change. The tasks and responsibilities can be changed, added to, removed, amended, deleted and modified at any time by the leadership group. We are proud equal opportunity employers and committed to providing an inclusive environment based on mutual respect for all candidates and team members.

Employment decisions, including hiring decisions, are not based on race, color, religion, national origin, sex, physical or mental disability, sexual orientation, gender identity or expression, age, military or veteran status or any other characteristic protected by state or federal law. We offer opportunities to all job seekers including individuals with disabilities and provide reasonable accommodation to qualified individuals with disabilities in accordance with state and federal law. If you need a reasonable accommodation to assist with your job search, application for employment, or interview, please visit our FAQ page at www.myrocketcareer.com/how-we-hire/faqs/ for details on how to contact our team.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos y SQL Server Integration Services (SSIS), Bases de datos, Ciencias de la computación, Herramientas de bases de datos y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3978158425/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=QCzU5V3cBM2bTp3y87QkKA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Boston, MA","Acerca del empleo
The Expertise You HaveB.S. degree or higher in Computer Science or equivalent experience
Conceptual understanding of Blockchain and crypto based analytics gained in a financial services enterprise environment
Experience with data analysis and database design
2+ years of recent experience with relational database technology, data Engineering and ETL/ELT skills
Working experience with Linux shell scripts and job scheduling tools like Control-M/Autosys
Ability to develop Data APIs to support varied application requirements
Experience with AWS services, Snowflake and Python
Exposure to GitHub, and Jenkins, is desirable.
Knowledge of Finance and Investing domains is a plus

Must Have's:Crypto knowledge
Strong Python experience
Experience with Data analysis/Database design (ideally Oracle)
Nice to Have's:Snowflake
AWS
GitHub and Jenkins",GitHub y Jenkins,Solicitud sencilla
https://www.linkedin.com/jobs/view/3817715855/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=unrUnh5Mxg%2BPA9ipHldC4g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 6 meses,"Jacksonville, FL","Acerca del empleo
Apply Now

<< Return to Search Results

Data Engineer

Contract 

Jacksonville (On Site Preferred)

Brooksource is looking for an experienced Data Engineer to join one of our leading global pharmaceutical clients. This person will create prototype data engineering solutions for use cases across critical business priorities. They will also manage new data sources and drive business impact across the US for the Americas Region. This is an exciting and meaningful position as an integral part of the overall Business Intelligence Team!

Types of Responsibilities

 Identify and resolve quality issues during complex data migration & transformation.
 Support BI analysts in basic data engineering capabilities enabling throughput, complex analytics support.
Accountable for data engineering use cases that may have “limited time” life cycles.
 Writing queries / coding real-time, fast deployment within sandbox environment
 Data Engineering to support acceleration of E2E Reporting and Analytics
 Manage data catalog for the business.
 Pull disparate data sources together in the sandbox.
 Manage new data sources (conform w/ HIPAA, HCC) and access protocols.
 Problem solving for data issues in the lake in partnership with IT team.
 Analytics to answer urgent business priority questions to drive action.

Qualifications

A bachelor’s or advanced degree is required (Computer Science, Data Science, Business Administration, Engineering, with an Information Technology focus or related field)
A minimum of two years of progressive, broad-based information systems and technology experience in data warehousing, and decision support/reporting environments
Minimum of two years of software development experience, specifically in the area of data transformation scripting languages – proprietary (INFORMATICA, SQL) or open source (TALEND, Python) – required; open source transformation scripting strongly preferred.
Demonstrable data lifecycle capability required around data ingestion (ingest data from disparate systems into cloud computing environment), data contextualization (merging large data sets, developing algorithms to merge and clean data); data insights and analytics is required.
Demonstrated familiarity with large datasets and understanding of data analysis workflows is required – experience with data visualization tools like Tableau preferred.
 Superior attention to detail, organization skills, and the ability to balance multiple tasks.
 Ability to talk with business partners and help determine problem statement.
 Strong oral and written communication skills.

Eight Eleven Group provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws.

For over 16 years, Brooksource has established and maintained relationships that are designed to meet your IT staffing needs. Whether it’s contract, contract-to-hire, or permanent placement work, we customise our search based upon your company’s unique initiatives, culture and technologies. With our national team of recruiters placed at 21 major hubs around the nation Brooksource finds the people best-suited for your business. When you work with us, we work with you. That’s the Brooksource promise.

Brooksource is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth, lactation and related medical conditions), gender identity or gender expression, sexual orientation, marital status, military service and veteran status, physical or mental disability, protected medical condition as defined by applicable state or local law, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances.

Benefits & Perks

Brooksource offers competitive medical, dental, vision, Health Savings Account, Dependent Care FSA, and supplemental coverage with plans that can fit each employee’s needs. We offer a 401k plan that includes a company match and is fully vested after you become eligible, paid time off, sick time, and paid company holidays. We also offer an Employee Assistance Program (EAP) that provides services like virtual counseling, financial services, legal services, life coaching, etc.

Pay Disclaimer

The pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.

JO-2311-141111

Apply Now","Almacenamiento de datos, Analítica, Analítica de datos, Ciencia de datos y Ingeniería de datos, Ciencias de la computación, Comunicación, Comunicación escrita, Datasets y Ingesta de datos",Solicitar
https://www.linkedin.com/jobs/view/3980466081/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=g89U6PiekNirMmSPX7mm%2BQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Tampa, FL","Acerca del empleo
- Tampa, FL (Hybrid from day 1) - Fulltime

Job Title: Data Engineer

Location: Tampa, FL(Hybrid)

Duration: Fulltime Permanent

Job Description

We are looking for a skilled Data Engineer with 8 to 10 years of experience to join our team

Data Migration The candidate will work on projects involving the migration of data from flat files to Redshift This will involve the use of GluePySpark and Datasync

Terraform Experience The candidate should have experience with Terraform This is a crucial requirement for the role

Amazon Data Migration Service DMS to Kafka Migration In the future the candidate will also be involved in a project to migrate away from Amazon DMS to Kafka

Requirements

Proven experience as a Data Engineer or similar role

8 to 10 years of experience in the field

Experience with AWS Glue PySpark and AWS DataSync

Strong knowledge of Terraform

Experience with Amazon DMS and Kafka

Proficiency in Python scripting

Excellent problemsolving abilities and detail orientation

Strong communication and teamwork skills

Responsibilities

Migrate data from flat files to Redshift using GluePySpark and Datasync

Utilize Terraform for various tasks and projects

Participate in a project to migrate from Amazon DMS to Kafka in the future

Skills

AWS Glue (with AWS Cloud)","Almacenamiento de datos, Apache Kafka, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , PySpark y Python, Amazon Redshift, Comunicación y Secuencia de comandos",Solicitar
https://www.linkedin.com/jobs/view/3969514762/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=tlE%2BrPrh5aJfRdj3BoiKmw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"50 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"London, OR","Acerca del empleo
Be part of a unique team, bridging commercial projects with academic research.

Work with a mix of data, ML and cloud technologies in a role that promises diverse technical challenges and learning opportunities.

Join a growing and successful Data Science team in a supportive atmosphere that values ambition and continuous skill development.

Are you ready to be at the forefront of data innovation? This company is collaborating with the University of Bristol and looking to bolster its dynamic Data Science Team.

This is your chance to blend commercial projects with academic research, working alongside Masters/Ph.D. students. If you’re passionate about Data Engineering, Machine Learning, MLOps and Cloud Development, this opportunity is for you.

What You’ll Be Doing

Develop end-to-end data & machine learning solutions vital for serving millions of customers.
Consult within the business, working with different areas to identify opportunities and develop solutions.
Liaise with business experts to understand their needs and data requirements.
Create next-generation data platforms for market-leading ML solutions.

Remember, it’s not just about the technical know-how; it’s about ambition and the desire to learn. The hiring manager welcome diverse backgrounds and will provide upskilling in key areas.

What Experience You’ll Need To Apply

A strong foundation in data engineering/ML.
Python and/or SQL
Cloud experience.
Excellent problem-solving skills and a knack for optimal solutions.
Ability to communicate and collaborate effectively.

What You’ll Get In Return For Your Experience

You’ll be a part of a team that’s pushing the boundaries. They offer:

A competitive salary up to £50k, plus bonus and great benefits including 30 days of holiday (plus bank holiday)
Opportunities for professional growth and learning.
Flexible work policies and a collaborative environment.
Engaging projects that combine commercial and academic pursuits.
The chance to be part of a team that’s making a significant impact across the business.

Locations: Bristol/London/Bournemouth 2 days per week

What’s next?

If this role sounds like a match for you, please apply with an up-to-date CV.","Almacenamiento de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3977774622/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=33smswBhGlDJeMw5%2F4nB8A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (REMOTE OR MA BASED),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Worcester, MA","Acerca del empleo
For more than 170 years, The Hanover has been committed to delivering on our promises and being there when it matters the most. We live our values every day, demonstrating we CARE through our values, ESG initiatives and IDE journey.

Hanover Technology Group is hiring  is hiring a Data Analyst & Reporting Engineering for our IT Service Management Operations team in our Worcester, MA or in a remote work arrangement.

POSITION OVERVIEW: 

As part of the IT Service Management Operations team, the Data Analyst & Reporting Engineer as part of the Enterprise Technology Operations office will develop and deliver data analytics, reports, and dashboards that drive performance improvement across the organization.

The Data Analyst & Reporting Engineer plays a key role in unlocking our operational data to enhance IT Service Management (ITSM) and other business processes using data from within the ServiceNow platform. The Data Analyst & Reporting Engineer will develop and maintain robust reporting solutions that provide actionable insights, drive informed decision making, and support continuous improvement.

The successful candidate will use their strong analytical skills along with a deep understanding of ServiceNow and Power BI to produce elegant and informative analytics that translate complex data into insightful information. A demonstrated command of ITIL principles along with the supporting metrics is required.

IN THIS ROLE, YOU WILL:

Gather requirements from stakeholders to design and develop reports and dashboards.
Collaborate with ITSM process owners and stakeholders to understand data analytics needs and produce actionable reports, dashboards, and data sets.
Analyze data to identify trends, patterns, and anomalies. Provide insights and recommendations for technology and process improvements.
Create and maintain dashboards to track and monitor ITSM and other business process performance, including but not limited to: incident, problem, change, release, asset, configuration, and request management.
Design and implement custom reports, dashboards, and metrics using ServiceNow reporting tools and other relevant technologies.
Create executive level artifacts to support weekly, monthly, quarterly, and annual reviews of key performance and risk indicators supported by a rich set of dashboards and reports.
Provide expertise and guidance on best practices for efficient and effective reporting and data visualization within ServiceNow.


WHAT YOU NEED TO APPLY:

Minimum 3 years of proven experience working with ServiceNow, excellent knowledge of reporting and analytics capabilities within ServiceNow, PowerBI, and other tools.
Bachelor's degree required
Familiarity with ServiceNow data structures and access methods.
Understanding of ITIL and working knowledge of core processes.
Experience building complex reports, dashboards, and metrics using ServiceNow Performance Analytics, native ServiceNow reporting, PowerBI, or other tools.
Strong analytical skills with the ability to gather, organize, and interpret data to pinpoint improvement opportunities and present insights in a clear and concise manner.
Ability to work independently and collaboratively in a team environment, managing multiple priorities and deadlines.
Attention to detail and a commitment to delivering high-quality, accurate reports.
Demonstrated experience in developing and implementing reporting solutions to support business objectives.
Advanced certifications or training in data analysis, business intelligence, or data visualization tools is a plus.
Alignment with our CARE values (Collaboration, Accountability, Respect, Empowerment).
Must be eligible to work in the United States without requiring sponsorship now or in the future (ie. US Citizen or green card holder)


TECHNICAL SKILLS REQUIRED:

Experience with ServiceNow advanced reporting such as ServiceNow Dashboards and Performance Analytics.
Experience with advanced reporting tools including Power BI and Tableau. 


CAREER DEVELOPMENT:

It's not just a job, it's a career, and we are here to support you every step of the way. We want you to be successful and fulfilled. Through on-the-job experiences, personalized coaching and our robust learning and development programs, we encourage you - at every level - to grow and develop.

BENEFITS:

We offer comprehensive benefits to help you be healthy, build financial security, and balance work and home life. At The Hanover, you'll enjoy what you do and have the support you need to succeed.

Benefits include:

Medical, dental, vision, life, and disability insurance 
401K with a company match 
Tuition reimbursement 
PTO 
Company paid holidays 
Flexible work arrangements 
Cultural Awareness Day in support of IDE 
On-site medical/wellness center (Worcester only) 
Click here for the full list of Benefits


EEO statement:

The Hanover values diversity in the workplace and among our customers. The company provides equal opportunity for employment and promotion to all qualified employees and applicants on the basis of experience, training, education, and ability to do the available work without regard to race, religion, color, age, sex/gender, sexual orientation, national origin, gender identity, disability, marital status, veteran status, genetic information, ancestry or any other status protected by law.

Furthermore, The Hanover Insurance Group is committed to providing an equal opportunity workplace that is free of discrimination and harassment based on national origin, race, color, religion, gender, ancestry, age, sexual orientation, gender identity, disability, marital status, veteran status, genetic information or any other status protected by law.""

As an equal opportunity employer, Hanover does not discriminate against qualified individuals with disabilities. Individuals with disabilities who wish to request a reasonable accommodation to participate in the job application or interview process, or to perform essential job functions, should contact us at: HRServices@hanover.com and include the link of the job posting in which you are interested.

Privacy Policy:

To view our privacy policy and online privacy statement, click here.

Applicants who are California residents: To see the types of information we may collect from applicants and employees and how we use it, please click here

Other Details

 Job Function Information Technology
 Pay Type Salary
 Required Education Bachelor's Degree


Apply Now","Analítica, Ciencia de datos, Tableau y Visualización de datos, Annual Reviews, Análisis de rendimiento de software, ITIL, Obtención de requisitos, Panel de control y ServiceNow",Solicitar
https://www.linkedin.com/jobs/view/3979104075/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=F7m4VrNYkhbfmQ%2BPDEhHWQ%3D%3D&trk=flagship3_search_srp_jobs,Data - Data Visualization Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Washington, DC","Acerca del empleo
Data Visualization Engineer 

Washington, D.C 

Must

Active Secret clearance required

5+ years of experience in IT and Data Visualization

3+ years' experience with Tableau, Power BI, or D3.js,

3+ years of data analysis and data management concepts

Ability to mentor junior engineers is a huge plus

Excellent communication and collaboration skills

Strong portfolio demonstrating data visualization expertise

Bachelors degree in Information Design, Data Science, Computer Science or related field

Duties

Working with stakeholders to understand data needs and requirements

Designing and develop visually compelling and informative dashboards, reports, and infographics

Developing and maintain documentation, standards, and best practices for data visualization

Performing data analysis to identify trends and insights, and use that knowledge to design effective visualizations

Working closely with cross-functional teams to develop and implement data visualization solutions

Continuously improve and innovate data visualization practices

Progression Inc. is an equal opportunity and affirmative action employer. Progression Inc. is committed to administering all employment and personnel actions on the basis of merit and free of discrimination based on race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or status as an individual with a disability. Consistent with this commitment, we are dedicated to the employment and advancement of qualified minorities, women, individuals with disabilities, protected veterans, persons of all ethnic backgrounds and religions according to their abilities. #indpro
Aptitudes y experiencia deseables
DATA VISUALIZATION, SECRET, TABLEAU, POWER BI, D3.JS, DATA, DASHBOARD, DASHBOARDS","Analítica, Analítica de datos, Análisis de datos, Ciencia de datos, D3.js, Minería de datos, Visualización y Visualización de datos, Ciencias de la computación y SC Clearance",Solicitar
https://www.linkedin.com/jobs/view/3980291419/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=T5iIJqPBsbYSlN1iejDylA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Quincy, MA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Hi,

Hope you are doing well

Number of position : 2

Only Full Time

Full-time

I, Shakib (i3 infotek) would like to share a job opportunity as Data Engineer based in Quincy, MA (Onsite) location for a Full-time position.

In case, if you are not comfortable with this location, please share your preference with contact details for further requirements

Kindly find the JD below and let me know if you are available for the same.

Job tittle – Data Engineer

Duration: Full-time

Location – Quincy, MA (Onsite)

Job Description

Data Engineer for databricks/ Hadoop/ scala

Hadoop, Databricks, Scala

Senior/Lead developer position who has extensive experience with development and designing of bigdata platform with Hadoop, Scala and Python experience.

 The candidate should have extensive experience in the bigdata developer with data engineering/big data developer.
 The candidate should have good experience in Scala & Python programming language for data bricks development
 Experience on Databricks
 Candidate should have very good experience in the Dataware housing (ETL Concepts, Types of SCD)
 Candidate should have very good experience in the data lake and delta tables concepts.
 Candidate should have experience in databases like SQL server/Oracle

Willing to work out of Quincy, MA

Please reply me with your updated resume and required details:

Full Name

Best number to reach you:

Work Authorization/Visa Status

Current Location:

Expected Compensation

Best time to call you:

Waiting for your earliest response

Sincerely,

Mohd Shakib

Sr. Technical Recruiter Direct: (phone number removed)","Big data, Extraer, transformar y cargar (ETL), Ingeniería de datos , Lenguajes de programación, Python, SQL y Scala, Azure Databricks, Bases de datos y Lagos de datos",Solicitar
https://www.linkedin.com/jobs/view/3977991487/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=h2iZ2vbOgHV7vHyThVKIgg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Pleasanton, CA","Acerca del empleo
Title: Data Engineer
Location: Dublin, CA (Onsite role – 4 days/Week)
Type: Fulltime with Intelliswift Software Inc.

Detailed Job Description
We are seeking a highly skilled Data Analyst with strong analytical skills and the ability to work independently. The ideal candidate will be a self-starter capable of piecing together data to answer complex questions. Experience in the banking domain is a plus.
 Key Responsibilities:
Perform data analysis to identify trends, patterns, and insights.
Utilize data analytics tools to process and interpret data.
Work with data lakes to gather and organize data.
Answer business questions through detailed data analysis.
 Must-Have Skills:
Strong analytical skills
Self-starter with the ability to work independently
Experience with Snowflake
Proficiency in Azure
Experience with Azure Data Factory (ADF)
 Preferred Qualifications:
Experience in the banking domain.","Analítica de datos, Lagos de datos, Microsoft Azure, Oracle Application Development Framework, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3963210812/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=YNGYsGhXV5ha6zRtjjLu1w%3D%3D&trk=flagship3_search_srp_jobs,Gen AI Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 1 mes,"Irving, TX","Acerca del empleo
Title- GenAI Data Engineer
Fulltime
Location – Irving TX/Piscataway New Jersey

 Role Description: Data Engineer/GenAI Data Engineer As a GenAI Developer at Incedo, you will play a critical role in shaping the future of AI by developing and deploying cutting-edge GenAI applications, specifically focusing on RAG applications. You will leverage your expertise in machine learning, deep learning, generative AI techniques, and software engineering principles to design, implement, optimize, and deploy these applications, ensuring seamless integration with existing workflows and infrastructure. Designation: GenAI Developer 

Role and responsibilities

• Design, develop, and deploy scalable data pipelines for GenAI and RAG applications using Python, to ingest, transform, and load both structured and unstructured data 
• Utilize Vector Databases and NoSQL databases for efficient storage and retrieval of structured and unstructured data 
• Write and execute complex SQL queries to extract, transform, and load structured data 
• Build and maintain ETL pipelines using tools like Pyspark to handle both structured and unstructured data
• Leverage cloud-based data warehousing and analytics solutions for structured data 
• Explore and implement tools for ingesting and processing unstructured data on cloud platforms 
• Apply knowledge of LangChain and prompt engineering techniques to optimize data preparation for GenAI applications 
• Automate data workflows using cloud-based workflow orchestration tools (e.g., Airflow)
• Integrate data pipelines with CI/CD pipelines using Jenkins
• Monitor and maintain data pipelines for performance and stability
• Document data pipelines and processes for future reference. Technical skills requirements The candidate must demonstrate proficiency in,
• Proficiency in Python
• Experience with Vector Databases
• Strong understanding of SQL and NoSQL databases 
• Experience with ETL processes and tools (Pyspark) for both structured and unstructured data
• Experience with cloud platforms (familiarity with AWS, Azure, or GCP is a plus) 
• Experience with CI/CD tools (Jenkins)
• Experience working with unstructured data processing tools 
• Knowledge of LangChain and prompt engineering techniques is a plus. Nice-to-have skills
• Experience working with RAG applications
• Experience with cloud-based data warehousing solutions (e.g., BigQuery, Redshift, Snowflake)
• Experience with cloud-based workflow orchestration tools (e.g., Airflow, Prefect)
• Familiarity with Kubernetes (K8S) is a welcome addition
• Google Cloud certification • Unix or Shell scripting Qualifications 
• B.Tech., M.Tech. or MCA degree from a reputed university We are an Equal Opportunity Employer We value diversity at Incedo. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status","NoSQL, Bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3984143769/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=goJGXhB2WE1ycaElLE7P2Q%3D%3D&trk=flagship3_search_srp_jobs,LLM Engineer with Prompt Engineering /Data Embedding and guardrails.,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Austin, TX","Acerca del empleo
Job Description

5+ years of industry experience with strong Client fundamentals

Technical Skills

Experience in Machine Learning, particularly on Large Language Models (LLMs) and Generative AI.
Comprehensive knowledge and hands-on experience with fine-tuning approaches and training models.
Strong understanding of prompt engineering, data embedding, and guardrails
Strong analytical and problem-solving skills
Strong programming skills in Python, C, and C++
Demonstrated leadership in both applied research and development.
Excellent written and verbal communication skills, comfortable presenting research to large audiences, and the ability to work hands-on in multi-functional teams.

Certifications Needed

Advanced degree (Master's or Ph.D.) in Computer Science, Artificial Intelligence, Machine Learning","Programación y Python, C++, Comunicación, Fundamentals, IA generativa, Investigación aplicada, Presentaciones, Programación en C y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3975110057/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=8urr%2FVFYs9ec5qo7euEbyA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"140 US$K/año - 160 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 2 semanas,"Nueva York, NY","Acerca del empleo
Data Engineer - Fintech Firm in the Healthcare Industry - Seed Funded Startup (Very Well Funded) - On Site (Financial District NYC)

A Seed Funded Startup that's looking to build out a Healthcare Platform that will revolutionize the way you interact with your healthcare provider are looking for their first Data Engineer
Currently at a Seed Stage and a step away from a Series A Funding, the firm is building a platform that will make your life very easy after visiting a medical professional by being able to pay any expenses on the platform.

They are looking for a Data Engineer to own the buildout of their Data Pricing Platform/Software and work on big data systems - scope of role can expand even further based on needs!
Compensation: Up to 160k base + (percent ownership) Equity + Phenomenal benefits.

If you are interested, feel free to apply or reach out to prince.agyemang@gogpac.com

All qualified applicants will receive consideration without regard to race, age, color, sex (including pregnancy), religion, national origin, disability, sexual orientation, gender identity, marital status, military status, genetic information, or any other status protected by applicable laws or regulations.
GPAC (Growing People and Companies) is an award-winning search firm specializing in placing quality professionals within multiple industries across the United States since 1990. We are extremely competitive, client-focused and realize that our value is in our ability to deliver the right solutions at the right time.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3979762931/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=zmr6MQGC6%2Fl%2BCSLR3oRIGg%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer - SQL, SPARK, Pyspark, R","110 US$K/año - 170 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Washington, DC","Acerca del empleo
Position Overview

The Data Engineer will be responsible for managing and optimizing the data infrastructure for a leasing defense company. This individual will work closely with the Data Science and Technical Support teams to ensure data integrity and accuracy, while also developing and optimizing data pipelines and applications. The ideal candidate will have a strong background in data integration, SPARK, Pyspark, and other programming languages, as well as experience with regulatory compliance and data management.

Current IRS employment is required to be considered for this Washington D.C./Remote position.

Key Responsibilities

Design, develop, and maintain data pipelines and applications
Optimize data integration processes to ensure accuracy and efficiency
Collaborate with Data Science team to identify and implement data transformation and processing techniques
Debug and troubleshoot data issues and provide technical support to resolve them
Manage data feeds and ensure data quality and integrity
Ensure compliance with regulatory requirements for data handling and processing
Collaborate with Application Development team to design and implement data-driven applications
Stay up-to-date on new technologies and tools in the field of data engineering
Work closely with cross-functional teams to understand data needs and requirements

Qualifications

Bachelor's degree in Computer Science, Software Engineering, Physics, or a related field
3+ years of experience in Data Engineering
Strong understanding of data management and data transformation techniques
Experience with data integration tools such as SPARK, Pyspark, and pandas
Proficient in SQL and other programming languages such as R and Python
Experience with application design and development
Knowledge of regulatory compliance and data privacy laws
Experience in distributed computing and open source technologies
Excellent problem-solving and debugging skills
Strong communication and collaboration skills
Experience working in a government or regulatory environment, specifically with the IRS, is a MUST

Benefits

Medical, Dental, Vision

401k

PTO, Sick Pay

Email Your Resume In Word To

Looking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:

connor.turnbow@cybercoders.com

Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : CT7-1811317 -- in the email subject line for your application to be considered.***

Connor Turnbow-Lindenstadt - Recruiter

Applicants must be authorized to work in the U.S.

CyberCoders is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, sexual orientation, gender identity or expression, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, status as a crime victim, disability, protected veteran status, or any other characteristic protected by law. CyberCoders will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. CyberCoders is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please contact a member of our Human Resources team to make arrangements.

CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance.","Canalizaciones de datos, Ciencia de datos, Ingeniería de datos , Integración de datos, Pandas (Software) y PySpark, Calidad de datos, Ciencias de la computación, Distributed Computing y Fuentes de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3973323156/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=11814Zqo0hpif8%2BbX02DLA%3D%3D&trk=flagship3_search_srp_jobs,,,,,,,
https://www.linkedin.com/jobs/view/3959917309/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=F%2B%2FwdLvw%2Fk1r8R8c0176jQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Remote / Telecommute,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Mineápolis, MN","Acerca del empleo
Job Description:
 
 Skills and Qualifications: 
 SQL. 
 Azure Data Bricks. 
 Azure Data factory. 
 Azure. 
 Spark. 
 PySpark. 
 Scala. 
 Python. 
 CI/CD.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL, Hive, Ingeniería de datos , Python, SQL y Scala",Solicitar
https://www.linkedin.com/jobs/view/3944903298/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=wuorBkmkgcanUfULFwT7Sg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Capital One Software (Remote),"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"McLean, VA","Acerca del empleo
Locations: US Remote, United States of AmericaData Engineer - Capital One Software (Remote)

Capital One has been a pioneer through our tech journey as the first large bank to go all in on the public cloud, while operating in a complex and highly regulated business environment. We have built out a large engineering organization, moved to the cloud, re-architected our applications and data platforms, and embraced machine learning at scale. Our AI/ML capabilities are now at the forefront of what’s possible in banking. (e.g., Capital One Eno).

Our teams have built and battle tested new capabilities to meet those needs. We’ve open sourced several of the software tools we built (e.g., Cloud Custodian, Hygieia) and forged new partnerships with other digital leaders (e.g., Microsoft, MSFT).

Through this journey, we've developed a suite of internal solutions uniquely designed to meet the challenges of a digital-first, cloud-first business at scale. We also recognize that many other businesses are facing similar data management needs as they accelerate their cloud and data journeys, and are exploring how best to bring some of the tools to market as enterprise B2B software solutions.

Capital One Software is seeking a Senior Associate, Data Engineer who is passionate about marrying innovation with emerging technologies. As a Capital One Senior Associate, Data Engineer you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.

What You’ll Do:

 Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies 
 Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems 
 Utilize programming languages like Java, Scala, Python and expertise in Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake 
 Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community 
 Collaborate with digital product managers & architects, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment 
 Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance 

Basic Qualifications: 

 Bachelor’s Degree 
 At least 2 years of experience in application development (Internship experience does not apply) 
 At least 1 years of experience in big data technologies 
 At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud) 

Preferred Qualifications:

 3+ years of experience in application development including Python, SQL, Scala, or Java 
 2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud) 
 2+ years experience with Distributed data/computing tools (EMR, Kafka, MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL) 
 2+ year experience working on real-time data and streaming applications 
 2+ years of experience with NoSQL implementation (Mongo, Cassandra) 
 2+ years of data warehousing experience (Redshift or Snowflake) 
 2+ years of experience with UNIX/Linux including basic commands and shell scripting 
 2+ years of experience with Agile engineering practices 
 1+ years of experience with Salesforce / other ERP solutions 

At this time, Capital One will not sponsor a new applicant for employment authorization for this position.

The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.

Remote (Regardless of Location): $117,400 - $134,000 for Data Engineer

Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.

This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.

Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.

This role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.

If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.

For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com

Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.

Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).","Apache Kafka, Google Cloud , Hadoop, Hive, NoSQL y Scala, Amazon Redshift, Cassandra, Guiones shell y Gurobi",Solicitar
https://www.linkedin.com/jobs/view/3978624622/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=phaByR9np6Jj5G0l2%2F%2BcRw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"120 US$K/año - 150 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Midland, TX","Acerca del empleo
Description

We are providing an opportunity for a proficient Data Engineer to join our team. This role is located in Midland, Texas and is part of the telecom industry. As a Data Engineer, your primary function will be to design, build, and optimize data pipelines from source systems into an analytics ready environment, amongst other responsibilities.

Responsibilities

 Design, build, and optimize data pipelines from various source systems into an analytics ready environment.
 Convert raw data into usable formats for data consumers.
 Monitor and support the development and production environments within Databricks to ensure system availability and quality.
 Continuously develop, test, and manage the data pipelines.
 Build data pipelines that transform, clean, and aggregate data from disparate systems.
 Develop data models and pipelines for reporting, dashboards, and machine learning.
 Collaborate with multiple business SMEs, data scientists and IT members to provide reliable and clean data.
 Work on multiple projects in a fast-paced environment.
 Utilize skills in Microsoft SQL Server, Data Pipelines, R Programming, Databricks, Python, and Snowflake.

Requirements

 Possess at least 3 years of experience in a Data Engineer or similar role
 Proficient in Microsoft SQL Server
 Demonstrable experience in designing and managing data pipelines
 Proficiency in R Programming is a must
 Experience with Databricks will be considered as an advantage
 Sound knowledge of Python language for data manipulation and analysis
 Expertise in Snowflake data warehousing platform
 Excellent problem-solving skills and attention to detail
 Strong communication and collaboration skills
 Bachelor's degree in Computer Science, Engineering or a related field.

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Almacenamiento de datos, Canalizaciones de datos, Ingeniería de datos y R (Lenguaje de programación), Ciencias de la computación, Comunicación, Manipulación de datos, Modelo de datos, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3971703979/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=h%2FxSdpXTI5M1rYRoNH4RiQ%3D%3D&trk=flagship3_search_srp_jobs,Data Visualization Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,"Indianápolis, IN","Acerca del empleo
Position: Data Visualization Engineer
Location: Hybrid; Indianapolis

Responsibilities 
Develop interactive and visually compelling dashboards, reports, and data visualizations using Power BI, Tableau, or related tools, to enable stakeholders to understand complex data insights easily
Collaborate with business cross-functional teams and client stakeholders to gather requirements, understand data needs, and translate them into effective visualization solutions
Stay up to date with the latest trends and advancements in data visualization, data engineering, and cloud technologies, and proactively recommend innovative solutions to enhance our analytics capabilities
Provide technical guidance and mentorship to junior team members, fostering a collaborative and knowledge-sharing environment
Requirements
Proven experience developing advanced dashboards, reports, and visualizations, with a strong portfolio displaying your work
Strong understanding of data warehousing concepts and best practices
Experience with data visualization tools, including both its design and data transformation capabilities as demonstrated by a portfolio of past reports and dashboards
Deep understanding of data visualization best practices, including principles of design, color theory, and readability
Excellent analytical and problem-solving skills
Effective communication and collaboration skills
Ability to work effectively in a fast-paced and dynamic environment
Familiarity with CI/CD pipelines and DevOps practices
Experience working in one or more Agile methodologies such as Scrum or Kanban 
Willingness to work in a hybrid work environment; work in our Indianapolis office or at the client location three days per week
Must be legally authorized to work in the United States
Must not require visa sponsorship
Preferred Qualifications 
Bachelor’s degree in computer science, engineering, or related field
Client-facing consulting experience
Certification in data visualization tools or relevant cloud technologies (e.g., Tableau, Power BI, Looker, Domo)
Extensive knowledge of SQL, data modeling, and ETL processes, with hands-on experience building data pipelines and working with large-scale datasets
Proficiency in at least one cloud platform (such as Azure, AWS, or GCP) with experience deploying, managing, and optimizing data solutions in a cloud environment","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud y Visualización de datos, Ciencias de la computación, Panel de control, Resolución de problemas y Teoría del color",Solicitud sencilla
https://www.linkedin.com/jobs/view/3974244316/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=hiNTEGe4pdBdBC%2BAHmvEpA%3D%3D&trk=flagship3_search_srp_jobs,Big Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 semanas,"Phoenix, AZ","Acerca del empleo
Big Data Engineer with mapreduce , spark , hive , sql skillset"" Expert in SQL and Data warehousing concepts. 
Hands-on experience with public cloud data warehouse (GCP, Azure, AWS). 
GCP certification will be very good to have. 
MapR experience is must Strong Hands on experience with one or more programming languages ( Python or Java).
Hands-on expertise with application design and software development in Big Data (Spark(Pyspark), HIVE). Experience with CICD pipelines, Automated test frameworks, DevOps and source code management tools (XLR, Jenkins, Git, Maven).
Strong communication and analytical skills including effective presentation skills.
Familiarity with Agile & scrum ceremonies.""
Digital : Big Data and Hadoop Ecosystems, Digital : Big Data and Hadoop Ecosystem - MapR

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","Almacenamiento de datos, Google Cloud , Hadoop, Hive, Lenguajes de programación, PySpark y Python, Aptitudes para hacer presentaciones, Comunicación y Java",Solicitar
https://www.linkedin.com/jobs/view/3818363720/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=AmR20pZSTfEAU5Lj1aLiHQ%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Programmer/Coder/Developer/Data Scientist/Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"San Antonio, TX","Acerca del empleo
SYNERGISTICIT is aware that the Job Market is Challenging due to almost 300,000 Tech Layoffs within the past year due to which The Job market is flooded with thousands of laid off Techies who are competing with existing Jobseekers. For entry level Job seekers to get client interviews and jobs they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.

Since 2010 we have helped Jobseekers differentiate themselves by providing the clients with candidates who have the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers. All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this economy no client wants or has the resources to take an entry level person and spend resources on upgrading their skills and on top of that pay the jobseeker. That's the specific reason there are so many techies both experience and freshers who are unemployed.

Clients have now the option to hire remote workers from anywhere so for a Jobseeker its important to introspect and see how they can become better and have the skills and technologies to meet client requirements. We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped thousands of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few. We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients. Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates looking to make their careers in IT Industry We welcome candidates with all visas and citizens to apply. We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then you should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

Candidates who Lack Experience or are freshers with No actual on Job experience with projects with clients Have had a break in careers Lack Technical Competency or skills being demanded by clients Different visa candidates (Like OPT/H4EAD/L2EAD )who want to get employed and settle down in the USA please check the below links to see success outcomes of our candidates https://www.synergisticit.com/candidate-outcomes/

We are also silver sponsors at Oracle Cloudworld , Las vegas from sept 18-21st— please visit us

https://www.oracle.com/cloudworld/sponsor-listing/#synergistic-it

Watch the below videos of us participating at Industry events with the Top companies in Technology at Oracle Cloud world /Oracle Java one (Las vegas) and at Gartner Data Analytics Summit (Florida)

Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube https://www.youtube.com/watch?v=OAFOhcGy9Z8

https://www.youtube.com/watch?v=EmO7NrWHkLM https://www.youtube.com/watch?v=NVBU9RYZ6UI

https://www.youtube.com/watch?v=Yy74yvjatVg SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube

If you have relevant skills and industry experience, please apply

For preparing for interviews please visit

https://www.synergisticit.com/interview-questions/

REQUIRED SKILLS For Java /Software Programmers

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Project work on the skills
 Knowledge of Core Java , javascript , C++ or software programming
 Spring boot, Microservices, Docker, Jenkins and REST API's experience
 Excellent written and verbal communication skills

For data Science/Machine learning

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Project work on the technologies needed
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
 Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis

Please understand skills and relevant experience on real world projects are required by clients for selection even if its Junior or entry level position the additional skills and Project work with hands on experience building projects at client site are the only way a candidate can be picked by clients. If not having the skills or hands on project work at client site then candidates can optionally opt for skill enhancement to gain the required skills and project work. No third party candidates or c2c candidates

please only apply to the posting

No phone calls please. Shortlisted candidates would be reached out.","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación y Programación, Ciencias de la computación, Comunicación, Cracking, Desarrollo de software y Lista de preseleccionados",Solicitar
https://www.linkedin.com/jobs/view/3983066846/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=VBB%2B8fJOshpuk5B1SJntwg%3D%3D&trk=flagship3_search_srp_jobs,"Sr., Hadoop Developer (Only W2)","55 US$/h - 60 US$/h En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.",Publicado de nuevo hace 17 horas,"Jacksonville, FL","Acerca del empleo
OpTech is seeking for strong Hadoop Developers for their direct client.

Position - Sr., Hadoop Developer.
Rate - $60/hour on W2.
Only W2, no C2C

A Hadoop developer is responsible for the design, development and operations of systems that store and manage large amounts of data. Most Hadoop developers have a computer software background and have a degree in information systems, software engineering, computer science, or mathematics. IT Developers are responsible for development, programming, coding of Information Technology solutions. IT Developers are responsible for documenting detailed system specifications, participation in unit testing and maintenance of planned and unplanned internally developed applications, evaluation and performance testing of purchased products. IT Developers are responsible for including IT Controls to protect the confidentiality, integrity, as well as availability of the application and data processed or output by the application. IT Developers are assigned to moderately complex development projects. Essential Functions:
Write code for moderately complex system designs. Write programs that span platforms. Code and/or create Application Programming Interfaces (APIs).
Write code for enhancing existing programs or developing new programs.
Review code developed by other IT Developers.
Provide input to and drive programming standards.
Write detailed technical specifications for subsystems. Identify integration points.
Report missing elements found in system and functional requirements and explain impacts on subsystem to team members.
Consult with other IT Developers, Business Analysts, Systems Analysts, Project Managers and vendors.
Scope time, resources, etc., required to complete programming projects. Seek review from other IT Developers, Business Analysts, Systems Analysts or Project Managers on estimates.
Perform unit testing and debugging. Set test conditions based upon code specifications. May need assistance from other IT Developers and team members to debug more complex errors.
Supports transition of application throughout the Product Development life cycle. Document what has to be migrated. May require more coordination points for subsystems.
Researches vendor products / alternatives. Conducts vendor product gap analysis / comparison.
Accountable for including IT Controls and following standard corporate practices to protect the confidentiality, integrity, as well as availability of the application and data processed or output by the application.
The essential functions listed represent the major duties of this role, additional duties may be assigned.

Must:
-Good SQL knowledge -CDP experience -PySpark, Scala -Python

OpTech is an equal opportunity employer and is committed to creating a diverse environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, status as a parent, disability, age, veteran status, or other characteristics as defined by federal, state or local laws.","Hadoop, PySpark, Python, SQL y Scala",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982372270/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=5lweqyypS5xVAnmIcp6JHA%3D%3D&trackingId=y5MPARabwWHYBCKxy4K4bQ%3D%3D&trk=flagship3_search_srp_jobs,Technology and Data - Software Engineer 4 - Contingent,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Concord, CA","Acerca del empleo
Job Description: In this contingent resource assignment, you may: Consult on complex initiatives with broad impact and large-scale planning for Software Engineering. Review and analyze complex multi-faceted, larger scale or longer-term Software Engineering challenges that require in-depth evaluation of multiple factors including intangibles or unprecedented factors. Contribute to the resolution of complex and multi-faceted situations requiring solid understanding of the function, policies, procedures, and compliance requirements that meet deliverables. Strategically collaborate and consult with client personnel. Required Qualifications: 5+ years of Software Engineering experience, or equivalent demonstrated through one or a combination of the following: work or consulting experience, training, military experience, education.

Comments for Suppliers (Use this area to provide any additional comments to all suppliers. For example, any specific skills to prioritize when looking for profiles.:

EEO:

“Mindlance is an Equal Opportunity Employer and does not discriminate in employment on the basis of – Minority/Gender/Disability/Religion/LGBTQI/Age/Veterans.”
Aptitudes y experiencia deseables
JAVA",Soporte técnico,Solicitar
https://www.linkedin.com/jobs/view/3927670224/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=zZNkZmdbhnyR6dCwSuyunA%3D%3D&trk=flagship3_search_srp_jobs,Remote Work - Need 3 Data Engineer and 1 data analyst,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 2 meses,Estados Unidos,"Acerca del empleo
3 Data Engineer and 1 data analyst ..100% remote job..send resume ASAP!!

Job Summary

Job Summary

The Senior Data Engineer will be responsible for design and development of ETL. The role will cover full systems development life cycle (SDLC) phases including requirements gathering, data analysis, system design, development, implementation, and post-implementation support. During the design phase, the role will work closely with Data Architects/Models. The developer will be responsible the ETL movement of data from originating source feeds (files, x12, etc) to target systems (SQL Server, Snowflake) using Data Bricks.

Key Responsibilities

Key Responsibilities

 Collaborate with users, business analysts, developers, database administrators, and project managers on reporting needs.
 Collaborate with data architect to create conceptual, logical, and physical data models for reporting databases.
 Implement changes and provide post-implementation user support and system support.
 Identify and advocates beneficial change opportunities.
 Acts as a technical resource for application and data users, data administrators and others.
 Ensures compliance with TriWest HIPAA, privacy and government security policies.
 Performs other duties as assigned.
 Regular and reliable attendance is required.

Working Conditions

Working Conditions

 Works non-standard hours as required
 Works within a standard office environment, with less than 10% travel
 Extensive computer work with prolonged periods of sitting

Required

Education & Experience

 Prior experience working in the Healthcare industry
 Bachelor's Degree in Business Administration, Computer Science, Mathematics, Engineering, or related field with programming and database systems coursework or equivalent database development experience
 5+ years of experience in a Data Engineering role
 2+ years of experience with Data Bricks preferably within Azure Cloud
 3+ years of experience with Python
 3+ years of experience with Azure
 Experience in gathering business requirements from cross functional teams
 Experience with SDLC processes and DevOps

Preferred

 Experience with Data Governance
 Experience with SQL Server and Snowflake in Azure Cloud
 Experience in ETL with both structured and semi-structured data
 Experience with x12 data file formats
 Understanding of ANSI and T-SQL
 Experience with data analysis in the healthcare industry
 Experience in a team or project lead role, with a focus on mentoring

Competencies

Technical Skills

Strong knowledge of data warehouse design; data modeling; proficiency with SQL; strong experience with data extraction, transformation, and loading (ETL); working knowledge of report and dashboard development; strong knowledge of SQL Server / Snowflake databases; working knowledge of Oracle databases.

Problem Solving / Analysis

Ability to solve problems through systematic analysis of processes with sound judgment; Has a realistic understanding of relevant issues.

Organizational Skills

Ability to organize people or tasks, adjust to priorities, learn systems, within time constraints and with available resources; Detail-oriented.

Multi-Tasking / Time Management

Prioritize and manage actions to meet changing deadlines and requirements within a high volume, high stress environment.

Information Management

Ability to manage large amounts of complex information easily, communicates clearly, and draws sound conclusions.

High Intensity Environment

Ability to function in a fast-paced environment with multiple activities occurring simultaneously while maintaining focus and control of workflow.

Coping / Flexibility

Resiliency in adapting to a variety of situations and individuals while maintaining a sense of purpose and mature problem-solving approach is required.

Computer Literacy

Ability to function in a multi-system Microsoft environment using Word, Outlook, TriWest Intranet, the Internet, and department software applications.

Communication / People Skills

Ability to influence or persuade others under positive or negative circumstances; Adapt to different styles; Listen critically; Collaborate. 

US Citizen Only 
100% remote
Candidates must sit in one of these states: AK, AR, AZ, *CO, FL, HI, IA, ID, IL, KS, LA, MD, MN, MO, MT, NE, NV, NM, NC, ND, OK, OR, SC, SD, TX, UT, VA/DC, *WA, WI, WY
When you do the Internal Recorded Interview – Tell candidates that the client will see the recording – They should act as if your screening call is an interview with the client.
CODERBYTE REQUIRED: Sr. Data Engineer -
SUBS MUST INCLUDE A FULLY COMPLETED SUBMITTAL TEMPLATE (all fields including skillset checklist)","Analítica de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Extracción de datos, Microsoft Azure, Modelado de datos, Modelo de datos, Snowflake y Snowflake cloud",Solicitud sencilla
https://www.linkedin.com/jobs/view/3903803058/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=UY1QqX8MzaJmjxvNvu1y0w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 meses,Estados Unidos,"Acerca del empleo
100% Remote

Need LinkedIn

W2 candidates only

Need only 2 strong candidates

There will be a short tech assessment on this role which needs to be completed and cleared to be considered

Skills Requirement | Success Criteria for all:

Hyper communication and transparency
Drivers with high level of self-motivation
Extreme accountability and ownership
Hands-on executionists vs theorists
Critical thinking and problem-solving skills

Skills Requirement | Success Criteria for Data Engineer: 

Must be hands-on with development and build. Need team members who are self-motivated and driven.
Must have deep experience with SSIS, Python, SQL, and ideally Azure and Azure Data Factory. Primary relevant experience would be coding complex SQL, building the integration packages (including logic development), flat file transfers, etc. There is no XML work and no experience needed with Pyspark, Spark, or any other big data platforms. The standard requirement is Rest or Soap-based APIs but the majority are flat files. There may be some ETL work moving data from various source systems to data warehouse.","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Pensamiento crítico, Bases de datos, Comunicación y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3961559198/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=%2FPn4GtxkzLHUwacwhVjTwg%3D%3D&trk=flagship3_search_srp_jobs,DATA ENGINEER,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
Jd

Top Skills: 

GCP 
Scala/Python
DataProc for running workloads – 6+ months 

5 years of experience in Data or BI Engineering, Data Warehousing/ETL, or Software Engineering

4 years of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983492972/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=ijew3Hi19YThP%2Fy9hHSeWA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (L5) - Growth,"170 US$K/año - 720 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Los Gatos, CA","Acerca del empleo
Netflix is one of the world’s leading entertainment services with 278 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.

The Role

About

Netflix is re-imagining entertainment with over 270 million paid memberships in over 190 countries on millions of devices. One of the ways we do this is by using insights from data to optimize for the best customer experience. The Growth Data Engineering team is responsible for data that is critical in driving customer acquisition and optimizing the signup experience for our prospective members. Our work empowers product managers and business leaders to rapidly experiment and innovate on product experiences that drive towards our goal to “Entertain the world”. In this role, you will partner closely with other engineers and data scientists to power experimentation, analytical data products, and machine learning models. This is an opportunity for someone with a strong data engineering background to demonstrate their thought leadership in crafting metrics and elegant insights that have a direct impact on the business. About you:

You are proficient in at least one major language (e.g., Java, Scala, Python) and SQL (any variant). You strive to write elegant and maintainable code, and you're comfortable with picking up new technologies.
You have a product mindset and are curious to understand what the business needs. You have a naturally collaborative style to work with product management, data science, engineering, etc in service of these needs. 
You possess a strong data intuition and apply your analytical and data engineering skills to translate business problems into scalable engineering solutions. 
You are proficient in working with distributed systems and have experience with different distributed processing frameworks that can handle data in batch and real-time e.g. Flink, Spark etc. 
You see patterns and new ways to innovate in existing spaces. As a result, you always look for opportunities to simplify, automate tasks, and build reusable components across multiple use cases and teams.
You build data products that are well-modeled, documented and easy to understand and maintain. 
You own what you build, beyond just your code. You have a passion for quality. 
You are comfortable working in the most agile of environments with vague requirements. You are nimble and can pivot easily when needed. You are unafraid to take smart risks.
You relate to and embody many aspects of Netflix's Culture. You love working independently while also collaborating and giving/receiving candid feedback.

Learn more about the team and technologies you’d get to work with! Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000. Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here. Netflix is a unique culture and environment. Learn more here.

We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.

Job is open for no less than 20 days and will be removed when the position is filled.","Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Automatización, Bases de datos, Nimble, Productos de datos y Tablas dinámicas",Solicitar
https://www.linkedin.com/jobs/view/3914855005/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=FmtdU1bNCn%2FrA3j14%2Fgugg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"200 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,"San Diego, CA","Acerca del empleo
Gunderson Dettmeris the only business law firm of its kind - exclusively serving the global venture capital and emerging technology marketplace. With 400 attorneys in eleven offices - from Silicon Valley to Singapore - we innovate for innovators, accelerate entrepreneurship, and help build companies at every stage of the growth lifecycle. We are committed to being the employer of choice by working together to create an environment, in which each of our people can grow, take initiative, and develop a fun, fulfilling and financially rewarding career.

Benefits

In addition to offering competitive salaries, we also offer an excellent benefit package, which includes full medical, dental and vision coverage; 401(k) Profit Sharing Plan; Flexible Spending Account and Paid Time Off.

Job Description

Join Gunderson Dettmer, the preeminent international law firm with an exclusive focus on the innovation economy, as a full time Data Engineer to work at the intersection of technology, law, data and innovation. We’re at the forefront of legal innovation and are actively developing and refining the law firm tech stack of the future. This unique position offers the opportunity to help architect the modern law firm’s data infrastructure, power key applications to solve problems for legal practitioners, and craft the integration layer between these bespoke tools and our data warehouse. We seek a talented individual who thrives in a collaborative, cross-functional environment and embodies a commitment to precision, repeatability, and quality.

Responsibilities:

Design, build and manage robust, end-to-end ETL processes with performance monitoring
Create data integrations between various cloud-based software solutions and platforms (including our enterprise data warehouse) using iPaaS tools
Manage and optimize enterprise data warehouse on RDBMS with a complex Dim/Fact data model for analytics
Manage and optimize cloud infrastructure to ensure scalability, reliability, and cost-effectiveness
Work with cross-functional teams—including legal engineers, data visualization specialists, business solutions and product specialists and subject-matter experts—to ensure efficient and coherent data processes and delivery of high-quality data products
Improve and maintain quality control, implementing best practices to ensure data products meet our standards of reliability, usability, and performance
Proactively identify opportunities to improve data quality, tooling, and version controls

Requirements:

5+ years of relevant experience designing, building and managing ETL processes with a variety of tools
Significant experience with:

iPaaS tools like Apache AirFlow

API technologies such as REST and GraphQL and transfering files using SFTP

Data transformation tools like dbt

RDBMS platforms like Snowflake

noSQL databases like Mongo

Dim/Fact data models for analytics

Proficiency writing Python scripts and SQL queries (with experience writing Mongo Atlas aggregation queries considered a plus)
Exceptional problem-solving skills, ability to work independently, and a collaborative spirit
Excellent communication skills, underpinning effective collaboration with various teams and subject-matter experts

Nice-to-haves:

Experience with Google Cloud Platform (particularly IAM permissions, as well as log analytics, queries and dashboards)
Experience with other Snowflake platform features like Snowsight and Cortex
Experience in a financial services, professional services or law firm environment
Experience in, or familiarity with, the venture-backed company or venture capital space, especially including familiarity with private company equity data schemas and transaction modeling

Location:

Work out of any of our firm’s U.S. offices or remotely.

Our Offer:

Join us as a Data Engineer and be at the forefront of innovation and change in the legal industry. We offer a dynamic, supportive, and collaborative work environment, where our contributions will shape the future of legal service delivery. Be part of our journey to optimize the way we serve our clients and make a lasting impact on the legal landscape. The expected starting salary for this position is $100,00 - $200,000 annually, dependent upon qualifications, experience and location.

Gunderson Dettmer is an Equal Opportunity Employer and does not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.

By applying to this job you acknowledge that you have read the California Consumer Privacy Act Applicant Notice

View HERE

Powered by JazzHR

EsY8BbXWZf","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Comunicación, Modelo de datos, Resolución de problemas y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3921553625/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=cyN2rT2VOQ%2Bx3XmFVnx%2BUQ%3D%3D&trk=flagship3_search_srp_jobs,AWS Glue Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 meses,Estados Unidos,"Acerca del empleo
Role: AWS Glue Data Engineer 

Location: Remote

AWS Data Engineer (Spark, AWS, Glue) 

Must Skill

 Work with development teams and other project leaders/stakeholders to provide technical solutions that enable business capabilities
 Design and develop data applications using big data technologies (AWS, Spark) to ingest, process, and analyze large disparate datasets
 Build robust data pipelines on the Cloud using AWS Glue, Aurora Postgres, EKS, Redshift, PySpark, Lambda, and Snowflake.
 Build Rest-based Data API using Python and Lambda.
 Build the infrastructure required for optimal extraction, transformation, and loading of data from various data sources using SQL and AWS ‘big data’ technologies.
 Work with data and analytics experts to strive for greater functionality in our data systems.
 Implement architectures to handle large-scale data and its organization
 Execute strategies that inform data design and architecture partnering with enterprise standard
 Work across teams to deliver meaningful reference architectures that outline architecture principles and best practices for technology advancement","AWS Lambda, Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Microsoft Power BI, AWS Glue, Amazon Redshift, Bases de datos y Datasets",Solicitar
https://www.linkedin.com/jobs/view/3980380267/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=6nKrGU9HUJQjCaBEip38Yg%3D%3D&trk=flagship3_search_srp_jobs,Sr Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,Estados Unidos,"Acerca del empleo
At SunnyData, a leading Databricks technology partner, our mission is to empower customers with scalable architectures, robust data engineering pipelines, seamless data consumption layers, and advanced ML and AI applications.

As a Senior Data Engineer, you will play a critical role during this customer journey. You will directly work with internal teams and customers to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence / insights.

The impact you will have

You will be part of a team responsible for supporting new and existing customers in their data engineering needs.
You will guide customers to make the best technical decisions to achieve their goals
You will actively work across multiple customer accounts which you would need to track and report on their progress.
You will build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing / quality solutions.
You will design data solutions.
You will analyze sources to determine value and recommend data to include in analytical processes.
You will incorporate core data management competencies including data governance, data security and data quality.
You will collaborate within and across teams to support delivery and educate end users on data products / analytic environments.
You will perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.
You will test data movement, transformation code, and data components.

What we look for

Bachelor’s Degree in STEM related field or equivalent
4 to 6 years of related experience in data engineering and data product development.
Programming Skills: Proficiency in Java, Python, and/or Scala.
Cloud Platforms: AWS, Azure, and/or GCP.
Current Data Engineering (hands on keyboard) experience with Apache Spark is a must have, along with one or more of the following:

 *Databricks platform - data engineering and/or ML ops experience would be a huge bonus
 *Data Science and Machine Learning technologies (e.g., pandas, scikit-learn, HPO)
 *Data Warehousing (e.g., SQL, OLTP/OLAP/DSS)

Solid understanding of the end to end data analytics workflow
Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on
Proven problem solving skills including debugging skills
Strong verbal and written communication skills
Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities
Excellent time management and prioritization skills Knowledge of public cloud platforms AWS, Azure or GCP would be a plus
Nice to have: Databricks Certification

Benefits & Compensation Overview

Health Insurance : Employees and their eligible family members including spouses, domestic partners, and children are eligible for coverage from the first day of employment.
Paid Time Off : Start your career at SunnyData with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.
An opportunity to grow your technical and people skills, lead teams on complex customer projects that are highly innovative and cutting edge. Great opportunity to grow in your career with the right level of focus, innovation and customer centricity with a high growth consulting company dedicated to Databricks.
The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.","Apache Spark, Ingeniería de datos , Python y SQL, Productos de Databricks",Solicitud sencilla
https://www.linkedin.com/jobs/view/3960292091/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=pe5YofPg8l7xOiBQuXKzgw%3D%3D&trk=flagship3_search_srp_jobs,Cloud Data Engineer - Healthcare,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Portland, ME","Acerca del empleo
Department: Professional Services

Employment Type: Full Time

Location: Portland, ME, USA

Description

Why Work Here?

At Arkatechture, we have a simple shared mission: to build a sustainable organization built upon three pillars: Do something meaningful, With a great team, Earning what you deserve.

We started in 2012 with a passion for data, business, and getting things done. We are a team of data lovers and technical experts who use our skills to help businesses big and small harness, utilize, and optimize their data. As New England's Data Resource, we are a small company constantly evolving to keep up with changing landscapes in the data world.

Benefits

We are proud of the community and culture that we've created at Arkatechture, and we have no intention of slowing down. We offer a competitive benefits package that includes:

A flexible work-from-home policy (work 100% remotely!)
Open-concept offices in Portland, ME with an easy-going dress code, and fresh pots and pops all day (that's coffee and popcorn!)
Training & certificate reimbursement
A competitive benefits package that includes medical, disability, life insurance and optional dental/vision
401K Retirement planning with company matching
Generous paid time off and eleven paid holidays
Employee recognition through milestone awards including annual PTO increases and a 4 day work-week at 3 years of service!

All employees share our core values: put the team first, practice humility, take pride in everything we do, stay curious, care for our community & environment, take work seriously; ourselves not so much.

The Position

As a Cloud Data Engineer specializing in Healthcare, you will be a key member of our engineering team, responsible for architecting, developing, and optimizing publish/subscribe messaging systems with a specific focus on Amazon Web Services (AWS) services. You will collaborate with cross-functional teams to design and implement robust publish/subscribe solutions that leverage a bi-directional data exchange architecture using services such as AWS messaging services, Apache Kafka, and other FedRamp-authorized Cloud Service Offerings along with industry-standard protocols, enabling efficient and real-time data exchange using a hub-and-spoke framework within our software ecosystem.

How To Apply

Please send a cover letter and resume with your application. You must have 3+ years of experience working for a Medicaid agency and you must submit all requested documents to be considered for the position.

Key Responsibilities

Experience with designing and implementing highly secure FedRamp-Authorized Cloud Service Offerings (CSO) such as AWS GovCloud, Amazon MSK, Okta, Snowflake, etc.
Experience designing and building data pipelines using AWS services
Experience with developing bidirectional data exchange systems
SQL/Python development experience especially for serverless computing and event-based triggers
Developing and Testing of code
Working with Senior Data engineers on the team for a full end-to-end delivery of projects/solutions
Communicate with both technical and non-technical collaborators
Follow Engineering best practices
Status reporting to Team lead on a regular cadence
Estimation and working with Project manager on task allocation
Additional responsibilities as assigned

Minimum Qualifications

Skills, Knowledge and Expertise

3+ years of experience working for a Medicaid agency as an employee or as a consultant working with Medicaid systems and data
3+ years of experience in a similar individual contributor role
Bachelor's degree in a related field or comparable work experience
Excellent SQL skills and understands implementation of conformed data models
Experience working on Snowflake along with 2 additional databases such as SQL Server, Oracle, Aurora, PostgreSQL, Redshift, MySQL etc
Experience with developing in Python; JavaScript is a nice to have
Experience working on Data Management projects for Data Lakes/Data Warehousing
Experience working with APIs, specifically REST APIs, SDKs and CLI tools as part of ETL/ELT provisioning
Experience working with multiple file formats such as JSON, XML, CSV, Flat, etc
Experience extracting data from databases using ODBC/JDBC
Strong understanding of Microservices architecture
A strong understanding of Agile software development life cycle and methodology

Preferred Experience

One or more of the following certifications:
AWS Solutions Architect Associate/Professional
AWS Developer Associate/Professional
Snowflake SnowPro Core/Advanced
Domain expertise in Healthcare and one or more of the following verticals: Financial Services, Retail, Telco, Digital Marketing, Supply Chain, or Transportation","Almacenamiento de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Comunicación, Disparadores de base de datos, Independent Contributor, Intercambio de datos, Medicaid, Modelo de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3956531860/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=LlO0qIx8%2BBlXfqYoFG1jJQ%3D%3D&trk=flagship3_search_srp_jobs,Associate Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 mes,"Buffalo Grove, IL","Acerca del empleo
Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.

Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.

Position Summary

Caremark LLC, a CVS Health company, is hiring for the following role in Buffalo Grove, IL: Associate Data Engineer to design, build and manage large scale data structures, pipelines and efficient Extract/Load/Transform (ETL) workflows to support business applications. Duties include: develop large scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs; write ETL (Extract/Transform/Load) processes, design database

systems, and develop tools for real-time and offline analytic processing; collaborate with Data Science team to transform data and integrate algorithms and models into automated processes; leverage knowledge of Hadoop architecture, HDFS commands, and designing and optimizing queries to build data pipelines; utilize programming skills in Python, Java, or similar languages to build robust data pipelines and dynamic systems; build data marts and data models to support Data Science and other internal customers; integrate data from a variety of sources and ensure adherence to data quality and accessibility standards; analyze current information technology environments to identify and assess critical capabilities and recommend solutions; and experiment with available tools and advise on new tools to provide optimal solutions that meet the requirements dictated by the model/use case. Telecommuting available. - Requirements: Master’s degree (or foreign equivalent) in Computer Science, Data Science, Statistics, Mathematics, Analytics, or a related field and one (1) year of experience in the job offered or related occupation. Requires one (1) year of experience in each of the following: GIT and Python; Agile methodologies; Web Service APIs; Redis; MySQL; Analyzing large data sets from multiple data sources; Statistical analysis; SQL programming languages; Designing data models and solutions for analytical and reporting use cases; Data Visualization; and GCP – Big Query. Telecommuting available.

Pay Range: $109,242.00/year to $150,000.00/year.

This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.

For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits

This job does not have an application deadline, as CVS Health accepts applications on an ongoing basis.","Analítica de datos, Ciencia de datos, Data Marts, Google Cloud , Hadoop y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Lenguaje de consulta (query) y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3973996870/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=OkxJ7%2FhnbPdDkKrOdkSY7w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 1 semana,"San Diego, CA","Acerca del empleo
Job Overview

We are looking for a self-motivated Data Engineer to join a growing team of mission oriented analysts and developers. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow from multiple operational domains across multiple network domains. The Data Engineer will support software developers, database architects, and data analysts and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Candidate must be self-directed and comfortable supporting the data needs of multiple projects and diverse data products.

Responsibilities

Create and maintain optimal data pipeline architecture,
Manage data flow pipeline across 3 networks
Prepare data in xml format for consumption by cross-domain solution (CDS) to other networks in accordance with security requirements/policy
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources via REST, SOAP, WSDL APIs
Create flowfiles in nifi that move data into Accumulo via Kafka

Qualifications / Skills

Must have TS clearance and be a US citizen

Applicants should also have a demonstrated understanding and experience using software and tools including big data tools like Nifi, Kafka, Spark and Hadoop; relational NoSQL and SQL databases including Accumulo and Postgres
Graph technology such as Neo4j is a bonus, but not required
Familiarity with Horton Data Flow (HDF) framework
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Candidate should have experience using the following software/tools:Experience with big data tools: Hadoop, Spark, Kafka, etc.
Experience with relational SQL and NoSQL databases, including Postgres and Accumulo
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, Jupyter Notebooks, Matlab

Education / Training / Experience

Bachelor’s Degree (Computer Science or related, relevant field)
3-5 years of experience building and optimizing data pipelines, architectures and data sets
Security + (preferred)
Vendor specific certifications are not required, but beneficial. The most relevant vendor specific certification would be the CCP Data Engineer for Cloudera certification that shows the individual has proven experience in ETL analytics and tools.","Extraer, transformar y cargar (ETL), Ingeniería de datos , Jupyter y Scala, Accumulo, Bases de datos, Ciencias de la computación, Datasets, MATLAB y Ordenador portátil",Solicitud sencilla
https://www.linkedin.com/jobs/view/3972810153/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=0zOo9fNBkU54roPBJjlo3w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"106,6 US$K/año - 154,6 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Newark, CA","Acerca del empleo
Experience Level: Experienced Hire

Categories:

Engineering & Technology

Location(s):

Remote - United States, US
Remote - United States, US
Remote - United States, US
7 World Trade Center, 250 Greenwich Street, New York, New York, 10007, US
7575 Gateway Blvd., Suite 300, Newark, California, 94560, US
Remote - United States, US
Remote - United States, US

At Moody's, we unite the brightest minds to turn today’s risks into tomorrow’s opportunities. We do this by striving to create an inclusive environment where everyone feels welcome to be who they are-with the freedom to exchange ideas, think innovatively, and listen to each other and customers in meaningful ways.

If you are excited about this opportunity but do not meet every single requirement, please apply! You still may be a great fit for this role or other open roles. We are seeking candidates who model our values: invest in every relationship, lead with curiosity, champion diverse perspectives, turn inputs into actions, and uphold trust through integrity.

The Compliance & 3rd-Party Risk unit at Moody’s leverages data-driven insights and cutting-edge technologies to help businesses deliver prudent risk management in the KYC, Supply Chain, and Trade Credit space. We are currently seeking a Data Engineer to join our dynamic and motivated Data & Analytics team. The ideal candidate will have a strong foundation in data engineering concepts such as pipelines, orchestration and streaming; as well as a passion for working with cutting-edge technologies to ensure clean, quality, and usable data is democratized to the business.

In this role, you will be highly involved working on creating foundational data models and data products within our data warehouse which supports various business and product reporting needs. In your day-to-day you will be working in a squad consisting of data engineers, analysts, software engineers, and product managers.

The Work:

Develop and maintain product-specific data pipelines to ensure the timely and accurate flow of data from various sources to our Databricks platform.
With the guidance of technical peers, learn to work with business and other cross-functional team members to identify, integrate, and distribute data data from sources to destinations.
Work on project squads adhering to project management best practices and participating in scrum ceremonies
Gain an understanding of data usage patterns and requirements within the organization to design and implement appropriate data models in the data warehouse, ensuring compliance with industry regulations.
Utilize Databricks and AWS/GCP services to manage, store, and process data, ensuring optimal performance, security, and scalability while adhering to data protection standards.
Leverage orchestration tools, such as Databricks Workflows or Airflow, to orchestrate and monitor data pipelines, ensuring seamless integration, data quality, and compliance with regulatory requirements.
Implement data transformation and validation processes using dbt and SQL to ensure data accuracy, consistency, and regulatory compliance across the organization.
Continuously monitor, troubleshoot, and optimize data pipelines to ensure data reliability, performance, and adherence to best practices and industry regulations
Stay current with industry trends, emerging technologies, and regulatory changes to ensure the company remains at the forefront of data engineering best practices in the compliance and AML space.

Qualifications

0-2+ years of experience in data, or backend engineering roles
Bachelor’s degree in Computer Science, Engineering, or a related field.
Some understanding of data engineering concepts such as orchestration & pipelining, data modeling, streaming and warehousing
Excellent problem-solving skills and the ability to work independently or as part of a team.
Eagerness to learn new technologies and adapt to industry trends and changes

For US-based roles only: the anticipated hiring base salary range for this position is $106,600.00 - $154,600.00, depending on factors such as experience, education, level, skills, and location. This range is based on a full-time position. In addition to base salary, this role is eligible for incentive compensation. Moody’s also offers a competitive benefits package, including not but limited to medical, dental, vision, parental leave, paid time off, a 401(k) plan with employee and company contribution opportunities, life, disability, and accident insurance, a discounted employee stock purchase plan, and tuition reimbursement.

Moody’s is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody’s also provides reasonable accommodation to qualified individuals with disabilities or based on a sincerely held religious belief in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications.

For San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance.

This position may be considered a promotional opportunity, pursuant to the Colorado Equal Pay for Equal Work Act.

Click here to view our full EEO policy statement. Click here for more information on your EEO rights under the law. Click here to view our Pay Transparency Nondiscrimination statement. Click here to view our Notice to New York City Applicants.

Candidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody’s Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.","Airflow, Almacenamiento de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Calidad de datos, Ciencias de la computación, Modelado de datos, Modelo de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3977204228/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=IgWlf9umZPA8TR4dPsid4Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Data Engineer V,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Nueva York, NY","Acerca del empleo
Job Description:Client is looking for a Data Engineer to join the Infrastructure Automation team. Client has over 70 million customers, and developers all over the world rely on our storage, compute, and virtualized services. Our success depends on our world-class network and hardware infrastructure; we’re handling massive scale and rapid integration of emergent technologies. Our goal is to become “The Infrastructure Platform” for the world. The Infrastructure Automation team is responsible for delivering the software that powers our infrastructure.

Responsibilities

 As a Data Engineer you will be working in one of the world's largest and most complex data warehouse environments.
 You will be developing and supporting the Data warehouse that give our customers timely, flexible and structured access to their data.
 You will be responsible for designing and implementing a data storage & reporting platform using AWS Cloud and in-house engineering tools, modeling metadata, building reports and dashboards in Quicksight.
 You will work with business customers in understanding the business requirements and implementing solutions to support analytical and reporting needs.

Required Skills & Experience

 7+ years of related experience in Data Engineering (Data Warehouse, Data Lake, ETL, etc).
 3+ years of experience with AWS Cloud development (Redshift, S3, VPCs/Subnets, Glue, EMR etc)
 Very Strong development experience with SQL, Spark, Python and a reporting tools (such as Quicksight).
 Should have experience developing complex data pipelines and a variety of reports & dashboards.
 A good candidate has strong analytical skills and enjoys working with large complex data sets.
 A good candidate can partner with business owners directly to understand their requirements and provide data which can help them observe patterns and spot anomalies.

Must Have Skills

SQL, Spark, Redshift, ETL, Data Warehouse, Data Lake, Quicksight Reporting

Python, Shell scripting

AWS Cloud development

Good To Have

Excel macros, SPSS","Amazon QuickSight, Apache Spark, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Memoria de datos, Amazon Redshift, Macro, Metadatos, Necesidades empresariales y Panel de control",Solicitar
https://www.linkedin.com/jobs/view/3976366696/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=Wo0E7e5ANZCGud2nVUdi2g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Houston, TX","Acerca del empleo
Position Summary

We are seeking a skilled Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and experience working with Microsoft SQL Server, as well as expertise in Snowflake data warehousing, Airbyte or other open-source ETL tools. The Data Engineer will be responsible for designing, developing, and maintaining our data architecture, ensuring that our data pipelines are efficient, scalable, and reliable. This position reports directly to the Data Solutions Manager and is located in downtown Houston, TX. Hybrid work schedule is an option for remote work on Mondays and Fridays. Level and salary commensurate with experience.

Essential Job Functions

Design, develop, and maintain data pipelines using Azure Data Factory, Microsoft SQL Server Integration Services (SSIS), Airbyte or other ETL tools
Implement data models and database designs, particularly in Snowflake
Optimize and tune SQL queries for performance
Collaborate with data scientists and analysts to understand data requirements
Ensure data quality and integrity
Manage and monitor data pipelines to ensure reliability and availability
Stay up to date with the latest technologies and trends in data engineering


This job description is not intended to be an all-inclusive list of duties and responsibilities of the position. Incumbents will be required to follow any other job-related instructions and duties outside of their normal responsibilities as assigned by their supervisor.

Minimum Qualifications

Bachelor's degree in Computer Science, Engineering, or related field
2+ years of experience in data engineering
Proficiency in Microsoft SQL & Fabric tools, including SSIS, SSRS, and T-SQL, Snowflake SQL, Azure Data Factory
Experience with upstream Oil & Gas data and processes
Experience with data modeling and database design
Familiarity with Airbyte or other open-source ETL tools
Strong analytical and problem-solving skills
Excellent communication and teamwork abilities
Ability to work in a fast-paced and fluid environment; flexible with the demands of a growing company
Ability to meet deadlines
Physical Requirements and Working Conditions: Must possess mobility to work in a standard office setting and to use standard office equipment, including a computer, stamina to maintain attention to detail despite interruptions, strength to lift and carry files weighing up to 10 pounds; vision to read printed materials and a computer screen, and hearing and speech to communicate in person and over the telephone


The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

EEO Statement:

Chord Energy does not discriminate in employment on the basis of race, color, religion, sex (including pregnancy and gender identity), national origin, political affiliation, sexual orientation, marital status, disability, genetic information, age, membership in an employee organization, retaliation, parental status, military service, or other non-merit factor.","Arquitectura de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL Server Integration Services (SSIS), Atención al detalle, Comunicación, Modelado de datos, Resolución de problemas, SQL Server Reporting Services (SSRS) y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3978453890/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=5S17aZgXYddjAV6uXzNJOA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Austin, TX","Acerca del empleo
Title : Data Engineer _ Snowflake, Python and Tableau (Fulltime Role) 

 Location : Austin, TX (Day one on site) work from office 3 days a week 

 Fulltime Role 

Job Description

 Data Engineer with Snowflake, Python and Tableau
 3 to 5 years in relevant technology. Total 8 to 10 years in IT.
 They will be building ETL's , Snowflake structures, Tableau reports and may be automation opportunities using Python.

Aptitudes y experiencia deseables
DATA ENGINEER, PYTHON, SNOWFLAKE, ETL, TABLEAU, AUTOMATION","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3978034693/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=qvIYatee58FIml417FHlrA%3D%3D&trk=flagship3_search_srp_jobs,Data Center Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Chicago, IL","Acerca del empleo
Fastly helps people stay better connected with the things they love. Fastly’s edge cloud platform enables customers to create great digital experiences quickly, securely, and reliably by processing, serving, and securing our customers’ applications as close to their end-users as possible — at the edge of the Internet. The platform is designed to take advantage of the modern internet, to be programmable, and to support agile software development. Fastly’s customers include many of the world’s most prominent companies, including Vimeo, Pinterest, The New York Times, and GitHub.

We're building a more trustworthy Internet. Come join us.

Posting Open Date: 7/16/2024

Anticipated Posting Close Date*: 9/16/2024

Job posting may close early due to the volume of applicants.

Fastly’s Datacenter Infrastructure Team is looking for a Data Center Engineer that is highly proficient in the deployment and maintenance of datacenter equipment, including servers, switches, routers, power distribution units, as well as all related cabling and connectivity. Ideal candidates should have experience with high-density datacenter configurations, project management, prioritizing work and excellent troubleshooting skills. Working on Fastly’s Infrastructure Team requires the ability to work autonomously through routine tasks while also working cross-functionally with Fastly teams for larger initiatives.

What You'll Do

Oversee the installation (rack, stack and cabling), configuration and deployment of data center equipment deployed to Fastly data centers worldwide
Order and handle cross connect circuit installs for in flight builds, and update internal circuit records
Organize and prioritize daily workload using the JIRA issue tracker.
Write and publish SOW / SOP for internal Fastly documentation to be used internally for DCI and externally for other Fastly teams
Enable team productivity by ensuring that hardware is onsite; work closely with Procurement team to resolve logistic issues that could impact timelines
Provide guidance and instructions to remote hands personnel and third-party vendors to carry out tasks such as POP deployments, hardware replacements, cable installations, and troubleshooting under your supervision
Actively participate in Fastly’s on-call pager rotation

What We're Looking For

3+ years of direct involvement in supporting data center operations at an Internet Service Provider (ISP), hosting provider, or other datacenter-oriented position or equivalent.
Knowledge of power and network cabling standards including but not limited to NEMA, IEC, and/or ANSI/TIA - crimping copper patch cords and troubleshooting fiber optic cabling is a required skill set at Fastly
Experience with working with vendors to oversee installations and hardware repairs
Proficient in Linux
Proficiency with hardware from Hyve, Supermicro, Dell, Arista, Ciena, and/or Cisco.
Proven experience as a Data Center Engineer with a focus on rotations and 24/7 operations
Experience working in a Linux environment 
Able to read and write English installation documentation (documents/drawings/spreadsheets)
Knowledge of safe working practices (e.g., familiar with proper tool safety, eye protection, ladder safety, etc.) and able to document practices
Able to lift heavy equipment
Able to work days/nights/weekends/holidays, if needed and/or required
Able to travel up to 25% of the time

We’ll be super impressed if you have experience in any of these: 

One or more of the following certifications preferred: Fluke CCTT Copper, Fluke CCTT Fiber, BICSI Level II Copper, Comp-TIA A+ or equivalent work experience
Understanding of DWDM architecture
Knowledge or fluency in Bash or Python scripting languages 

Work Location & Travel Requirements:

This position is a remote role and open to candidates residing in the following locations: Chicago, IL.

This position may require travel as required by your role or requested by your manager.

Pursuant to the San Francisco Fair Chance Ordinance and the Los Angeles Fair Chance Initiative for Hiring Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

Salary

The estimated salary range for this position is $111,000 to $138,750.

Starting salary may vary based on permissible, non-discriminatory factors such as experience, skills, qualifications, and location.

This role may be eligible to participate in Fastly’s equity and discretionary bonus programs.

Benefits

We care about you. Fastly works hard to create a positive environment for our employees, and we think your life outside of work is important too. We support our teams with great benefits that start on the first day of your employment with Fastly. Curious about our offerings?

We offer a comprehensive benefits package including medical, dental, and vision insurance. Family planning, mental health support along with Employee Assistance Program, Insurance (Life, Disability, and Accident), a Flexible Vacation policy and up to 18 days of accrued paid sick leave are there to help support our employees. We also offer 401(k) (including company match) and an Employee Stock Purchase Program. For 2024, we offer 10 paid local holidays, 11 paid company wellness days.

Why Fastly?

We have a huge impact. Fastly is a small company with a big reach. Not only do our customers have a tremendous user base, but we also support a growing number of open source projects and initiatives. Outside of code, employees are encouraged to share causes close to their heart with others so we can help lend a supportive hand. 
We love distributed teams. Fastly’s home-base is in San Francisco, but we have multiple offices and employees sprinkled around the globe. As a new hire, you will be able to attend our IN-PERSON new hire orientation in our San Francisco office! It is an exciting week-long experience that we offer to new employees to build connections with colleagues across Fastly, participate in hands-on learning opportunities, and immerse yourself in our culture firsthand. 
We value diversity. Growing and maintaining our inclusive and diverse team matters to us. We are committed to being a company where our employees feel comfortable bringing their authentic selves to work and have the ability to be successful -- every day. 
We are passionate. Fastly is chock full of passionate people and we’re not ‘one size fits all’. Fastly employs authors, pilots, skiers, parents (of humans and animals), makeup geeks, coffee connoisseurs, and more. We love employees for who they are and what they are passionate about. 

We’re always looking for humble, sharp, and creative folks to join the Fastly team. If you think you might be a fit please apply! A fully completed application and resume or CV are required when applying.

Fastly is committed to ensuring equal employment opportunity and to providing employees with a safe and welcoming work environment free of discrimination and harassment. Our employment decisions are based on business needs, job requirements and individual qualifications. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, family or parental status, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.

Consistent with the Americans with Disabilities Act (ADA) and federal or state disability laws, Fastly will provide reasonable accommodations for applicants and employees with disabilities. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact your Recruiter, or the Fastly Employee Relations team at candidateaccommodations@fastly.com or 501-287-4901. 

Fastly collects and processes personal data submitted by job applicants in accordance with our Privacy Policy. Please see our privacy notice for job applicants.","Cableado, Crimping, Gestión de centros de datos, Infaestructura del centro de datos, Ingeniería de redes, Operaciones de centro de datos y Resolución de incidencias",Solicitar
https://www.linkedin.com/jobs/view/3965421380/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=SH%2B4c1TZC0RHktjiOOceag%3D%3D&trk=flagship3_search_srp_jobs,Data Analytics Engineer - Remote | WFH,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 3 semanas,"Washington, DC","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
We're a leading technology company (Washington D.C. area) on a mission to deliver cutting-edge solutions for both government and commercial clients.

Our expertise spans data analytics, cloud computing, collaboration tools, and human-centered design.

What You'll Do As a Data Analytics Engineer

Partner with our federal client to analyze data from their knowledge management platform.

Transform complex data into clear, actionable insights using data visualization tools.

Design and optimize data pipelines for efficient information flow.

Develop and maintain user-friendly reports and dashboards using Power BI.

Implement cutting-edge data security practices to ensure data integrity.

Leverage your expertise in Azure services to enhance analytics capabilities, including AI and machine learning.

You're a Great Fit If You

Have 5+ years of experience with Power Apps, Power BI, and Dataverse.

Hold a bachelor's degree in a relevant field like software engineering, computer science, or data analytics.

Are a whiz at Azure data extraction, transformation, and loading (ETL) processes.

Have a passion for data visualization and creating compelling reports using Power BI and Power Platforms.

Thrive in a collaborative and fast-paced environment.

Bonus Points For

Experience integrating with third-party data sources.

Certifications in Power BI, Azure, and Dynamics 365.

What We Offer

Competitive salary and benefits package.

Opportunity to work on challenging and impactful projects.

Collaborative and supportive work environment with a strong focus on work-life balance (flexible workdays, generous paid time off).

Supportive leadership team that's invested in your professional development.

Ready to join a dynamic team that values your contributions?

Employment Type: Full-Time","Analítica, Analítica de datos, Ciencia de datos, Hive, Ingeniería de datos , Inteligencia empresarial, Microsoft Power BI, Microsoft PowerApps y Visualización de datos, Ciencias de la computación",Solicitar
https://www.linkedin.com/jobs/view/3985202829/?eBP=BUDGET_EXHAUSTED_JOB&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=iB5iqZfP6mRUBE%2F7QVm%2BzA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Houston, TX","Acerca del empleo
Your Job

As a Data Engineer at INVISTA, you will be a vital contributor to our Enterprise Data Platform, responsible for designing and implementing data pipelines, optimizing data workflows, and ensuring data reliability and accessibility. Your work will be instrumental in empowering our organization to make data-driven decisions and fueling innovation across the company.

Our Team

Joining our Enterprise Data Platform team means becoming a key player in a dynamic and innovative group of professionals dedicated to unlocking the power of data. Here, you'll find a diverse and collaborative environment where your ideas and expertise will shape the future of our data-driven organization. We pride ourselves on fostering a culture of continuous learning, creativity, and teamwork, and we're looking for individuals who are eager to contribute their skills and passion to our shared mission.

What You Will Do

Collaborate closely with business partners and data scientists to align data engineering efforts with their specific needs and objectives, ensuring that data solutions contribute effectively to the organization's overall goals
Develop, maintain, and optimize orchestration and data pipelines using tools such as Snowflake, DBT, GitHub, AWS
Design and implement data modeling and ETL/ELT processes to ensure data quality, consistency, and availability
Monitor and troubleshoot data pipelines, resolving issues to ensure uninterrupted data flow
Optimize and fine-tune data pipelines for performance and efficiency
Manage and maintain data warehouses and ensure data security and compliance with company policies and relevant regulations
Perform data transformations, aggregations, and data cleansing to support analytics and reporting needs
Implement data integration strategies for streaming and batch data sources
Automate and orchestrate data workflows to improve efficiency and reduce manual intervention
Document data engineering processes, pipelines, and architecture for knowledge sharing and compliance
Stay up-to-date with industry best practices, emerging technologies, and trends in data engineering
Be tenacious, self-directed, curious, problem-solver and learner


Who You Are (Basic Qualifications)

Bachelor's degree in Computer Science, Information Technology, or a related field
Proficiency in designing software and data solutions that scale
Proficiency in SQL, profiling, and data modeling
Experience with Snowflake data warehouse technology
Strong programming skills in Python
Hands-on experience with AWS cloud computing platform
This role is not eligible for visa sponsorship


What Will Put You Ahead

Certification in data engineering or related AWS certifications
Experience with DevOps practices and tools, such as continuous integration (CI) and continuous deployment (CD) pipelines, version control systems (e.g., Git)
Experience in using Agile project management and collaboration tools (e.g., Jira, Confluence) to track progress and manage work in an Agile environment
Proficiency in data orchestration, integration concepts, and workflow management tools (e.g., Step Function)
Knowledge of data governance and compliance best practices


At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.

Hiring Philosophy

All Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.

Who We Are

As a Koch company, INVISTA has a long history of working to make the world around you a better place. From parts for the automotive industry to medical equipment, air bags, food packaging and clothing, our ingredients in the nylon 6,6 and polypropylene value chains help bring many of life’s essential products to market.

At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.

Our Benefits

Our goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.

Additionally, everyone has individual work and personal needs. We seek to enable the best work environment that helps you and the business work together to produce superior results.

Equal Opportunities

Equal Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, some offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please click here for additional information.","Almacenamiento de datos, Analítica de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Limpieza de datos, Modelado de datos, Resolución de cuestiones, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3980207723/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=Uy3Jaf5SETpXotiYxEFWSA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Dallas, TX","Acerca del empleo
Only W2 and Healthcare Exp is MUST 

Job Title: Data Engineer
Location: Dallas, TX(Onsite Position)
Direct Client
Visa Status: Any

Job Summary:
We are seeking a highly skilled Data Engineer with extensive experience in data profiling, data validation, ETL processes using SSIS, SQL, and Business Intelligence (BI) tools. The ideal candidate will have a strong background in the healthcare industry, with a deep understanding of healthcare data standards, compliance, and best practices. This role is critical in ensuring the integrity, availability, and accuracy of our data systems, contributing to data-driven decision-making across the organization.

Key Responsibilities:
Conduct comprehensive data profiling to understand data quality, structure, and content.
Implement data validation processes to ensure accuracy, consistency, and completeness of data.
Identify data anomalies and discrepancies, and work with relevant stakeholders to resolve issues.
Design, develop, and maintain ETL processes using SQL Server Integration Services (SSIS) to extract, transform, and load data from various sources.
Optimize ETL workflows for performance, scalability, and reliability.
Monitor ETL jobs, troubleshoot issues, and implement enhancements as needed.
Develop, optimize, and maintain SQL databases to support data warehousing and reporting needs.
Ensure data integrity, security, and compliance with healthcare data standards (e.g., HIPAA).
Perform regular database performance tuning and maintenance.
Develop and maintain BI solutions to support data analysis and reporting requirements.
Collaborate with business users to understand reporting needs and deliver actionable insights.

Qualifications:
Minimum of 2-4 years of experience as a Data Engineer, Data Analyst, or in a similar role within the healthcare industry.
Proficient in SQL, including complex queries, stored procedures, and performance tuning.
Extensive experience with ETL tools, particularly SQL Server Integration Services (SSIS).
Strong understanding of data profiling and validation techniques.
Experience with BI tools such as Power BI, Tableau, or similar.
Knowledge of healthcare data standards and regulations (e.g., HIPAA, HL7).
Excellent problem-solving skills and attention to detail.
Strong communication and collaboration skills.","Arquitectura de datos, Extraer, transformar y cargar (ETL), Herramientas ETL, Microsoft Power BI, SQL y SQL Server Integration Services (SSIS), Perfiles de datos y Validación de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3963462759/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=UMDy1LZLfJpvEL2quVf01w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,"Atherton, CA","Acerca del empleo
Role: Data Engineer

Location: Menlo Park, CA (100% Work From Office)

Type: Full-time

Experience: 10+ years

Skills

Candidate should have a good knowledge of Influxdb, Hive and has experience working in Lab Environment","Almacenamiento de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3979932560/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=RO%2FYZ6kqg2PW795TYUs4%2BQ%3D%3D&trk=flagship3_search_srp_jobs,LLM Engineer with Prompt Engineering /Data Embedding and guardrails,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Austin, TX","Acerca del empleo
Job Description

5+ years of industry experience with strong Client fundamentals

Technical Skills:

Experience in Machine Learning, particularly on Large Language Models (LLMs) and Generative AI.
Comprehensive knowledge and hands-on experience with fine-tuning approaches and training models.
Strong understanding of prompt engineering, data embedding, and guardrails
Strong analytical and problem-solving skills
Strong programming skills in Python, C, and C++
Demonstrated leadership in both applied research and development.
Excellent written and verbal communication skills, comfortable presenting research to large audiences, and the ability to work hands-on in multi-functional teams.

Certifications Needed:

Advanced degree (Master's or Ph.D.) in Computer Science, Artificial Intelligence, Machine Learning","Programación y Python, C++, Comunicación, Fundamentals, IA generativa, Investigación aplicada, Presentaciones, Programación en C y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3979097809/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=HLj80r%2BsmscXR3HVxN0DfQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Quincy, MA","Acerca del empleo
Senior/Lead developer position who has extensive experience with development and designing of bigdata platform with Hadoop, Scala and Python experience.

 The candidate should have extensive experience in the bigdata developer with data engineering/big data developer.
 The candidate should have good experience in Scala & Python programming language for data bricks development
 Experience on Databricks
 Candidate should have very good experience in the Dataware housing (ETL Concepts, Types of SCD)
 Candidate should have very good experience in the data lake and delta tables concepts.
 Candidate should have experience in databases like SQL server/Oracle

Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.","Analítica, Big data, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python, SQL y Scala, Azure Databricks, Bases de datos y Lagos de datos",Solicitar
https://www.linkedin.com/jobs/view/3983429944/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=9GA5zFGFyzqkhxfRa7lwHQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"50 US$/h - 70 US$/h Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 5 días,"Greenwood Village, CO","Acerca del empleo
Requirements
Top 3 Requirements:
- Advanced SQL skills (they are all code based)
- AWS (EMR services)
- Spark, Python / PySpark
- ETL experience - ingestion part (no data science backgrounds)
Plusses:
- Airflow (tool to visualize your data flow instead of their current Amazon solution)
Day to Day/Project:
Data Engineer responsible for designing and developing new ETL pipelines, maintain existing pipelines, support data loads, perform data analysis, testing and documentation. Interpreting data for the mobile/wifi product group (how to enhance their products, i.e., hotspots). Seeking a data engineer with SQL expertise.
Airflow is a new tool that they are going to utilize. 
JOB SUMMARY
Enable data-driven decision making and in-time health monitoring of our Client's products by building and deploying product and feature specific analytics capabilities. Support product teams with the complex design, development and implementation of automated reporting and dashboards. Produce complex ad hoc reports and analytics. Develop real-time analytics for measuring product health, usage and customer value.

MAJOR DUTIES AND RESPONSIBILITIES
Design, develop, and implement complex reports and analytics
Leverage various data and technology resources to augment analysis
Expertly use analytical tools to provide insight and make recommendations
Use advanced data visualizations techniques to provide concise and compelling summary of analysis findings in reports and presentations
Interact with various stakeholders to understand their business needs, communicate project status and develop relationships to ensure satisfaction
Be a thought partner within the group, supporting the developments of insights, processes, and information that is valued by the business
QA and validate new and existing reports and analytics for proper and efficient execution and adherence to business and technical requirements
Write SQL code that meets the production requirements and design specifications
Provide ad-hoc analysis to address specific business questions from stakeholder groups
Participate in visualization design and deployment review sessions to ensure all technical and functional requirements have been met
Maintain accurate documentation of reporting processes and monitor and resolve any failures
Mentor and lead junior developers in both design and technology
 
Beacon Hill is an Equal Opportunity Employer that values the strength diversity brings to the workplace. Individuals with Disabilities and Protected Veterans are encouraged to apply.

If you would like to complete our voluntary self-identification form, please click here or copy and paste the following link into an open window in your browser: https://jobs.beaconhillstaffing.com/eeoc/

Completion of this form is voluntary and will not affect your opportunity for employment, or the terms or conditions of your employment. This form will be used for reporting purposes only and will be kept separate from all other records.



Company Profile:

Beacon Hill Technologies, a premier National Information Technology Staffing Group, provides world class technology talent across all industries utilizing a complete suite of staffing services. Beacon Hill Technologies' dedicated team of recruiting and staffing experts consistently delivers quality IT professionals to solve our customers' technical and business needs. 

Beacon Hill Technologies covers a broad spectrum of IT positions, including Project Management and Business Analysis, Programming/Development, Database, Infrastructure, Quality Assurance, Production/Support and ERP roles.


Learn more about Beacon Hill Staffing Group and our specialty divisions, Beacon Hill Associates, Beacon Hill Financial, Beacon Hill HR, Beacon Hill Legal, Beacon Hill Life Sciences and Beacon Hill Technologies by visiting www.beaconhillstaffing.com. 

We look forward to working with you.

Beacon Hill. Employing the Future™","Airflow, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , PySpark, SQL y Visualización, Comunicación y Especificaciones de diseño",Solicitud sencilla
https://www.linkedin.com/jobs/view/3963915462/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=4D6Ojtl726XtT7JkH6Plzg%3D%3D&trk=flagship3_search_srp_jobs,SCALA Data Engineer - (U.S. remote),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,"Carolina del Norte, Estados Unidos","Acerca del empleo
#Hiringnow We are actively hiring (Data Engineers)
We are seeking a Senior Data Engineer to be a strong technical resources on a dynamic and growing team of engineers. Our ideal candidate is passionate about creating well-architecture solutions containing thoroughly tested code. The ability to communicate effectively and create relationships by empathizing with client goals is a highly valued skill within our company culture.
Core Responsibilities:
Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions
Skills & Experience:
Scala 2.12 + development experience, Zio, Quill, Circe, Doobie, Scalaz
Passionate about developing clean and maintainable code with little or no side-effects
Experience building Restful APIs in Scala using Spark 2.4
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3.
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
Bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required
$140,000 - $170,000 a year
Salary is commensurate with experience.
We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.","Apache Spark, Ingeniería de datos y Scala, Anuncios, Bases de datos, Comunicación, Comunicación escrita, Diseño técnico, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3833868086/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=A0G%2FAvW9UdiJhaua0VnM5A%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Programmer/Coder/Developer/Data Scientist/Analyst/Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Chandler, AZ","Acerca del empleo
At SynergisticIT, we aim to bring aboard IT professionals to help them build a rewarding career in cutting-edge technologies. Being in the industry for more than 10 years, we provide a splendid range of lucrative opportunities to sustain a position in our top tech clients like Google, Apple, Cognizant, Client, PayPal, to name a few.

Our seasoned team firmly believes that the new tech talent can scale any business if given the right opportunity. We value your integrity, hard work, and commitment to make a difference in the technical sphere. For this reason, we focus on providing end-to-end career assistance and enhancing your already existing IT skills and knowledge.

Currently, we are looking for qualified entry-level Data Scientists who can apply Data Science principles to design, test, implement, and develop data-based solutions, including reporting, auditing, and preparing large databases for statistical analysis.

Qualifications

 Minimum Background and Qualifications Requirement 

 Bachelor's degree or Master's degree in Computer Engineering, Computer Science, Mathematics, Electrical Engineering, Information Systems, or IT 
 Must have Mathematics or Statistics background 

Required

 Technical and Soft Skills Required 

 Experience in Python programming and understanding of the software development life cycle. 
 Knowledge of Linear Algebra, Statistics, and Mathematics concepts. 
 Excellent written and verbal communication skills. 
 Highly motivated, self-learner, team player, and technically inquisitive. 
 Strong work ethics and creative problem-solving abilities. 

 Preferred skills 

 Deep Learning 
 Data visualization 
 NLP 
 Scala 
 Django 

 Roles and Responsibilities 

 Collaborate with dynamic teams of engineers, developers, and scientists who research and integrate algorithms to develop an application, software, and computer system solutions to address complex data problems. 
 Assess project requirements and develop data analysis algorithms. 
 Engage developers to share their opinions, knowledge, and recommendations to meet the deliverables. 
 Contribute to technical solutions and implement software analyses to unlock the secrets held by big data sets. 
 Integrate components like web-based UI, commercial indexing products, and access control mechanisms to create operational information and knowledge discovery systems. 

Benefits

 Competitive salary 
 Flexible work schedule & part-time off 
 E-verified 
 No relocation 
 H1B filing 
 On job technical support 
 Skill Enhancement 
 Opportunity to work with Fortune 500 Companies 

 Who Should Apply? 

Recent IT graduates looking to build a solid career in the tech industry. If you're lured by the endless possibilities presented by AI, Machine Learning, IoT, and Data Science, this job opportunity can be the right career path for you.

 Candidate's Outcome  : Best Programmers in USA | Best Coding Bootcamp - SynergisticIT

 No third-party candidates or c2c candidates 

 If you are interested, please apply to the posting. 

 No phone calls please, Shortlisted candidates would be reached out.","Analítica de datos, Análisis de datos, Programación y Python, Ciencias de la computación, Competencias transversales, Comunicación, Desarrollo de software, Matemáticas y Resolución creativa de problemas",Solicitar
https://www.linkedin.com/jobs/view/3970454667/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=V2dHTbnOYyaoYdpPsqSNEw%3D%3D&trackingId=O2d1PYsFy8c%2BaZS7LYBmrA%3D%3D&trk=flagship3_search_srp_jobs,Data Analytics & Cloud Solutions Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,"Bellevue, WA","Acerca del empleo
Technical/Functional Skills

Digital : Cloud DevOps; bicep; Digital : PySpark; Digital : Kafka; Digital : Azure Databricks; ETL; Digital : BI Data Visualization - Tableau; SQL; kusto; Digital : Puppet; Digital : Cloud Computing (General)

Roles & Responsibilities

Gather data from primary and secondary sources, ensuring the upkeep of databases and data systems.

Detect, examine, and decode trends or patterns within intricate datasets.

Cleanse data and scrutinize computer-generated reports and outputs to identify and rectify coding errors.

Coordinate with management to align business and informational priorities.

Identify opportunities for process enhancements.

Employ statistical techniques to scrutinize data and produce actionable business insights.

Collaborate with the management team to determine and rank the needs of different business units.

Develop data dashboards, charts, and visual aids to support decision-making across departments.

Convey insights through both reports and visual presentations.

Partner with engineering and product development teams to understand business requirements.

Engage with managers from various departments to specify data requirements for analysis projects tailored to their unique business processes.


Aptitudes y experiencia deseables
Puppet , SQL , Cloud Computing","Analítica, Analítica de datos, Apache Kafka, Capacidad de análisis, Ciencia de datos y Extraer, transformar y cargar (ETL), Azure Kusto, Datasets y Presentaciones",Solicitar
https://www.linkedin.com/jobs/view/3981858469/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=sCjr30KYmzj4ucY17H4F5Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,Estados Unidos,"Acerca del empleo
Overview

Tekwissen group, is a workforce management provider throughout the USA and many other countries in the world. This client is a German multinational Pharmaceutical and biotechnology company and one of the largest pharmaceutical companies in the world, headquartered in Leverkusen, and areas of business include pharmaceuticals; consumer healthcare products, agricultural chemicals, seeds and biotechnology products.

Job Title: Data Engineer

Location: Creve Coeur, MO

Duration: 12 Months 

Job Type: Contract 

Work Type: Remote

Job Description

What you will do is why you should join us:

Be a critical senior member of a data engineering team focused on creating distributed analysis capabilities around a large variety of datasets 
Take pride in software craftsmanship, apply a deep knowledge of algorithms and data structures to continuously improve and innovate 
Work with other top-level talent solving a wide range of complex and unique challenges that have real world impact 
Explore relevant technology stacks to find the best fit for each dataset 
Pursue opportunities to present our work at relevant technical conferences 
Google Cloud Next 2019 
GraphConnect 2015 
Google Cloud Blog 
Project your talent into relevant projects. Strength of ideas trumps position on an org chart 

If You Share Our Values, You Should Have

At least 7 years experience in software engineering 
At least 2 years experience with Go 
Proven experience (2 years) building and maintaining data-intensive APIs using a RESTful approach 
Experience with stream processing using Apache Kafka 
A level of comfort with Unit Testing and Test Driven Development methodologies 
Familiarity with creating and maintaining containerized application deployments with a platform like Docker 
A proven ability to build and maintain cloud based infrastructure on a major cloud provider like AWS, Azure or Google Cloud Platform 
Experience data modeling for large scale databases, either relational or NoSQL 

Bonus Points For

Experience with protocol buffers and gRPC 
Experience with: Google Cloud Platform, Apache Beam and or Google Cloud Dataflow, Google Kubernetes Engine or Kubernetes 
Experience working with scientific datasets, or a background in the application of quantitative science to business problems 
Bioinformatics experience, especially large scale storage and data mining of variant data, variant annotation, and genotype to phenotype correlation 

TekWisen® Group is an equal opportunity employer supporting workforce diversity.

TekWissen is an emerging global human capital, recruitment and IT services organization. Operating since 2009, we draw upon more than a decade of staffing experience to deliver critical talent acquisition solutions and IT engagements for our clients. We’re founded on a culture that is passionate about delivering tailored solutions, that create lasting partnerships.

Our global footprint covers six countries: United States, Canada, Australia, India, United Kingdom and the Philippines. This allows us to work in close partnership with organizations and manage everything from global talent needs with demanding resourcing strategies, to single sites with lower recruitment volumes.

TekWissen® is an equal opportunity employer supporting workplace diversity.","Almacenamiento de datos, Apache Kafka, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Datasets, Desarrollo de software, Modelado de datos y gRPC",Solicitud sencilla
https://www.linkedin.com/jobs/view/3980643722/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=2AlsqqzD2RlZs7qPv3gAYw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 6 días,"Dallas, TX","Acerca del empleo
About Us:
Arch Energy Partners is an energy investment group based in Dallas, TX, focused on the acquisition of non-operated and mineral assets. We are looking to add a Data Engineer with a strong software engineering and DevOps background to our team.

 Responsibilities:
Design, build, and maintain robust data warehousing solutions to support the storage of all enterprise data sources. Experience with BigQuery is a plus.
Develop and manage ELT/ETL processes for new and existing data sources.
Create and maintain dashboarding solutions for the enterprise, including type curve fit, EUR calculation, and other parameters as requested by engineering, as well as pay status analysis for the Accounting team.
Lead automation initiatives and streamline workflow execution.
Manage existing cloud infrastructure and add new solutions using platforms such as Azure, GCP, and AWS. Experience with GCP is a plus.
Develop and deploy containerized applications using Google Cloud Functions, Google Cloud Run, and Docker.
Model effective data designs and SQL queries. Experience with BigQuery/Snowflake is a plus.
Craft resilient data pipelines in Spark-based systems. Experience in Databricks is a plus.
Implement and manage infrastructure as code.
Maintain and optimize Linux & Windows virtual machines.
Build rich data sets that drive innovation in data-driven insights at scale within the company.
Demonstrate strong experience with IT systems as a whole.
Qualifications:
Bachelor's degree in Computer Science, Software Engineering, Information Technology, or a related discipline.
5+ years of experience in software engineering, data engineering, DevOps, or IT, preferably in the oil & gas industry.
Proficiency in Python, SQL, and other programming languages.
Experience with Docker and infrastructure as code.
Experience with big data tools such as Spark.
Experience with data visualization tools such as Spotfire, PowerBI, or similar software.
Strong experience with web scraping, API-based data ingestion, and seamless data system-to-data system replication.
Proficient in working with shapefiles, using tools like geopandas and Shapely is a plus.
Experience with setting up and maintaining SQL, NoSQL, and data warehouses.
Strong written and oral communication skills.
Ability to collaborate effectively with cross-functional teams such as Operations, Land, Accounting, and Marketing.
Familiarity with Spark and data processing frameworks.
Preferred Skills:
Experience with infrastructure as code tools (e.g., Terraform, Ansible).
Familiarity with container orchestration tools (e.g., Kubernetes).
Strong understanding of cloud-native architecture and best practices.
Strong background in designing and maintaining data products and warehouses.
Proven track record of developing resilient and scalable data pipelines.","Almacenamiento de datos, Analítica de datos, Apache Spark, DevOps, Extraer, transformar y cargar (ETL), Google BigQuery, Google Cloud , Ingeniería de datos y SQL, Comunicación",Solicitud sencilla
https://www.linkedin.com/jobs/view/3919916954/?eBP=BUDGET_EXHAUSTED_JOB&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=9pyJ5k%2FpQKtl%2FTHitFcb7Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 3 días,Washington DC-Baltimore y alrededores,"Acerca del empleo
Please Note: We are hiring for multiple positions. 

Since Resonate’s founding in 2008, the company has been driven by a simple but powerful idea: understanding people. Resonate empowers leading brands and agencies with deep consumer intelligence that ignites unbreakable relationships through better connections, more meaningful engagement and compelling customer experiences.

We are “people-centric.” It is at the heart of what we do and drives how we operate. We reveal the Human Element—a holistic understanding of a person that starts with what makes us the most human—our values and motivations. In other words, we help you understand the “what” that drives the “why” people decide to choose, buy, endorse or abandon a brand or cause. By combining this human, person-based ""why"" with relevant data about your industry, brand or product, we help you create powerful marketing engagement that drives results.

As an engineering team member for the big data team, you will be responsible for the technical direction and development of critical data ingestion pipelines that power the Resonate business. You will be able to provide support and technical direction to a team of highly skilled data engineers and facilitate interaction with the technical product management team members. You will be a member of a high-quality, dedicated and friendly team passionate about delivering quality products, solving hard problems, and transforming the data and insights industry.

Responsibilities
Collaborate in the design, planning and development of products and initiatives to deliver on product requests for the data engineering squad
Oversee production, quality assurance, quality control, testing and maintenance as required.
Bachelor’s degree in Computer Science, Computer Engineering, or equivalent
5+ years experience professional experience in software engineering, data engineering, or a related field
3+ years of experience in Spark/Scala DataFrame/Dataset APIs
Prior experience with debugging and problem solving in Big data ecosystem
Proven experience in tuning multi terra byte / Peta byte scale Spark Applications
Prior relational database experience is required
Deep knowledge of software architecture and engineering standard methodologies, especially modern cloud computing stacks for processing Big Data.
Experience with AWS as a cloud provider, Spark, Kafka, Hadoop, Elastic Stack, Java, Docker for containers, etc.
Strong understanding of the different parts of the software development lifecycle, from exploration and design to delivery to production
Good understanding of principles of solution architecture, technical design, data structures, and data modeling
Prior experience with Non-functional requirement is required
Excellent communication skills
Positive attitude and excellent teamwork

Preferred Qualifications
Demonstrated success in implementing scaled high cardinality data system
Experience in using probabilistic data structures
Love for open-source technologies

Benefits
Besides the opportunity to work with smart, fun, hard-working Resonate employees, you will have uncapped growth potential, a work/life balance, and a competitive suite of benefits.

Location
At Resonate, we're proud to offer a flexible work environment that combines the best of both worlds. Our team is made up of talented individuals who collaborate seamlessly across physical locations, thanks to our innovative hybrid and remote work policies. Whether you're working from home or from one of our state-of-the-art offices, you'll have access to the tools and resources you need to succeed.

Resonate is headquartered in Reston, VA with offices in New York City, and Washington, D.C. Be a part of the team that changes the industry!

Our EEO Statement:
Resonate is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outline by federal, state, or local laws.
 Find out more about our story at www.resonate.com.","Amazon Web Services (AWS), Apache Spark y Scala",Solicitar
https://www.linkedin.com/jobs/view/3888451479/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=JPWweZSh7XzOk1RF9t3snw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,"San Diego, CA","Acerca del empleo
Job Title : Data Engineer

Location : San Diego, CA (Remote Is Fine)

Duration : 6+ Months (Possibility Of Extension)

Implementation Partner : TCS

End Client : To Be Disclosed

Jd

Working with our stakeholders to develop end to end Cloud based solutions with a heavy focus on applications and data.

Collaborate with BI/BA analyst, Data scientists, Data Engineers, Product Managers and other stakeholders across the organization.

Ensure the delivery of reliable software and data pipelines using data engineering best practices, including secure automation, version control, continuous integration/delivery, proper testing.

Ownership of the product and will significantly influence on our strategy by helping define the next wave of data insights and system architecture.

A commitment to teamwork and excellent business and interpersonal skills are essential.

You will be an essential part of our growing analytics and data insights team, and be responsible for our technological and architectural vision.

The Ideal Candidate

You have a minimum of 3 years' experience and hands on practical experience in data integration, engineering and technological analytics.

You have a degree in Science, Technology, Engineering, or Mathematics Related Discipline

Excellent skills in; SQL, Python, distributed source control such as GIT in an Agile-Scrum environment.

Experience with ETL pipelines and Airflow

Has a strong understanding of dimensional modelling and data warehousing methodologies.

Can identify ways to improve data quality And reliability.

Can use data to discover different tasks for automation.

Is aligned with the latest data trends and ways to simplify data insights.

Is passionate about data and the insights that large amounts of data sets can provide

Experience within the retail industry is a plus.","Airflow, Almacenamiento de datos, Analítica, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Integración de datos, Bases de datos, Calidad de datos y Habilidades sociales",Solicitar
https://www.linkedin.com/jobs/view/3864813365/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=WYFFx9kg3ZN4d8tPPYC%2FnA%3D%3D&trk=flagship3_search_srp_jobs,Data Visualization Engineer (PowerBI),"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 meses,"Omaha, TX","Acerca del empleo
Summary 

Join Aperia Solutions, a leader in SaaS solutions for the Payments and Compliance industries. Aperia is a Texas-based fintech and managed consultancy firm that creates custom SaaS applications and other software-based solutions for the payments, banking, and processing industry. Founded in 1999, Aperia offers business intelligence, risk management, compliance, and customer intelligence platforms. With offices in Dallas, Washington DC, and Vietnam, Aperia is a fast-paced, global organization that strives to improve efficiency in compliance, risk, and customer service operations. Aperia's clients include banks, processors, payment facilitators, merchant service providers, independent sales organizations, and government entities. A career at Aperia promises a great challenge, culture, and opportunities to forge your own path.

Job Description 

At Aperia, we rely on a dynamic team of developers. We're seeking a Data Visualization Engineer who is ready to work with new technologies and architectures in a forward-thinking organization that's always pushing boundaries.

Key Responsibilities:

Design and develop Power BI reports and dashboards to meet the business stakeholders' needs
Gather and understand business requirements for data visualization and analysis
Recommend optimal visualizations for insightful, actionable business use
Collaborate with data engineers and analysts to acquire and transform data for reporting purposes
Ensure compliance with data security best practices
Troubleshoot and resolve issues in Power BI reports

Required:

5+ years creating data visualizations
3+ years with Power BI report/dashboard development
Expertise in using different Power BI functionalities
Experience developing DAX calculations and M (Power) Queries
Experience with developing interactive dashboards –e.g., parameters, actions, navigation –for ease-of-use.
Proficiency in creating dataflows and data-marts. Experience in composite models
Knowledge of Power BI contexts and its types
Exposure to applying security to Reports, Apps and workspaces
3+ years SQL experience (Oracle preferred)
Experience in connecting to and leveraging multitude data sources including Hive, Impala, and Oracle
Experience in creating pixel perfect reports using Power BI report builder
Excellent analytical thinking for translating data into informative visuals and reports

Preferred:

Familiarity with Cloudera Hadoop
Knowledge in emerging cloud technologies related to Big Data
Previous experience in Finance/Banking industry

Location:

The Six8 Building: 24-26 Phan Dinh Giot, Tan Binh District, HCMC
VDB Tower: 74 Quang Trung, Hai Chau, Danang
Omaha
Coral Springs
Berkeley Heights



Schedule: Monday to Friday

Benefits 

Health insurance 
Dental insurance
Paid time off
Parental leave
Childcare assistance

This job description is not intended to be all-inclusive. An employee may also perform other reasonable related business duties as assigned by their immediate supervisor or management. Principals only. 

Recruiters please don't contact this job poster. DO NOT contact us with unsolicited services or offers.","Analítica, Analítica de datos, Ciencia de datos, Hive, Minería de datos y Visualización de datos, Apache Impala, Dashboard Building, Expresiones de análisis de datos (DAX) y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3943915471/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=E%2BfJI7n07WdTYGzDx9zRRA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Palo Alto, CA","Acerca del empleo
About AppLovin

AppLovin makes technologies that help businesses of every size connect to their ideal customers. The company provides end-to-end software and AI solutions for businesses to reach, monetize and grow their global audiences. For more information about AppLovin, visit: www.applovin.com.

To deliver on this mission, our global team is composed of team members with life experiences, backgrounds, and perspectives that mirror our developers and customers around the world. At AppLovin, we are intentional about the team and culture we are building, seeking candidates who are outstanding in their own right and also demonstrate their support of others.

Fortune recognizes AppLovin as one of the Best Workplaces in the Bay Area, and the company has been a Certified Great Place to Work for the last four years (2021-2024). Check out the rest of our awards HERE.

A Day in the Life

As a member of the Platform team, you will take ownership of projects and work with large-scale data processing systems. We are seeking a motivated engineer to join the team responsible for AppLovin's core products which process over 6PB of data and reach 1B users daily. We are responsible for scaling a platform that produces hundreds of billions of unique events consumed trillions of times throughout our geographically distributed data centers every day. The technical stack includes Java, Scala, Spark, Airflow, GCP and working with a variety of databases.

The Impact You’ll Make

Design, develop, and maintain large-scale distributed systems
Collaborate with various engineering teams to meet a wide range of technological challenges
Influence and inspire team members

Who You Are

Minimum 2 years of meaningful professional experience
Have a Bachelor’s and/or Master’s Degree in Computer Science or a related field
Have used Java or Scala in a professional environment for at least 1 year
Strong algorithms experience
Have some experience with big data systems, like Apache Spark, big data processing, big data processing, big data pipelines, HDFS, etc.
Have a desire to solve large, complex problems. You look beyond the surface to understand root causes so that you can build long-term solutions for the whole ecosystem

Additional Strengths

Knowledge of Airflow
Practical experience working with big data systems (Apache Spark, SparkSQL, HDFS)
Practical experience with broker systems (Apache Kafka, RabbitMQ, etc)
Practical experience working in the cloud (GCP, AWS, etc) or with kubernetes

We use Covey as part of our hiring and / or promotional process for jobs in NYC and certain features may qualify it as an AEDT. As part of the evaluation process we provide Covey with job requirements and candidate submitted applications. We began using Covey Scout for Inbound on March 12, 2024.

Please see the independent bias audit report covering our use of Covey here.

AppLovin is proud to be an equal opportunity employer that is committed to inclusion and diversity. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status, or other legally protected characteristics. Learn more about EEO rights as an applicant here.

If you need assistance and/or a reasonable accommodation due to a disability during the application or recruiting process, please send us a request at accommodations@applovin.com.

AppLovin will consider for employment all qualified applicants with criminal histories in a manner consistent with applicable law. If you’re applying for a position in California, learn more here.

Please read our Global Applicant Privacy Notice to learn more about how AppLovin processes your personal information.","Airflow, Apache Kafka, Apache Spark, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Ciencias de la computación y Java",Solicitar
https://www.linkedin.com/jobs/view/3964926678/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=ScOU7uDvnmbokmTNappO7Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"120 US$K/año - 140 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,Atlanta y alrededores,"Acerca del empleo
Title: Data Engineer
Compensation: $120k - $140k
Benefits: Medical, Dental, Vision, 401k, and more
Location: Atlanta Metro Area (hybrid)

Our growing client in the Atlanta Metro area is looking for a strong Data Engineer to expand their team. The role requires 1-2 days in office.

The ideal candidate will have:
Experience at smaller to mid-sized companies ($20M - $125M revenue).
Strong knowledge of SQL for managing data.
Experience with Power BI for creating visualizations and using DAX, M, and Power Query.
Experience in BI/analytics for retail or DTC metrics like inventory, sales reporting, and lifetime value.
Experience with data warehouses like BigQuery or Snowflake.

KEYWORDS: Data Engineer, SQL, Power BI, Data Management, Retail Analytics, Data Warehouse","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery, Ingeniería de datos y SQL, Expresiones de análisis de datos (DAX), Lenguaje de consulta (query) y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984295849/?eBP=BUDGET_EXHAUSTED_JOB&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=TjkJL1OIpKTzJBJCV6zB6A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Virginia Beach, VA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
POSITION

Data Engineer

LOCATION

100% Remote

DURATION

6+ months

INTERVIEW TYPE

Video

VISA RESTRICTIONS

None

Required Skills

Must have ALL (if your consultant does not have all 5 listed, they will not be considered)

Scala

Spark

Hadoop

GCP

Python

Cloud BC Labs Inc is a digital transformation organization aimed at creating seamless solutions for clients to effectively manage their business operations. The company specializes in Business and Management Consulting, AI/ML, Data Analytics & Visualization, Cloud Data Warehouse Migration, Snowflake Implementation, Informatica Implementation & Upgrade, Staffing Services and Data Management Solutions","Apache Spark, Ciencia de datos, Extraer, transformar y cargar (ETL), Hadoop y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3952968773/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=suUcGGjgzi4LKH%2BFgOhm7w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"120 US$K/año - 125 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Cape Coral y alrededores,"Acerca del empleo
About Client:

The client is a global technology, consulting, and digital solutions company with problem-solving abilities and an emphasis on developing ingenious solutions that allow its clients to remain competitive, profitable, and secure in an evolving business environment.

Client anticipates and leads change to remain in the leader’s quadrant for profitable growth, driven by partnerships with globally leading hyperscales like AWS, Google Cloud, and Microsoft. It has built strong capabilities in new as well as existing technologies such as cloud, data, and digital, pioneering new frontiers.

Salary Range: $120k-$125k per annum + Benefits

Job Description:

Skills:

Oracle & Mongo DB

About ApTask:

ApTask is a leading global provider of workforce solutions and talent acquisition services, dedicated to shaping the future of work. As an African American-owned and Veteran-certified company, ApTask offers a comprehensive suite of services, including staffing and recruitment solutions, managed services, IT consulting, and project management. With a focus on excellence, collaboration, and innovation, ApTask provides unparalleled opportunities for professional growth and development. As a member of the ApTask team, you will have the chance to connect businesses with top-tier professionals, optimize workforce performance, and drive success across diverse industries. Join us at ApTask and be part of our mission to empower organizations to thrive while fostering a diverse and inclusive work environment.

Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.

Candidate Data Collection Disclaimer:

At ApTask, we prioritize safeguarding your privacy. As part of our recruitment process, certain Personally Identifiable Information (PII) may be requested by our clients for verification and application purposes. Rest assured, we strictly adhere to confidentiality standards and comply with all relevant data protection laws. Please note that we only collect the necessary information as specified by each client and do not request sensitive details during the initial stages of recruitment.

If you have any concerns or queries about your personal information, please feel free to contact our compliance team at businessexcellence@aptask.com .
Aptitudes y experiencia deseables
DATA ENGINEER","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos",Solicitar
https://www.linkedin.com/jobs/view/3972306194/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=DYac8nfVdzhpXJsPEQ0v3w%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Analytics Engineer,"85,6 US$K/año - 152,6 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Illinois, Estados Unidos","Acerca del empleo
At Allstate, great things happen when our people work together to protect families and their belongings from life’s uncertainties. And for more than 90 years our innovative drive has kept us a step ahead of our customers’ evolving needs. From advocating for seat belts, air bags and graduated driving laws, to being an industry leader in pricing sophistication, telematics, and, more recently, device and identity protection.

Job Description

This role is responsible for the development and management of data infrastructure projects & proof of concept business solutions for users in analytics and Data Science. This person works directly with data scientists and analytic engineers on needs and tactical solutions. This role will execute new data engineering work tracks for Data Science and analytics from inception and prototyping to fully developed solutions and transition to Allstate technology. This role will also begin to manage projects of medium complexity.

Key Responsibilities:

 Work closely with Data Science and business partners requiring subject matter expertise in the business as well as advanced analytics.
 Participate in requirement development, breaking down business problems into solvable components and assists in documenting requirements
 Participates in developing test plans and scripts, preparation of test data, reconciliation of data sources, and execution of functional tests.
 Help develop and deliver the data infrastructure required to support needs of predictive modeling and analytics with minimal supervision
 Execute rapid development of new data and analytic work tracks with fast iteration over quick sprints
 Leverage agile data analysis with technology fluency in parallel processing/programming, software/programming languages and technologies (i.e. Oracle, MongoDB, SQL, Python, Spark, Kafka, Scala and Hadoop) paired with a high degree of analytic agility to be able to meet fluid and dynamic business needs in this space.
 Provide input on the development of enterprise data assets, information platforms or data spaces designed for exploring and understanding the data. Participate in supporting the data engineering environment, and execute monitoring, quality and validation processes to ensure data accuracy.
 Execute data exploration, data engineering, testing and implementation.
 Participate in and help execute the development of new concepts, proof of concept designs, and prototypes for business or research data solutions so that business users or predictive modelers may visually understand and explore a new feature or functionality before implementation to expose design assumptions and drive ideation.
 Mentor other team members in a business technical environment and promote an environment that supports innovation and process improvement.

Functional Skills:

 Strong database development experience and knowledge (i.e. SQL, Oracle, DB2) with ability to learn new database technologies
 Intermediate programming skills (i.e. Python, R, Java) with ability to learn new technologies
 Computer Proficiency in Oracle, UNIX / Linux
 Ability to extract data from various data sources.
 Advanced analytic, data sourcing and data management skills
 Experience in designing and managing the nuisances of an array of data sources
 Strong attention to detail
 Ability to learn and teach new technologies
 Intermediate written and verbal communication skills including the ability to effectively collaborate with multi-disciplinary groups and all organizational level

Supervisory Responsibilities:

This job does not have supervisory duties.

Education and Experience:

 4 year Bachelors Degree (Preferred)
 3 or more years of experience (Preferred)
 In lieu of the above education requirements, an equivalent combination of education and experience may be considered.

Certificates, Licenses, Registrations:

 No Certification, License or Registration is required for the job.

Notes:

The preceding description is not designed to be a complete list of all duties and responsibilities. May be required to perform other related duties as assigned. Regular, predictable attendance is an essential function of this job.

Skills

Apache Kafka, Communication, Data Analytics, Data Engineering, Data Science, Digital Literacy, Inclusive Leadership, Learning Agility, Python (Programming Language), Results-Oriented

Compensation

Compensation offered for this role is $85,600.00 - 152,650.00 annually and is based on experience and qualifications.

The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.

Joining our team isn’t just a job — it’s an opportunity. One that takes your skills and pushes them to the next level. One that encourages you to challenge the status quo. And one where you can impact the future for the greater good.

You’ll do all this in a flexible environment that embraces connection and belonging. And with the recognition of several inclusivity and diversity awards, we’ve proven that Allstate empowers everyone to lead, drive change and give back where they work and live.

Good Hands. Greater Together.® 

Allstate generally does not sponsor individuals for employment-based visas for this position.

Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.

For jobs in San Francisco, please click “here” for information regarding the San Francisco Fair Chance Ordinance.

For jobs in Los Angeles, please click “here” for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.

To view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance Programs

To view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.

It is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment.","Analítica, Analítica de datos, Apache Kafka, Ciencia de datos, Ingeniería de datos , Modelos predictivos y Scala, Bases de datos, Comunicación y Infraestructura de datos",Solicitar
https://www.linkedin.com/jobs/view/3984142979/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=3myyaqSM0My3ztUSdpoPbw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Berkeley Heights, NJ","Acerca del empleo
As a Data Engineer, you will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure to support the organization's data needs. You will work closely with data scientists, analysts, and other stakeholders to ensure that data is efficiently collected, stored, processed, and made accessible for analysis and reporting purposes. Your role will involve collaborating with cross-functional teams to understand requirements, identify opportunities for data integration and automation, and implement solutions that drive actionable insights and enable data-driven decision-making.

Key Responsibilities

Design, develop, and deploy data pipelines and ETL processes to ingest, transform, and load structured and unstructured data from various sources into data storage systems.
Optimize data pipelines for performance, reliability, and scalability, ensuring efficient processing and storage of large volumes of data.
Work with stakeholders to understand data requirements and translate them into technical specifications and solutions.
Implement data quality checks and monitoring to ensure the accuracy, completeness, and integrity of data throughout the pipeline.
Collaborate with data scientists and analysts to provide them with access to high-quality data for analysis and modeling.
Evaluate and integrate new technologies and tools to improve data infrastructure, processing efficiency, and analytics capabilities.
Document data pipelines, workflows, and technical specifications to facilitate knowledge sharing and maintainability.
Stay current with industry trends, best practices, and emerging technologies in data engineering and big data processing.

Qualifications:

Bachelor's degree in Computer Science, Engineering, Mathematics, or a related field. Advanced degree preferred.
Proven experience as a Data Engineer or similar role, with a strong background in building and maintaining data pipelines and infrastructure.
Proficiency in programming languages such as Python, Java, Scala, or SQL for data processing and manipulation.
Hands-on experience with big data technologies and frameworks such as Hadoop, Spark, Kafka, and NoSQL databases.
Familiarity with cloud platforms (e.g., AWS, Azure, GCP) and their data services (e.g., S3, Redshift, BigQuery).
Strong understanding of data modeling, database design principles, and data warehousing concepts.
Excellent problem-solving skills and attention to detail, with the ability to analyze complex data issues and implement effective solutions.
Good communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams.

This job description outlines the key responsibilities and qualifications expected from a Data Engineer position. The role plays a critical part in ensuring the organization can effectively manage and leverage its data assets for informed decision-making and business insights.

Axis Group® is an Equal Opportunity/Affirmative Action employer. Qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, veteran status, age, marital status, genetic information, or any other protected characteristic under applicable law. All candidates must be legally authorized to work in the United States. Axis Group® will not consider Visa Sponsorship for this position.
Aptitudes y experiencia deseables
DATA PIPELINES, PIPELINES, ETL, ETL FRAMEWORK, ETL PROCESS, DEVELOP, SQL, JAVA, PYTHON","Almacenamiento de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Bases de datos, Comunicación, Especificaciones técnicas, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3959054703/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=hQg2IapMapWMaququE31iQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Databricks),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Atlanta, GA","Acerca del empleo
Title 

Data Engineer (Databricks)

Overview

This is a full-time, employee position. Please do not apply if you are seeking a C2C or 1099/W2 contract. 

 Must be located in one of the following: Atlanta, Chicago, Columbus, Dallas, Minneapolis, New York Area, St. Louis. This position is mostly Remote. 

Daugherty Business Solutions brings a fresh approach to data engineering by delivering results through unmatched innovation and world-class technology and talent. This is why many of the most well-known companies in the world trust us with their mission-critical projects. As a team member at Daugherty, you will play an integral role in our company’s success and are recognized and valued for your contributions. We have an entrepreneurial culture with the maturity and security of a 35+ year company helping you to create your best work and take your career to the next level.

We are seeking a skilled and experienced Data Engineer with expertise in Databricks, Azure, and Spark to join our dynamic team. The ideal candidate will play a key role in designing, developing, and maintaining scalable data pipelines and analytics solutions to support our growing business needs. This position offers an exciting opportunity to work with cutting-edge technologies and collaborate with cross-functional teams to drive actionable insights from data.

Responsibilities

 Design, develop, and deploy end-to-end data pipelines using Databricks, Azure Data Factory, and Spark to ingest, process, and analyze large volumes of structured and unstructured data. 
 Collaborate with data scientists, analysts, and other stakeholders to understand business requirements and translate them into technical solutions. 
 Optimize and tune data pipelines for performance, reliability, and scalability to ensure efficient processing of data. 
 Implement data governance and security best practices to ensure the integrity and confidentiality of data. 
 Develop and maintain documentation, including data flow diagrams, technical specifications, and user guides. 
 Stay up-to-date with emerging technologies and best practices in data engineering, cloud computing, and big data analytics. 


Qualifications

 Proven experience as a Data Engineer, with at least 5 years of hands-on experience in designing and building data pipelines. 
 Proficiency in Databricks, Azure services (e.g., Azure Data Lake Storage, Azure Synapse Analytics), and Spark for big data processing and analytics. 
 Strong programming skills in Python, Scala, or Java, with experience in writing complex SQL queries. 
 Experience with data modeling, ETL/ELT processes, and data warehousing concepts. 
 Excellent problem-solving skills and attention to detail, with the ability to troubleshoot and debug complex data engineering issues. 
 Strong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. 
 Able to articulate ideas clearly, present findings persuasively, and build rapport with clients and team members. 
 Ability to effectively communicate technical concepts to non-technical stakeholders .
 Relevant certifications in Databricks, Azure, or Spark (e.g., Azure Data Engineer, Databricks Certified Associate) are a plus. 
 Bachelor's degree in Computer Science, Engineering, or related field; Master's degree preferred. 


 What We Commit to YOU: 

 We provide many training opportunities like certifications, hackathons, lunch and learns and free access to Pluralsight, Udemy and other digital learning platforms. 
 You will get to work with some of the most innovative teams in the IT marketplace and solve real strategic problems. 
 We will invest in things that are important to you professionally and personally. 
 We will build a relationship with you to accelerate your career. 
 We will provide you with a team environment like no other. We are consistently ranked as a Top Workplace in many of our regions as voted by our employees. 
 We provide opportunities to build community, be social and have fun with your colleagues. 
 We provide a comprehensive compensation and benefits package. 


Daugherty Business Solutions is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.

If you require accommodations or assistance to complete the online application process, please inform any recruiter you are working with (or send an email to careers@daugherty.com) and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The recruiting team will respond to your email promptly.","Almacenamiento de datos, Apache Spark, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , SQL y Scala, Modelado de datos, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3984608632/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=Fo9V0T8rWPEq4MAyRmA%2Bsw%3D%3D&trk=flagship3_search_srp_jobs,BI Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, TalTeam, is seeking the following. Apply via Dice today!

BE/B.Tech in Computer Science or related field

5+ years of enterprise software design and development experience

Proficiency in SQL, with experience in writing and optimizing complex queries

Professional, practical programming using Spark, SQL, Scala/Python

Strong technical expertise in Azure, Big Data, Apache Nifi, Hadoop, HDInsights, ADF, ADW etc.

Experience working with reporting and visualization tools like Power BI, Tableau, or similar.

Hands-on experience with scripting languages like Powershell/Bash for automation and data manipulation tasks.

Extensive knowledge and experience in data warehousing, Streaming data processing (ETL), e-metrics/measurement, business intelligence, information retrieval, parallel and distributed computation

Experience in implementation of Cloud Computing concepts and platforms

Experience in analyzing very large real world datasets and hands-on approach in data analytics

Experience with Test Driven Development, Continuous Integration, Continuous Deployment, Telemetry etc.

Great design and problem-solving skills, with a strong bias for quality and engineering excellence

Excellent verbal and written communications skills

Excellent problem-solving and debugging skills with a solid understanding of testing practices

Proven sense of high accountability and self-drive to take on and see through big challenges

Experience working in a global delivery model

Experience with SCRUM, Devops or similar Agile development/implementation methodologies

Familiarity with automation tools Visual Studio etc

BI Data Engineer","Analítica, Analítica de datos y Extraer, transformar y cargar (ETL), Buenas prácticas de pruebas, Ciencias de la computación, Comunicación, Comunicación escrita, Diseño de software, Manipulación de datos y Resolución de problemas",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3888424940/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=9RcTUguAfVIv%2FStjbAFO4Q%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer WITH INSURANCE BACKGROUND, Remote","En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,Estados Unidos,"Acerca del empleo
Data Engineer WITH INSURANCE

Remote (100%)

Phone+skype

Job Description

Understanding data in its context/insurance world sales/insurance/broker data for example

Ideal candidate 2-3 years of insurance p&c side/insurance

5-10 years of data engineering

Python and R

Azure Databricks needed (platform being used)

DAX or AAS is not required, but helpful

ETL

Will take sharp young talent who knows insurance and data engineering.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Azure Databricks, Bases de datos, Programación en C, Seguros de bienes y responsabilidad civil y Skype",Solicitar
https://www.linkedin.com/jobs/view/3980221360/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=pfCk6f3t3%2BbGtPbbnyxzsg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer 1,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Dayton, OH","Acerca del empleo
What you should know about the Data Engineer 1 position: 

This is a FT position M-F 8am-5pm

This position does require onsite availability 1 day per week at our Centerville OH location 

We provide superior care and superior services to patients at their end of life journey. Only those who have a heart for hospice will succeed

Job Summary/Purpose

As a Data Engineer I, you will be responsible for supporting the data infrastructure and pipelines within the organization. You will work closely with data scientists and analysts to ensure efficient data flow and storage. Your duties may include data cleaning, transformation, and loading tasks, as well as troubleshooting data pipeline issues. You will also assist in the maintenance and optimization of databases and data warehouses.

Key Responsibilities

Develop and maintain data pipelines for efficient data extraction, transformation, and loading (ETL).
Collaborate with data scientists and analysts to understand data requirements and ensure data quality.
Perform data cleaning and transformation tasks to prepare raw data for analysis.
Assist in the development and maintenance of databases and data warehouses.
Troubleshoot data pipeline issues and perform root cause analysis.
Implement data governance and security measures to ensure data integrity and compliance.
Stay up to date with emerging technologies and best practices in data engineering.

Qualifications

Bachelor’s degree in information systems, computer science, engineering or an equivalent field required. Equivalent work experience will be considered.
Healthcare industry experience preferred.
2+ years’ professional experience or internship.
Knowledge of SQL Server 2016/2019/2022 and strong T-SQL skills is required.
Strong discipline in following SQL coding styles which improves code readability and supportability.
Experience with ETL tools and techniques.
Knowledge of database systems (e.g., SQL, MongoDB, MySQL).
Strong analytical and problem-solving skills.
Excellent oral and written communication skills and interpersonal skills including the ability to effectively communicate technical information to both technical and non-technical personnel.
Ability to work successfully and independently in an atmosphere of multiple projects, shifting priorities, and deadline pressures.
Ability to work effectively with both users and technical staff in team-oriented environments.
Translation of business questions and requirements into reports, views, and SQL queries.
Ability to work an on-call schedule that includes being available nights and weekends. 

Benefits & Perks

 Competitive Pay (we actually mean it!) 
 Competitive Health, Dental, and Vision Insurance 
 Short- & Long-Term Disability 
 Life Insurance 
 Paid Time Off 
 Matching Retirement Plans 
 Tuition Reimbursement 
 Preparation for certification and pay incentive on Hospice certification achievement 
 Scrubs provided 
 Mileage reimbursement 
 Organizational preceptor to assist with orientation and ongoing education 
 Educational programs geared toward career advancement 
 Career growth 
 And much, much, more! 

Ohio’s Hospice offers opportunity, advancement, and a great foundation for growth to energetic people looking to serve our mission. Those who join our RN Extended team are committed to providing superior care and service so our patients and their families can celebrate life. We provide our staff members with the resources and support to contribute and make a difference in the lives of patients and families every day. Come join a group of people that are wildly passionate about taking care of our patients and each other!

As a member of our team, you'll have a chance to impact many lives. You may find a deeper meaning in your work or rediscover why you chose your profession in the first place. The passion you may have been missing in previous workplaces can be found at Ohio’s Hospice!

 Ohio’s Hospice complies with applicable Federal civil rights laws and does not discriminate on the basis of race, color, national origin, age, disability, or sex. 

 Ohio’s Hospice is proud to be platinum certified through SAGECare, which provides training and consulting on LGBT aging issues to service providers. Ohio’s Hospice welcomes those in the LGBT community to join our team.","Extraer, transformar y cargar (ETL), Herramientas ETL, Ingeniería de datos y SQL, Calidad de datos, Ciencias de la computación, Comunicación, Habilidades sociales, Limpieza de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3977771251/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=5U1cDDH%2BbkEYBnpKo2BIog%3D%3D&trk=flagship3_search_srp_jobs,BEST Program Data Analyst / BI Engineer,"50 US$/h - 57 US$/h Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Algo de responsabilidad",hace 1 semana,"Boston, MA","Acerca del empleo
Responsibilities

A Kforce client in Boston, MA is seeking a BEST Program Data Analyst/BI Engineer to drive the configuration of standard reports and dashboards delivered through the new Financials solution. Specific Duties:

 Contribute to the development of data reporting and data access through database queries, reports, and dashboards
 Gather information from agency business users and technical staff regarding the types of data, frequency of reports or dashboards, distribution of reports or dashboards, and other information as needed to understand the current reporting needs of agencies
 Participate in developing an approach to identifying reporting and data analytics needs across the new solution user community
 Document configuration decisions for each standard report and dashboard and provide guidance to report developers during the configuration stage
 Participate in testing of configured reports and dashboards and assist in coordinating user testing of these materials
 Support and manage enterprise architecture's data operations (optimize data processing, query performance and data quality) and data pipelines
 Integrate data from diverse sources using ETL processes and transform raw data into datasets used for analysis
 Implement robust security measures, monitor data access, and develop access policies
 Provide technological expertise to capture, store and employ data catalogue and lineage to support governance initiatives
 Understand business requirements in the BI context and design data models to convert raw data to meaningful insights
 Perform SQL, DAX queries and functions in Power BI
 Receive training and hands on guidance from the system integrator and product vendor in how to build queries, reports, and dashboards in order to assist reports developers in their work
 Assist in the creation of job aids and other training materials to support report writing by agency users

Requirements

 Bachelor's degree in Computer Science, System Analysis or a related study, or equivalent experience
 Minimum of 5 years of design and implementation experience in the areas of business intelligence/reporting and data warehousing
 Experience in building complex queries, reports and dashboards using a range of web-based tools
 Knowledge of structured data, such as entities, classes, hierarchies, relationships, and metadata

Proven ability to collaborate with business owners, information architects, content architects and other stakeholders to support common goals and approaches including:

 Interviewing managers and other stakeholders to understand their data needs
 Interviewing users to understand what they need from data systems to boost their performance
 Translating business goals, user needs and process improvements into data management functions and requirements
 Explaining the capabilities of data systems to business managers and users as the new solution is designed and configured
 Assessing whether data is fit for use by performing initial validation of data delivered as part of the project
 In-depth exposure to data quality concepts, best practices, and tools
 Experience in designing, developing, and deploying business analytics dashboards using Power BI
 Experience in Cloud Data warehouses like Snowflake, Redshift
 Familiarity with Snowflake Data sharing, Snowpipe and Snowpark
 Experience in ETL tools such as Pentaho/Informatica and proficient in SQL
* Well-developed system analysis skills
 Understanding and knowledge of IT standards and controls
 Proficient written and verbal communication and interpersonal skills

Preferred Qualifications

 Experience with Software as a Service cloud implementations particularly those in which legacy on premise applications have been migrated to cloud delivery options

The pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. We may ultimately pay more or less than this range. Employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. This range may be modified in the future.

We offer comprehensive benefits including medical/dental/vision insurance, HSA, FSA, 401(k), and life, disability & ADD insurance to eligible employees. Salaried personnel receive paid time off. Hourly employees are not eligible for paid time off unless required by law. Hourly employees on a Service Contract Act project are eligible for paid sick leave.

Note: Pay is not considered compensation until it is earned, vested and determinable. The amount and availability of any compensation remains in Kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law.

This job is not eligible for bonuses, incentives or commissions.

Kforce is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.","Almacenamiento de datos, Analítica, Analítica de datos, Extraer, transformar y cargar (ETL) y Herramientas ETL, Amazon Redshift, Experiencia en implementación, Modelo de datos, Necesidades empresariales y Pentaho",Solicitud sencilla
https://www.linkedin.com/jobs/view/3980695667/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=b7LaAQlRKp%2FuQaGhzFaSAQ%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,Estados Unidos,"Acerca del empleo
About Apexon

Apexon is a digital-first technology services firm specializing in accelerating business transformation and delivering human-centric digital experiences. We have been meeting customers wherever they are in the digital lifecycle and helping them outperform their competition through speed and innovation.

Apexon brings together distinct core competencies – in AI, analytics, app development, cloud, commerce, CX, data, DevOps, IoT, mobile, quality engineering and UX, and our deep expertise in BFSI, healthcare, and life sciences – to help businesses capitalize on the unlimited opportunities digital offers. Our reputation is built on a comprehensive suite of engineering services, a dedication to solving clients’ toughest technology problems, and a commitment to continuous improvement.

Backed by Goldman Sachs Asset Management and Everstone Capital, Apexon now has a global presence of 15 offices (and 10 delivery centers) across four continents.

We enable #HumanFirstDIGITAL

Role Description

You’ll be responsible for (Responsibilities):

Technical Expertise:

Proven experience with Apache Spark, including Spark SQL, Spark Streaming, and PySpark.

Strong proficiency in using Apache Airflow for workflow orchestration.

Hands-on experience with cloud platforms such as AWS, Azure, or Google Cloud.

Programming Skills

Proficiency in programming languages such as Python, Scala, or Java.

Solid understanding of SQL and experience with relational databases.

Data Engineering

Experience with data modeling, data warehousing, and data integration.

Familiarity with Big Data technologies and frameworks.

Knowledge of containerization technologies like Docker and orchestration tools like Kubernetes is a plus.

Understanding of DevOps practices and CI/CD pipelines.

Strong problem-solving skills and attention to detail.

Ability to work in a fast-paced, agile environment.

You’ll Have (Qualification & Experience)

 Bachelor's Degree in related field is required. 

Don’t worry if you don’t check all the boxes; we’d still love to hear from you.

Our Commitment To Diversity & Inclusion

Did you know that Apexon has been Certified™ by Great Place To Work®, the global authority on workplace culture, in each of the three regions in which it operates: USA (for the fourth time in 2023), India (seven consecutive certifications as of 2023), and the UK.

Apexon is committed to being an equal opportunity employer and promoting diversity in the workplace. We take affirmative action to ensure equal employment opportunity for all qualified individuals. Apexon strictly prohibits discrimination and harassment of any kind and provides equal employment opportunities to employees and applicants without regard to gender, race, color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law.

You can read about our Job Applicant Privacy policy here Job Applicant Privacy Policy (apexon.com)

Our Perks And Benefits

Our benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones.

As an Apexon Associate, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.

We Also Offer

Health Insurance with Dental & Vision
401K Plan
Life Insurance, STD & LTD
Paid Vacations & Holidays
Paid Parental Leave
FSA Dependent & Limited Purpose care
Learning & Development","Airflow, Almacenamiento de datos, Base de datos relacional, Ingeniería de datos , PySpark y Scala, Apache Airflow, Bases de datos, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3977058877/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=g1RoAW7Gw0O1Od3Q%2FcAS3g%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"160 US$K/año - 200 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,San Francisco y alrededores,"Acerca del empleo
Revinate is one of the largest and most innovative providers of direct revenue-generating solutions in the hospitality industry. Revinate's mission is to deliver hoteliers scalable direct revenue and profits from data-driven solutions that cultivate deeper relationships with guests. Revinate’s Direct Booking Platform helps capture, convert and retain guests with strategies and services that maximize direct booking revenue. This combination maximizes the lifetime value of each guest through personalized and targeted campaigns across the guest journey. Revinate Marketing has won 1st place for Hotel CRM & Email Marketing in the HotelTechAwards five years in a row!

About Revinate

We support full remote work but also maintain offices in Amsterdam, Singapore and Bend Oregon, Revinate seeks to build specialized and easy-to-adopt technology to solve these challenges. Revinate enables hoteliers to transform their guest data into revenue. With Revinate Marketing and Revinate Guest Feedback, hoteliers are empowered to make smarter decisions, resulting in increased direct revenue and guest engagement. Much like the industry we serve, we are a team of hard-working and passionate individuals who love our customers and are committed to surprising and delighting them with every new innovation and disruption.

The company is backed by leading Silicon Valley investors, including Serent Capital, Benchmark Capital, Tenaya Capital, and Sozo Ventures. Headquartered in San Francisco with regional offices in Amsterdam and Singapore, Revinate counts tens of thousands of the world’s leading hotels as customers.

To learn more, please visit www.revinate.com

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status

What We Believe

We believe that hoteliers deserve better. The global hotel sector is a booming $500B+ industry, yet hotels are facing many complex challenges, including increased pressure from online travel agencies and intense competition from ever-growing room inventory and the shared economy. That coupled with aging, cumbersome technology is making the job of the hotelier more difficult than ever. At Revinate, we use cutting edge technology to build powerful software for hotels to take back control and drive direct revenue. The simplicity and beautiful UX of our solutions are a breath of fresh air in an industry of old technology.

Role Overview

Revinate is seeking an experienced Data Engineer to join our Pipeline team. This team is responsible for building and maintaining reactive microservices to enable a highly-available data pipeline that processes millions of events, in real-time, each day, from thousands of hotels across the globe. A passion for building a high-volume, cloud-native, distributed stream-processing pipeline is a must. Additionally, the ability to collaborate with others, regardless of title, to drive cross-team technical and process improvements is a requirement. If you enjoy coming up with thoughtful solutions to challenging problems then we may be a match.

What You'll Do

Build, scale, and maintain a high-volume, cloud-native, distributed stream-processing pipeline that runs 24x7
Contribute to a culture of rapid iteration and frequent production deployments, without sacrificing quality
Play a hands-on role investigating issues, asking questions, and prioritizing tech debt
Promote quality and best practices through active participation in code reviews and knowledge sharing
Collaborate with product managers, project managers, and other engineers to understand, document, and deliver solution requirements
Monitor system performance and ensure timely recovery from incidents
Work from home most days, but meet in person (in our downtown San Francisco office) once every two weeks

What You'll Bring

Exceptional written and verbal communication skills
Ability to build relationships and influence colleagues
Strong understanding of distributed systems, microservice architecture, and reactive design patterns
Unwavering commitment to engineering excellence and craftsmanship
Experience with data engineering technologies (Java, Kafka, Kafka Streams, Cassandra, Spark, Elasticsearch, S3, Protocol Buffers) and supporting technologies (Docker, Kubernetes, Helm, Gradle, GitLab, AWS)
Systems thinking mindset with a strong desire to understand the entire end-to-end architecture and data flow
Ability to challenge existing designs and propose solutions for highly-scalable distributed systems while minimizing technology costs and maintenance obligations
Intuitive sense for spotting, investigating, and prioritizing data processing anomalies

Benefits

Health insurance-employee premium paid 100% by Revinate
Dental insurance-employee and dependents’ premium paid 100% by Revinate
Vision insurance-employee and dependents’ premium paid 100% by Revinate
401(k) with employer match
Short & Long Term Disability insurance
Life insurance
Paid time off
Monthly work from home stipend
Telehealth access
Employee Assistance Program (EAP)

$160,000 - $200,000 a year

This salary range may be inclusive of several career levels at Revinate and will be narrowed during the interview process based on a number of factors, including (but not limited to) the candidate’s experience, qualifications and location.

Revinate values the flexibility of a remote workforce and the benefits of localized hiring. We focus on specific cities to foster local communities and enhance team cohesion, allowing employees to collaborate, attend local events, and build a strong sense of community and company culture.

Candidates must be located in the city listed in the job application. Thank you!

Revinate is not open to third party solicitation or resumes for our posted FTE positions. Resumes received from third party agencies that are unsolicited will be considered complementary.

Important Security Alert

We have been made aware of fraudulent activities involving individuals impersonating our HR team and offering fake job opportunities. Please be vigilant and ensure your safety by verifying all job offers.

For Authentic Opportunities: Only refer to our official careers page on our company website. Your security is our priority. If you encounter any suspicious activity, please report it immediately. Stay safe and secure! You can confirm or inquire with any questions by reaching out to recruiting@revinate.com

Excited?! Want to learn more? Apply Now!

Our Core Values:

One Revinate - United & Strong, on a single mission together

Built on Trust - It’s the foundation of everything we do

Expect Amazing - We think, dream & deliver big

Customer Love -- When the customer wins, we win

Make it Simpler -- Apply it to everything we do

Hungerness -- Feel it, follow it, be relentless about our success

Grounded in Gratitude - We’re glad to be here & make the most of every day

Revinate Inc. provides Equal Employment Opportunity to all employees and applicants for employment without regard to race, color, religion, gender identity or expression, sex, sexual orientation, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. Revinate complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.

Revinate is not open to third party solicitation or resumes for our posted FTE positions. Resumes received from third party agencies that are unsolicited will be considered complementary.

If you are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e-mail with your request to recruiting@revinate.com.

By submitting your application you acknowledge that you have read Revinate's Privacy Policy ( https://www.revinate.com/privacy/ )","Apache Spark, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Comunicación oral, Establecer prioridades del trabajo, Gradle, Intercambio de conocimientos y System Performance",Solicitar
https://www.linkedin.com/jobs/view/3964487491/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=J%2BVmpMRy9jYYUA1XQTXh9Q%3D%3D&trk=flagship3_search_srp_jobs,Reliability Data Engineer (HYBRID Schedule!) (Pension offered!),"100 US$K/año - 115 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,"Chicago, IL","Acerca del empleo
A stable and growing equipment leasing company located in the heart of Chicago is seeking a Reliability Engineer. The goal of this position is to analyze data and optimize the reliability of life-cycle maintenance on the rental equipment

This company has a rich history, being over 100 years old! This company offers a competitive salary, amazing benefits, pension and a true opportunity to grow your career!

Key duties include:
Analyzing and tracking all KPIs to drive strategic improvement
Set up and monitor quantitative metrics to meet reliability goals
Optimize the maintenance schedule for planned events
Analyze the root cause of failures and prevent unplanned maintenance events
Support additional reliability goals

Education or Experience required:
B.S. or M.S. in Engineering or other Technical discipline
2 + years of relevant experience
Advanced skills in Power BI and MS Excel
Advanced level skills with statistics and data analysis","Analítica de datos, Microsoft Excel y Microsoft Power BI, Fiabilidad",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3961528162/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=DVfS%2Fe%2BwczXgwcE0O5iKVg%3D%3D&trk=flagship3_search_srp_jobs,ML with LLM Data Engineer- with Python and SQL,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Cupertino, CA","Acerca del empleo
Technical/Functional Skills

12+ years of experience in ML Engineering ( NLP) and Data Engineering
Very good knowledge and experience in Python and SQL
Expereince in deploying ML models
Strong understanding of machine learning principles, especially in the context of LLMs.
Experience building chatbots using LLM’s
Managing Datapipeline/Transforms
Experience in Data injestion and Data Visualisation through Tableau or related tools.
Experince with Data Robot is recommended
Strong verbal and written communications skills with the ability to work effectively across internal and external organizations and virtual teams.

Aptitudes y experiencia deseables
Python , Tableau , Machine Learning","Aprendizaje automático, Ciencia de datos, Ingeniería de datos , Procesamiento de lenguaje natural, Python, SQL, Tableau, Visualización y Visualización de datos, Bots conversacionales",Solicitar
https://www.linkedin.com/jobs/view/3974078227/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=7EuYk1t3%2BETXRFRFEFiTDw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 semanas,"Jackson, MI","Acerca del empleo
Description

As an Azure Data Engineer, your role involves designing, implementing, monitoring, and maintaining data and analytical solutions on Microsoft Azure.

Data Pipeline Design And Implementation

Create and maintain data pipelines that facilitate data movement, transformation, and integration.

Ensure efficient and reliable data flow from source systems to target destinations.

Data Storage Solutions

Design and manage data storage solutions within Azure, including:

Azure Data Lake: Storing large volumes of raw data.

Azure SQL Database: Providing structured data storage.

Azure Blob Storage: Handling unstructured data.

Data Processing And Transformation

Use tools like Databricks and dbt to process and transform data.

Leverage Azure Databricks for running dbt transformations in production

Data Integration

Collaborate with data analysts and scientists to understand data requirements.

Design effective data workflows that enable data-driven decision-making.

Performance Optimization

Optimize SQL queries generated by dbt for better performance.

Ensure consistency between development (using a Databricks SQL warehouse) and production (using Azure Databricks compute) environments

Security And Compliance

Implement secure and compliant data processing pipelines.

Use Azure services and frameworks to produce cleansed and enhanced datasets for analysis

Skills

Microsoft Azure, Azure Databricks, and DBT

Knowledge of data catalog tools (e.g., Microsoft Purview)

Proficiency in data processing languages, including SQL and Python

Excellent problem-solving skills and ability to work in a collaborative environment

Education

Bachelor’s degree in Computer Science, Engineering, or related field.

Experience as an Azure Data Engineer or similar role.

Certifications & Licenses

Proficiency in data processing languages, including SQL and Python

Skills Required

Microsoft azure

Problem-solving

Data pipelines

Python

SQL

Additional

Datasets

Performance optimization

SQL queries

Data integration

Languages

English (Speak, Read, Write)

As an equal opportunity employer, ICONMA provides an employment environment that supports and encourages the abilities of all persons without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local laws.
Aptitudes y experiencia deseables
DATA PIPELINES, SQL, PYTHON, MICROSOFT AZURE, DATA FLOW, DATA BRICKS, DBT","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Bases de datos, Ciencias de la computación, Procesamiento de datos y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979929084/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=jg8ooM5iM2qysJPbul6xmw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer with Mattillion,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,"Newport Beach, CA","Acerca del empleo
We are seeking a highly motivated Business Associate with 5 to 7 years of experience in business development. The ideal candidate will have strong expertise in Snowflake and ETL processes. This role involves driving business growth, identifying new opportunities, and leveraging data to inform strategic decisions. The candidate will play a crucial role in expanding our market presence and achieving our business objectives. 3.Experience : 5to7Yrs 4.Required Skills : ,Snowflake,ETL 5.Nice to have skills : 6.Technology : -Not Applicable 7.Shift : Day 8.Roles & Responsibilities : - Drive business growth by identifying and pursuing new business opportunities. - Utilize Snowflake and ETL processes to analyze and interpret complex data sets. - Develop and implement strategic plans to achieve business development goals. - Collaborate with cross-functional teams to align business strategies with company objectives. - Provide insights and recommendations based on data analysis to inform decision-making. - Oversee the development of proposals and presentations for potential clients. - Maintain and expand relationships with existing clients to ensure ongoing business. - Conduct market research to identify trends and opportunities for growth. - Monitor and report on the effectiveness of business development strategies. - Ensure compliance with industry regulations and company policies. - Manage and prioritize multiple projects to meet deadlines and achieve targets. - Utilize CRM tools to track and manage business development activities. - Prepare regular reports and presentations for senior management. -Qualifications - Possess strong expertise in Snowflake and ETL processes. - Have a proven track record in business development with 5 to 7 years of experience. - Demonstrate excellent analytical and problem-solving skills. - Exhibit strong communication and interpersonal skills. - Show proficiency in using CRM tools and data analysis software. - Display the ability to work independently and as part of a team. - Have a deep understanding of market research and competitive analysis. - Show capability to manage multiple projects and meet deadlines. - Demonstrate a proactive and results-oriented approach. - Possess a bachelors degree in Business, Marketing, or a related field. - Have experience in preparing and delivering presentations to clients and stakeholders. - Exhibit strong organizational and time management skills. - Show commitment to continuous learning and professional development.

Note: Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence, and performance of the essential functions of their positions. We promote and support a diverse workforce at all levels in the company. This is not an unsolicited mail and if it is not intended for you or you are not interested in receiving our e-mails, you can unsubscribe . We are sorry for any inconvenience.

 Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.
Aptitudes y experiencia deseables
DATA ENGINEER WITH MATTILLION, ETL, SNOWFLAKE, CRM","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Habilidades sociales, Resolución de problemas, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3977206333/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=%2BFJ38XqvCvGifw3B28eb0g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer III,"79,43 US$/h - 81,56 US$/h Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Cupertino, CA","Acerca del empleo
Responsibilities

Develop and maintain Python scripts for data analysis and automation.

Design, implement, and manage ETL pipelines using tools such as Apache Airflow.

Analyze large datasets to identify trends, patterns, and insights.

Collaborate with cross-functional teams to understand data requirements and deliver actionable insights.

Conduct quality assurance and validation of data to ensure accuracy and reliability.

Prepare reports and visualizations to communicate findings effectively.

Desired Skills And Qualifications

Proficiency in Python programming.

Hands-on experience with data ETL processes, especially using Airflow.

Strong analytical and problem-solving skills.

Experience with human evaluation, prompt engineering, machine learning, statistical analysis is a plus.

Excellent communication and teamwork abilities.

Bachelor’s degree in Computer Science, Data Science, or a related field.

As an equal opportunity employer, ICONMA provides an employment environment that supports and encourages the abilities of all persons without regard to race, color, religion, gender, sexual orientation, gender identity or express, ethnicity, national origin, age, disability status, political affiliation, genetics, marital status, protected veteran status, or any other characteristic protected by federal, state, or local laws.
Aptitudes y experiencia deseables
DATA ENGINEER, PYTHON, SCRIPTS, DATA ANALYSIS, ETL, APACHE, AIRFLOW","Airflow, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, Ciencias de la computación, Comunicación, Datasets y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3979003201/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=d2v9pw5J9gFHRhuq0%2B%2FyPw%3D%3D&trk=flagship3_search_srp_jobs,Teamcenter PLM Data Migration Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Role: Teamcenter PLM Data Migration Engineer

Location: Remote

Interview: Phone/Skype

Job Type: Contract 
   Technical Skills:

Experienced in the Data migration projects in PLM industry in specific to TeamCenter

Proficiency in data migration tools and technologies (e.g., ETL tools, SQL, scripting languages).

In-depth knowledge of Teamcenter PLM architecture, data models, and integration points.

Strong understanding of data mapping, transformation, and validation techniques.
 Key Responsibilities:

Develop and implement comprehensive data migration strategies and plans for Teamcenter PLM.

Assess the current data environment and identify potential challenges and risks associated with migration.

Analyze and map source data to target Teamcenter PLM structures.

Define and document data transformation rules to ensure data integrity and consistency during migration.

Lead the execution of data migration activities, including data extraction, transformation, and loading (ETL) processes.

Utilize migration tools and scripts to automate data migration tasks and improve efficiency.

Conduct data validation and reconciliation to ensure the accuracy and completeness of migrated data.

Identify and resolve data discrepancies and issues during and after migration.

Collaborate with cross-functional teams, including IT, PLM developers, business analysts, and end-users, to gather requirements and ensure successful migration.

Provide regular updates and reports on migration progress, issues, and resolutions to stakeholders.

Monitor and support post-migration activities, addressing any issues that arise to ensure a smooth transition.

Provide training and documentation to end-users and support teams on new data structures and processes within Teamcenter PLM.
 Essential Skills:

Excellent problem-solving and analytical skills.

Strong communication and interpersonal skills.

Ability to work independently and collaboratively in a team environment.
 Essential Skills:

Excellent problem-solving and analytical skills.

Strong communication and interpersonal skills.

Ability to work independently and collaboratively in a team environment.""

Strong general IT skills with experience with both the Windows and Linux OS platforms

Experience performing Teamcenter systems data migration skills

Strong customer focus, including the ability to manage customer needs and multiple work priorities.

Needs strong oral and written communication, analytical, and problem-solving skills, as well as

excellent judgment and self-motivation.

Lead the execution of data migration activities, including data extraction, transformation, and loading (ETL) processes.

Ability to work independently with little to no supervision researching new technologies or comparing.

At Least years of strong TCE admin experience""
 Qualifications:

Education: Bachelor s degree in Computer Science, Information Technology, Engineering, or a related field.

Aptitudes y experiencia deseables
Teamcenter,PLM,Migration","Extraer, transformar y cargar (ETL) y Herramientas ETL, Asignación de datos, Comunicación, Habilidades sociales, Migración de datos, Modelo de datos, Resolución de problemas, Teamcenter y Validación de datos",Solicitar
https://www.linkedin.com/jobs/view/3981308116/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=AeeEZfAVWr8Dpejn0HBbdA%3D%3D&trackingId=fmuwq6kwx6%2BHDQdFE9i7vA%3D%3D&trk=flagship3_search_srp_jobs,CS - Data Scientist / Machine Learning Engineer Student,"Híbrido Media jornada
Coincide con tus preferencias de empleo. El tipo de empleo es Media jornada.
Prácticas",hace 5 días,"Phoenix, AZ","Acerca del empleo
Job Description

Join Intel and build a better tomorrow. Intel is in the midst of an exciting transformation, with a vision to create and extend computing technology to connect and enrich the lives of every person on Earth. So join us and help us create the next generation of technologies that will shape the future for decades to come.

Corporate Services (CS) touches the lives of every Intel employee every day. CS creates an environment where employees can prosper while creating the innovative technologies that make amazing possible. Our scope is vast and includes operating and maintaining all Intel sites, offices, labs and factories globally as well onsite services and experiences that help employees stay safe and productive. CS also helps to make Intel and our community a greener place by supporting Intel commitment to environmental sustainability, including investing in conservation projects, setting company-wide environmental targets and driving reductions in greenhouse emissions, energy use, water use and waste generation.

Intel Corporation's state of the art facilities gives you the opportunity to learn a variety of technical and operational skills to develop, operate, maintain, and repair the world’s most advanced facility equipment, in a demanding and challenging operations environment.

Life at Intel link: https://www.intel.com/content/www/us/en/jobs/life-at-intel.html

As a CS - Data Scientist / Machine Learning Engineer Student your responsibilities will include but are not limited to: 

Work alongside our multi-disciplinary Automation team and apply data science techniques to predict anomalies and faults in complex, large-scale time-series data.
Apply your data science skills and statistical knowledge to build an autonomous fault detection and root cause platform using Machine Learning methodologies.
Support consulting activities to identify data availability, quality, and modeling requirements and participate in our customers' engagement.
Build and set up; AI environments, workloads, and frameworks to deploy the AI use cases.

The Successful Candidate Should Exhibit The Following Behavioral Traits

A proactive, innovative, and pragmatic approach to problem-solving.
Skills to think critically and independently.
Work as part of a cross-functional team and communicate clearly to provide insights based on analysis outputs.
Ability to acquire fast technical acumen and business.

Intel invests in our people and offers a complete and competitive package of benefits employees and their families through every stage of life.

See https://www.intel.com/content/www/us/en/jobs/benefits.html for more details.

Qualifications

You must possess the below minimum qualifications to be initially considered for this position. Preferred qualifications are in addition to the minimum requirements and are considered a plus factor in identifying top candidates. Experience would be obtained through a combination of prior education level classes, and current level school classes, projects, research, and relevant previous job and/or internship experience.

This position is not eligible for employment-based visa/immigration sponsorship.  Intel sponsors individuals for employment-based visas for positions where we experience a shortage of US Workers. These skills shortage roles are typically STEM contributing positions requiring a Master's or Ph.D. degree, or a Bachelor’s degree with three years’ related  job experience. This position does not qualify for Intel Sponsorship because it is either (1) a non-STEM contributing position, or (2) a STEM position that only requires a Bachelor’s degree and less than three years’ experience.

Minimum Qualifications

BS student in Computer Science, System or Software Engineering, or a related field.

With 3+ months of experience or related coursework in:

Statistical analysis, machine learning, and deep learning methodologies understanding.
Computing fundamentals in algorithm design, data structures, complexity analysis, problem-solving, and diagnosis. 
Software development skills. (Python, SQL, and some scripting). 

Preferred Qualifications

Understanding what it takes to write clean code and experience with software development lifecycle.
Knowledge of system identification of dynamic models and control systems.
Experience with Vibration analysis, Fault Detection, and Root Cause Diagnosis.
Practical experience of predicting Remaining Useful Life.
Extraction of conditional indicators using signal processing techniques and analyzing data from physical sensors.
DevOps.

Inside this Business Group

As the world's largest chip manufacturer, Intel strives to make every facet of semiconductor manufacturing state-of-the-art -- from semiconductor process development and manufacturing, through yield improvement to packaging, final test and optimization, and world class Supply Chain and facilities support. Employees in the Technology Development and Manufacturing Group are part of a worldwide network of design, development, manufacturing, and assembly/test facilities, all focused on utilizing the power of Moore’s Law to bring smart, connected devices to every person on Earth.

Posting Statement

All qualified applicants will receive consideration for employment without regard to race, color, religion, religious creed, sex, national origin, ancestry, age, physical or mental disability, medical condition, genetic information, military and veteran status, marital status, pregnancy, gender, gender expression, gender identity, sexual orientation, or any other characteristic protected by local law, regulation, or ordinance.

Benefits

We offer a total compensation package that ranks among the best in the industry. It consists of competitive pay, stock, bonuses, as well as, benefit programs which include health, retirement, and vacation. Find more information about all of our Amazing Benefits here.

Working Model

This role will be eligible for our hybrid work model which allows employees to split their time between working on-site at their assigned Intel site and off-site. In certain circumstances the work model may change to accommodate business needs.","Análisis predictivo, Ciencia de datos, Pensamiento crítico y Visualización de datos, Análisis de vibraciones, Análisis estadístico, Ciencias de la computación, Clean Coding, Root Cause y System Identification",Solicitar
https://www.linkedin.com/jobs/view/3970356046/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=G9drRcU%2B%2Fo87YQC6xNyKyg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,Estados Unidos,"Acerca del empleo
Figma is growing our team of passionate people on a mission to make design accessible to all. Born on the Web, Figma helps entire product teams brainstorm, design and build better products — from start to finish. Whether it’s consolidating tools, simplifying workflows, or collaborating across teams and time zones, Figma makes the design process faster, more efficient, and fun while keeping everyone on the same page. From great products to long-lasting companies, we believe that nothing great is made alone—come make with us!

We are looking for an experienced Data Engineer to partner with our Data Science and Data Infrastructure teams to own and scale our data pipelines. You’ll also work closely with stakeholders across business teams including sales, marketing, and finance to ensure that the data they need arrives promptly and reliably. You’ll play an integral role in building the metrics and self-serve reporting capabilities to unlock Figma’s next phase of growth.

This is a great role for an individual who is passionate about working with data and data systems, and who loves solving problems. You’ll have a good sense for when it makes sense to build fast, scrappy solutions to unblock a key stakeholder vs. when to push back or bring in an outside service. The ideal candidate will be a great communicator who can help coordinate across multiple internal and external teams and takes pride in building end-to-end projects.

What you'll do at Figma:

Own, build, and maintain scalable data pipelines that connect various cloud data sources.
Develop a deep understanding of Figma’s core data models and optimize data pipelines for scale.
Partner with the Data Science and Data Infrastructure teams to build new foundational data sets that are trusted, well understood, and enable self-service.
Work with a wide range of cross-functional stakeholders to derive requirements and architect shared datasets; ability to document, simplify and explain complex problems to different types of audiences.
Establish best practices for the development of specialized data sets for analytics and modeling. 

We'd love to hear from you if you have:

4+ years in a relevant field
Fluency with both SQL and Python
Familiarity with Snowflake, dbt, Dagster, and ETL/reverse ETL tools.
Excellent judgment and creative problem solving skills
A self-starting mindset along with strong communication and collaboration skills

While not required, it’s an added plus if you also have:

Knowledge in data modeling methodologies to design and build robust data architectures for insightful analytics
Experience with business systems such as Salesforce, Customer IO, Stripe, NetSuite is a big plus.

At Figma, one of our values is Grow as you go. We believe in hiring smart, curious people who are excited to learn and develop their skills. If you’re excited about this role but your past experience doesn’t align perfectly with the points outlined in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.

Pay Transparency Disclosure

If based in Figma’s San Francisco or New York hub offices, this role has the annual base salary range stated below.

Job level and actual compensation will be decided based on factors including, but not limited to, individual qualifications objectively assessed during the interview process (including skills and prior relevant experience, potential impact, and scope of role), market demands, and specific work location. The listed range is a guideline, and the range for this role may be modified. For roles that are available to be filled remotely, the pay range is localized according to employee work location by a factor of between 80% and 100% of range. Please discuss your specific work location with your recruiter for more information.

Figma offers equity to employees, as well a competitive package of additional benefits, including health, dental & vision, retirement with company contribution, parental leave & reproductive or family planning support, mental health & wellness benefits, generous PTO, company recharge days, a learning & development stipend, a work from home stipend, and cell phone reimbursement. Figma also offers sales incentive pay for most sales roles. Figma’s compensation and benefits are subject to change and may be modified in the future. You may view our Pay Transparency Policy by clicking on the corresponding link.

Annual Base Salary Range (SF/NY Hub):

$149,000—$317,000 USD

At Figma we celebrate and support our differences. We know employing a team rich in diverse thoughts, experiences, and opinions allows our employees, our product and our community to flourish. Figma is an equal opportunity workplace - we are dedicated to equal employment opportunities regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity/expression, veteran status, or any other characteristic protected by law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements.

We will work to ensure individuals with disabilities are provided reasonable accommodation to apply for a role, participate in the interview process, perform essential job functions, and receive other benefits and privileges of employment. If you require accommodation, please reach out to accommodations-ext@figma.com. These modifications enable an individual with a disability to have an equal opportunity not only to get a job, but successfully perform their job tasks to the same extent as people without disabilities.

Examples of accommodations include but are not limited to:

Holding interviews in an accessible location
Enabling closed captioning on video conferencing
Ensuring all written communication be compatible with screen readers
Changing the mode or format of interviews 

By applying for this job, the candidate acknowledges and agrees that any personal data contained in their application or supporting materials will be processed in accordance with the applicable candidate section of Figma's Privacy Policy.","Ciencia de datos, Extraer, transformar y cargar (ETL), Herramientas ETL y Ingeniería de datos, Comunicación, Datasets, Modelado de datos, Modelo de datos, Resolución creativa de problemas y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3952682523/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=bUZhtjE%2BKMF4yZeTrJFOAw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer/Scientist,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 mes,"Mineápolis, MN","Acerca del empleo
Bachelor's or master's degree in Computer Science, Data Science, Computer Engineering, or a related field.
 At least 5 years of experience in software development and data engineering, with a proven track record in data warehouse management.
 Advanced skills in JavaScript, and Python, and familiarity with modern programming frameworks and technologies.
 In-depth knowledge of Snowflake, data lakes, and other data warehousing technologies.
 Expertise in developing and maintaining RESTful APIs and web service integration.
 Solid understanding of Docker/Kubernetes for deployment, and databases (Postgres, MySQL).

Preferred Qualifications

 Ability to work autonomously, efficiently, and accurately in a dynamic, fast-paced environment.
 Excellent problem-solving skills with a keen analytical mindset suitable for tackling complex data challenges.
 Demonstrated experience in architecting full-stack applications for Business Intelligence (BI) insights in an enterprise environment, emphasizing the integration of front-end analytics platforms with back-end data warehousing and processing technologies to drive data-driven decision-making.","Almacenamiento de datos, Ciencia de datos, Ingeniería de datos y Minería de datos, Arquitectura técnica, Ciencias de la computación, Desarrollo de software, Lagos de datos, Resolución de problemas y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3962986923/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=127fM0to95IMAQYYJoFJ1g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,Estados Unidos,"Acerca del empleo
Azure Data Engineer
Full-Time (40 hrs/week)
Remote
Note: We are unable to provide sponsorship at this time.


Job Summary:

Serves as a CI/CD Services Architect with responsibilities to architect and manage deployment pipelines.

Recommended Years of Experience: Three (3) to five (5) years of experience in cloud-based software testing and test automation.


Experience Requirements:
Microsoft Certified: Azure Data Engineer Associate (DP 203) certification required.
Experience developing, deploying, and managing data pipelines in cloud environments.
Experience using Azure data services (e.g., Azure Data Factory, Azure Data Lake Storage, Azure Synapse, Azure Blob Storage).
Experience using Databricks, including data ingestion, transformation, and analytics using Databricks notebooks and clusters.
Experience using continuous integration and continuous deployment (CI/CD) practices and tools (e.g., Azure DevOps, Jenkins).
Experience using scripting languages (e.g., Python, Bash, PowerShell) for automating data pipeline deployments and management.
Experience working in an Agile environment – working as a part of teams to iteratively develop and deliver data products.

Preferred:
Microsoft Certified: Azure DevOps Engineer Expert (AZ 400).


Eight Eleven Group provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, sexual orientation, gender identity, marital status, amnesty or status as a covered veteran in accordance with applicable federal, state, and local laws.","Azure Data Lake, Azure Databricks y Azure DevOps",Solicitud sencilla
https://www.linkedin.com/jobs/view/3964672731/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=NLgNjghmk%2Fy0PDZ7rxLwrg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Nueva York, NY","Acerca del empleo
Kargo creates breakthrough cross-screen ad experiences for the world's leading brands and publishers. Everyday, our 600+ employees bring the power of their creativity and diversity to radically raising the bar on what mobile, CTV, AI, social, and eCommerce can do to wow consumers and build businesses. Now 20 years strong, Kargo has offices in NYC, Chicago, Austin, LA, Dallas, Sydney, Auckland, London and Waterford, Ireland. Humble brag: In 2024, Kargo was recognized as a Best Place to Work by Ad Age and Built In.

Who We Hire

Success takes all kinds. Diversity describes our workforce. Inclusion defines our culture. We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, national origin, protected veteran status, disability or other legally protected status. Individuals with disabilities are provided reasonable accommodation to participate in the job application process, perform essential job functions, and receive other benefits and privileges of employment.

Title: Data Engineer

Job Type: Full-time; In-Office Hybrid Required

Job Location: NYC

Salary Range: 100,000 - 130,000 OTE

The Opportunity

As cookie deprecation looms, Kargo must adapt to new ways of matching the right audience with the right ads. We are looking to add a data engineer to our Targeting and Audiences team to help evaluate existing methodology, try out new solutions, and build processes that scale to petabytes of data. This role involves communication across engineering, data, product, and client-facing teams to ensure alignment on what we're building and how it will be used. It involves performing experiments on how data is captured, stored, and interpreted, and evaluating the results once the methodology is applied to campaigns.

The Daily To-Do

Understanding and maintaining existing identity and contextual data obtained from internal logs and third parties, dealing with data volume and performance issues as they arise, and refactoring legacy processes to be more reliable/fit current best practices.
Working with product and data science teams to expand our identity and contextual targeting capabilities; communicating needs with the engineering teams to ensure we can obtain this data and properly log it.
Performing analytics on efficacy and accuracy of our current targeting strategies, data vendors, and internal reports.
Building data pipelines to ingest, process, and load data to an in-memory database used by our real-time bidder. Current technologies used: Airflow, Snowflake, Aerospike, APIs. Current languages used: SQL and Python; need proficiency in both.

Qualifications 

Strong experience with Airflow, Snowflake, Python and SQL
Experience with Aerospike is a plus
Experience with petabytes of data, including building complex methods for loading this data quickly and efficiently, is a must
Experience in AdTech, and particularly around targeting, is highly preferred
Past programming experience in Python is highly preferred
 Extreme attention to detail, great verbal and written communication skills, and a proactive attitude

Follow Our Lead

Big Picture: kargo.com
The Latest: Instagram (@kargomobile) and LinkedIn (Kargo)","Airflow, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Aerospike, Bases de datos, Comunicación, Comunicación escrita, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3917542957/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=vnL%2FIJLaWZwx1zNh3QEUCw%3D%3D&trk=flagship3_search_srp_jobs,DBT Data Engineer - REMOTE,"82 US$K/año - 178,1 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,Estados Unidos,"Acerca del empleo
Job Description

We currently have a career opportunity for a DBT Data Engineer to join our Data Solutions team.

Job Overview:

The Technical Architect provides technology direction, ensures project implementation compliance, and utilizes technology research to innovate, integrate, and manage technology solutions. As a Technology Architect, you will significantly contribute to identifying best-fit architectural solutions for one or more projects; you will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions, participate in Sales and various pursuits focused on our clients' business needs. This role is considered part of the Business Unit Leadership team and may mentor Junior Architects and /or development team members.

Perficient is always looking for the best and brightest talent and we need you! We’re a quickly-growing, global digital consulting leader, and we’re transforming the world’s largest enterprises and biggest brands. You’ll work with the latest technologies, expand your skills, and become a part of our global community of talented, diverse, and knowledgeable colleagues.

Responsibilities

Responsible for day-to-day development process and overseeing the implementation of the technical design, data profiling and data analysis. 
Deploy, Run, and Debug the ETL engine running the scripts built by the ETL Engineering team. 
Exposure towards Source to Target Mapping and Function Requirement Specifications Docs. 
Develop a deep understanding of systems and processes to extract insights from existing data leveraging rigorous project management discipline to ensure the highest possible quality of delivery while managing team capacity. 
Ability to work in a fast-paced and deadline driven environment and mentor team members. 
Conduct business due-diligence activities to identify analytics opportunities for transformation. 
Ability to manage high volume and high complexity data projects across multiple delivery location in virtual environment with excellent client communication skills. 
Strong problem solving and conceptual thinking abilities. 

Qualifications

Candidate must have the min of 10+ years of experience working with DBT core or DBT cloud
Must have Snowflake experience. 
Must have relevant experience of min 2 years in Data vault 2.0 implementation. 
Experience of working in healthcare domain would be a huge plus. 
 Experience in defining new architectures and ability to drive an independent project from an architectural standpoint. 
 Experience in complete project life cycle activities on development and maintenance projects. 
 Knowledge and experience in developing software using agile methodologies. 
 Proven track record of technical leadership roles delivering solutions within defined timeframes. 
 Skilled in building relationships with clients and in practice development activities. 
 Demonstrated success in performing work and managing complex and/or large consulting projects. 
 Excellent written and oral communication skills; Ability to communicate effectively with technical and non-technical staff. 
 Bachelor’s degree in computer science or related field. 
 Must be open to travel. 

Preferred Skills and Education:

 Experience working with a globally distributed team and managing off-shore teams. 
 Master’s degree in Computer Science or related field. 
 Certification in (Insert technologies). 
 Experience with authoring, editing and presenting technical documents. 

The salary range for this position takes into consideration a variety of factors, including but not limited to skill sets, level of experience, applicable office location, training, licensure and certifications, and other business and organizational needs. The new hire salary range displays the minimum and maximum salary targets for this position across all US locations, and the range has not been adjusted for any specific state differentials. It is not typical for a candidate to be hired at or near the top of the range for their role, and compensation decisions are dependent on the unique facts and circumstances regarding each candidate. A reasonable estimate of the current salary range for this position is $81,978 to $178,090. Please note that the salary range posted reflects the base salary only and does not include benefits or any potential equity or variable bonus programs. Information regarding the benefits available for this position are in our benefits overview .

Who We Are

Perficient is a leading global digital consultancy. We imagine, create, engineer, and run digital transformation solutions that help our clients exceed customers’ expectations, outpace competition, and grow their business. With unparalleled strategy, creative, and technology capabilities, our colleagues bring big thinking and innovative ideas, along with a practical approach to help our clients – the world’s largest enterprises and biggest brands succeed.

What We Believe

At Perficient, we promise to challenge, champion, and celebrate our people. You will experience a unique and collaborative culture that values every voice. Join our team, and you’ll become part of something truly special.

We believe in developing a workforce that is as diverse and inclusive as the clients we work with. We’re committed to actively listening, learning, and acting to further advance our organization, our communities, and our future leaders… and we’re not done yet.

Perficient, Inc. proudly provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, genetic information, marital status, amnesty, or status as a protected veteran in accordance with applicable federal, state and local laws. Perficient, Inc. complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including, but not limited to, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. Perficient, Inc. expressly prohibits any form of unlawful employee harassment based on race, color, religion, gender, sexual orientation, national origin, age, genetic information, disability, or covered veterans. Improper interference with the ability of Perficient, Inc. employees to perform their expected job duties is absolutely not tolerated.

Disability Accommodations:

Perficient is committed to providing a barrier-free employment process with reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or accommodation due to a disability, please contact us.

Disclaimer: The above statements are not intended to be a complete statement of job content, rather to act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time. 

About Us

Perficient is always looking for the best and brightest talent and we need you! We’re a quickly growing, global digital consulting leader, and we’re transforming the world’s largest enterprises and biggest brands. You’ll work with the latest technologies, expand your skills, experience work-life balance, and become a part of our global community of talented, diverse, and knowledgeable colleagues.

 

Select work authorization questions to ask when applicants apply

1. Are you legally authorized to work in the United States?

2. Will you now, or in the future, require sponsorship for employment visa status (e.g. H-1B visa status)?","Analítica de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL) y Visualización de datos, Bóveda de datos, Diseño técnico, Especificaciones de requisitos, Perfiles de datos, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3968898186/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=PbeHXJDVCTMZ%2FvDk4QHFJQ%3D%3D&trk=flagship3_search_srp_jobs,Imagery Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 semanas,Estados Unidos,"Acerca del empleo
Overview

Tekwissen group, is a workforce management provider throughout the USA and many other countries in the world. This client is a German multinational Pharmaceutical and biotechnology company and one of the largest pharmaceutical companies in the world, headquartered in Leverkusen, and areas of business include pharmaceuticals; consumer healthcare products, agricultural chemicals, seeds and biotechnology products.

Job Title: Imagery Data Engineer 

Location: St Louis, MO 63146 

Duration: 12 Months 

Job Type: Contract 

Work Type: Remote 

Job Description

As an Imagery Data Engineer, you will play a crucial role in designing, implementing, and maintaining Imagery Team data assets and capabilities in a growing multi-cloud environment to ensure the efficient and reliable operation of client’s Imagery pipelines and services. 
You will collaborate closely with data engineers, data stewards, data scientists, platform engineers and software developers across the organization to deploy large-scale cloud-based solutions, implement and maintain scalable cloud infrastructures, optimize data pipelines, monitor, and identify issues in the cloud environment, enhance system performance and ensure data integrity and security. 

Key Responsibilities Include

Work with team and project management to manage priorities, deadlines, and deliverables including collaborative identification and assignment of tasks. 
Implement data solutions according to design documentation using a variety of tools and programming languages, like GIT, Kafka, SQL and no-SQL databases, object storage, Python, Typescript/Javascript, C++, Go etc., and following the team’s established processes and methodologies, like SCRUM or Kanban. 
Participate in incident response and troubleshooting issues for Imagery data pipelines and API services. 
Learn and share code quality practices, and participate in code reviews, retrospectives, functional and integration testing, and other team activities focused on improving quality of delivery. 
Assist in developing any technical documentation needed to accurately represent application design and code. 
Gain understanding of the business operations and functions for the product(s) owned within the team. 
Collaborate with data steward, stake holders, and platform engineers to optimize data pipelines for performance, reliability, and cost-effectiveness. 
Actively seek opportunities to discover new and better solutions. 

Requirements

Bachelor’s degree in Computer Science, Software Engineering, Data Science or related field including geospatial, environmental, remote sensing/earth observation or other STEM related disciplines 
Experience engineering data intensive solutions using streaming and/or resource based (i.e. API) design principles 
Experience developing pipeline solutions for deploying to cloud environments like Amazon Web Services, Google Cloud Platform, Azure, etc using respective cloud services like GCS/S3, GCE/EC2, Cloud Functions/Lambda, Pub/Sub/SQS/SNS, GKE/EKS, etc. 
Demonstrated understanding of data architecture and modeling, including designing both logical and physical models for datasets 
Proven experience writing queries and building data structures from cloud-based datastores like AWS Aurora Postgres, Google BigQuery, Elasticsearch, etc. 
Knowledge of at least one NoSQL database such as Elasticsearch, Neo4j, Cassandra, DynamoDB, Spanner, etc. 
Experience with containerization technology and orchestration platforms such as Docker and Kubernetes. 
Experience with monitoring and logging tools such as Grafana, Prometheus, ELK stack, Datadog or equivalent. 
Strong interpersonal skills and desire to work in a fast-paced and highly collaborative environment. 

Desirable Qualifications

Highly proficient in Golang or Python and the respective geospatial libraries associated with each (i.e. GoDAL/GDAL, Rasterio, PyProj, PDAL, PySTAC, etc.) 
Hands on experience developing  HTTP APIs (Open API, REST, gRPC, and/or GraphQL) 
Experience with processing UAV (drone) or satellite imagery for modeling and analysis 
Experience with cloud-based machine learning platforms (i.e. AWS Sagemaker , Vertex AI, etc.) and familiarity with data science practices , tools, and libraries like Jupyer Notebooks, TensorFlow, PyTorch, Scikit-Learn, Pandas, etc. 
Demonstrated experience with Continuous Integration, Continuous Delivery (CICD) concepts and applications like GitHub Actions, Argo, etc. 
Proven interest/curiosity in agriculture, life sciences, bioinformatics, biochemistry, environmental sciences, biology, or other STEM related disciplines 
Experience working with varied geospatial datasets and formats like WKT, geojson, cloud-optimized geotiffs(COGs), OGC services, etc. 
Familiarity with SpatioTemporal Asset Catalog (STAC) specification and implementations 
Experience building Software Development Kit (SDKs) that advance the adoption of specific software or resource capabilities 
Hands-on experience with Infrastructure as Code (IaC) tools such as Terraform. 
Experience with standard cloud authentication and authorization patterns 

TekWisen® Group is an equal opportunity employer supporting workforce diversity. 

TekWissen is an emerging global human capital, recruitment and IT services organization. Operating since 2009, we draw upon more than a decade of staffing experience to deliver critical talent acquisition solutions and IT engagements for our clients. We’re founded on a culture that is passionate about delivering tailored solutions, that create lasting partnerships.

Our global footprint covers six countries: United States, Canada, Australia, India, United Kingdom and the Philippines. This allows us to work in close partnership with organizations and manage everything from global talent needs with demanding resourcing strategies, to single sites with lower recruitment volumes.

TekWissen® is an equal opportunity employer supporting workplace diversity.","Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Autenticación, Ciencias de la computación, Datasets, Documentación técnica, Imágenes satelitales y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3961557826/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=llkyCJOOQUclQ0tiDC5F9g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer I,"65 US$K/año - 83 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Beverly, NJ","Acerca del empleo
LOCATION 4287 Route 130 S Edgewater Park NJ US 08010

Overview

Come join our growing team of data practitioners and be on the leading edge of Burlington’s digital transformation! Burlington is seeking a self-driven and highly motivated individual to join a dynamic team with a passion for data, software, and engineering. At Burlington you will have the opportunity to work with the latest technologies in a goal-oriented environment.

As a Data Engineer I you will be a member of the Enterprise Data and Analytics team supporting business areas including Merchandising, Allocations, Marketing, IT and Supply Chain Analytics teams with insights gained from analyzing Burlington and external data. To be successful in this position you will have strong experience pulling data from various internal and external data sources and preparing it for advanced analytics, segmentation, and modeling. Additionally, you should have strong interpersonal and relationship building skills as well as written and verbal communication skills.

Experience:

3-5 years of experience in designing and implementing large scale data loading, manipulation, processing, analysis, and exploration solutions
Experience developing SQL based data processing
3+ years of experience with Data Architecture, Data Warehouse, Data Lake, Data Marts and Data Stores with focus on AI/ML techniques
Experience with Snowflake, Oracle Databases, Azure/AWS and ADLS

Skills and Abilities:

Technical expertise with pulling and massaging data
Great understanding of first/third party data
Agile Development methodology
Database Normalization
Advanced SQL skills
Understanding of data management principles and processes
Passion for data, analytics and pushing business innovation

Education:

Bachelor’s or master’s degree in Computer Science / Engineering, Informatics, or related areas

Come join our team. You’re going to like it here!

You will enjoy a competitive wage, flexible hours, and an associate discount. Burlington’s benefits package includes medical, dental and vision coverage including life and disability insurance. Full time associates are also eligible for paid time off, paid holidays and a 401(k) plan.

We are a rapidly growing brand and provide a variety of training and development opportunities so our associates can grow with us. Our teams work hard and have fun together! Burlington associates make a difference in the lives of customers, colleagues, and the communities where we live and work every day. Burlington Stores, Inc. is an equal opportunity employer committed to workplace diversity.

\

Posting Number 2024-225572

Location US-NJ-Edgewater Park

Address 4287 Route 130 S

Zip Code 08010

Workplace Type Hybrid

Position Type Regular Full-Time

Career Site Category Corporate

Position Category Information Technology

Evergreen Yes

Min USD $65,000.00/Annual

Mid USD $83,000.00/Annual","Almacenamiento de datos, Arquitectura de datos y Data Marts, Ciencias de la computación, Data Loading, Edición de imágenes, Lagos de datos, Modelado de datos, Snowflake y Snowflake cloud",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3875515250/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=MQ9tZK2rz%2BMCsu%2F7Y5mLZQ%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist-AI/ML Engineer,"90 US$K/año - 115 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Wilmington, DE","Acerca del empleo
Best Egg is the leading financial confidence platform that provides flexible solutions to real-life challenges for people with limited savings. We leverage real-time customer insights and data science to connect more people with the right products for their financial needs. Our offers include a growing suite of products such as personal loans, a credit card, and flexible rent, which are complemented by a suite of financial health tools to help customers make smart financial decisions and stay on track, so they can be money confident no matter what life throws at them.

Our culture and values inspire our employees and customers to embrace Best Egg. We are committed to championing a culture of inclusiveness and diversity of thought, and we focus on providing a safe, flexible, and collaborative work environment. Our employees are encouraged to engage in creative problem solving, and we promote opportunities for growth and enrichment across the organization.

If you are inspired by inspiring others, Best Egg is the place for you.

Best Egg promotes diversity and equal opportunity. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the better we will grow.

We are seeking a Data Scientist who can identify business problems, translate insights into actionable recommendations, develop AI/ML stack solutions, drive test/learn plans, present recommendations, and independently deploy these solutions into production – with the capability to lead, coach, mentor or upskill other team members to do the same. The ideal candidate has a high sense of urgency, a creative mind, great communication skills, and a strong background in programming and AI/ML.

Qualifications

Bachelor’s degree in Statistics, Mathematics, Operational Research, Computer Science/Engineering, Data Science or other quantitative major. Masters or PhD is preferred. Other quantitative fields with experience in building Cloud Services/Architect solutions, and data/CI/CD pipelines for AI/ML solutions will be considered. 
2+ years of work experience in AI/ML engineering. 
Experience with building and deploying AI/ML stacks in cloud production environments/containers in ECS/EKS – including data pipeline, CDK/SDK, Cloudformation/Terraform, and familiar with AWS services and various ML/Deep Learning tools. 
Intermediate to advanced experience in AI/ML modeling in Python/R. 
Strong SQL background
Experience with dev tools – Git, JIRA, etc. 
Ability to work effectively independently and as a team member. 
Excellent English written and verbal communication skills; ability to translate technical concepts to a non-technical audience. 
Nice to have, but not required: experience in the financial industry. 

Responsibilities

Partner with various business units to develop AI/ML solutions aimed at optimizing and automating business tasks relating to, but not limited to, underwriting, pricing, customer service, fraud, verification, loss forecasting, and/or operational strategies. 
Spearhead the development of AI tools for Best Egg
Build end-to-end AI/ML pipelines including data ingestion, extraction, feature engineering, model training and inference, performance evaluation and implementation. Remain engaged in R&D efforts to stay current with AI/ML advancements. 
Take ownership of a rich library of AI/ML tools and enjoy authority to continue to research efficient and effective strategies. 
Use Best Egg’s deep data sets (application, bureau, web, and alternative data) to derive actionable insights. 
Assess the capabilities and values of 3rd party vendors. 
Contribute to a culture focused on continuous learning and celebrate frequent wins, while providing valuable insights to inform decisions across the entire Best Egg organization. 

$90,000 - $115,000 a year

This position is also eligible for an annual incentive bonus based on individual and company performance. Yearly incentive bonus target 20% of base salary.

Our Brand: 

At Best Egg, we believe money should be accessible so people can reach their goals, live a fuller life, and feel pride in knowing they have taken control of their finances. For those who need extra money to achieve the progress they seek in life, Best Egg is the modern solution-minded finance provider that mixes decades of banking experience with smart technology and deep customer insight to create products designed for today’s borrower, so that people can establish a smoother financial path. The egg symbolizes protection and a fresh start.

Employee Benefits

Best Egg offers many additional benefits for our employees, including (but not limited to):

 Pre-tax and post-tax retirement savings plans with a competitive company matching

program

 Generous paid time-off plans including vacation, personal/sick time, paid short--

term and long-term disability leaves, paid parental leave, and paid company

holidays

 Multiple health care plans to choose from, including dental and vision options
 Flexible Spending Plans for Health Care, Dependent Care, and Health

Reimbursement Accounts

 Company-paid benefits such as life insurance, wellness platforms, employee

assistance programs, and Health Advocate programs

 Other great discounted benefits include identity theft protection, pet insurance,

fitness center reimbursements, and many more!

In compliance with the CCPA, Best Egg is fully committed to handling the personal information and data of employees and job applications responsibly with respect and due care. Review our CCPA Employee Policy here","Análisis predictivo, Aprendizaje automático, Ciencia de datos, Inteligencia artificial, Procesamiento de lenguaje natural, Python y SQL, Comunicación, Comunicación oral y Inglés",Solicitar
https://www.linkedin.com/jobs/view/3982901991/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=tc0HmrCjj%2BaPFQhFrjqcTQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"118,9 US$K/año - 205,6 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 6 días,"Bellevue, WA","Acerca del empleo
Description

AWS Databases, Analytics and AI/ML Products and Services is one of the largest and fast growing business unit within AWS. We are working to rebuild and revolutionize data engineering and business intelligence systems within database, analytics and AI/ML organization to support fast growing business needs.

We are looking for experienced, self-driven Data Engineer. In this role, you will be building complex data engineering and business intelligence applications using AWS big data stack. You should have deep expertise and passion in working with large data sets, data visualization, building complex data processes, performance tuning, bringing data from disparate data stores and programmatically identifying patterns. You should have excellent business acumen and communication skills to be able to work with business owners to develop and define key business questions and requirements. You will provide guidance and support for other engineers with industry best practices and direction. Amazon Web Services (AWS) has culture of data-driven decision-making, and demands timely, accurate, and actionable business insights.

Key job responsibilities

 Design, implement, and support data warehouse / data lake infrastructure using AWS big data stack, Python, Redshift, Quicksight, Glue/lake formation, EMR/Spark/Scala, Athena etc.
 Develop and manage ETLs to source data from various commercial, sales and operational systems and create unified data model for analytics and reporting
 Use business intelligence and visualization software (e.g., Quicksight.) to develop dashboards those are used by senior leadership.
 Empower technical and non-technical, internal customers to drive their own analytics and reporting (self-serve reporting) and support ad-hoc reporting when needed.
 Develop deep understanding of vast data sources and know exactly how, when, and which data to use to solve particular business problems.
 Work with Product Managers, Finance, Service Engineering Teams and Sales Teams on day to day basis to support their new analytics requirements.
 Manage numerous requests concurrently and strategically, prioritizing when necessary
 Partner/collaborate across teams/roles to deliver results.
 Mentor other engineers, influence positively team culture, and help grow the team.

A day in the life

Our team puts a high value on work-life balance. It isn’t about how many hours you spend at home or at work; it’s about the flow you establish that brings energy to both parts of your life. We believe striking the right balance between your personal and professional life is critical to life-long happiness and fulfillment. We offer flexibility in working hours and encourage you to find your own balance between your work and personal lives.

About The Team

Inclusive Team Culture

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Here at AWS, we embrace our differences. We are committed to furthering our culture of inclusion. We have ten employee-led affinity groups, reaching 40,000 employees in over 190 chapters globally. We have innovative benefit offerings, and we host annual and ongoing learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences. Amazon’s culture of inclusion is reinforced within our 14 Leadership Principles, which remind team members to seek diverse perspectives, learn and be curious, and earn trust.

Mentorship & Career Growth

Our team is dedicated to supporting new members. We have a broad mix of experience levels and tenures, and we’re building an environment that celebrates knowledge sharing and mentorship. Our senior members enjoy one-on-one mentoring and thorough, but kind, code reviews. We care about your career growth and strive to assign projects based on what will help each team member develop into a better-rounded engineer and enable them to take on more complex tasks in the future.

Basic Qualifications

 3+ years of data engineering experience
 Experience with data modeling, warehousing and building ETL pipelines

Preferred Qualifications

 Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions
 Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $118,900/year in our lowest geographic market up to $205,600/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon Development Center U.S., Inc. - B02

Job ID: A2707134","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Amazon Kinesis, Amazon Redshift, Base de datos orientada a grafos, Bases de datos, Informes ad hoc y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3928000311/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=QET1mtRdwu%2FS5jn7ncL6dw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 semana,Filadelfia y alrededores,"Acerca del empleo
Company Description

IntegriChain is the data and application backbone for market access departments of Life Sciences manufacturers. We deliver the data, the applications, and the business process infrastructure for patient access and therapy commercialization. More than 250 manufacturers rely on our ICyte Platform to orchestrate their commercial and government payer contracting, patient services, and distribution channels. ICyte is the first and only platform that unites the financial, operational, and commercial data sets required to support therapy access in the era of specialty and precision medicine. With ICyte, Life Sciences innovators can digitalize their market access operations, freeing up resources to focus on more data-driven decision support. With ICyte, Life Sciences innovators are digitalizing labor-intensive processes – freeing up their best talent to identify and resolve coverage and availability hurdles and to manage pricing and forecasting complexity.

We are headquartered in Philadelphia, PA (USA), with offices in: Ambler, PA (USA); Pune, India; and Medellín, Colombia. For more information, visit www.integrichain.com, or follow us on Twitter @IntegriChain and LinkedIn.

Job Description

Responsible for designing, developing, and optimizing data pipeline in AWS/Snowflake/Agile data engine environment to manage incremental data ingestion, data migration, data staging, loading, update, archive and restatement etc. 
Responsible for building data model and data catalog and maintaining data integrity across different platforms. 
Work with Agile methodologies to ensure delivering quality solutions to the problems without compromising on the planned deadlines. 
Work with UI team to develop OLAP Datamart and Views for data reporting. 
Comfortable providing technical solutions and guidance to other team members. 
Maximize the opportunity to excel in an open and recognizing work culture. Be a problem solver, leader, and a team player to make significant contribution to the achievements. 

Qualifications

Minimum 2 years’ work experiences as a data engineer, experiences in data modeling, data warehouse and ETL. 
Working knowledge on AWS cloud services and especially Snowflake data warehouse is a MUST. 
Having at least 2 years of programming experience with Python, SQL, Spark is a MUST. 
Experience in writing relatively complex DB queries (any relational DB) is a MUST. 
Experience in building solutions with containerization like Docker is nice to have. 
Experience in using Lucidchart or other similar tools is nice to have. 
Comfortable with Agile methodologies, such as Scrum, Kanban
Key competencies required: Problem-Solving, Analytical, Collaboration, and Accountability
An influencer by always advocating for technical excellence and innovation while being open to change when needed. 
Resilient in ambiguous situations and can approach challenges from multiple perspectives. 
AWS® and Snowflake* certification would be an advantage. 

Additional Information

What does IntegriChain have to offer?

Mission driven: Work with the purpose of helping to improve patients' lives! 

Excellent and affordable medical benefits + non-medical perks including Student Loan Reimbursement, Flexible Paid Time Off and Paid Parental Leave
401(k) Plan with a Company Match to prepare for your future
Robust Learning & Development opportunities including over 700+ development courses free to all employees

IntegriChain is committed to equal treatment and opportunity in all aspects of recruitment, selection, and employment without regard to race, color, religion, national origin, ethnicity, age, sex, marital status, physical or mental disability, gender identity, sexual orientation, veteran or military status, or any other category protected under the law. IntegriChain is an equal opportunity employer; committed to creating a community of inclusion, and an environment free from discrimination, harassment, and retaliation.

Our policy on visa sponsorship for US based positions: Applicants for employment in the US must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the US by IntegriChain.","Almacenamiento de datos, Apache Spark, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Integridad de la información, Modelado de datos, Resolución de problemas y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3966736229/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=A8VJEaGwTFYq1w29n4vLpA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - Collision Avoidance System,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 4 días,"Foster City, CA","Acerca del empleo
The Collision Avoidance System (CAS) is responsible for detecting and reacting to imminent collision situations in support of our vehicle’s overall safety goals. CAS Perception is responsible for processing raw sensor data from our vehicle’s world-class sensor suite using a combination of geometric, interpretable algorithms and deep learning to detect near-collisions with obstacles along our intended driving path, in the most challenging dense urban environments and under tight compute resource constraints. Overall CAS is parallel and complementary to our Main AI autonomy stack, and has a close relationship with our vehicle hardware and safety teams in order to architect redundancy into our overall driving system.

As a Data Engineer on the Collision Avoidance system, you will play a crucial role in driving dat-driven decision making by ensuring the availability of high-quality, reliable data and metrics for the team and Zoox as a whole. By doing so, you will contribute significantly to making autonomous vehicles safer for all passengers.

In This Role, You Will

Design the data pipelines to support Zoox’s machine learning systems and data mining at scale
Develop and maintain ETL (Extract, Transform, Load) processes for data ingestion and transformation to make data readily available for ML models training and validation.
Maintain high quality data pipelines implementing data quality checks, ensuring data consistency and accuracy and adherence to data engineering best practices.
Design and implement data models and data storage solutions for fast, reliable and user-friendly data querying.
Build self-serve data dashboards for quick fact checking and ongoing reporting purposes.
Collaborate with data scientists, software engineers, and other stakeholders to ensure that the data pipelines meet the requirements for machine learning models and make recommendations for changes or upgrades.
Collaborate with legal, infrastructure, platform teams to develop effective solutions that aligns with data access, retention, privacy protection policies and regulations.

Qualifications

BS/MS degree in a technical field
Experience designing and building complex data infrastructure at scale
Advanced Structure Query Language (SQL) and data warehousing experience
Experience operating a workflow manager such as Airflow
Experience with large scale streaming platforms (e.g. Kafka, Kinesis), processing frameworks (e.g. Spark, Hadoop) and storage engines (e.g. HDFS, HBase)

Bonus Qualifications

Exceptional Python or Scala skills
Basic fluency in C++
Familiarity with or exposure to experimentation platforms
A strong DataOps mindset and opinions on next-generation warehousing tools

Compensation

There are three major components to compensation for this position: salary, Amazon Restricted Stock Units (RSUs), and Zoox Stock Appreciation Rights. The salary will range from $164,000 - $265,000. A sign-on bonus may be part of a compensation package. Compensation will vary based on geographic location, job-related knowledge, skills, and experience.

Zoox also offers a comprehensive package of benefits including paid time off (e.g. sick leave, vacation, bereavement), unpaid time off, Zoox Stock Appreciation Rights, Amazon RSUs, health insurance, long-term care insurance, long-term and short-term disability insurance, and life insurance.","Airflow, Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Calidad de datos, Lenguaje de consulta (query) y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3967684161/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=W8Hf0GL%2F1jN402nZ42ycug%3D%3D&trk=flagship3_search_srp_jobs,AWS DE-Data Engineer - US,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 semanas,Estados Unidos,"Acerca del empleo
Role: AWS DE-Data Engineer

Location: NY preference, remote is also fine

Duration: Fulltime

Job Description

Experience Level: 8 to 14

Must Have

AWS Cloud, SQL, Data Engineering, Python","Almacenamiento de datos, Base de datos relacional, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Hojas de estilos en cascada (CSS)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3977245756/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=aVg784nCMOF26uC5zKYH3Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Databricks),"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Foster City, CA","Acerca del empleo
Expected compensation: $72.00 - $77.00 per hour

HireArt is helping an on-demand, autonomous ride-hailing company hire an experienced Databricks Administrator to manage and maintain company data stack tools.

We are looking for a Databricks Administrator who will be responsible for administering and maintaining different workspaces on Databricks running on AWS.

Responsibilities

Run Databricks at a workspace and account level. 
Set up and advise on architecture and scaling for the Databricks environment, including administering, configuring and installing libraries. 
Collaborate with data scientists, analysts, and other stakeholders to understand and deliver data solutions. 
Troubleshoot end user and platform-level issues. 
Document and maintain Databricks best practices and standards. 
Manage/monitor the following:
Tenants, including workspace creation, user management, cloud resources, and account usage. 
Cluster and jobs, policies, templates, and pools configuration options. 
Auto-scaling to ensure resources are allocated efficiently. 
Workspace users and groups, including single sign-on, provisioning, and access control to workspace storage accounts across a large user base. 
Requirements

5+ years of experience working as Databricks administrator/architect
Strong fluency with Python or Java
Advanced understanding of SQL to extract data from the databases
Must have hands-on experience with the following:
Serving as the Databricks account owner, including security and privacy setup, marketplace plugins and integration with other tools
Unity Catalog migration, workspaces and audit logs
Amazon Web Services (AWS) accounts and high-level usage monitoring
Hadoop and EMR administration
Preferred Qualifications

Experience optimizing usage for performance, including monitoring cluster health checks and cost
Experience with Looker/LookML 

Benefits

Pre-tax commuter benefits 
Employer (HireArt) Subsidized healthcare benefits
Flexible Spending Account for healthcare-related costs
HireArt covers all costs for short and long term disability and life insurance
401k package

Commitment: This is a full-time, 6-month, ongoing contract position staffed via HireArt. This role is hybrid requiring at least 3 days onsite. It will be available to candidates who are local to the Foster City, CA area.

HireArt values diversity and is an Equal Opportunity Employer. We are interested in every qualified candidate who is eligible to work in the United States. Unfortunately, we are not able to sponsor visas or employ corp-to-corp.","Almacenamiento de datos, Amazon Web Services (AWS), Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Python, Azure Databricks, Bases de datos, Java y LookML",Solicitar
https://www.linkedin.com/jobs/view/3889959724/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=TzMKF9bI508mlJNpr1AXdw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer with Machine learning and Python,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 3 meses,"Georgia, Estados Unidos","Acerca del empleo
Data Visualization: Creating visual representations of data to extract insights and communicate findings effectively.

Predictive Analysis: Analyzing historical data to make predictions about future outcomes.

Statistical Modeling: Building mathematical models to analyze relationships within data.

Data Preprocessing: Cleaning, transforming, and preparing data for analysis.

Clustering and Classification: Grouping data points into clusters or assigning them to predefined classes.

Time Series Analysis and Forecasting: Analyzing time-dependent data and making predictions about future values.

Machine Learning: Designing and implementing algorithms that enable computers to learn from data.

Deep Learning Algorithms: Utilizing neural networks with multiple layers to solve complex problems.

Python Packages

Pandas: For data manipulation and analysis.

Numpy: For numerical computing with arrays and matrices.

Matplotlib: For creating static, interactive, and animated visualizations in Python.

Scikit-Learn: For machine learning algorithms and tools.

Scipy: For scientific computing and advanced mathematics.

Spacy: For natural language processing (NLP) tasks.

TensorFlow: For building and training deep learning models.

PySpark: For working with big data and distributed computing using Apache Spark with Python.","Análisis predictivo, Extraer, transformar y cargar (ETL), Ingeniería de datos , Matplotlib, Modelos estadísticos, Pandas (Software) y Python, Ciencias de la computación, Manipulación de datos y Matemáticas",Solicitar
https://www.linkedin.com/jobs/view/3960149577/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=9eSPx%2F3jdJ9Kw5%2BGH%2BixjQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 6 días,"Chicago, IL","Acerca del empleo
JobID: 44987

Job Title: Data Engineer

Location: Remote

Duration: 6 + month contract

Responsibilities

Analyze and understand data from disparate sources – these include databases, raw files, other structured data
Work with Architects to design logical and physical models to house this data
Create migration scripts that will allow this data to move from these source systems to these new structures
Develop any code (stored procedures, triggers, views, etc) that are needed to make the process as efficient as possible
Improve system performance by evaluating current processes to find inefficiencies
Work with business partners to define needs and objectives
Collaborate across multiple systems and groups","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Procedimientos de almacenado",Solicitar
https://www.linkedin.com/jobs/view/3975421066/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=rV5KxzJmNV%2B%2BhIyARaOS5A%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst III (BI Test Engineer),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 2 semanas,"Dallas, TX","Acerca del empleo
How will this role have an impact?

The Data Analyst reports to the Sr. Director of Business Intelligence and Analytics. The Data Analyst is the sole point of contact responsible for the data quality initiatives of the Data Management organization with respect to all healthcare data produced by the organization. The Data Analyst will help establish the vision for data quality standards that are maintained by development teams within the Data Management organization. The Data Analyst will work closely with the Business Analysts to ensure that data contracts between data suppliers (external and internal) and the Data Management Team are satisfied as well as that the data contracts for data pushed downstream to internal consumers of the data are established and communicated. The Data Analyst will also work closely with the leadership of Business Intelligence to ensure that semantic layer processing is in line with high data quality standards and clear business approval/review. The Data Analyst will work closely with other leaders in the company to help formulate strategic healthcare data partnerships where new data must be ingested, processed and disseminated downstream into new and existing reporting or new and existing data contracts. The Data Analyst will also help formulate the strategy for what value lies in the new and existing data in order to achieve insightful solutions. The Data Analyst will review CMS-related changes that may impact the use of data in the organization and translate that information into tactical approaches to be taken by the development teams. The Data Analyst will be responsible for collaborating with team members to collect and analyze data. The Data Analyst will be responsible to structure large datasets to find usable information and be able to work with the team of analysts and other associates to process information. The Data Analyst will also help establish KPIs to measure the effectiveness of business decisions and create reports and presentations based on recommendations and findings.

Qualifications

Education/Licensing Requirements:

High school diploma or equivalent.
Bachelor’s degree in Healthcare related discipline, Management Information Systems, Computer Science or related field or equivalent work experience.
Master’s degree in business or technology preferred
Healthcare experience is required; preferably within payer organizations and CMS; prior CI or nursing experience will be considered

Experience Requirements

Strong experience interfacing directly with members of executive and senior management
Self-driven – able to identify data quality initiatives in the organization, work with the VP of Data Management and other Directors on prioritizing, and lead those initiatives while providing frequent updates on status
A minimum of 3 years of experience in the data quality space, ideally within business intelligence applications and data warehousing solutions
A minimum of 3 years of experience in the healthcare space with key positions in being accountable for high levels of data quality and accuracy.
A minimum of 3 years of experience in working with CMS or other healthcare-related agencies as it pertains to use of healthcare data
Minimum of 3 years of experience in working with users to understand business processes and data quality-affecting requirements
Has experience in reporting progress of team activities to leadership with clear and concise attention to work performed, value provided and initiatives completed
Has demonstrated significant skill required to work effectively across internal functional areas in situations where clear parameters may not exist

Essential Skills

Fluently speak, read, and write English
Coding skills in languages such as SQL, Python, and/or R
Analytical and Problem-Solving skills
Knowledge of data gathering, cleaning and transforming techniques
Reporting and data visualization skills using software like Tableau
Understanding of data warehousing and ETL techniques
Passion for solving complex system and data challenges and desire to thrive in a constantly innovating and changing environment
Excellent interpersonal skills, including teamwork and negotiation
Excellent external client-facing skills for assessing data-related impacts and new business
Superior analytical abilities, problem solving skills, technical judgment, risk assessment abilities and negotiation skills
Proven ability to prioritize and multi-task
Advanced skills in MS Office

Essential Characteristics

Self-directed and organized
Discrete/ability to maintain confidentiality
Team player
Detail-oriented
Sense of urgency
Customer service orientation
Ability to work under pressure
Ability to work well independently
Ability to take direction

Essential Job Responsibilities

Supports the Customer Success team and Operations as needed
Establishes standards for data quality
Assist in the design and development of data-quality frameworks
Develop, direct and manage efforts related to data quality initiatives
Leads system subject matter experts and the Business and Data Analysts to establish data-quality related requirements.
Establishes best practice standards for data quality initiatives
Builds or leads the development of data quality related solutions that present the user stories on the level of data quality
Establishes the vision and implements best practices for a data quality environment

Additional Job Responsibilities

Maintains a neat, orderly work area
Performs other incidental and related duties as required

Working Conditions

Fast-paced environment
Requires working at a desk and use of a telephone and computer
Use office equipment and machinery effectively
Ability to ambulate to various parts of the building
Work effectively with frequent interruptions
May require occasional overtime to meet project deadlines
Lifting requirements of <20 lbs. occasionally

The base salary hiring range for this position is $ 92,300 to $160,800. Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, and benefits.

In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. Eligible employees may enroll in a full range of medical, dental, and vision benefits, 401(k) retirement savings plan, and an Employee Stock Purchase Plan. We also offer education assistance, free development courses, paid time off programs, paid holidays, a CVS store discount, and discount programs with participating partners.

About Us

Signify Health is helping build the healthcare system we all want to experience by transforming the home into the healthcare hub. We coordinate care holistically across individuals’ clinical, social, and behavioral needs so they can enjoy more healthy days at home. By building strong connections to primary care providers and community resources, we’re able to close critical care and social gaps, as well as manage risk for individuals who need help the most. This leads to better outcomes and a better experience for everyone involved.

Our high-performance networks are powered by more than 9,000 mobile doctors and nurses covering every county in the U.S., 3,500 healthcare providers and facilities in value-based arrangements, and hundreds of community-based organizations. Signify’s intelligent technology and decision-support services enable these resources to radically simplify care coordination for more than 1.5 million individuals each year while helping payers and providers more effectively implement value-based care programs.

To learn more about how we’re driving outcomes and making healthcare work better, please visit us at www.signifyhealth.com

Diversity and Inclusion are core values at Signify Health, and fostering a workplace culture reflective of that is critical to our continued success as an organization.

We are committed to equal employment opportunities for employees and job applicants in compliance with applicable law and to an environment where employees are valued for their differences.","Almacenamiento de datos, Analítica, Analítica de datos, Extraer, transformar y cargar (ETL) y Visualización de datos, Calidad de datos, Ciencias de la computación, Expertos en la materia, Historias de usuarios y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3967133066/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=peB9t38HL%2FcC2acgmOJxQA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"85 US$K/año - 92 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Madison, IN","Acerca del empleo
Hybrid - WILL NEED TO BE ONSITE at least 2 times a week in Madison, Indiana.

- No Sponsorship. 

The Data Engineer would leverage their expertise in engineering solutions that work with structured and unstructured datasets to optimize company-wide business problems, Machine Learning Models and bring data-driven solutions into production to make real business impact. The Data Engineer would build data pipelines that drive analytic solutions. This role requires deep understanding of data architecture, data engineering and a basic understanding of data analytics & data science techniques and workflows. The ideal candidate is a skilled data and software engineer with experience creating data products supporting analytic solutions for multiple teams, systems & products.

Essential & Secondary Functions:

Responsibility & Customer-Focused:
Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals using Azure Data Factory and SSIS
Have working experience with the Big Data technology like Azure.
Advanced working SQL knowledge and experience working with relational databases, DB2 databases, working familiarity with the variety of databases.
Experience writing complex T-SQL scripts in SQL Server Management Studio (SSMS) to interact with relational databases using DDL, DML, DQL, and TCL commands.
Advanced working Python or R knowledge.
Basic understanding of Machine Learning techniques.
Solve complex data problems to deliver insights that helps the organization’s business to achieve their goals.
Create data products for analytics team.
Prepare data for prescriptive and predictive modeling.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Advice, consult, mentor and coach other data and analytic professionals on data standards and practices.
Foster a culture of sharing, re-use, design for scale stability and operational efficiency of data and analytical solutions
Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve the organization’s productivity as a team.
Experience supporting and working with cross-functional teams in dynamic environment.
Partner with business analysts and solution architects to develop technical architectures for strategic enterprise projects and initiatives.
Responsible to follow and carry out corporate procedures, policies, guidelines, legal requirements, etc.

Integrity:
Use skills, training, and experience to provide a safe work environment for the production and on-time delivery of quality products.

Safety:
Responsible to follow all safety procedures and practices.
Use skills, training, and experience to provide a safe work environment for the production and on-time delivery of quality products

Quality:
Ensures and maintains the quality guidelines, procedures, and standards are consistently followed throughout the manufacturing operation.

Other Functions:
Required to follow any other instructions and to perform any other duties as requested by supervisor.

Competencies:

Safety Awareness
Quality Driven
Respectful
Honesty
Customer Driven
Family Oriented
Responsible
Innovative
Responsible Corporate Citizen
Team Player
Positive Attitude
Detail Oriented
Effective Communication (written and verbal)
Results Oriented
Process Improvement
Cooperation with Others","Analítica de datos, Ingeniería de datos , Python y SQL, Aplicaciones en la nube, Microsoft Azure y SSI",Solicitud sencilla
https://www.linkedin.com/jobs/view/3953272234/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=dewQgkh9SObbcMJk3SI2pQ%3D%3D&trk=flagship3_search_srp_jobs,Data Warehouse Engineer,"116 US$K/año - 130 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",Publicado de nuevo hace 2 semanas,"Bakersfield, CA","Acerca del empleo
Kern Energy is hiring a Data Warehouse Engineer who will design, construct, install, and maintain scalable data management systems. Collaborating with the IT Architect, business analysts, and other stakeholders, the engineer will understand data requirements and implement solutions to support the organization's needs. This role is crucial for ensuring the availability, reliability, and performance of the data infrastructure. Additionally, the engineer will optimize the information flow within the company to ensure data is accessible and reliable for analysis. The ideal candidate will embody Kern’s core values: Teamwork, Safety, Excellence, Integrity, and Connection.
Job Function 
Design and implement robust, scalable, and efficient data pipelines and architectures.
Work closely with IT Architect, project champions, and end users to understand data requirements and translate them into technical specifications.
Develop and maintain data integration processes to collect, process, and transfer data between systems.
Implement ETL (Extract, Transform, Load) processes for efficient data movement.
Manage and optimize databases for performance, reliability, and scalability.
Implement and maintain database schemas, indexes, and queries.
Implement data quality standards and governance processes to ensure the accuracy and consistency of data.
Monitor and troubleshoot data issues, implementing corrective actions as needed.
Implement and maintain security protocols for data storage, transmission, and access.
Ensure compliance with data protection and privacy regulations.
Collaborate with cross-functional teams including data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions.
Identify and implement optimizations to improve the performance of data processing and analytics.
Document data architecture, processes, and configurations for future reference and knowledge sharing.

Knowledge Skills
Strong proficiency in programming languages such as Python, R, DAX.
Expertise in working with databases (e.g., SQL, NoSQL) and data warehousing, data lakes, and/or data bricks, and/or Snowflake.
Experience with big data technologies like Hadoop, Spark, or Kafka.
Experience with data visualization tools (e.g., Tableau, Power BI).
Knowledge of cloud platforms such as Azure, AWS, or Google Cloud.
Knowledge of machine learning concepts.
Familiarity with data modeling and schema design.
Strong attention to detail and commitment to delivering high-quality solutions.
Demonstrated ability to manage time effectively and meet deadlines collaboratively in a team and independently.
Experience with unstructured data and correlating into relational data.
Manage projects with minimal supervision.
Excellent verbal, written, and social skills to communicate effectively with all levels of the organization.
Excellent problem-solving skills. 
Experience with cloud-based presentation services and on-prem reporting tools.
Education/ Experience 
B.S. in Computer Science Engineering, Information Technology, or relevant degree
2+ years of proven experience as a Data Engineer or in a similar role. 
Background in data warehouse design and data mining, including an in-depth understanding of Relational database design, is preferred.

Kern Energy is an equal-opportunity employer.
All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws and will not be discriminated against on the basis of disability.","Almacenamiento de datos, Amazon Web Services (AWS), Canalizaciones de datos, Google Cloud , Memoria de datos, NoSQL, Programación, SQL y Visualización de datos, Comunicación, Especificaciones técnicas, Modelado de datos y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3958572775/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=bC9ITx4yXX9hPzHRzu%2BZxQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - (Hybrid),"160 US$K/año - 200 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 1 semana,"Hoboken, NJ","Acerca del empleo
Have a Perm Role for Walmar

Pays 160-200k to the candidate

Position sits twice a week in Hoboken, NJ

Needs

PYTHON

SPARK

Senior level

pyspark

Nice To Haves

Tableau

GCP

Scala

IV Process

Live Code Pair

2 step Virtual client Iv

Job Description

They build algorithms and Pricing Models to show where anomalies are occurring. They also do platform develop for specific Platforms that do the same thing. Candidate will be responsible for designing, developing, testing, and deploying their code. A lot of the work also revolves around pipeline development and job development","Almacenamiento de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud , Ingeniería de datos y Scala, Bases de datos y LiveCode",Solicitar
https://www.linkedin.com/jobs/view/3977544358/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=au%2FswYsKxsunlcL2CKLo%2Bg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Hybrid) - 19888,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"San Antonio, TX","Acerca del empleo
Enlighten, honored as a Top Workplace from the Baltimore Sun, is a leader in big data solution development and deployment, with expertise in cloud-based services, software and systems engineering, cyber capabilities, and data science. Enlighten provides continued innovation and proactivity in meeting our customers’ greatest challenges.

We recognize that the most effective environment for your projects doesn’t always look the same. Our hybrid work approach ensures that you can make lasting relationships with your team and collaborate in-person to get the job done—while having the flexibility to be working from home when needed to achieve focused results.

Why Enlighten?

Benefits

At Enlighten, our team’s unwavering work ethic, top talent and celebration of innovative ideas have helped us thrive. We know that our employees are essential to our company’s success, so we seek to take care of you as much as you take care of us. Here are a few highlights of our benefits package:

 100% paid employee premium for healthcare, vision and dental plans.
 10% 401k benefit.
 Generous PTO + 10 paid holidays.
 Education/training allowances.

Job Description

Enlighten is looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets.The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is performed in a hybrid environment on customer site (on average 3 days a week) with a great team.

#Mid-Senior Level

Essential Job Responsibilities

The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past. 
To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation. 
The candidate will work both independently and as part of a large team to accomplish client objectives. 
Additional duties as assigned.

Minimum Qualifications

Security Clearance - Must have a current TS/SCI level security clearance; U.S. Citizenship required. 
5+ years of experience as a developer, analyst, or engineer with Bachelors in related field; 3 years relevant experience with Masters in related field; in related field; or High School Diploma or equivalent and 9 years relevant experience.
Experience with programming languages such as Python and Java.
Proficiency with acquisition and understanding of network data and the associated metadata.
Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
Experience with Kibana and Elasticsearch.
Familiarity with various log formats such as JSON, XML, and others.
Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
Ability to decompose technical problems and troubleshoot system and dataflow issues.
Must be able to work in a hybrid environment, spending an average of 3 days per week on customer site in San Antonio, TX. However, flexibility is essential to accommodate any changes in the schedule.

Preferred Requirements

DoD 8140 / 8570 compliance certifications may be required in this position as directed by the customer.
Experience with NOSQL databases such as Accumulo desired OR experience querying and optimizing SQL databases
Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.

We have many more additional great benefits/perks that you can find on our website at www.eitccorp.com [eitccorp.com].

Enlighten, an HII Company, is an Equal Opportunity/Veterans and Disabled Employer. U.S. citizenship may be required for certain positions. HII Is committed to cultivating an inclusive company culture to promote collaboration and enhance creativity by hiring a diverse work force.","Apache Kafka y JSON, Amazon S3, Amazon Simple Queue Service (SQS), Apache NiFi, Atención al detalle, Flujo de datos, Kibana, Soluciones de almacenamiento y TS",Solicitar
https://www.linkedin.com/jobs/view/3906647120/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=YKKhIZmRNK046IugYOaJ8w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 día,Salt Lake City y alrededores,"Acerca del empleo
With our strong investment in research and development and our culture of continuous improvement, Hexcel is the industry leader in the manufacturing of advance composite materials, including carbon fiber, woven reinforcements, resins, prepregs, honeycombs and additive manufactured parts. We invite you to join the Hexcel team at various manufacturing sites, sales offices and R&T centers around the globe. Become a part of the “strength within.”

Hexcel is currently seeking a hybrid Data Engineer for our Salt Lake City, UT, USA location. May also be located at Casa Grande, AZ, Decatur, AL, Seguin, TX, Pottsville, PA.

The selected individual will be responsible for but not limited to the following obligations:

Working with product owners to identify and design data solutions 
Engineering data pipelines to pull data from on premises and cloud data sources
Create & maintain optimized data pipeline architecture
Extracting data and consolidating with data lakes, data warehouse and data marts
Engineering models to connect different data sources together
Constructing a library of data components that can easily be reused to answer different business questions
Troubleshooting and fixing production issues

Qualifications:

Three plus years of relevant ETL, data engineering or similar experience preferred; open to various levels of experience
Bachelor’s degree or equivalent, relevant experien
Experience with cloud data platforms required
Experience with Azure SQL Server, Azure Synapse, Azure Analysis Services and Azure Data Factory highly desirable, but not essential
appetite for new technologies and methodologies
Strong understanding of Power BI platform and Power BI paginated
Design complex data sets to meet the needs of business requirements
Troubleshoot and optimize data pipelines
Strong knowledge of SQL; Understanding of DAX and/or MDX a plus
Experience with Databricks desirable, but not essential

This position is restricted to U.S. citizens due to U.S. federal government contracts that require the employment of only persons who are U.S. citizens. Hexcel (NYSE: HXL) is a global leader in advanced composites technology, a leading producer of carbon fiber, and the world leader in honeycomb manufacturing for the commercial aerospace industry.

Hexcel is an Equal Opportunity Employer of Minorities/Females/Protected Veterans/Individuals with Disabilities/Sexual Orientation/Gender Identity. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, physical or mental disability, status as protected veteran, or any other protected class.","Almacenamiento de datos, Analítica de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Necesidades empresariales y Resolución de incidencias",Solicitar
https://www.linkedin.com/jobs/view/3982247507/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=hpCKVxxKJTmZBB7Nsi2aYg%3D%3D&trk=flagship3_search_srp_jobs,HDE 2 (Data Engineer IV) #: 24-05067,"Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 4 días,"Mountain View, CA","Acerca del empleo
Job Title: HDE 2 (Data Engineer IV)

Job Location: Remote

Job Duration: 12 Months on W2

Summary 

The main function of a hardware engineer is to research, design, develop, test computer or computer-related equipment for commercial, industrial, military, or scientific use. 
A typical hardware engineer is responsible for the manufacture and installation of computer or computer-related equipment and components. 
Job Responsibilities:
Analyze information to determine plan layout, including type of computers and peripheral equipment modifications. 
Assemble and modify existing pieces of equipment to meet special needs. 
Build, test, and modify product prototypes using working models or theoretical models constructed with computer simulation. 
Evaluate factors such as reporting formats required, cost constraints, and need for security restrictions to determine hardware configuration. 
Monitor functioning of equipment and make necessary modifications to ensure system operates in conformance with specifications. 
Skills

Creativity, verbal and written communication skills, analytical and problem solving ability. 
Team player and detail oriented. 
Basic knowledge of design techniques, tools and principles involved in production of precision technical plans, blueprints, drawings and models. 
Basic knowledge of transmission, broadcasting, switching, control and operation of telecommunications systems. 
Basic knowledge of the practical application of engineering science and technology. 
Basic knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming. 
Previous experience with computer applications and design software related to engineering field. 

Top 3 Hard Skills Required &plus; Years of Experience

8-10&plus; Years of experience with C Sharp Required (Python would be nice to have) 
8-10 &plus; Years of Experience File Formatting XML or HTML or SQL (How the data is encoded within them & How to use them as a transport layer) 
8-10&plus; Years of Experience with data interpretation layers: Power BI content, R (programming language for statistical analysis), JMP 

Hard Skills Assessments

Expected Dates that Hard Skills Assessments will be scheduled: ASAP 
Hard Skills Assessment Process: The assessment process will include Panel with ~3 people on the team, 45mins-1hr 
Required Candidate Preparation: Candidates should be prepared to talk about their previous work

Education/Experience

Bachelor's degree in engineering required. 
8-10 years experience required.","Lenguajes de programación y Python, Análisis estadístico, C#, Configuración de hardware, Data Interpretation, Equipos informáticos, JMP, Resolución de problemas y Sistemas de telecomunicaciones",Solicitar
https://www.linkedin.com/jobs/view/3974931081/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=Vjbrs31dBDwxhybV9UUr7w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Locals Only),"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,"Woodlawn, MD","Acerca del empleo
T+S

USC/GC

 Selected candidate must reside within two (2) hours of SSA Headquarters in Woodlawn, MD 
 Selected candidate must be willing to work on-site at least 2 days a week. 

Key Required Skills

Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics

Position Description

Coordinate with EDW MI/BI Teams, EDW Technical Architects and EDW PMs in creating ETL documents as per the project requirements.
Collaborate with EDAs to understand the MI/BI requirements and support in data modeling needs.
Facilitate technical meetings with MI/BI teams and other required team members.
Provide design and development recommendations to MI/BI teams.
Proactively keep EDW PMs and Technical Leads up to date on any project issues or concerns or change of directions.
Able to multitask and support multiple projects
All other duties as assigned or directed. 

Detailed Skills Requirements

FOUNDATION FOR SUCCESS (Basic Qualifications)

Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Analytical and problem-solving skills.
Must be able to obtain and maintain a Public Trust. Contract requirement. 

FACTORS TO HELP YOU SHINE (Required Skills)

These skills will help you succeed in this position:

Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development (i.e., Tableau and other BI tools)
Strong working experience with ETL development and methodologies.
Strong oral and written communication skills and ability to communicate with all levels within the organization.
Strong data analysis and problem-solving skills.
Strong interpersonal skills with ability to collaborate with others effectively and efficiently. 

HOW TO STAND OUT FROM THE CROWD (Desired Skills)

Showcase your knowledge of modern development through the following experience or skills:

Experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.

Education

 Bachelor's degree in a training related field with 7+ years of experience
Must be able to obtain and maintain a Public Trust. Contract requirement.","Analítica de datos, Análisis de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL) y Gobierno de datos, Calidad de datos, Confianza ciudadana, DB2, Modelado de datos y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979448322/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=kVj3lnBVeIV9RpuNB2hVug%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer I - Enterprise Payment and Billing (Analyst),"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Hartford, WI","Acerca del empleo
Who Are We?

Taking care of our customers, our communities and each other. That's the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.

Job Category

Data Analytics, Technology

Compensation Overview

The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.

Salary Range

$105,100.00 - $173,400.00

Target Openings

1

What Is the Opportunity?

Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. This position is ideal for someone who enjoys doing Data Analysis and Data Profiling. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights. What Will You Do?

Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.
Design data solutions.
Analyze sources to determine value and recommend data to include in analytical processes.
Incorporate core data management competencies including data governance, data security and data quality.
Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.
Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.
Test data movement, transformation code, and data components.
Perform other duties as assigned.


What Will Our Ideal Candidate Have?

Bachelor's Degree in STEM related field or equivalent
Six years of related experience
Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices.
The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.
Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.
Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.
Strong verbal and written communication skills with the ability to interact with team members and business partners.
Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.


What is a Must Have?

Bachelor's degree or equivalent training with data tools, techniques, and manipulation.
Four years of data engineering or equivalent experience.


What Is in It for You?

Health Insurance: Employees and their eligible family members - including spouses, domestic partners, and children - are eligible for coverage from the first day of employment.
Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.
Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.
Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs.
Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.


Employment Practices

Travelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.

In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions.

If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an email so we may assist you.

Travelers reserves the right to fill this position at a level above or below the level included in this posting.

To learn more about our comprehensive benefit programs please visit http://careers.travelers.com/life-at-travelers/benefits/.","Arquitectura de datos, Gobierno de datos y Ingeniería de datos, Análisis de sistemas, Calidad de datos, Depuración de programas, Limpieza de datos, Perfiles de datos, Prácticas recomendadas en ingeniería de software y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982988898/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=oFuHICE99vLk6RwUMVJM9Q%3D%3D&trackingId=GYCEkOHnnFdhluE9wf6ncA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Fayetteville, AR","Acerca del empleo
Employment Type: Full-Time, Mid-level

Department: Business Intelligence

CGS is seeking a passionate and driven Data Engineer to support a rapidly growing Data Analytics and Business Intelligence platform focused on providing solutions that empower our federal customers with the tools and capabilities needed to turn data into actionable insights. The ideal candidate is a critical thinker and perpetual learner; excited to gain exposure and build skillsets across a range of technologies while solving some of our clients’ toughest challenges.

CGS brings motivated, highly skilled, and creative people together to solve the government’s most dynamic problems with cutting-edge technology. To carry out our mission, we are seeking candidates who are excited to contribute to government innovation, appreciate collaboration, and can anticipate the needs of others. Here at CGS, we offer an environment in which our employees feel supported, and we encourage professional growth through various learning opportunities.

Skills and attributes for success:

Complete development efforts across data pipeline to store, manage, store, and provision to data consumers
Being an active and collaborating member of an Agile/Scrum team and following all Agile/Scrum best practices
Write code to ensure the performance and reliability of data extraction and processing
Support continuous process automation for data ingest
Achieve technical excellence by advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
Work with program management and engineers to implement and document complex and evolving requirements
Help cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
Collaborate with others as part of a cross-functional team that includes user experience researchers and designers, product managers, engineers, and other functional specialists


Qualifications:

Must be a US Citizen
Must be able to obtain a Public Trust Clearance
7+ years of IT experience including experience in design, management, and solutioning of large, complex data sets and models
Experience with developing data pipelines from many sources from structured and unstructured data sets in a variety of formats
Proficiency in developing ETL processes, and performing test and validation steps
Proficiency to manipulate data (Python, R, SQL, SAS)
Strong knowledge of big data analysis and storage tools and technologies
Strong understanding of the agile principles and ability to apply them
Strong understanding of the CI/CD pipelines and ability to apply them
Experience with relational database, such as, PostgreSQL
Work comfortably in version control systems, such as, Git Repositories


Ideally, you will also have:

Experience creating and consuming APIs
Experience with DHS and knowledge of DHS standards a plus
Candidates will be given special consideration for extensive experience with Python
Ability to develop visualizations utilizing Tableau or PowerBI
Experience in developing Shell scripts on Linux
Demonstrated experience translating business and technical requirements into comprehensive data strategies and analytic solutions
Demonstrated ability to communicate across all levels of the organization and communicate technical terms to non-technical audiences


Our Commitment:

Contact Government Services (CGS) strives to simplify and enhance government bureaucracy through the optimization of human, technical, and financial resources. We combine cutting-edge technology with world-class personnel to deliver customized solutions that fit our client’s specific needs. We are committed to solving the most challenging and dynamic problems.

For the past seven years, we’ve been growing our government-contracting portfolio, and along the way, we’ve created valuable partnerships by demonstrating a commitment to honesty, professionalism, and quality work.

Here at CGS we value honesty through hard work and self-awareness, professionalism in all we do, and to deliver the best quality to our consumers mending those relations for years to come.

We care about our employees. Therefore, we offer a comprehensive benefits package:

Health, Dental, and Vision
Life Insurance
401k
Flexible Spending Account (Health, Dependent Care, and Commuter)
Paid Time Off and Observance of State/Federal Holidays


Contact Government Services, LLC is an Equal Opportunity Employer. Applicants will be considered without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.

Join our team and become part of government innovation!

Explore additional job opportunities with CGS on our Job Board:

https://cgsfederal.com/join-our-team/

For more information about CGS please visit: https://www.cgsfederal.com or contact:

Email: info@cgsfederal.com","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación, Datos no estructurados, Estrategia de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3981907002/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=76LdWVmQUAhVhXb1OQGK%2Fg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"125 US$K/año - 200 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Underdog.io is a modern recruiting platform. One of our hiring partners is looking to hire a Data Engineer. In this role, you'll work on tools, features, and applications that are critical to the company's needs with opportunities to switch teams and projects as you and the business grow and evolve. The ideal candidate will be versatile, display leadership qualities, and be enthusiastic to take on new problems as the company continues to push technology forward.

The company is looking to hire someone with (1) a Bachelor’s degree or equivalent practical experience, (2) experience with software development in one or more programming languages, and (3) experience with data structures or algorithms in either an academic or industry setting. The ideal candidate should also have experience building and developing large-scale infrastructure, accessible technologies, distributed systems or networks, and/or have experience with compute technologies.

On any given day, you will (1) write product or system development code, (2) participate in or lead design reviews with peers and stakeholders to decide amongst available technologies, (3) review code developed by other developers and provide feedback to ensure best practices, (4) contribute to existing documentation and adapt content based on product/program updates and user feedback, and (5) triage product or system issues and debug/track/resolve by analyzing the sources of issues and the impact on the system.

The company uses popular languages, frameworks, and tools, including JavaScript, Python, Golang, Scala, Ruby on Rails, SQL, AWS, Kubernetes, and Docker.

The Underdog.io team is available if you have any questions or would like to discuss other potential roles.

, C++,Python (Programming Language),Java,Data Products,Pipelines,Hadoop,MapReduce,SQL,Data Warehousing,Data Architecture","Almacenamiento de datos, Arquitectura de datos, Hadoop, MapReduce, Python y SQL, C++, Java, Productos de datos y Tuberías",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3812812801/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=dm7%2F4S%2F2zPtmOxuLqI9qfw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - (Full Time / Part Time),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 meses,"Los Ángeles, CA","Acerca del empleo
Data Engineer

Mavan is seeking a data engineer who will build data solutions for various use cases including tracking, reporting, product analytics, marketing optimization and financial reporting. By implementing dashboards, data pipelines, data structures, and data warehouse architecture, you will help shape how Mavan functions and empower clients to do the same.

Responsibilities

Work with business partners and stakeholders to understand data/reporting requirements
Work with engineering, analytics and marketing, and 3rd parties to build pipelines and collect required data
Design, develop and implement large scale, high volume, high performance data models and pipelines for Data Lake and Data Warehouse.
Develop and implement data quality checks, conduct QA and implement monitoring routines.
Build and implement ETL frameworks to improve code quality and reliability
Build and enforce common design patterns to increase code maintainability
Manage reliability and scaling of portfolio of pipelines and data marts
Document new and existing models, solutions, and implementations
Code new automations for common processes
Mentor and coach team members to improve their designs and solutions

Qualifications

7+ years of professional experience
5+ years experience working in data engineering, business intelligence, or a similar role
Proficiency in programming languages such as Python/Java
3+ years of experience in ETL orchestration and workflow management tools 
Expert in Database fundamentals, SQL and distributed computing
Experience working with Snowflake, BigQuery, Redshift, and/or PostgreSQL 
Familiarity with CDPs like Segment.io and mParticle
Familiarity with Data Pipelines such as Funnel.io, Fivetran, and Stitch
Excellent communication skills and experience working with technical and non-technical teams
Strong in Google tracking products - particularly Google Tag Manager and Google Analytics - with comfort setting up new instances from scratch 
Knowledge of reporting tools such as Tableau, Google Data Studio, and Looker
Comfortable working in fast paced environment, self starter and self organizing
Ability to think strategically, analyze and interpret market and consumer information

About MAVAN

MAVAN is a growth studio that operates across the entire funnel. We unlock growth through consumer/competitive research, creative strategy/production, paid acquisition/data and analytics, landing page/product optimization, lifecycle marketing, and technology. MAVAN's exclusive on-demand talent network of 250+ specialized experts, allows us to pull in specialists as needed to scale with the unique needs of any company. Our specialists are from some of the most successful tech and consumer companies, including Google, Apple, Uber, Square, Activision, Nike, Red Bull, Dropbox, and more.

We've unlocked multibillion-dollar businesses, scaled massive global teams, and collectively managed over $2.7 billion dollars in paid media over the last 5 years. Our process is informed by structured testing, competitive research, and detailed analysis informed by experts across the entire funnel.

We are profitable, have shown exponential growth over the last 2 years, and have a wide variety of benefits including:

Full Health, Medical, Dental, Vision
401k matching (up to 5%)
Unlimited Vacation
""No meeting Fridays"" (Experimenting with moving to a 4-day work week in 2023
All of the fun events, swag, joys of working at a fast growing start-up

Our Website

Explainer Video

DISCLAIMER: MAVAN will only contact you via LinkedIn or email using the mavan.com domain for job openings and job offers. Any communication from other domains, applications, or platforms is NOT from the MAVAN team and is not representative of any communication with the MAVAN team. If you receive any communication from parties pretending to be MAVAN using domains other than mavan.com, MAVAN is not responsible for the communications contained within. If you suspect someone is impersonating the MAVAN team, please forward those communications to legal@mavan.com.","Data Marts, Extraer, transformar y cargar (ETL), Google BigQuery, Google Data Studio y Ingeniería de datos, Amazon Redshift, Gestión de flujos de trabajo, Modelo de datos, Requisitos de información y Snowflake",Solicitar
https://www.linkedin.com/jobs/view/3804452253/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=gHkvMDZW5B%2FyAoRRBtIdng%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 7 meses,"San Diego, CA","Acerca del empleo
A NauWork client is seeking a Data Engineer to join their team. The position is fully remote or hybrid based in San Diego, California.

This client is a leading medical staffing agency with a mission to help others live better by helping healthcare professionals and the patients they serve. They’ve received multiple awards and accolades for “Best Places to Work” from companies like Glassdoor and Modern Healthcare.

As a Data Engineer II with a specialization in MS Power BI, you will lead the development and maintenance of data-driven solutions, creating compelling visualizations, and ensuring data integrity. Your deep understanding of data infrastructure and data visualization will help drive strategic decisions, streamline operations, and empower our team with actionable insights.

Responsibilities:

Data Pipeline Development & Management:

Design, construct, install, and maintain large-scale processing systems and other infrastructure. 
Manage and optimize data pipelines, ensuring data availability, accuracy, and optimal performance. 


MS Power BI Development & Management:

Develop, maintain, and optimize Power BI dashboards and reports tailored to business needs. 
Collaborate with stakeholders to identify opportunities for data-driven decision-making and to define metrics and KPIs. 
Ensure consistency and integrity of data visualizations across all Power BI reports. 


Data Analysis & Optimization:

Work with cross-functional teams to gather requirements, understand business challenges, and provide data-driven solutions. 
Continuously analyze data processes and tools for improvement and scalability. 


Data Governance & Integrity:

Collaborate with data governance teams to ensure data quality, compliance, and consistency. 
Develop and maintain documentation on data pipelines, data models, and data dictionaries. 


Team Collaboration & Leadership:

Collaborate with IT, analytics, and business teams to ensure seamless integration of systems and tools. 


Required Experience:

5+ years of experience in data engineering with a strong emphasis on data visualization. 
Bachelor’s degree in computer science, engineering, information systems, or a related field. 
Proven expertise in MS Power BI development, including DAX, data modeling, and performance tuning. 
Strong experience in SQL, ETL processes, and data warehouse design. 
Familiarity with cloud platforms like Azure or AWS. 


Preferred Experience:

Experience in the healthcare or recruiting industry. 
Familiarity with data governance principles and practices. 
Excellent communication skills, both written and verbal. 
Strong analytical and problem-solving skills with a keen attention to detail. 


To Learn More:

503-388-9585 
833-NAU-WORK 
nauwork.com/careers 


Category: Technology - System Software

Position: Data Engineer

Location: [Remote] San Diego, California

Job Type: Direct-Hire, Full-Time","Almacenamiento de datos, Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Ciencias de la computación, Comunicación y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982462774/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=wVGwK7CYpR5xmk%2FarsWAwA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (L5),"170 US$K/año - 720 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 5 días,"Los Gatos, CA","Acerca del empleo
Netflix is one of the world’s leading entertainment services with 278 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.

The Role

At Netflix, our mission is to entertain the world. With 278 million paid members in over 190 countries on millions of devices; enjoying TV series, documentaries, and feature films across a wide variety of genres and languages - Netflix is reinventing entertainment from end to end. We are revolutionizing how shows and movies are produced, pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet, and continuously improving the end-to-end user experience with Netflix across their member journey.

We pride ourselves on using data to inform our decision-making as we work towards our mission. This requires curating data across various domains such as Growth, Finance, Product, Content, and Studio. All of this data collection and curation is made possible thanks to the amazing Data Engineers of Netflix who bring this data to life.

Data Engineering at Netflix is a role that requires building systems to process data efficiently and modeling the data to power analytics. These solutions can range from batch data pipelines that bring to life business metrics to real-time processing services that integrate with our core product features. In addition, we require our Data Engineers to have a rich understanding of large distributed systems on which our data solutions rely. Candidates should have knowledge across several of these skill sets and usually need to be deep in at least one. As a Data Engineer, you also need to have strong communication skills since you will need to collaborate with business, engineering, and data science teams to enable a culture of learning. Learn more about the work of data engineers at Netflix.

Location of work: We are considering candidates who are willing to relocate to Los Gatos, California, as well as fully-remote candidates (remote in the US with occasional visits to Los Gatos) depending on the team your skills are most aligned with.

Who are you?

You strive to write elegant code, and you're comfortable with picking up new technologies independently
You are proficient in at least one major programming language (e.g. Java, Scala, Python) and comfortable working with SQL
You enjoy helping teams push the boundaries of analytical insights, creating new product features using data, and powering machine learning models
You have a strong background in at least one of the following: distributed data processing or software engineering of data services, or data modeling
You are familiar with big data technologies like Spark or Flink and comfortable working with web-scale datasets
You have an eye for detail, good data intuition, and a passion for data quality
You appreciate the importance of great documentation and data debugging skills
You relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedback
You are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risks

Our compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for is $170,000 - $720,000.

Netflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.

Netflix is a unique culture and environment. Learn more here.

We are an equal-opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.

Job is open for no less than 20 days and will be removed when the position is filled.","Ciencia de datos y Scala, Calidad de datos, Datasets, Depuración de programas, Desarrollo de software, Modelado de datos, Nimble, Procesamiento de datos y Servicios de datos",Solicitar
https://www.linkedin.com/jobs/view/3974635606/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=ESJ5eXdGmoSk9GrcjOdx5A%3D%3D&trk=flagship3_search_srp_jobs,Data Scientist - Python Data Analyst/ Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Copper Canyon, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Location: Remote must work Eastern hours

Long Term

Interview Process: 2 rounds 1 st interview w/ Sr. Data Engineer and 1 other person on the team 1-1.5hrs VERY technical and will include live coding exercises in Python. 2 nd interview panel w/ members from the team minimal tech questions, mainly team fit questions.

This is will be a backfill role for a candidate we currently have on billing with this team, resume attached.

Python skills + Data Analytics. Proven ability to work independently.

With the Sr. Lead Data Engineer on the team out on maternity leave, they have not been able to keep pace on their own.

But we need to jump on this to get candidates over by eod today so we can schedule interviews next week !

This role will work very close with the Sr. Engineer on the team to clean up the database and ensure data ethnicity and validity.

End goal of this role/ project clean up that databases. Cleaning up low hanging names that are not linked correctly in the database, determining if naming conventions need to be linked together or not, to ensure accurate data matches. Currently the security alerting system is not fully providing all of the security matches and proper licensing requirements and expirations notifications because there are multiple data/ naming conventions in the system that are not correct, therefore there are no notifications being generated.

Looking for someone that is not only a self-starter but a self-learner that can think outside of the box. someone that feels comfortable and confident to bring up ideas around what they can do to improve the quality and/or processes.

Must be a good communicator and researcher need to really dig in and come up with creative solutions. This role will be A LOT of researching!

Responsible for pulling data from the API in Python, doing Data analysis on what comes back from the API.

This role will be focused on natural language processing. Example how names can be matched to each other and knowing how to do that kind of work in Python.

Need to be good at explaining thought processes, researching, presenting findings their case and the data/ research behind it.

Full Job Description

Data Analyst/ Engineer - Python

Collaborate with team members to analyze data

Maintain database/metadata and adherence to conventions and data governance

Clean up current backlogged data management items

Eventually, become an SME in the field of software component analysis

Work with leadership to identify current data management issues and opportunities for improvement in the current process used to ratify data

Devise and implement solutions to speed up workflow in data cleaning in Python

Qualifications - Required

Intermediate practical knowledge of using regex and fuzzy matching to do string similarity mapping

3+ years of hands-on experience with data analysis tools including Python using the Pandas module

3+ years of experience using APIs in Python, including managing unstructured data

Experience using secured APIs (credentialed/tokenized)

Intermediate practical experience using command-line Linux

Qualifications - Desired

Bachelor's degree in science, engineering, statistics, mathematics, economics, or other fields related to the position

Intermediate practical general knowledge of Linux, including commonly used software packages and Distros/Operating Systems

Experience with MongoDB or other NoSQL databases is a plus

|

,

5208 Windsor Ln, Copper Canyon, Texas, 75077","Analítica de datos, Análisis de datos, Pandas (Software), Procesamiento de lenguaje natural y Python, Datos no estructurados, Expresiones regulares, Limpieza de datos, Presentaciones y Resolución creativa de problemas",Solicitar
https://www.linkedin.com/jobs/view/3946963929/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=zQWPeEAmaGR11myFBCLujg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 4 días,"Chicago, IL","Acerca del empleo
Who We Are

Apex Fintech Solutions (AFS) powers innovation and the future of digital wealth management by processing millions of transactions daily, to simplify, automate, and facilitate access to financial markets for all. Our robust suite of fintech solutions enables us to support clients such as Stash, Betterment, SoFi, and Webull, and more than 20 million of our clients' customers.

Collectively, AFS creates an environment in which companies with the biggest ideas in fintech are empowered to change the world. We are based in Dallas, TX and also have offices in Austin, New York, Chicago, Portland, and Belfast.

If you are seeking a fast-paced and entrepreneurial environment where you'll have the opportunity to make an immediate impact, and you have the guts to change everything, this is the place for you.

AFS has received a number of prestigious industry awards, including:

2021, 2020, 2019, and 2018 Best Wealth Management Company - presented by Fintech Breakthrough Awards
2021 Most Innovative Companies - presented by Fast Company
2021 Best API & Best Trading Technology - presented by Global Fintech Awards

About This Role

The Data Engineer will operate, maintain, and improve the data quality, accuracy, timeliness, accessibility, and processing speed to meet Apex Fintech Solution's goal in enhancing financial services through technology. By working on innovative data solutions, this role supports the company's mission to empower dynamic fintech initiatives and change the industry landscape. The Data Engineer will bridge the complex world of data with the company's strategic objectives, making information a powerful tool for business transformation.

Duties/Responsibilities:

 Design, build, and optimize data pipelines, ensuring robustness and scalability for handling vast amounts of data
 Develop and maintain real-time database systems and data warehousing solutions that align with business needs
 Collaborate with stakeholders to understand data requirements and implement systems that provide critical business insights through data analytics
 Enhance data quality and reliability by implementing modern data management techniques and technologies
 Drive continuous improvement by identifying and implementing system optimizations to reduce downtime and improve data processing efficiency
 Document all data architecture processes and systems while ensuring compliance with industry standards and company policies
 Support data platform operations in all environments during business hours. 

Required Skills/Abilities:

 Proficient in Python and minimum of 2+ years’ experience with Cloud Data technologies. 
 Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy
 Experienced in ETL processes using micro services

Education and/or Experience:

 Bachelor's degree in Computer Science, Computer Engineering, or a related field. 
 At least 2 years of experience in data engineering with a proven track record in data management systems
 Experience in the financial services industry preferred but not required

Work Environment:

 This job operates in a modern hybrid office environment
 Collaborative team settings with open communication channels across various departments
 Fast-paced atmosphere that values creativity, innovation, and results, with a strong emphasis on data-driven decision making

#engineering #full-time #APEX #associate

Our Rewards

We offer a robust package of employee perks and benefits, including healthcare benefits (medical, dental and vision, EAP), competitive PTO, 401k match, parental leave, and HSA contribution match. We also provide our employees with a paid subscription to the Calm app and offer generous external learning and tuition reimbursement benefits. At AFS, we offer a hybrid work schedule for most roles that allows employees to have the flexibility of working from home and one of our primary offices.

Diversity, Equity, Inclusion, and Belonging (DEIB) Commitment

We're looking for all kinds of people.

At Apex, we believe that wealth management and investing should be accessible to everyone, and we strive to create spaces to democratize investing for folks of all walks of life. Internally, we embrace diversity and are dedicated to creating an inclusive and equitable workplace, which reflects our company vision and mission. We value every team member's unique perspective and are committed to fostering a culture where everyone belongs. Join us in our mission to empower and celebrate individual differences.

Apex is committed to being an equal opportunity employer. We ensure that qualified applicants receive fair consideration for employment without discrimination based on sex, gender identity, gender expression, sexual orientation, race, color, natural or protective hairstyle, genetics, religion, national origin, disability, protected veteran status, age, or any other characteristic protected by law. Know your rights: workplace discrimination is illegal. We stand by this commitment to promote a diverse, equitable, and inclusive workforce.","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Gestión de datos, Ingeniería de datos y Python, Calidad de datos, Ciencias de la computación y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3970385841/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=7g2%2BdN%2F9OvaZlMReEKs9jw%3D%3D&trk=flagship3_search_srp_jobs,IT-Data Engineer,"90 US$K/año - 110 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Jacksonville, FL","Acerca del empleo
Job Description

SNI Technology’s top client is seeking to hire a talented Data Engineer for an exciting full-time, permanent opportunity! This is a remote opportunity but candidates must be willing to work normal business hours on eastern standard time.

This position is not open to C2C or non-US permanent residents.

Job Description

Data Engineer

The Data Engineer is responsible for designing, building and optimizing systems for data collection, storage, access, and analytics at scale.

Technical Skills, Knowledge, Abilities

Minimum 3-5 years of data engineering experience. 
Experience with Snowflake, SQL, Python and Java. 
Work on SQL performance measuring, query tuning, and database tuning 
Work on Data Masking / Encryption / Tokenization, Data Wrangling / ECreLT / Data Pipeline orchestration (tasks). 
Setup AWS S3/EC2, Configure External stages and SQS/SN 
Preferred hospitality industry experience with expertise in point-of-sale systems and experience with rolling up individual businesses (and multiple systems) into a consolidated, corporate platform 
Bachelor's Degree is required 

Compensation & Benefits:

The compensation for this position is in the range of $90k-$110k annually + bonus and benefits for this direct-hire opportunity. Please note your actual pay rate will be determined based upon your skills, knowledge and abilities including work experience - talk with your recruiter to learn more.","Ingeniería de datos y SQL, Ajuste de consultas, Autenticación, Bases de datos, Data Masking, Disputas de datos , Optimización de bases de datos, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3967146933/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=dn25HV830DRagcLReQf78g%3D%3D&trk=flagship3_search_srp_jobs,Remote Data Quality Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Austin, TX","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Remote Data Quality Engineer needs 8 years experience

Remote Data Quality Engineer Requires

Bachelor's degree in computer science or related field.

Data engineer with testing knowledge

PySpark/Python

Snowflake Or Other Data Warehouse Experience

AWS

Data engineer with QE experience

Remote Data Quality Engineer Duties

Develop and maintain data pipelines using ETL processes.
Take responsibility for Apache Hadoop development and implementation.
Work closely with data science team to implement data analytics pipelines.
Help define data governance policies and support data-versioning processes.

Employment Type: Full-Time","Aseguramiento de la calidad, Calidad de datos y Control de calidad",Solicitar
https://www.linkedin.com/jobs/view/3984786043/?eBP=BUDGET_EXHAUSTED_JOB&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=ABciA88E5hcsLhS1Pj7sXg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (AWS),"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Santa Clara, CA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Role: Data Systems Engineer

Key Skills: Python, Java, AWS, Azure

Location: Santa Clara, CA ( onsite only from Day 1 )

Expected annual pay for this role ranges from $66,000 to $112,000.

Based on the position, the role is also eligible for Client's standard benefits including a full range of medical and dental benefits options, disability insurance, paid time off (inclusive of sick leave), other paid and unpaid leave options.

Role Description

Bachelor's or Master's degree in computer science or information systems, or equivalent experience with programming knowledge (i.e Python, Java, etc)
Strong experience with systems, managing VMs, and tools and Adept at queries, reporting and summarizing findings.
Experienced in SQL
Significant experience as a catalyst in building and improving enterprise-class systems
Technical expertise regarding data models, database design development, data analytics and segmentation techniques
Experienced with cloud environments (AWS, Azure, etc)
Ability to communicate optimally with business users and translate business needs into technology solutions. Ability to connect with all team members successfully and effectively.
High energy with a positive problem-solving attitude, and strong multi-tasking and interpersonal skills.
Independent and motivated individual who requires little supervision and demonstrates good judgment and decision-making skills
Strong analytical skills, logical problem solver, results driven with the ability to collect, organize, and disseminate significant amounts of information accurately and at a detailed level.
Knowledge in operational processes in chips, boards, systems, and servers with a view of the data landscape
Mandatory Skills:
Strong ability to drive continuous improvement of systems and processes.
Ability to communicate at all levels of the organization and turn complex ideas into simple to understand solutions
Self-starter, self-confident individual with integrity and accountability, highly motivated, driven, high-reaching, and attracted to a meaningful opportunity.
A proven ability to work in a fast-paced environment where strong organizational skills are essential.
Data science / ML experience

Qualifications: Bachelor's Degree","Arquitectura de datos, Ciencia de datos y Ingeniería de datos, Aptitudes de organización, Comunicación, Confianza en uno mismo, Modelado de datos, Modelo de datos, OpenVMS y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3918927210/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=6du9sMihMI2cWKsRGQjpRA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 meses,"Bellevue, WA","Acerca del empleo
Summary: 

We are seeking an experienced, self-driven, analytical, and strategic Sr. Data Engineer. In this role, you will be working in one of the world's largest and most complex data warehouse environments. You should be passionate about working with huge datasets and be someone who loves to bring data together to answer business questions. You should have deep expertise in creation and management of data lake and the proven ability to guide continuous enhancement of data architecture by identifying efficiency gaining solutions and leveraging evolving new technologies. In this role, you will have ownership of end-to-end development of data engineering solutions to complex questions and you’ll play an integral role in strategic decision-making.

The right candidate will possess excellent business and communication skills, be able to work with business owners to tackle ambiguous business questions with creative data/science solution designs, and be able to collaborate with BIEs and DSs to build those solutions or answer business questions. You should have a solid understanding of how to build efficient and scalable data infrastructure and data models, and have the capability or the desire to learn and implement Elastic MapReduce (EMR)-based solutions where appropriate.

Key job responsibilities:

Design, implement, and support an analytical data infrastructure providing ad hoc access to large datasets and computing power
Managing AWS resources including EC2, RDS, Redshift, etc.
Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies
Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency
Collaborate with BIEs to recognize and help adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation
Collaborate with DS to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learning
Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers

Compensation:

The pay rate range above is the base hourly pay range that Aditi Consulting reasonably expects to pay someone for this position (compensation may vary outside of this range depending on a number of factors, including but not limited to, a candidate’s qualifications, skills, competencies, experience, location and end client requirements).

Benefits and Ancillaries:

Medical, dental, vision, PTO benefits and ancillaries may be available for eligible Aditi Consulting employees and vary based on the plan options selected by the employee.","Almacenamiento de datos, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Datasets, Informes y análisis, Lagos de datos, Modelado de datos y Modelo de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3959057126/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=gx8GDLmSgNRmW%2FRF%2BdJKyQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Databricks),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Chicago, IL","Acerca del empleo
Title 

Data Engineer (Databricks)

Overview

This is a full-time, employee position. Please do not apply if you are seeking a C2C or 1099/W2 contract. 

 Must be located in one of the following: Atlanta, Chicago, Columbus, Dallas, Minneapolis, New York Area, St. Louis. This position is mostly Remote. 

Daugherty Business Solutions brings a fresh approach to data engineering by delivering results through unmatched innovation and world-class technology and talent. This is why many of the most well-known companies in the world trust us with their mission-critical projects. As a team member at Daugherty, you will play an integral role in our company’s success and are recognized and valued for your contributions. We have an entrepreneurial culture with the maturity and security of a 35+ year company helping you to create your best work and take your career to the next level.

We are seeking a skilled and experienced Data Engineer with expertise in Databricks, Azure, and Spark to join our dynamic team. The ideal candidate will play a key role in designing, developing, and maintaining scalable data pipelines and analytics solutions to support our growing business needs. This position offers an exciting opportunity to work with cutting-edge technologies and collaborate with cross-functional teams to drive actionable insights from data.

Responsibilities

 Design, develop, and deploy end-to-end data pipelines using Databricks, Azure Data Factory, and Spark to ingest, process, and analyze large volumes of structured and unstructured data. 
 Collaborate with data scientists, analysts, and other stakeholders to understand business requirements and translate them into technical solutions. 
 Optimize and tune data pipelines for performance, reliability, and scalability to ensure efficient processing of data. 
 Implement data governance and security best practices to ensure the integrity and confidentiality of data. 
 Develop and maintain documentation, including data flow diagrams, technical specifications, and user guides. 
 Stay up-to-date with emerging technologies and best practices in data engineering, cloud computing, and big data analytics. 


Qualifications

 Proven experience as a Data Engineer, with at least 5 years of hands-on experience in designing and building data pipelines. 
 Proficiency in Databricks, Azure services (e.g., Azure Data Lake Storage, Azure Synapse Analytics), and Spark for big data processing and analytics. 
 Strong programming skills in Python, Scala, or Java, with experience in writing complex SQL queries. 
 Experience with data modeling, ETL/ELT processes, and data warehousing concepts. 
 Excellent problem-solving skills and attention to detail, with the ability to troubleshoot and debug complex data engineering issues. 
 Strong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. 
 Able to articulate ideas clearly, present findings persuasively, and build rapport with clients and team members. 
 Ability to effectively communicate technical concepts to non-technical stakeholders .
 Relevant certifications in Databricks, Azure, or Spark (e.g., Azure Data Engineer, Databricks Certified Associate) are a plus. 
 Bachelor's degree in Computer Science, Engineering, or related field; Master's degree preferred. 


 What We Commit to YOU: 

 We provide many training opportunities like certifications, hackathons, lunch and learns and free access to Pluralsight, Udemy and other digital learning platforms. 
 You will get to work with some of the most innovative teams in the IT marketplace and solve real strategic problems. 
 We will invest in things that are important to you professionally and personally. 
 We will build a relationship with you to accelerate your career. 
 We will provide you with a team environment like no other. We are consistently ranked as a Top Workplace in many of our regions as voted by our employees. 
 We provide opportunities to build community, be social and have fun with your colleagues. 
 We provide a comprehensive compensation and benefits package. 


Daugherty Business Solutions is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law.

If you require accommodations or assistance to complete the online application process, please inform any recruiter you are working with (or send an email to careers@daugherty.com) and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The recruiting team will respond to your email promptly.","Almacenamiento de datos, Apache Spark, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , SQL y Scala, Modelado de datos, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3962891710/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=N%2FdcYE14GDpf2vQrQt6Pvw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer / Machine Learning Engineer,"120 US$K/año - 130 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Houston, TX","Acerca del empleo
Data Engineer / Python Data Engineer 
 Key Responsibilities: 
Python
Artificial intelligence
AI
Google Cloud Platform
Google Cloud Platform
DevOps
Kubernetes / Docker

Experience:
Python
Artificial intelligence
AI
Google Cloud Platform
Google Cloud Platform
DevOps
Kubernetes / Docker
Design and Develop Data Pipelines
Machine Learning Integration
Data Modelling and Data Warehouse

My client is the largest distributor of Electricity and Gas across Central America. They are searching for a Data Engineer with a particular focus on Google Cloud Platform to join their team.

For more information, please apply directly.","Almacenamiento de datos, Big data, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Microsoft Power BI y Minería de datos, Bases de datos y Modelado de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3935002786/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=%2FHriLvYg7o49eOWIQNFa3g%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"180 US$K/año - 200 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,Estados Unidos,"Acerca del empleo
About Us

Since 2016, dbt Labs has been on a mission to help analysts create and disseminate organizational knowledge. dbt Labs pioneered the practice of analytics engineering, built the primary tool in the analytics engineering toolbox, and has been fortunate enough to see a fantastic community coalesce to help push the boundaries of the analytics engineering workflow. Today there are 30,000 companies using dbt every week, 100,000 dbt Community members, and over 4,100 dbt Cloud customers. You can learn more about our values here.

About the role:

As a Senior Data Engineer at dbt Labs, you will take the lead in building and owning data infrastructure (e.g. infrastructure, pipelines, data products). This data ecosystem is crucial for powering analyses, guiding business decisions, accelerating growth, and driving efficiency across business operations. This team combines strategic, operational, and problem-solving skills with a pragmatic sense of how to get things done and drive change across the organization.

In this role, you can expect to:

Design, build and manage our data pipelines, ensuring all user and product event data is seamlessly integrated into our data warehouse. 
Develop canonical datasets to track key product metrics including user growth, engagement, and revenue. 
Work collaboratively with various teams, including Infrastructure, Product, Marketing, Finance, and GTM to understand their data needs and provide solutions. 
Implement robust and fault-tolerant systems for data ingestion and processing. 
Participate in data architecture and engineering decisions, bringing your strong experience and knowledge to the table. 
Ensure the security, integrity, and compliance of data according to industry and company standards. 

You are a good fit if you have:

Worked asynchronously as part of a fully-remote, distributed team. 
Have 5+ years of experience as a data engineer and 8+ years of any software engineering experience (including data engineering). 
Proficiency in at least one programming language commonly used within Data Engineering, such as Python, Scala, or Java. 
Strong data infrastructure and data architecture skills. 
Expertise with any of ETL schedulers such as Airflow, Dagster, Prefect or similar frameworks. 
Solid understanding of Spark and ability to write, debug and optimize Spark code. 
A bias for action and urgency, not letting perfect be the enemy of the effective. 
A “full-stack mindset”, not hesitating to do what it takes to solve a problem end-to-end, even it it requires going outside the original job description. 

You'll have an edge if you have:

Experience developing and scaling a dbt project while leveraging engineering best practices (e.g. data quality tests, unit tests, etc)
Master’s degree in a quantitative field (e.g., Computer Science, Engineering, Statistics, Math). 
Have experience at a SaaS company. 

Compensation & Benefits 

Salary: $180,000 - $200,000 USD
Equity Stake
Benefits: In the US, dbt Labs offers unlimited vacation (and yes we use it!), 401k w/3% guaranteed contribution, excellent healthcare, paid parental leave, wellness stipend and a home office stipend. For employees outside the United States, dbt Labs offers a competitive benefits package
Benefits - dbt Labs offers:
Unlimited vacation (and yes we use it!)
401k w/3% guaranteed contribution
Excellent healthcare
Paid Parental Leave
Wellness stipend
Home office stipend, and more!
Equity or comparable benefits may be offered depending on the legal limitations

What to expect in the hiring process (all video interviews unless accommodations are needed):

Interview with Talent Acquisition Partner
Interview with Hiring Manager
Team Interviews

Who We Are

At dbt Labs, we have developed strong opinions on how companies should practice analytics.

Specifically, we believe that:

Code-based transformations offer unmatched flexibility and transparency across various “multi-player” development to power everyone in the organization to collaborate on a common language
Data analysts should adopt similar practices and tools to software developers
Critical analytics infrastructure should be controlled by its users as open source software
Analytic code itself — not just analytics tools — will increasingly be open source

It turns out that a lot of other people believe this too! Today, there are 30,000 companies using dbt every week, 100,000 dbt Community members, and 4,100 companies paying for dbt Cloud. Our customers include JetBlue, Hubspot, Vodafone New Zealand, and Dunelm. dbt is synonymous with the practice of analytics engineering, defining an entire industry. We’re backed by top investors including Andreessen Horowitz, Sequoia Capital, and Altimeter.

dbt Labs is an equal opportunity employer. We're committed to building an inclusive team that welcomes a diversity of perspectives, people, and backgrounds regardless of race, color, national origin, gender, sexual orientation, age, religion, disability, citizenship, veteran status, or any other protected status. We feel strongly that whether or not your experience exactly fits the job description, your passion and skills will stand out and set you apart even if your career has taken some twists and turns. If you are on the fence about whether you meet our requirements, we encourage you to apply anyway! Please reach out to us directly at recruiting@dbtlabs.com if you need assistance or an accommodation. 

Want to learn more about our focus on Diversity, Equity and Inclusion at dbt Labs? Check out our DEI page here.

dbt Labs reserves the right to amend or withdraw the posting at any time. For employees outside the United States, dbt Labs offers a competitive benefits package. Equity or comparable benefits may be offered depending on the legal or country limitations.

Privacy Notice

Supplement to Privacy Notice - Californians

Supplement to Privacy Notice - EEA/UK","Almacenamiento de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y Scala, Datasets, Desarrollo de software, Java y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3977206302/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=evmUHvNVY428jSIz1Z2SmA%3D%3D&trk=flagship3_search_srp_jobs,Data - BI Tableau Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Baltimore, MD","Acerca del empleo
BI (Tableau) Data Engineer 

 Baltimore, MD (Hybrid) 

Must

Must be able to obtain and maintain a Public Trust

Experienced BI (Tableau) Data Engineer

Must have 7+ years of overall experience

Strong working experience with BI Development (i.e., Tableau and other BI tools)

Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.

Strong working experience with SQL and other databases (i.e., DB2 and Oracle)

Strong working experience with ETL development and methodologies.

Bachelors degree and 7+ years of experience, 15 years without degree

Duties

Coordinate with EDW MI/BI Teams, EDW Technical Architects and EDW PMs in creating ETL documents as per the project requirements.

Collaborate with EDAs to understand the MI/BI requirements and support in data modeling needs.

Facilitate technical meetings with MI/BI teams and other required team members.

Provide design and development recommendations to MI/BI teams.

Proactively keep EDW PMs and Technical Leads up to date on any project issues or concerns or change of directions.

Able to multitask and support multiple projects

All other duties as assigned or directed.

 Quadrant is an affirmative action/equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, status as a protected veteran, or status as an individual with a disability.
Aptitudes y experiencia deseables
ETL, BUSINESS INTELLIGENCE, BI, TABLEAU, SQL","Arquitectura de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y SQL, Almacén de datos empresarial, Bases de datos, Calidad de datos, Confianza ciudadana, DB2 y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3830225224/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=y89AglcrZ4gUJL7xd14xYQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer MYMOVE | Home,"80 US$K/año - 120 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,"Charlotte, NC","Acerca del empleo
This role is not open to visa sponsorship or transfer of visa sponsorship including those on OPT and STEM-EXT OPT, nor is it available to work corp-to-corp.

The Movers team is seeking an experienced Data Engineer to join our growing Data Team. You'll play an instrumental role in developing core data infrastructure and reporting to help determine the efficacy of products for internal and external brands.

The Movers team helps customers during the moving process by offering assistance with change of address and services required at the new place. We guide you through updating your address with relevant organizations, help set up utilities, find reliable home service providers, provide local information, and offer ongoing support. Our goal is to ensure a smooth and seamless moving experience.

The right candidate for this role will be highly-organized and collaborative; with a passion for solving problems cross-functionally. But you won’t do it alone — you’ll have a collaborative cross-disciplinary team of developers, designers, product managers, and other engineers to support you.

What You'll Do

Working with a cross-functional team of data engineers, software engineers, data analysts, and data scientists to understand business requirements.
Contribute to key architectural decisions.
Conceptualizing and generating infrastructure that allows data to be accessed and analyzed effectively.
Documenting database design including data modeling, metadata, and process flow for business integrations.
Documenting technical ETL/ELT specifications for a data warehouse. Perform periodic code reviews and test plans to ensure data quality and integrity.
Design and build data pipelines from various sources to data warehouse using batch or incremental loading strategies utilizing cutting-edge cloud technologies like Fivetran, Redshift, Airflow, Databricks and others

What We're Looking For

Minimum of 4 years of experience in data engineering.
Good understanding of warehouse concepts
3 + years of sql experience
2+ years of experience working with cloud data warehouses.
2+ years of experience working on AWS or GCP
1+ years of experience working on Spark (RDDs / Data Frames / Dataset API)
2+ experience with GitHub and CI/CD processes (CircleCI, Jenkins, GitHub Actions).
Has experience working on Airflow, Prefect, or Dagster
Excellent understanding of development processes and agile methodologies
Enthusiastic, highly motivated, and ability to learn quick
Ability to work through ambiguity in a fast-paced, dynamically changing business environment

Even Better

Bachelor’s degree in Computer Science, Information Technology, or a related field.
Understanding of data analytics and basic knowledge of reporting tools

Compensation

Cash Compensation Range: $80,000-$120,000*
Note actual salary is based on geographic location, qualifications, and experience. 

Additionally, the following benefits are provided by Red Ventures, subject to eligibility requirements.

Health Insurance Coverage (medical, dental, and vision)
Life Insurance
Short and Long-Term Disability Insurance
Flexible Spending Accounts
Paid Time Off
Holiday Pay
401(k) with match
Employee Assistance Program
Paid Parental Bonding Benefit Program

Who We Are

Founded in 2000, Red Ventures (RV) is home to a diverse portfolio of industry-leading brands and businesses, strategic partnerships and proprietary technology – including Bankrate, Lonely Planet, CNET, The Points Guy, BestColleges and more. Together, RV helps millions of people worldwide make life’s most important decisions, accelerates digital adaptation, and innovates the online consumer experience by improving every step of the consumer journey – from first discovery of information, throughout the decision-making process, to transactions. Headquartered south of Charlotte, NC, Red Ventures employs thousands of people across the US and Puerto Rico, with international offices in the UK and Brazil. For more information, visit https://redventures.com and follow @RedVentures on social platforms.

We offer competitive salaries and a comprehensive benefits program for full-time employees, including medical, dental and vision coverage, paid time off, life insurance, disability coverage, employee assistance program, 401(k) plan and a paid parental leave program.

Red Ventures is an equal opportunity employer that does not discriminate against any employee or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or any other basis protected by law. Employment at Red Ventures is based solely on a person's merit and qualifications.

We are committed to providing equal employment opportunities to qualified individuals with disabilities. This includes providing reasonable accommodation where appropriate. Should you require a reasonable accommodation to apply or participate in the job application or interview process, please contact accommodation@redventures.com.

If you are based in California, we encourage you to read this important information for California residents linked here.

#MM

Click here for more details regarding the employee privacy policy: https://www.redventures.com/legal/us-emp-privacy-notice 

Questions about this Privacy Notice can be directed to employeerights@redventures.com. Alternatively, you may raise any questions or concerns to your manager, HR Business Partner, or through the Privacy Team.","Airflow, Analítica de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL), Google Cloud y Ingeniería de datos, Amazon Redshift, Ciencias de la computación, Modelado de datos y Necesidades empresariales",Solicitar
https://www.linkedin.com/jobs/view/3953549690/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=8wzWfX0yMQ6GR%2F%2Fijjx7Vw%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer, Analytics","46,63 US$/año - 134 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 2 semanas,"Seattle, WA","Acerca del empleo
Are you passionate about Facebook’s product, analytics and technology? The Analytics, Data Engineering team is looking for fast-moving analytics candidates and data junkies who want to make an impact. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will work with some of the brightest minds in the industry, and you'll have the opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.

Data Engineer, Analytics Responsibilities:

Architect, implement and deploy new data models and data processes in production
Perform data analysis to generate business insights
Interface with Engineers, Product Managers and Product Analysts to understand product goals and data needs
Build data expertise and own data quality for allocated areas of ownership
Manage data warehouse plans for a product or a group of products
Support critical data processes running in production

Minimum Qualifications:

Currently has, or is in the process of obtaining, a Bachelors or Masters degree in Computer Science, Mathematics, or related technical field
Programming knowledge in Python or Java
Knowledge of SQL
Knowledge of database systems
Must obtain work authorization in country of employment at the time of hire, and maintain ongoing work authorization during employment
Currently has, or is in the process of obtaining a Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience. Degree must be completed prior to joining Meta.

Preferred Qualifications:

Experience thriving in a fast-paced work environment
Curious, self-driven, analytical and excited to play with data
Experience in collaborating with individuals and organizations

About Meta:

Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps like Messenger, Instagram and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. People who choose to build their careers by building with us at Meta help shape a future that will take us beyond what digital connection makes possible today—beyond the constraints of screens, the limits of distance, and even the rules of physics.

Meta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.

Meta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.

$46.63/hour to $134,000/year + bonus + equity + benefits

Individual compensation is determined by skills, qualifications, experience, and location. Compensation details listed in this posting reflect the base hourly rate, monthly rate, or annual salary only, and do not include bonus, equity or sales incentives, if applicable. In addition to base compensation, Meta offers benefits. Learn more about  benefits  at Meta.","Almacenamiento de datos, Analítica de datos, Análisis de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Calidad de datos, Ciencias de la computación, Ingeniería informática y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3960510764/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=DsPaxRYH2Pfv21JNAznREw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer Intern/Co-op,"Presencial Prácticas
Coincide con tus preferencias de empleo. El tipo de empleo es Prácticas.
Prácticas",Publicado de nuevo hace 1 semana,"Brentwood, MO","Acerca del empleo
ABOUT YOU

Are you an out of the box thinker? Are you passionate about solving business problems with the latest technology? If the answer is, “Yes!” then we have an exciting, internship opportunity for you. Who are we? We are ARCO, a Family of Construction Companies.

We are looking for a Data Engineer Intern to work part-time during the school year with our Technology Data & Analytics team in Brentwood, MO.

WHAT WE CAN OFFER YOU

The internship program in ARCO’s Technology Department is designed to provide hands-on on enterprise environments in a practical setting. Students get an opportunity to gain insight and meaningful experience and develop additional soft skills necessary while working at ARCO. Work part time during the school year while attending college classes. Full time working hours available in the summer.

We are dedicated to the well-being of our associates and are proud to be consistently recognized as a Best Place to Work.

Competitive hourly rate 
Flexible hours to accommodate your class schedule
Medical, dental, and vision insurance coverage when you Intern or Co-op with us for at least 3 months full time
Company sponsored lunches, and events
Stocked kitchen with coffee, soda, and snacks

At ARCO, our first core value is to treat people fairly and do the right thing. We are committed to building and sustaining a culture that supports diversity and inclusion. We are proud to be an equal opportunity employer, and all qualified applicants will receive consideration for employment.

From recruiting, training, and hiring practices to selecting our subcontractors, we understand that diversity of all those involved in the construction process enhances our ability to deliver the best solutions to our customers. We hire the best and the brightest from across the country - constructing a team of experts in architecture, design, engineering, project management, and business services.

A DAY IN THE LIFE

Coordinate activities with data source application owners to ensure integration, data integrity, and data quality
Translate business requirements into specifications and solutions
Uses azure data tools and applied principles to assist in deploying integration and production code
Create, update, and maintain system based on incoming change requests and break-fixes
Interact with team members when developing interfaces and researching and troubleshooting technical issues

NECESSARY QUALIFICATIONS

Enrolled in an Undergraduate or Graduate program in Computer Science, Engineering, Information Systems, Statistics, or relevant work experience
Must be able to work part-time during the school year. 
Experience with SQL, Azure Synapse or Fabric, and ADF
Experience with Python preferred but not required 
Proficient in Microsoft Office programs 
Excellent written and verbal communication skills, experience working with dispersed teams
Willingness and ability to work independently, as well as collaborate in team settings
Ability to take ownership of assigned project from start to finish
Eager to learn and expand skillset
Experience with or desire to learn how to use APIs
Strong troubleshooting, diagnostic and analytical skills
Attention to detail, exceptional follow-through, the ability to prioritize, stay organized, and multi-task in a fast-paced environment

MAKE YOUR MOVE 

With 38 offices in major cities across the United States, we are one of the most dynamic and fastest-growing construction companies in the nation. As a leader in the design/build industry ranking #3 on the ENR's Top 100 Design-Build Companies list, we know that the best work requires the best talent - that’s why we are always looking for the best and brightest to join our team of experts.

At ARCO, we are committed to taking care of our greatest asset - our team. From Project Managers to Interns and Co-ops, ARCO is dedicated to creating a workplace in which associates feel valued and empowered to succeed. From paid sabbaticals to premier compensation and everything in-between, we pride ourselves in making ARCO, not only a great place to work but the best place to build a career. From the moment an associate begins his or her career at ARCO, the opportunity for success is limitless.

ARCO does not accept unsolicited resumes from individual recruiters or third-party recruiting agencies without pre-approval from ARCO’s Human Resource team. Pre-approval is required before any external candidate can be submitted. ARCO will not be responsible for fees related to unsolicited resumes and for candidates who are sent directly to our hiring managers.","Ingeniería de datos y SQL, Calidad de datos, Change Requests, Ciencias de la computación, Microsoft Azure, Necesidades empresariales, Oracle Application Development Framework, Resolución de incidencias y Textiles",Solicitar
https://www.linkedin.com/jobs/view/3957013196/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=9Yu2QBUpCted5f2ims%2F2ow%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,"Rockville, MD","Acerca del empleo
fusionSpan is a dynamic, fast-paced organization. We are a team of highly committed individuals who are inspired by the role technology plays in society. 
This position is 100% remote across the US. Working hours align with EST timezone.

A data engineer at fusionSpan will be part of the cross-functional Data & Integrations team at fusionSpan. This team handles all data analysis and ETL issues across multiple projects.The ideal candidate needs to be able to work autonomously and adapt to an evolving work structure.

This is a client facing role. You will also be responsible for capturing requirements from the client and converting those into requirement and technical documentation and acting as a liaison between the client and the development team.

Responsibilities
Utilize extract/transform/load ETL technologies
Interpret data, analyze results using statistical techniques and provide ongoing reports,
Acquire data from primary or secondary data sources and maintain databases/data systems,
Evaluate and optimize data structures,
Identify, analyze, and interpret trends or patterns in complex data sets,
Filter and “clean” data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems,
Produce field mapping and translation documentation for use in both manual and scripted migrations,
Work within Agile methodology managing tasks and tickets as assigned,
Document work and work processes for use by team members.
Required Qualifications
3+ years of experience with an enterprise ETL tool is required
3+ Experience using SQLand RDBMS is required
Experience with AWS/Azure cloud databases/data lakes
Experience working with Salesforce ETL
Mastery of Excel
What We Offer:
Health (PPO) dental & vision plan – 100% covered for employee
Long/Short-term disability insurance – 100% covered for employee
Life and AD&D insurance – 100% covered for employee
IRA with 3% matching contribution
15days of paid vacation – increases with tenure
10 paid federal holidays
12 weeks for parental leave","Almacenamiento de datos, Analítica de datos, Análisis de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Sistema de gestión de bases de datos relacionales, Bases de datos, Documentación técnica y Microsoft Azure",Solicitar
https://www.linkedin.com/jobs/view/3965652042/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=O3aV78gIEdChfWnXIbfP3Q%3D%3D&trk=flagship3_search_srp_jobs,Data Center Engineer II,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 5 días,Estados Unidos,"Acerca del empleo
Overview

As a Managed Service Center Tier II Datacenter Engineer, you will provide ePlus MSC and clientele with Data Center infrastructures (physical/virtual), storage, and LAN/SAN). Responsible for diagnostics, resolution, tracking, escalation of Data Center systems incidents and functionality within the production environment. As a customer-interfacing contact, you will work to solve pressing business issues and drive positive change. You will leverage technology, advance ePlus MSC services and provide feedback.

Responsibilities

CORPORATE VALUES

Respectful communication and cooperation: We prioritize respectful communication, fostering an environment where everyone is treated with dignity and respect.

Teamwork and employee participation: Collaboration and teamwork thrive through diverse perspectives, both within our teams and in our interactions with our customers.

Work/life balance that supports our employees’ varying needs: We value the well-being of our employees, recognizing that a healthy work-life balance is pivotal to our collective success.

Embracing communities: We embrace and support the communities that nurture us. Our employees' dedication to fostering positive change is a source of immense pride for us.

COMMITMENT TO DIVERSITY, INCLUSION AND BELONGING

We are an equal opportunity employer that does not discriminate or allow discrimination based on race, color, religion, sex, sexual orientation, gender identity, age, national origin, citizenship, disability, veteran status, or any other classification protected by federal, state, or local law.

ePlus is dedicated to fostering, cultivating, and preserving a culture that represents diversity, enables inclusion, and makes our employees feel comfortable bringing their full, unique selves to work.

YOUR IMPACT

The essential functions of this position include:

Service Delivery — Ensure IT assets are highly available and properly deployed in order to maximize ePlus service capability. Utilize company applications like Service Now and email to ensure prompt reliable coverage.
Provide systems support functions for client end systems, which include troubleshooting, diagnosis, and resolution of issues. Follows through until problems are resolved.
Security and Continuity — Strive where possible to mitigate geographic and environment risks. Set a high standard for technical implementations to ensure the security and confidentiality of ePlus and customer assets and intellectual properties.
Change Management — Assess ePlus MSC and customer conditions and recommend operational changes. Log all material work, tasks and system changes in Service Now.
Documentation - Periodic reviews internal process and control documentation as relates to IT Infrastructure, IT Operations and to Sarbanes-Oxley compliance. Ensure proper recordkeeping of all licenses, agreements, entitlements, and invoice approvals as relates to the MSC infrastructure.
Administration — Ensure access control rights to all individuals are properly authorized and evidenced.

Qualifications

Strong background in NetApp and UCS
Ability to perform basic performance analysis using sysstat, statit, and perfstat
Understanding of NAS (CIFS and NFS), SAN (iSCSI and FCP), protocols and implementation
Knowledge of LAN and SAN Concepts
Strong communication skills-written and verbal
Strong troubleshooting skills
Hardware NetApp FAS, Cisco UCS, drive and controllers
VMware vSphere knowledge of HA, DRS, vMotion, Storage vMotion, SRM

Education and Experience:

3+ years of hands-on experience in datacenter engineering support (troubleshooting)
Bachelor’s degree in related field or equivalent experience

Greatly Preferred Experience: 

NetApp Storage Array (FAS, AFF, SAN and E-Series)
Cisco UCS Servers
VMware vSphere
Technical Certification in NetApp NCDA with significant progress toward a NCIE certification
Technical Certifications like Cisco, Pure, VMware or Nutanix are a plus, but not required

POSITION SPECIFICS

The base salary range for this position at commencement of employment is expected to be between $70,000 and $105,000 annually; however, base pay offered may vary depending on multiple individualized factors, including market location, job-related knowledge, skills, and experience. The total compensation package for this position may also include other elements, including commissions and discretionary bonuses, in addition to a full range of medical, financial, and/or other benefits (including 401(k) eligibility and various paid time off benefits, such as vacation, sick time, and personal leave), dependent on the position offered. Details of participation in these benefit plans will be provided if an employee receives an offer of employment. ePlus Benefits highlights can be viewed here.

If hired, employee will be in an “at-will position” and the Company reserves the right to modify base salary (as well as any other discretionary payment or compensation program) at any time, including for reasons related to individual performance, Company or individual department/team performance, and market factors.

Notice to Recruiting Agencies: ePlus only accepts unsolicited resumes when presented directly by a candidate. Unsolicited resumes submitted to ePlus from any other source will be considered ePlus property, and will not qualify for any placement or referral fees. ePlus will only pay such fees in connection with a valid written agreement between ePlus and the referring agency, and then only after providing advance written approval to the referring agency to submit resumes in connection with a particular opportunity.

Physical Requirements

While performing this role, you will engage in both seated and occasional standing or walking activities. We provide reasonable accommodations, in accordance with relevant laws, to support success in this position.

By embracing our values, you will contribute to our collective mission of making a positive impact within our organization and the broader community. We understand that this job description serves as a guide and is not an employment contract.

#IND1","Análisis de rendimiento de software, Comunicación, Infaestructura del centro de datos, Ingeniería de redes, NCDA, NCIE, Red de área local y Resolución de incidencias",Solicitar
https://www.linkedin.com/jobs/view/3863605235/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=1EeJLZpzNtUK2gLQMGPvKw%3D%3D&trk=flagship3_search_srp_jobs,SQL/Python - Data Engineer Tools (Finance Industry a Must) - USC OR Green Card is a must (140-150K),"140 US$K/año - 150 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 meses,"Manhattan, NY","Acerca del empleo
Refer a friend: Referral fee program

Career Developers Inc, a well-established staffing agency/consulting firm, is celebrating 30 years in business. Previously in Ramsey, NJ for 25 years and now headquartered in sunny West Palm Beach, FL, we offer comprehensive commercial and government staffing services nationwide (GSA Contract holder). With a portfolio of carefully chosen clients to represent, we ensure a productive partnership that exceeds most others. Our commitment and goal for our candidates lie in efficiently managing your expectations through business intelligence, spending time for interview preparations, providing open communication, and delivering exceptional feedback throughout the process.We look forward to helping advance your career!

-----------------------------------------------------------------------------------------------------------------------------------------------

Data Engineer (Capital Markets / Financial Services)
Location: NYC - (3 days a week on-site)
Salary: 140-150K + 7% Bonus / Pension / Full Benefits

Green Card and USC is a must due to US FHFA regulations that this company follows.
Must possess a STRONG Capital Markets or similar finanical background with Fixed Income, Bonds, swaps, etc, supporting a large data warehouse and provide Python tools within this team. 
You must have a strong demonstrated experience with supporting and implementing tools withn the data infrastruture. 
The ideal candidate will be responsible for designing and implementing a robust data infrastructure and analytical toolset to support Balance Sheet management and optimization activities of the Capital Markets group.
This role is ideal for someone passionate about leveraging data to drive high-impact business decisions.
Technical Expertise:
Advanced Proficiency in SQL and Python are essential. Proven track record of deploying and managing python codebase in a production setting.
Experience with Data Warehousing Solutions: Proven experience with Snowflake or similar platforms.
Data Visualization Tools: Proficiency in using tools like Plotly, QlikSense, Tableau for data visualization.
Working knowledge of a Git repository for version control and collaborative development
Data Engineering Skills:
Data Processing: Experience in handling, processing, and extracting value from large, disconnected datasets.
Data Pipeline Development: Skilled in designing and implementing data pipelines for efficient data flow.
Quantitative Skills: 
Statistical and Mathematical Methods: Familiarity with PCA, linear regression, logistic regression, KNN algorithm, and Monte Carlo simulations.
Analytical Abilities:
Capability to analyze complex sets of data and provide actionable insights.
Experience in building high impact analytical tools.
Collaborative Skills:
Experience working closely with various groups across an organization.
Excellent communication skills for effective cross-team collaboration.
At least 3 years of relevant experience in a similar role

INDH

Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing, Capital Markets data analytics, SQL python, data warehousing,
Aptitudes y experiencia deseables
DATA ENGINEER","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Ingeniería de datos y Visualización de datos, Plotly, Qlik Sense, Revisión del balance general, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3978138436/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=TpYANr488d6oNTvDYx0YXQ%3D%3D&trk=flagship3_search_srp_jobs,Engineer - Data Visualization Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Washington Highlands, MD","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Data Visualization Engineer

Washington, D.C

Must

Active Secret clearance required

5+ years of experience in IT and Data Visualization

3+ years experience with Tableau, Power BI, or D3.js,

3+ years of data analysis and data management concepts

Ability to mentor junior engineers is a huge plus

Excellent communication and collaboration skills

Strong portfolio demonstrating data visualization expertise

Bachelors degree in Information Design, Data Science, Computer Science or related field

Duties

Working with stakeholders to understand data needs and requirements

Designing and develop visually compelling and informative dashboards, reports, and infographics

Developing and maintain documentation, standards, and best practices for data visualization

Performing data analysis to identify trends and insights, and use that knowledge to design effective visualizations

Working closely with cross-functional teams to develop and implement data visualization solutions

Continuously improve and innovate data visualization practices

Progression Inc. is an equal opportunity and affirmative action employer. Progression Inc. is committed to administering all employment and personnel actions on the basis of merit and free of discrimination based on race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or status as an individual with a disability. Consistent with this commitment, we are dedicated to the employment and advancement of qualified minorities, women, individuals with disabilities, protected veterans, persons of all ethnic backgrounds and religions according to their abilities. #indpro","Analítica, Analítica de datos, Análisis de datos, Ciencia de datos, D3.js, Minería de datos, Visualización y Visualización de datos, Ciencias de la computación y SC Clearance",Solicitar
https://www.linkedin.com/jobs/view/3959338707/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=AEMDojA0%2FtADiZp7G9%2BFhg%3D%3D&trk=flagship3_search_srp_jobs,Remote - Data Scientist/Analyst/Engineer(ENTRY LEVEL),"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 mes,"Detroit, MI","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers get employed in the tech job market by providing candidates the requisite skills, experience, and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this Job market also, our candidates are able to achieve multiple job offers and $100k + salaries. 

 please check the below links to see the success outcomes and salaries of our candidates .

 https://www.synergisticit.com/candidate-outcomes/ 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage visit the below videos exhibiting at Oracle Cloud World/Oracle Java one (Las Vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

 https://synergisticit.wistia.com/medias/tmwjwchxz5 

 https://synergisticit.wistia.com/medias/n8487768di 

 https://synergisticit.wistia.com/medias/o5gmv7i9eu 

 https://synergisticit.wistia.com/medias/k6t6a1n4kb 

 https://synergisticit.wistia.com/medias/pgrvq4fgni 

 https://synergisticit.wistia.com/medias/ce4syhm853 

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, Walmart lab s etc to name a few.

Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers  for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates 

 For data Science/Machine learning Positions 

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow 

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java, Javascript, C++, or software programming

Spring boot, Microservices, Docker, Jenkins, Github, Kubernates, and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates","Analítica de datos, Ciencia de datos, Lenguajes de programación, Programación y Visualización de datos, Ciclo de vida de desarrollo de software (SDLC), Desarrollo de software, Java, JavaScript y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3980222197/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=4%2FrPORW6lr%2B0QlkwcHf6Fg%3D%3D&trk=flagship3_search_srp_jobs,Salesforce Data Cloud/Marketing Cloud Software Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Atlanta, GA","Acerca del empleo
Must Haves

Proven experience as a Software Engineer with proficiency in Salesforce Marketing Cloud and Data Cloud.

Strong knowledge of Salesforce Marketing Cloud components such as Journey Builder, Email Studio, Mobile Studio, and Social Studio.

Experience with Salesforce Data Cloud, including data integration, management, and analytics.

Proficiency in programming languages such as Apex, JavaScript, and SQL.

Familiarity with REST and SOAP APIs, and experience with API integrations.

Plus

Salesforce Marketing Cloud and Data Cloud certifications

Summary

Client is seeking a highly skilled and motivated Software Engineer with expertise in Salesforce Marketing Cloud and Data Cloud technologies to join a dynamic team. The successful candidate will be responsible for designing, developing, and implementing solutions that leverage Salesforce Marketing Cloud and Data Cloud to drive customer engagement and enhance our marketing efforts.

Key Responsibilities

Design and Development:

Design, develop, and implement Salesforce Marketing Cloud solutions, including Journey Builder, Email Studio, Mobile Studio, and Social Studio.

Develop custom solutions using Salesforce Data Cloud to manage and analyze customer data.

Integration

Integrate Salesforce Marketing Cloud with other systems, such as CRM, CMS, and e-commerce platforms.

Ensure seamless data flow between Salesforce Marketing Cloud and Data Cloud for consistent and accurate customer insights.

Optimization

Optimize marketing campaigns and customer journeys to improve engagement and conversion rates.

Utilize A/B testing and data analytics to refine marketing strategies.

Collaboration

Work closely with marketing, sales, and IT teams to understand business requirements and translate them into technical solutions.

Provide technical guidance and support to team members and stakeholders.

Maintenance And Support

Monitor and maintain the performance of Salesforce Marketing Cloud applications.

Troubleshoot and resolve technical issues in a timely manner.

Stay up-to-date with the latest Salesforce Marketing Cloud and Data Cloud features and best practices

Education

Bachelors Degree","Analítica, Analítica de datos, Integración de datos y SQL, Desarrollo Salesforce.com, JavaScript, Necesidades empresariales, Oracle Application Express, SOAP y Salesforce Marketing Cloud",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3979152876/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=VPAG7mcpkx0ZYYF5D4AZtg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 5 días,"East Merrimack, NH","Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Judge Group, Inc., is seeking the following. Apply via Dice today!

Location:  East Merrimack, NH

Description: 

My Client is looking for an experienced Data Engineer @ MERRIMACK, NH

Senior Data Engineer @ REMOTE - (75%-50%) 

Locations:

Merrimack, NH 03054

HYBRID: 3 weeks REMOTE in a Month until September; starting in September, will go to 2 weeks.

Project/ Team Description: 

Data Services Platform:

Building brand new data platform in Snowflake for crypto data analytics- load data from various sources into Snowflake.

Things to be excited about: 

Exploring and making decisions. Greenfield project, building from Scratch, candidates will have some autonomy to implement their ideas and be very hands-on and collaborative with solutions.

Candidate Profile / Top 3 Skills: 

Snowflake

Python

SQL

How much hands-on experience with Snowflake and Python? 

Important that they have hands-on real experience (not just hypothetical).

Nice to Have skills: 

In-Depth AWS knowledge, Reporting, and Data modeling.

Interview Process: 

1st round - meet with 2 tech lead 45 min with each

One will cover Python and Snowflake

One will cover SQL - 45 min

2nd round- 

30 min with Chapter Leader

30 min with the Group Chapter Leader

Mix of Technical and behavioral questions discuss the team and role in more detail.

Contact:

This job and many more are available through The Judge Group. Please apply with us today!","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Modelado de datos, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3982795660/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=4xvMi3Zl7k8rpzQkNbmXuA%3D%3D&trackingId=XNbUHkVpP%2FqUSAghtlrYeA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 días,"Bristol, CT","Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Reliable Software Resources, is seeking the following. Apply via Dice today!

This is Srikanth from Reliable Software. We have a position with one of our clients. Below are a few details pertaining to the job. Please take a look at it and let me know if you would like to be considered for the opportunity. Please share with me your updated resume.

Job Title: Data Engineer

Location: Bristol, CT

Duration: Long term

Qualifications:

Experience:
Proven experience as a Data Engineer or in a similar role.
Extensive experience with Java or Python programming languages.
Strong experience with Apache Spark and Databricks for data processing and analytics.
Proficiency in working with Snowflake for data storage and management.
Skills:
Strong understanding of data warehousing concepts, ETL processes, and data integration techniques.
Proficiency in SQL for data manipulation and querying.
Familiarity with data modeling, schema design, and data partitioning strategies.
Excellent problem-solving skills and attention to detail.
Strong communication and collaboration skills.
Ability to manage multiple tasks and projects simultaneously.
Educational Qualifications:

Required - Bachelor s degree in Computer Science, Information Technology, Computer Engineering or closely related or equivalent.
Preferred - Master s degree in Management Information Systems (MIS), Computer Science, Big Data or Analytics or equivalent.

Travel:

Open to travel based-up on the nature of the engagement.

Thanks & Regards

Srikanth Donkani

Sr. Talent Acquisition Specialist

(w):

(E):

;/p>

2260 Haggerty Road, Suite 285 Northville, MI 48167

Equal Employment Opportunity

Reliable Software employment does not discriminate on the basis of race, religion, gender, sexual orientation, age or any other basis as covered by federal, state, or local law.

Employment decisions are based solely on qualifications, merit and business needs.","Almacenamiento de datos, Apache Spark, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL, Comunicación, Manipulación de datos, Modelado de datos, Resolución de problemas y Snowflake",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3964714634/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=gvQbwphG2dLES82UTP%2Ffag%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer- Atlanta, GA ONLY","Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Atlanta, GA","Acerca del empleo
About Rev.io

Rev.io provides configurable software for billing, customer management, business management, payments, analytics, provisioning, and automation to service providers in telecommunications, Wireless & IoT, managed IT services, A/V, security integration, and other related industries. Our modern, cloud-based software delivers the industry’s most complete quote-to-cash experience, enabling our clients to grow their revenue efficiently.

Rev.io is an Atlanta-based company with employees based in more than 20 states. We have more than 21 years’ in business serving our clients. While we are very experienced in what we do, we are still growing rapidly and we are looking for exceptional people who are excited to join us on a career-defining journey.

In all of our daily work, each of us are guided by our mission, vision and “ACT TOP” values.

Our Mission: To help clients grow revenue efficiently.

Our Vision: To be the best billing & back-office software company in the world by providing innovative solutions and extraordinary service to our clients and end users.

Our ACT TOP Values:

Achieving Extraordinary Results: Doing more than expected and pursuing a higher standard of work without compromising your job responsibilities
Caring For Employees, Clients and Community: Demonstrating empathy and concern by helping those who are most in need
Take Responsibility and Act Like An Owner: Being accountable and doing the right thing even when no one is watching
Transparent Leadership: Communicating openly, regardless of title or situation
Opportunity To Make a Positive Difference: Recognizing situations and taking action to produce a better outcome
Passion For Innovation: Enthusiastically finding new solutions to improve efficiency. 

In a recent national survey, 97% of our employees agreed that Rev.io is a Great Place to Work. Compared to 57% agreement at comparable US-based companies, Rev.io stands out as a place where team, values and culture combine to create an amazing work environment!

About The Role: 

We are looking for a Data Engineer to join our team in Atlanta, GA. The purpose of a Data Engineer is to migrate and curate a client's data from their legacy system into the Rev.io system in a structured, scalable manner that provides our clients with accurate billing and superior customer management capabilitiesThis is a dynamic role with a responsibility for the detailed design, coding and maintenance of migration projects to move data from a variety of legacy systems. Problem solving will be a big part of the role, as legacy systems need to be deciphered, documented and methods of accessing them designed. The role requires you to be a self-driven, highly motivated individual with strong customer facing, teamwork, and support skills. This is an IT Billing Operations role supporting client data migrations and billing analytics.

Job Objectives and Responsibilities:

Work with client teams of Billing and Mediation Specialists, and Database Administrators responsible for all clients billing and rating operations
Migration of legacy data to specified SQL Server database structure
Configure database and DBMS parameters
Monitor successful completion of all scheduled jobs for implementation; troubleshooting any job failures and DBMS issues
Communicate and update on task due dates, requirement needs and overall project status to the Implementation Team and Vice President of Operations
Provide hands-on assistance in research and resolution of client billing issues and inquiries
Create, modify and test custom SQL queries and stored procedures to provide maximum visibility into billing details and accuracy and customer billing formats to large wholesale and enterprise customers
Manage complex call records mediation and rating engine to ensure all usage elements are properly applied to customer accounts
Validate results by testing Rev.io instance for each implementation, including standard and custom fields
Dual bill run testing and rating to ensure the customer is billing correctly out of Rev.io
Analyzing rating, MRCs, NRCs for accurate billing between a customers legacy system and Rev.io
Assist clients with guidance on data migration standards and best practices for utilizing Rev.io functionality
Foster ideas on how to improve client relationships and operational efficiencies
Develop a foundation and the recommendation to steam line and define the data migration process by product type and monthly recurring revenue

Skills Needed:

A Bachelor’s degree in Computer Science, Business, or Information Technology recommended
Knowledge and working experience of SQL Server and DTS packages are essential
Familiarity with CDR formats (e.g. BAF, EMI), types and jurisdictions
Familiarity with telecom tax engines and data feeds (e.g. SureTax, Avalara)
Experience of analysis of data structures and development of tools to parse and restructure data
Client-facing skills together with the ability to work independently in the field interpreting specific requirements
In-depth understanding of SQL queries, statements, and stored procedures
Experience of problem solving, database installation and management procedures in development and production environments
Knowledge of ODBC/ADO and accessing remote systems using various methods
T-SQL experience and programming in SQL Server, SQL scripting and SQL database design. Experience of applying data modeling tools would be an advantage
Management of database systems

Benefits and Perks:

Excellent medical, dental and vision coverage, with rates comparable to larger companies
Company paid for life and disability insurance
401k with generous company match and immediate vesting
Unlimited PTO
Monthly tech and fitness reimbursements
Professional development allowance

At Rev.io no employee or applicant will be treated less favorably on the grounds of their sex, marital status, race, color, nationality or ethnic or national origin, disability, gender, sexual orientation, gender identity, age, pregnancy or maternity, marital or civil partner status, or religion or belief. By clicking submit below, you consent to allow Rev.io to store and process the personal information submitted above.



Powered by JazzHR

Pra3a5AnsO","Arquitectura de datos y Base de datos SQL, Ciencias de la computación, Diseño de bases de datos, Modelado de datos, Open DataBase Connectivity, Procedimientos de almacenado, SQL database design, Servicios de transformación de datos y Sistema de gestión de bases de datos (SGBD)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3962389900/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=WQSTxGxIk7WP6GnLZT8SFQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"85 US$K/año - 150 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 semanas,"Nueva York, Estados Unidos","Acerca del empleo
This inclusive employer is a member of myGwork – the largest global platform for the LGBTQ+ business community.  

You Lead the Way. We’ve Got Your Back.

With the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, you’ll learn and grow as we help you create a career journey that’s unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.

At American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong.

Join Team Amex and let's lead the way together.

As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skill fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.

Let’s share success. 

Global Infrastructure Product Services aims to establish and reinforce a culture of effective metrics, data-driven business processes, architecture simplification, and cost transparency.

A successful Data Analyst knows that delivering on that promise takes foresight, planning and agility. We are seeking Data Analysts who are not only technically adept, but also understand the importance of harnessing the power of data to streamline financial operations, enhance decision-making and improve business outcomes.

This role is focused on developing and maintaining a dynamic technology infrastructure cost allocation model, and requires a blend of technical prowess in SQL and database technologies, understanding of enterprise infrastructure components, and the ability to continuously integrate and improve our financial modeling processes.

Let’s Build On What You Know.

If you are a pioneer in developing robust cost allocation models and infrastructure analysis, you'll find a fit within our Global Infrastructure Cost Transparency team responsible for establishing consistent on-premises workload attribution, unit economics and business value metrics. By leveraging ApptioOne and TBM Studio, you will ensure that our technology spending is transparent, efficient, and properly aligned with our strategic objectives. Here’s just some of what you’ll do:

Architect and refine a robust technology infrastructure cost allocation model using ApptioOne and TBM Studio, with a focus on continuous integration and improvement
Extract, transform and load (ETL) data from diverse sources, including general ledger fields and other systems of record into a coherent, actionable format 
Maintain seamless data links between internal systems and ApptioOne, ensuring high data quality and consistency 
Work closely with stakeholders across Technology, Finance and business unit portfolio leaders to define data and analysts requirements and incorporate cost drivers, allocation methods, and infrastructure nuances 
Regularly review and update the cost model to reflect changes in technology infrastructure, business strategy and financial policies 
Develop SQL queries, scripts and routines to automate data processing and enhance the model’s accuracy and efficiency 
Generate insightful data visualization and reports to aid in decision-making and demonstrate the cost model’s value to stakeholders 
Lead training sessions and create comprehensive documentation to empower end users to leverage the cost model effectively 
Function as an active member of an agile team through consistent development practices (tools, components, and documentation) 
Find opportunities to adopt innovative technologies 
Take part in reviews of individual or team contributions 
Work on prioritized/assigned features for ongoing sprints 
Communicate and work reciprocally with business and product teams to support changes and implementation 

Are you up for the challenge? Here’s what you should have: 

Hands-on expertise with distributed (multi-tiered) systems and relational database design, development, troubleshooting and automated testing 
Demonstrated experience in data visualization with a focus on financial data analysis and cost modeling 
Advanced proficiency in SQL and familiarity with other relational database technologies 
Proficiency with industry recognized ETL methods, processes and standards 
Proficiency with KPI and metric design 
Thorough understanding of enterprise infrastructure technologies such as Compute, Storage, Network, Mainframe to inform model development and adjustments 
Strong analytical, problem-solving and project management skills, coupled with a continuous improvement mindset 
Excellent communication skills, capable of explaining complex data issues in simple terms to a diverse audience 
Strong proficiency in data manipulation and analysis using programming languages such as Python, R, or Scala 
Demonstrable experience with data visualization and reporting tools (e.g.: MS SQL Reporting Services, Tableau, Informatica, Cognos, Microstrategy, PowerBI, matplotlib, etc.) 
Bachelor’s or Master’s degree in Computer Science, Engineering, Finance or a related discipline is a plus 
Deep experience with Apptio One and TBM Studio is highly desirable

Salary Range: $85,000.00 to $150,000.00 annually + bonus + benefits

The above represents the expected salary range for this job requisition. Ultimately, in determining your pay, we’ll consider your location, experience, and other job-related factors.

Benefits

We back our colleagues and their loved ones with benefits and programs that support their holistic well-being. That means we prioritize their physical, financial, and mental health through each stage of life. Benefits include:

Competitive base salaries 
Bonus incentives 
6% Company Match on retirement savings plan 
Free financial coaching and financial well-being support 
Comprehensive medical, dental, vision, life insurance, and disability benefits 
Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need 
20+ weeks paid parental leave for all parents, regardless of gender, offered for pregnancy, adoption or surrogacy 
Free access to global on-site wellness centers staffed with nurses and doctors (depending on location) 
Free and confidential counseling support through our Healthy Minds program 
Career development and training opportunities

For a full list of Team Amex benefits, visit our Colleague Benefits Site.

American Express is an equal opportunity employer and makes employment decisions without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, disability status, age, or any other status protected by law.

We back our colleagues with the support they need to thrive, professionally and personally. That's why we have Amex Flex, our enterprise working model that provides greater flexibility to colleagues while ensuring we preserve the important aspects of our unique in-person culture. Depending on role and business needs, colleagues will either work onsite, in a hybrid model (combination of in-office and virtual days) or fully virtually.

US Job Seekers/Employees - Click here to view the “Know Your Rights” poster and the Pay Transparency Policy Statement.

If the links do not work, please copy and paste the following URLs in a new browser window: https://www.dol.gov/agencies/ofccp/posters to access the three posters.

Employment eligibility to work with American Express in the U.S. is required as the company will not pursue visa sponsorship for this position.","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Scala y Visualización de datos, Financial Data Analysis, Manipulación de datos, Modelado de datos y Modelos de coste",Solicitar
https://www.linkedin.com/jobs/view/3984106519/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=Qgd7CG00yNNm4%2Fo2ScugwQ%3D%3D&trk=flagship3_search_srp_jobs,Data Warehouse Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 días,"Wadsworth, OH","Acerca del empleo
Job Details

Description

Job Description:

A data warehouse architect designs and maintains data management solutions for an organization to store and retrieve processed data. They use their technical and analytical skills to create strategies for data warehouse systems, multidimensional networks, and enterprise databases. They also set standards for database operations, programming, query processes, and security.

Responsibilities

Analyze data needs: Understand the company's data needs
Design data models: Create and optimize data models for warehouse infrastructure and workflow
Design and implement data warehouses: Design and implement data warehouses, data marts, and data stores while ensuring high levels of data availability
Select infrastructure components: Evaluate and select all infrastructure components such as software, hardware, database management systems, and networking capabilities
Develop tools: Develop tools for data mining and data analysis
Test and improve: Test and improve data warehouse systems, and perform unit, system, performance, and regression testing on ETL mappings
Report: Report on systems performance and project progress
Support: Provide technical and business knowledge support to the team, and support and improve production data integration system and environment
Evangelize: Develop white papers, blogs, reference implementations, labs, and presentations to evangelize design patterns and best practices

Essential Technical Skills

SQL (Programming Language)
Data Warehousing – Concepts, Practice, Architecture, Design and Implementation
Data Modeling
Data Management
Azure
PowerBI

Other Essential Skills

Communications
Problem Solving 
Leadership
Management
Operations
Trouble Shooting","Almacenamiento de datos, Analítica de datos, Data Marts y Extraer, transformar y cargar (ETL), Bases de datos, Comunicación, Modelado de datos, Modelo de datos, Resolución de incidencias y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3978406658/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=3BiJaFwzKM9cFi%2FTV9UsLA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Nueva Jersey, Estados Unidos","Acerca del empleo
A law firm is looking for a Data Engineer to join their team. This role is remote.

Compensation: $120-130k

The Data Engineer must have a strong technical background in the Azure stack, including Azure Synapse Analytics, Azure Data Factory, Azure Databricks, and other Azure services. As the firm implements an aggressive and exciting cloud strategy, this role will facilitate and develop streamlined processes and work collaboratively with cross-functional teams of devops engineers, product and project managers, business analysts, and infrastructure architects. This individual must thrive in a fast-paced environment and be self-motivated with a passion for problem-solving and innovation. This IT department is project-focused, undergoing a complete digital business transformation for the firm. This position will drive new data engineering strategies and opportunities for the firm.

Responsibilities

Ability to diagnose and analyze dataflows and applications.
Create queries for the business team that translate custom in-house applications into useful reports or outputs for discussions.
Strong data modeling and data warehousing skills with the ability to optimize report and query performance.
Designing and implementing scalable and secure data processing pipelines using Azure Data Factory, Azure Databricks, and other Azure services.
Automating data pipelines and workflows to streamline data ingestion, processing, and distribution tasks.
Experience with DevOps + automation is a big plus.
Code management with Git.

Qualifications 

At least 3 - 5 years of experience working on cloud data platforms, preferably in the Azure stack.
Power BI proficiency, data modeling, and DAX experience.
Managing and optimizing data storage using Azure Data Lake Storage and Azure SQL.
At least 2 years of experience with:
Python / PySpark / Scala
Azure Synapse Analytics
Agile Scrum principles and practices
SQL Query, SP, and SSIS packages 
Azure Cosmos DB
Azure SQL solutions (Azure SQL DB, Managed Instances, etc.)
1 to 2 years of Boomi ETL is a plus.
An eye for automation and templating for reusable solutions.
Self-starter.
Curious problem solver who is willing to investigate on their own.
Strong desire for quality + best practice improvement.
Likes to collaborate across teams.
EDUCATION

Bachelor of science degree (or equivalent) in computer science, engineering, or relevant field","Almacenamiento de datos, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y SQL Server Integration Services (SSIS), Ciencias de la computación, Lenguaje de consulta (query), Modelado de datos y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3980042715/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=xI3wPSWltljiQg0HHLBKiw%3D%3D&trk=flagship3_search_srp_jobs,Python Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 6 días,"Dallas, TX","Acerca del empleo
Python Data Engineer OR Azure Data Engineer with Python
Preferred locations:
Dallas/Irving, Chicago, Boston/Wellesley, NYC, Hartford CT, Blue Bell PA, Scottsdale AZ, or Woonsocket RI.

Required Qualifications:
Strong in Python with 3+ years hands-on coding experience
3+ years’ experience in designing and building Microservices
3+ years’ experience with REST API design and development using Python
3+ years’ experience with deployment/scaling of apps on containerized environment (i.e., Kubernetes, AKS)
3+ years’ experience with real-time and streaming technology (i.e., Azure Event Hubs, Azure Functions Kafka, Spark Streaming)
3+ years’ experience with cloud-based platforms (i.e., Azure, GPC, AWS)
Experience building automated big data pipelines
Experience performing data analysis and data exploration
Experience working in an agile delivery environment
Experience with Snowflake
Experience with RDBMS and NoSQL Databases and hands-on query tuning/optimization
Experience working in multi-developer environment, using version control (i.e., Git)
Experience with orchestrating pipelines using tools (i.e., Airflow, Azure Data Factory)
Experience with data modeling and system architecture design
Experience partnering cross-functionally with other technical teams (i.e., data ingestion, data science, operational systems) to align priorities and achieve deliverable outcomes
Experience with setting coding standards, performing code reviews, and mentoring junior developers
Exposure/understanding DevOps best practice and CICD (i.e., Jenkins)
Ability to understand complex systems and solve challenging analytical problems
Strong critical thinking, communication, and problem-solving skills and a quick learner

Preferred Qualifications:
Experience in Azure/GCP.
Solid understanding of Kafka architecture and building consumers for high volume data streams.
Experience with bash shell scripts, UNIX utilities & UNIX Commands
Previous healthcare experience and domain knowledge

Education:
Bachelor’s degree in computer science, Engineering, Statistics, Physics, Math, or related field or equivalent experience
Preferred: master’s degree with coursework focused on advanced algorithms, mathematics in computing, data structures, etc.","Azure Data Factory y Python, Ajuste de consultas, Arquitectura de sistemas, Azure Kubernetes Service (AKS), Microservicios, Modelado de datos, Revisión de código, Snowflake cloud y Transferencia de Estado Representacional (REST)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979779259/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=vuB8qbq1J22RAnWluAs68A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Atlanta, GA","Acerca del empleo
Position Summary

The Data Engineer role within the Workforce Planning and Analytics team presents an exciting opportunity to work on large-scale data infrastructure projects through development and maintenance of our new workforce analytics data platform. The ideal candidate will have a strong background in data engineering and data architecture with an interest in performing data analysis and reporting. You will collaborate with cross-functional teams supporting Human Resources and Information Technology with responsibility to build and optimize data pipelines, data integration processes, and data storage solutions.

Responsibilities

Conduct exploratory analysis of workforce data to ensure a deep understanding of workforce data structure, data availability, and data integrity
Establish and maintain metadata about workforce data to ensure clarity and alignment on workforce data definitions, workforce metrics, and workforce data availability
Build data models that support dynamic and efficient data analysis and collaborate with cross functional teams to maintain and evolve those models over time
Create and enable the generation of ad-hoc / on-demand data sets for use by analysts, BI developers and data scientists
Support data integrity through ownership of automated monitoring that identifies occurrence of data problems
Ensure alignment to coding standard methodologies and development of reusable code
Provide ad-hoc reporting and visualization development

Job Requirements

Minimum 3 years of experience working in the following fields: Data Engineer, Analytics Development, Data Science, or Business Intelligence Development
Bachelors’ or above in quantitative discipline: MIS, Data Science, Economics, Mathematics, Engineering, or related field required
Experience performing data engineering to support reporting, analysis and data science 
Experience harmonizing disparate enterprise data into single version of truth data model for downstream analysis and reporting
Experience supporting data governance, master data management, data catalogs, and enterprise data warehouse architecture
High Proficiency in:
SQL
Python
Dimensional Modeling and Relational Databases
Data pipeline development, workflow management, and orchestration tools
ETL optimization and best practices
Proven ability to succeed in collaborative environments with cross-functional stakeholders and also in independent work environments
Proven ability to communicate clearly and effectively to audiences of varying technical levels
Flexibility to function in a full stack data development capacity including analysis and visualization development as needed
Experience with data visualization platforms preferred including Tableau, Power BI.
Experience with Oracle HCM Analytics platform preferred

Southern Company (NYSE: SO) is a leading energy provider serving 9 million residential and commercial customers across the Southeast and beyond through its family of companies. Providing clean, safe, reliable and affordable energy with excellent service is our mission. The company has electric operating companies in three states, natural gas distribution companies in four states, a competitive generation company, a leading distributed energy infrastructure company with national capabilities, a fiber optics network, and telecommunications services. Through an industry-leading commitment to innovation, resilience, and sustainability, we are taking action to meet our customers’ and communities’ needs while advancing our commitment to net zero emissions by 2050. Our uncompromising values ensure we put the needs of those we serve at the center of everything we do and are the key to our sustained success. We are transforming energy into economic, environmental and social progress for tomorrow. Our corporate culture and hiring practices have earned the company national awards and recognition from numerous organizations, including Forbes, Military Times, DiversityInc, Black Enterprise, J.D. Power, Fortune, Human Rights Campaign and more. To learn more, visit www.southerncompany.com.

Southern Company is an equal opportunity employer where an applicant's qualifications are considered without regard to race, color, religion, sex, national origin, age, disability, veteran status, genetic information, sexual orientation, gender identity or expression, or any other basis prohibited by law.

Job Identification: 6720

Job Category: Information Technology

Job Schedule: Full time

Company: Southern Company Services","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Bases de datos, Comunicación, Informes ad hoc, Modelado de datos y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3983527236/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=1P1e6Th1B%2F8M86HjHXsxqw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer (Python + ETL) - Only W2 & Local,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 4 días,"Atlanta, GA","Acerca del empleo
Title: Data Engineer (Python + ETL)
Location: Atlanta, GA 30354
Work Type: Hybrid (2-3 days/week)
Job Type: Contract (12 months with potential for extension)

Required Skills:
4+ years development experience building and maintaining ETL pipelines
3+ years of experience working with database technologies and data development such as Python, PLSQL, etc.
Deep understanding of writing test cases to ensure data quality, reliability and high level of confidence
Bachelor of Science degree in Computer Science or equivalent

Preferred: 
Airline industry experience","Extraer, transformar y cargar (ETL) y Python",Solicitud sencilla
https://www.linkedin.com/jobs/view/3960665502/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=Y7EzP7NWbYX7fDNGA96cpQ%3D%3D&trk=flagship3_search_srp_jobs,ETL Data engineer,"100 US$K/año - 110 US$K/año Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 mes,"Dallas, TX","Acerca del empleo
Skills
· Proficiency in SQL, relational and non-relational databases, query optimization and data modelling.
· Experience with Teradata (Vantage) or any RDBMS system/ ETL Tools
· Experience with AWS capabilities – Glue, dynamoDB, Lambda, elastic search.
· Software development experience with Python, PySpark, Apache Spark and Kafka
· Experience with GitHub, Jenkins, and Terraform.
· Excellent in trouble shooting the performance and data skew issues.
· Excellent communication skills","AWS Lambda, Almacenamiento de datos, Apache, Extraer, transformar y cargar (ETL), PySpark, Python y SQL, Bases de datos, Modelado, Modelado de datos y Teradata",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3985077794/?eBP=BUDGET_EXHAUSTED_JOB&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=ZDeKUlcfnfBw4hU6B1bHIQ%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer I, Intl. Seller Growth","Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",Publicado de nuevo hace 3 días,"Seattle, WA","Acerca del empleo
Description

Come and be part of the International Seller Services (ISS) Central Analytics Data Engineering (DE) team and work on solving cutting edge problems!

We are a team of DEs who support Applied Scientists, Data Scientist, and Economists who experiment, research, and turn machine/deep learning and AI research into great products for our customers.

ISS is seeking a smart, highly-motivated, and experienced Data Engineer to join our team. In this role, you'll help us create the right Data and ML infrastructure.

As a Data Engineer, you will provide technical leadership, lead data engineering initiatives and build end-to-end analytical solutions that are highly available, scalable, stable, secure, and cost-effective.

You are passionate about working with huge datasets and have experience with the organization and curation of data for analytics. You have a strategic and long term view on architecting advanced data eco systems.

You are experienced in building efficient and scalable data services and have the ability to integrate data systems with AWS tools and services to support a variety of customer use cases/applications.

Key job responsibilities

As a Data Engineer, you will provide technical leadership, lead data engineering initiatives and build end-to-end analytical solutions that are highly available, scalable, stable, secure, and cost-effective.

You are passionate about working with huge datasets and have experience with the organization and curation of data for analytics. You have a strategic and long term view on architecting advanced data eco systems.

You are experienced in building efficient and scalable data services and have the ability to integrate data systems with AWS tools and services to support a variety of customer use cases/applications.

Design, implement and operate large-scale, high-volume, high-performance data structures for analytics and data science.

Implement data ingestion routines using best practices in data modeling, ETL/ELT processes by leveraging AWS technologies and big data tools.

Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions with a flexible and adaptable data architecture.

Collaborate with engineers to help adopt best practices in data system creation, data integrity, test design, analysis, validation, and documentation

Identify opportunities in existing data solutions for improvements

Basic Qualifications

 Experience with data modeling, warehousing and building ETL pipelines
 2+ years of data engineering experience
 Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala)
 Experience with one or more scripting language (e.g., Python, KornShell)
 Knowledge of writing and optimizing SQL queries in a business environment with large-scale, complex datasets

Preferred Qualifications

 Experience with big data technologies such as: Hadoop, Hive, Spark, EMR
 Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc.

Amazon is committed to a diverse and inclusive workplace. Amazon is an equal opportunity employer and does not discriminate on the basis of race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, or other legally protected status. For individuals with disabilities who would like to request an accommodation, please visit https://www.amazon.jobs/en/disability/us.

Our compensation reflects the cost of labor across several US geographic markets. The base pay for this position ranges from $91,200/year in our lowest geographic market up to $185,000/year in our highest geographic market. Pay is based on a number of factors including market location and may vary depending on job-related knowledge, skills, and experience. Amazon is a total compensation company. Dependent on the position offered, equity, sign-on payments, and other forms of compensation may be provided as part of a total compensation package, in addition to a full range of medical, financial, and/or other benefits. For more information, please visit https://www.aboutamazon.com/workplace/employee-benefits. This position will remain posted until filled. Applicants should apply via our internal or external career site.


Company - Amazon.com Services LLC

Job ID: A2711693","Apache Spark, Arquitectura de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Hadoop, Hive y Ingeniería de datos, Arquitectura técnica, Datasets y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3970468398/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=jVLHon4Xxip5u77BT5dE6A%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer I, Professional Services, Google Cloud","118 US$K/año - 174 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 2 semanas,"Atlanta, GA","Acerca del empleo
The application window will be open until at least July 18, 2024. This opportunity will remain online based on business needs which may be before or after the specified date.

Note: By applying to this position you will have an opportunity to share your preferred working location from the following: Austin, TX, USA; Atlanta, GA, USA; Boulder, CO, USA; Chicago, IL, USA.Minimum qualifications:

Bachelor's degree in Computer Science, Mathematics, a related field, or equivalent practical experience.
3 years of experience with data processing software (e.g., Hadoop, Spark, Pig, Hive) and algorithms (e.g., MapReduce, Flume).
3 years of experience in Google Cloud.
Experience managing client-facing projects, troubleshooting technical issues, and working with Engineering and Sales Services teams.
Experience programming in Python and SQL.

Preferred qualifications:

Experience in technical consulting.
Experience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT, and reporting/analytic tools and environments.
Experience working with Big Data, information retrieval, data mining, or machine learning.
Experience in building multi-tier high availability applications with modern web technologies (e.g., NoSQL, MongoDB, SparkML, TensorFlow).
Experience architecting, developing software, or internet scale production-grade Big Data solutions in virtualized environments.

About The Job

The Google Cloud Consulting Professional Services team guides customers through the moments that matter most in their cloud journey to help businesses thrive. We help customers transform and evolve their business through the use of Google’s global network, web-scale data centers, and software infrastructure. As part of an innovative team in this rapidly growing business, you will help shape the future of businesses of all sizes and use technology to connect with customers, employees, and partners.

As a Data Engineer, you will guide customers on how to ingest, store, process, analyze, explore, and visualize data on Google Cloud Platform. You will lead data migrations and transformations, partner with clients to architect scalable data processing systems, build efficient data pipelines, and resolve platform challenges.

In this role, you will collaborate with Google's strategic cloud customers and our team to successfully implement Google Cloud products.

Google Cloud accelerates every organization’s ability to digitally transform its business and industry. We deliver enterprise-grade solutions that leverage Google’s cutting-edge technology, and tools that help developers build more sustainably. Customers in more than 200 countries and territories turn to Google Cloud as their trusted partner to enable growth and solve their most critical business problems.

The US base salary range for this full-time position is $118,000-$174,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.

Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.

Responsibilities

 Act as a trusted technical advisor to customers and solve complex Big Data challenges. 
 Create and deliver best practice recommendations, tutorials, blog articles, sample code, and technical presentations, tailoring approach and messaging to varied levels of business and technical stakeholders. 
Analyze on-premises and cloud database environments and consult on the optimal design for performance and deployment on Google Cloud Platform.
Travel regularly up to 30% of the time, in-region for meetings, technical reviews, and onsite delivery activities.
Communicate effectively via video conferencing for meetings, technical reviews, and onsite delivery activities.


Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .","Ciencia de datos, Computación en la nube, Extraer, transformar y cargar (ETL) y Google Cloud, Ciencias de la computación, Comunicación, Evaluaciones técnicas, Presentaciones técnicas, Resolución de incidencias y Tutoriales",Solicitar
https://www.linkedin.com/jobs/view/3978058777/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=YMI9qRG5pA13%2Fai3cfGgmg%3D%3D&trk=flagship3_search_srp_jobs,Data Analyst/Engineer,"80 US$K/año - 115 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Raleigh, NC","Acerca del empleo
Description

We are seeking a Data Engineer to join our team, based in Raleigh, North Carolina. The role will involve a hybrid work schedule with three days in the office per week. As a Data Analyst/Engineer, your main focus will be on data modeling, ETL processes, SQL querying, as well as some reporting.

Responsibilities

 Design data modeling for databases including relational databases, data warehouses, and data marts.
 Develop and maintain ETL processes, with hands-on experience in Microsoft SSIS ETL.
 Write SQL queries using SQL statements for data extraction and manipulation.
 Generate reports using Microsoft SSRS or other reporting tools.
 Effectively communicate and present on topics such as database design, data migration, and ETL process design.
 Utilize your strong SQL skills to manage and manipulate data.
 Apply your knowledge of data modeling to design and implement data solutions.
 Create dashboards using Tableau to present data in a visual and interactive manner.
 Collaborate with other team members to ensure data accuracy and consistency.

Requirements

Must have:

SQL queries
ETL and experience in SSIS
Reporting experience, ideally SSRS
Strong communication skills
Data modeling

Nice To Have

creating dashboards using tableau.

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Analítica de datos, Arquitectura de datos, Data Marts, Extraer, transformar y cargar (ETL) y SQL Server Integration Services (SSIS), Bases de datos, Comunicación, Extracción de datos, Modelado de datos y Panel de control",Solicitar
https://www.linkedin.com/jobs/view/3983541043/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=3KO1pGWOR0QGBnKZwTBJ3g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"71,25 US$/h - 82,50 US$/h Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Houston, TX","Acerca del empleo
Description

We are offering a contract to permanent employment opportunity for a Data Engineer in the Energy/Natural Resources industry in Houston, Texas. This role is essential in standing up enterprise data warehouses, setting up data pipelines, and implementing AI components. You will be working in an on-site environment, utilizing tools like Azure Databricks and SSIS for ETL processes.

Responsibilities

 Develop and maintain data pipelines using GIT
 Collaborate with the team to stand up an enterprise data warehouse
 Utilize Azure Databricks and SSIS ETL for data engineering tasks
 Handle some AI components to enhance the efficiency of the data infrastructure
 Ensure the accuracy and efficiency of data brick through Power BI function
 Employ your knowledge of Apache Spark, Database, EO/IR systems, Erwin Data, and HDFS in daily tasks
 Apply AB Testing, Analytics, AWS Technologies, and Business Intelligence (BI) to optimize data processing
 Comprehend and implement Business Requirement Documents in relation to data engineering tasks
 Use SSIS - SQL Server Integration Services effectively for various data tasks.

Requirements

 Proven experience in the Energy/Natural Resources industry
 Demonstrated skills in Apache Spark and Database management
 Proficiency in working with EO/IR systems
 Experience with Erwin Data and HDFS
 Familiarity with AB Testing and Analytics methodologies
 Proficient in AWS Technologies and Business Intelligence (BI)
 Ability to write and interpret Business Requirement Documents
 Experience with Azure Databricks, SSIS ETL, and SSIS - SQL Server Integration Services
 Strong problem-solving and analytical skills
 Excellent communication and collaboration skills
 A degree in Computer Science, Information Technology, or related field is preferred
 Willingness to stay updated with the latest industry trends and advancements.

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Apache Spark, Extraer, transformar y cargar (ETL), Hadoop, Ingeniería de datos y SQL Server Integration Services (SSIS), Bases de datos, Comunicación, Erwin, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3955639735/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=th8XSVXO4abhUHcKwXzR1Q%3D%3D&trk=flagship3_search_srp_jobs,Big Data/ML/Python Controls Engineer - Lead_W2,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 mes,"McLean, VA","Acerca del empleo
Position: Big Data/ML/Python Controls Engineer - Lead_W2

Location: Hybrid to Mclean, VA, Richmond, VA, or Plano, TX

Required Skills

A strong analytical mindset is essential.
The ability to interpret data and derive meaningful insights is necessary.
Effective communication skills are also crucial due to the need to discuss data quality and remediation plans with external team members.
Proficiency in Pandas and Numpy is required.","Analítica, Capacidad de análisis, Gestión de datos, Ingeniería de datos , NumPy y Pandas (Software), Calidad de datos, Comunicación, Conocimientos comerciales y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3943855990/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=IE6r2XCtbuiuABuFEf6y6A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer- Need to be Local to VA - only W2,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 mes,"McLean, VA","Acerca del empleo
Job Description

 

Project Details/Day2Day

ETL data transformation project
Self-service data transformation – onboard their use cases and data transformations
Maintaining existing application
Feature developments – here and there
API’s – data brick clusters
New developments on the Python side","Almacenamiento de datos, Análisis de datos, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL), Hive, Ingeniería de datos y Python, Bases de datos y Transformación de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3977049128/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=h%2FifBYKvY%2B8Fbx0xi5Ti4Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"120 US$K/año - 130 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Midland, TX","Acerca del empleo
Description

We are offering an exciting opportunity for a Data Engineer in Midland, Texas. The Data Engineer will design, build, and optimize data pipelines from various source systems into an analytics-ready environment. This role involves transforming raw data into usable formats, monitoring and supporting development and production environments, and collaborating with various team members to provide reliable and clean data.

Responsibilities

 Design, build, and optimize data pipelines from source systems into an analytics-ready environment.
 Transform raw data into usable formats for data consumers.
 Monitor and support the development and production environments to ensure system availability and quality.
 Continuously develop, test, and manage the data pipelines.
 Build data pipelines that transform, clean, and aggregate data from disparate systems.
 Develop data models and pipelines for reporting, dashboards, and machine learning.
 Collaborate with various business SMEs, data scientists, and IT members to provide reliable and clean data.
 Work on multiple projects in a fast-paced environment.
 Maintain proficiency in SQL, Python, R or other similar languages.
 Use Microsoft SQL Server, Data Pipelines, R Programming, Databricks, Python, Snowflake as key tools for the job.

Requirements

 Must have a minimum of three years of experience in a Data Engineer role or similar.
 Proficiency in using Microsoft SQL Server is required.
 Experience in building and optimizing Data Pipelines is essential.
 Must possess skills in R Programming.
 Experience with Databricks is highly desirable.
 Proficiency in Python is required.
 Experience with Snowflake data warehousing platform is a plus.
 Ability to work effectively in a team and independently.
 Strong problem-solving skills are necessary.
 Excellent communication and presentation skills.
 Bachelor's degree in Computer Science, Engineering or a related field is required.
 Must have a deep understanding of database structure principles.
 Knowledge of data mining and segmentation techniques is necessary.

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Canalizaciones de datos, Ciencia de datos y R (Lenguaje de programación), Aptitudes para hacer presentaciones, Bases de datos, Ciencias de la computación, Comunicación, Resolución de problemas, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3960580230/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=QQAZtdiMINxBVX9fgKS%2Fow%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer- Plano TX,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 mes,"Plano, TX","Acerca del empleo
Position- Data Engineer

Location- Plano TX

Duration- Contract

Job Description

Required Skills: Unix,Python,PySpark,SQL,AWS

Roles & Responsibilities

Analyze business processes to identify areas for improvement and optimization.
Develop and maintain data pipelines using Python and PySpark to ensure efficient data processing.
Utilize AWS services to manage and deploy scalable data solutions.
Write and optimize SQL queries to extract and manipulate data for analysis.
Implement Unix scripts to automate routine tasks and enhance system performance.
Collaborate with stakeholders to gather requirements and translate them into technical specifications.
Provide actionable insights through data analysis to support strategic decision-making.
Create and maintain comprehensive documentation for data processes and business workflows.
Conduct data validation and quality checks to ensure accuracy and reliability of data.
Lead data-driven projects from inception to completion, ensuring timely delivery and alignment with business goals.
Communicate findings and recommendations to both technical and non-technical audiences.
Stay updated with industry trends and best practices to continuously improve data solutions.
Work closely with IT and development teams to integrate data solutions into existing systems

Qualifications

Possess strong analytical skills with the ability to interpret complex data sets.
Demonstrate proficiency in Python and PySpark for data processing and analysis.
Have hands-on experience with AWS services for data management and deployment.
Show expertise in writing and optimizing SQL queries for data extraction and manipulation.
Be skilled in Unix scripting for automation and system performance enhancement.
Exhibit excellent communication skills to effectively collaborate with cross-functional teams.
Display a proactive approach to problem-solving and continuous improvement.
Hold a Bachelor's degree in Computer Science, Information Technology, or a related field.
Have a proven track record of delivering data-driven projects on time

Aptitudes y experiencia deseables
UNIX, PYTHON, PYSPARK, SQL, AWS","Amazon Web Services (AWS), Capacidad de análisis, PySpark, Python y SQL, Especificaciones técnicas, Linux, Possess strong analytical, Procesamiento de datos y Unix",Solicitar
https://www.linkedin.com/jobs/view/3983404952/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=ch3%2FFgUofPPqFdjDC6uatg%3D%3D&trk=flagship3_search_srp_jobs,"Senior Data Engineer, Core","En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,Estados Unidos,"Acerca del empleo
We're transforming the grocery industry

At Instacart, we invite the world to share love through food because we believe everyone should have access to the food they love and more time to enjoy it together. Where others see a simple need for grocery delivery, we see exciting complexity and endless opportunity to serve the varied needs of our community. We work to deliver an essential service that customers rely on to get their groceries and household goods, while also offering safe and flexible earnings opportunities to Instacart Personal Shoppers.

Instacart has become a lifeline for millions of people, and we’re building the team to help push our shopping cart forward. If you’re ready to do the best work of your life, come join our table.

Instacart is a Flex First team 

There’s no one-size fits all approach to how we do our best work. Our employees have the flexibility to choose where they do their best work—whether it’s from home, an office, or your favorite coffee shop—while staying connected and building community through regular in-person events. Learn more about our flexible approach to where we work.

Overview

At Instacart, our mission is to create a world where everyone has access to the food they love and more time to enjoy it together. Millions of customers every year use Instacart to buy their groceries online, and the Data Engineering team is building the critical data pipelines that underpin all of the myriad of ways that data is used across Instacart to support our customers and partners.

About The Role

Instacart’s Core Data Engineering team plays a critical role in defining and maintaining company-wide datasets, standardized for uniform, reliable, timely and accurate insights from our data. This is a high impact, high visibility role owning critical data integration pipelines and models across all of Instacart’s products. This role is an exciting opportunity to join a key team shaping our most critical data.

About The Team

Core Data Engineering is part of the Infrastructure Engineering pillar, working closely with data engineers, data scientists and senior leaders across the company on developing and standardizing critical company-wide datasets. Our team also collaborates closely with other data infrastructure teams on designing and building key data platforms, systems and tools to make everyone at Instacart more productive with data.

About The Job

You will be part of a team with a large amount of ownership and autonomy.
Large scope for company-level impact working on critical data.
You will work closely with engineers and both internal and external stakeholders, owning a large part of the process from problem understanding to shipping the solution.
You will ship high quality, scalable and robust solutions with a sense of urgency.
You will have the freedom to suggest and drive organization-wide initiatives.

About You

Minimum Qualifications

6+ years of working experience in a Data/Software Engineering role, with a focus on building data pipelines.
Expert knowledge of SQL and Python.
Experience building high quality ETL/ELT pipelines.
Experience with cloud-based data technologies such as Snowflake, Databricks, Trino/Presto, or similar.
Adept at fluently communicating with many cross-functional stakeholders to drive requirements and design shared datasets.
A strong sense of ownership, and an ability to balance a sense of urgency with shipping high quality and pragmatic solutions.
Experience working with cross functional stakeholders on metric development, including data scientists, analysts, finance and senior leaders.
Experience working with a large codebase on a cross functional team.

Preferred Qualifications

Bachelor’s degree in Computer Science, Computer Engineering, Electrical Engineering OR equivalent work experience.
Experience with Snowflake, dbt and Airflow
Experience with data quality monitoring/observability, either using custom frameworks or tools like Great Expectations, Monte Carlo or similar

Instacart provides highly market-competitive compensation and benefits in each location where our employees work. This role is remote and the base pay range for a successful candidate is dependent on their permanent work location. Please review our Flex First remote work policy here.

Offers may vary based on many factors, such as candidate experience and skills required for the role. Additionally, this role is eligible for a new hire equity grant as well as annual refresh grants. Please read more about our benefits offerings here.

For US based candidates, the base pay ranges for a successful candidate are listed below.

CA, NY, CT, NJ

$192,000—$213,000 USD

WA

$184,000—$204,000 USD

OR, DE, ME, MA, MD, NH, RI, VT, DC, PA, VA, CO, TX, IL, HI

$176,000—$196,000 USD

All other states

$159,000—$177,000 USD","Airflow, Canalizaciones de datos y Extraer, transformar y cargar (ETL), Calidad de datos, Ciencias de la computación, Datasets, Monte Carlo, Presto, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3980791786/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=9qL4TSaOUZjuA7PxML7WSw%3D%3D&trk=flagship3_search_srp_jobs,Big Data Engineer,"140 US$K/año - 160 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Red Bank, NJ","Acerca del empleo
Big Data Engineer (Hybrid position)

Job Summary
 As a Big Data Engineer at Throtle, you will be responsible for designing and developing complex software systems that integrate with Throtle big data processing solutions. You will work closely with cross-functional teams to architect scalable, secure, and maintainable software systems that meet the needs of our business.

 Duties/Responsibilities
· Design and develop software architectures for large-scale data processing and analytics platforms
· Collaborate with data engineers to design and implement data pipelines and transformations using big data technologies such as Hadoop, Spark, Kafka, and related ecosystems
· Develop and maintain complex SQL queries for data extraction and reporting, and optimize query performance and scalability
· Design and develop software components that integrate with our identity graphs, client data, and keying processes
· Work with cross-functional teams to ensure software systems meet business requirements and are scalable for future growth
· Develop and maintain technical documentation for software architectures and designs
· Ensure data integrity and quality in development process
· Position will require that the individual understands all regulations and laws applicable to their assigned roles and responsibilities. Additionally, the individual will be responsible for the development, implementation, and regular maintenance of policies and procedures that govern the work of assigned roles and responsibilities, including compliance with the security requirements of ePHI.


Required Skill and Abilities

· Strong understanding of software design patterns, architecture principles, and big data technologies
· Proficiency in programming languages such as Java, Python, Scala, or similar languages
· Experience with large-scale data processing and analytics platforms, including Hadoop, Spark, Kafka, and related ecosystems
· Knowledge of distributed computing concepts and experience with cloud-based infrastructure (AWS)
· Strong analytical and problem-solving skills to address complex software architecture challenges
· Ability to work effectively with cross-functional teams and communicate technical designs and solutions to non-technical stakeholders

Education and Experience

· Bachelor's Degree in Computer Science, Engineering, or a related field
· Master's Degree or certification in Software Architecture or a related field preferred but not required
· 5+ years of professional experience in software development, architecture, or a related field
· Bachelor’s Degree in related field such as computer science, Engineering, or related field.
· Experience in data modeling, relational and NoSQL databases, SQL and programming languages for building big data pipelines and transformations.

About Throtle:
 Throtle is a leading identity company trusted by the world’s top brands and agencies located in Red Bank, NJ. At Throtle, we empower brands at scale with true individual-based marketing using a data-centric identity and onboarding approach.
 Throtle is a company that truly values its employees and their work-life balance. We offer a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being:

Competitive compensation.
Comprehensive benefits include Medical, Dental, and Vision.
Life insurance.
Long-Term Disability
A generous PTO program.
A 401k plan supported by a company match.
Half Day Summer Fridays (close at 1 p.m. Memorial Day to Labor Day).
Early Fridays (office closes at 3 p.m.).
Hybrid Schedule (Mondays and Fridays WFH)
The office is closed between Christmas and New Year.
Company-sponsored lunch at least 1x a month.
Professional Development Policy!
And much MORE!

Throtle is an equal-opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.",NoSQL y SQL,Solicitud sencilla
https://www.linkedin.com/jobs/view/3972224324/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=%2FFBc0HoLLvs0mUwnn%2F5HEg%3D%3D&trk=flagship3_search_srp_jobs,ETL/BI Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,"Baltimore, MD","Acerca del empleo
Centurion is hiring a Data Engineer to support one of our clients in the Woodlawn, MD area. Selected candidate must reside within 2 hours of headquarters and be willing to work on-site at least 2 days a week. 

Key Required Skills: 
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics
Position Description: 
Coordinate with EDW MI/BI Teams, EDW Technical Architects and EDW PMs in creating ETL documents as per the project requirements.
Collaborate with EDAs to understand the MI/BI requirements and support in data modeling needs.
Facilitate technical meetings with MI/BI teams and other required team members.
Provide design and development recommendations to MI/BI teams.
Proactively keep EDW PMs and Technical Leads up to date on any project issues or concerns or change of directions.
Able to multitask and support multiple projects
All other duties as assigned or directed.
Basic Qualifications: 
Bachelor's or master's degree in a training related field or 8 years of experience in lieu of a degree.
Technical experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Analytical and problem-solving skills.
Must be able to obtain and maintain a Public Trust. Contract requirement.
Required Skills: 
Strong working experience with SQL and other databases (i.e., DB2 and Oracle)
Strong working experience with BI Development (i.e., Tableau and other BI tools)
Strong working experience with ETL development and methodologies.
Strong oral and written communication skills and ability to communicate with all levels within the organization.
Strong data analysis and problem-solving skills.
Strong interpersonal skills with ability to collaborate with others effectively and efficiently.
Desired Skills: 
Experience in information architecture, data architecture, data modeling, data governance, ETL design, data quality and BI analytics.
Education: 
Bachelor's degree in a training related field with 7+ years of experience
Must be able to obtain and maintain a Public Trust. Contract requirement.
Position Details:  
US Citizenship or Authorization to work in US required 
Travel: < 10% (CONUS) 
Centurion Consulting Group, LLC is an Equal Opportunity Employer EOE M/F/D/V 
No third parties or subcontractors","Análisis de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Inteligencia empresarial y Tableau, Confianza ciudadana y Modelado de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3979659087/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=boqVDOpOw5S6RrnKn3islw%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"American Fork, UT","Acerca del empleo
About Lvt

LVT is on a mission to make the world safer and more secure through rapidly deployable security hardware that runs on our proprietary SaaS platform. Our enterprise-grade safety and security ecosystem makes it easy to secure essentially any physical environment through intelligent automation and actionable insights. As an industry leader in the IoT space, our systems are deployed in every state and adopted by Fortune 500 enterprise companies who share this vision.

About This Role

As a Senior Data Engineer at LVT, you will play a pivotal role in designing, building, and maintaining robust data infrastructure and pipelines. You will work closely with cross-functional teams to ensure the seamless flow of data, enabling advanced analytics and machine learning capabilities. This position requires exceptional technical expertise, strong problem-solving skills, and a passion for creating high-quality data solutions.

Responsibilities

Technical Leadership:

Provide technical leadership and mentorship to a growing team of data engineers.
Help set the technical direction, define best practices, and drive the adoption of modern data engineering methodologies and technologies.

Data Pipeline Development

Design, build, and maintain scalable and efficient data pipelines for batch and real-time data processing.
Ensure data pipelines are robust, reliable, and capable of handling large volumes of data.

Data Infrastructure Management

Develop and manage data storage solutions (e.g., data warehouses, data lakes) to support analytics and machine learning workflows.
Evaluate and make build vs. buy decisions for data tools and platforms.

Data Quality And Governance

Implement data quality, validation, and governance processes to ensure data integrity and consistency.
Establish and enforce data management policies and best practices.

Collaboration

Collaborate with data scientists, software engineers, and product managers to understand data requirements and deliver high-quality data solutions.
Foster effective communication channels and promote a culture of collaboration and knowledge sharing.

Continuous Improvement

Drive continuous improvement initiatives to enhance data engineering processes, productivity, and efficiency.
Identify bottlenecks, streamline workflows, and implement tools and methodologies to optimize the data development lifecycle.

Qualifications

Bachelor’s or Master’s degree in Computer Science, Software Engineering, Electrical/Computer Engineering, or a related field.
5+ years of professional experience in data engineering or related fields.
Strong expertise in data engineering technologies and frameworks (e.g., Apache Spark, Kafka, Hadoop).
Proficiency in programming languages such as Python, Java, or Scala.
Experience with cloud platforms (e.g., AWS, GCP, Azure) and data storage solutions (e.g., SQL, NoSQL, data lakes).
Experience with one or more of the following: PostgreSQL, ELK stack, Snowflake.
Excellent problem-solving, debugging, and analytical skills, with the ability to navigate complex technical challenges.
Strong interpersonal and communication skills, with the ability to collaborate effectively with diverse stakeholders.
Demonstrated ability to thrive in a fast-paced and dynamic environment, managing multiple priorities simultaneously.
Experience with containerization and orchestration tools (e.g., Docker, Kubernetes).

Preferred Qualifications

Familiarity with machine learning workflows and tools.
Knowledge of data security best practices.
Experience with monitoring and observability tools (e.g., Prometheus, Grafana).

WHY JOIN US

Founder-led and employee-driven company 
The opportunity to build where you stand
Value centric decision making
Both an economically stable and hyper-growth environment (ask us how this is possible)
The market leader in redefining how B2B does security

Benefits

On top of the obvious benefit of getting paid to work with great people who are laser-focused on a mission that matters, we also offer the following benefits:

Comprehensive health, vision, and dental benefits for you and your family. Including supplemental and life insurance, company-paid HSA contributions, and an Employee Assistance Program (EAP). 
401(k) With up to 4% match
Time Off & Paid Holidays - Ask us how we empower employees to take control of their well-being
Stock Options - Every full-time employee has the opportunity to be an owner of the company and benefit from our success. 
Paid Parental Leave - To help your growing family while you're away from work. 
Company Events - Christmas Party, Summer Party, and other parties to celebrate whenever we can find an excuse. 
Charitable Opportunities - LVT sends groups of employees to help the Daybreak Vision Project restore sight to thousands of people a year. 
Wellness - We regularly host dentists, chiropractors, financial experts, and other professionals to provide services and seminars to help promote physical, mental, emotional, and financial wellness. 
And More - Scholarship opportunities for employees and their dependents, discounted cell services, and opportunities to score tickets to Utah Jazz games and other Delta Center events.

HR Policy 

We’re an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status. All Candidates must pass a drug screening and background check upon employment. Some roles may also require passing a federal background check and fingerprinting. Must be authorized to work in the U.S.","Analítica de datos, Ingeniería de datos , SQL y Scala, Calidad de datos, Ciencias de la computación, PostgreSQL, Resolución de problemas, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3981626035/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=dSn9ZBtEGOoiW1evF%2Fg2Lg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 5 días,Washington DC-Baltimore y alrededores,"Acerca del empleo
Position Title: Data Engineer
 Skillset Required: 
Design, develop, and maintain scalable data pipelines for batch and streaming data, handling both structured and unstructured formats effectively.
Utilize advanced analytics tools and methodologies to supplement and enhance the analytics platform, contributing to our journey towards advanced analytical capabilities.
Collaborate with business and technical teams to gather requirements and design comprehensive data solutions that align with organizational goals and DT strategies.
Implement complex data operations using Python, Java, Spark, and SQL, optimizing ETL/ELT processes for performance and efficiency across diverse data types.
Manage data across various platforms, leveraging cloud services, big data technologies, and non-relational databases to ensure comprehensive data integration and accessibility.
Work closely with data scientists and analysts to facilitate the operationalization of Client/AI models, ensuring seamless data flow and integration for predictive and prescriptive analytics.
Champion data quality and observability within the organization, employing robust controls and monitoring systems to ensure data accuracy, consistency, and reliability.
Develop and maintain data virtualization solutions to minimize physical data movement and streamline data integration processes, reducing overall system complexity and cost.
Engage in continuous learning and development to keep up with the latest trends and best practices in data engineering, virtualization, and analytics.
Develop self-service data tools and capabilities enabling non-technical users to acquire (BYOD), access, store and analyze data independently.

***Lumen and / or its clients will not provide equipment (Laptop, monitor, etc.) to the selected contractor. The contractor must have their own equipment. Access to a virtual desktop set up (software) will be provided by Lumen’s client, allowing the user access to the required systems and technology.***","Análisis de datos, Extraer, transformar y cargar (ETL), Python y SQL, Arquitectura de integración y Virtualización de centros de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3818367551/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=yKkTHm5LkTms6TDid2JK%2Bw%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Developer/Coder/Programmer/Data Scientist/Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Provo, UT","Acerca del empleo
SYNERGISTICIT is aware that the Job Market is Challenging due to almost 300,000 Tech Layoffs within the past year due to which The Job market is flooded with thousands of laid off Techies who are competing with existing Jobseekers. For entry level Job seekers to get client interviews and jobs they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.

Since 2010 we have helped Jobseekers differentiate themselves by providing the clients with candidates who have the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers. All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this economy no client wants or has the resources to take an entry level person and spend resources on upgrading their skills and on top of that pay the jobseeker. That's the specific reason there are so many techies both experience and freshers who are unemployed.

Clients have now the option to hire remote workers from anywhere so for a Jobseeker its important to introspect and see how they can become better and have the skills and technologies to meet client requirements. We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped thousands of candidates get jobs at technology clients like apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few. We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients. Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates looking to make their careers in IT Industry We welcome candidates with all visas and citizens to apply. We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then you should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

please check the below links to see success outcomes of our candidates https://www.synergisticit.com/candidate-outcomes/

We are also silver sponsors at Oracle Cloudworld , Las vegas from sept 18-21st— please visit us

https://www.oracle.com/cloudworld/sponsor-listing/#synergistic-it

Watch the below videos of us participating at Industry events with the Top companies in Technology at Oracle Cloud world /Oracle Java one (Las vegas) and at Gartner Data Analytics Summit (Florida)

Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube https://www.youtube.com/watch?v=OAFOhcGy9Z8

https://www.youtube.com/watch?v=EmO7NrWHkLM https://www.youtube.com/watch?v=NVBU9RYZ6UI

https://www.youtube.com/watch?v=Yy74yvjatVg SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube

If you have relevant skills and industry experience, please apply

For preparing for interviews please visit

https://www.synergisticit.com/interview-questions/

REQUIRED SKILLS For Java /Software Programmers

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Project work on the skills
 Knowledge of Core Java , javascript , C++ or software programming
 Spring boot, Microservices, Docker, Jenkins and REST API's experience
 Excellent written and verbal communication skills

For data Science/Machine learning

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
 Project work on the technologies needed
 Highly motivated, self-learner, and technically inquisitive
 Experience in programming language Java and understanding of the software development life cycle
 Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
 Excellent written and verbal communication skills

Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis

Please understand skills and relevant experience on real world projects are required by clients for selection even if its Junior or entry level position the additional skills and Project work with hands on experience building projects at client site are the only way a candidate can be picked by clients. No third party candidates or c2c candidates

please only apply to the posting

If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.

No phone calls please. Shortlisted candidates would be reached out.","Lenguajes de programación y Programación, Ciencias de la computación, Cracking, Desarrollo de software, Java, Lista de preseleccionados, Microservicios, Plataforma Java y Transferencia de Estado Representacional (REST)",Solicitar
https://www.linkedin.com/jobs/view/3829611881/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=LwufEiB%2FI%2Bm%2FyvSdAICofQ%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Programmer/Coder/Developer/Data Scientist/Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Beaumont, TX","Acerca del empleo
Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  Apple, google, Paypal, Western Union, Client, visa, walmart lab s, etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java full-stack developers, Python/Java developers, Data analysts/ Data Scientists, and Machine Learning engineers for full-time positions with clients.

Who Should Apply Recent Computer Science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry?

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates 

 If you applied for a job and got emails from our Job Placement Program team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

 please check the below links to see the successful outcomes of our candidates our  participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/ 

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage see us exhibiting at Oracle Cloud World/Oracle Java One (Las Vegas) -2023/2022 and Gartner Data Analytics Summit (Florida)-2023 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 https://www.youtube.com/watch?v=OFoqPTNORew 

 https://www.youtube.com/watch?v=-HkNN1ag6Zk 

 https://www.youtube.com/watch?v=OAFOhcGy9Z8 

 https://youtu.be/bJJl27D8bh0 

To prepare for interviews please visit  https://www.synergisticit.com/interview-questions/

 We are looking for the right matching candidates for our clients 

 Please apply via the job posting 

Required Skills

 REQUIRED SKILLS For Java /Full stack/Software Programmer 

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Project work on the skills 
 Knowledge of Core Java, Javascript, C++, or software programming 
 Spring boot, Microservices, Docker, Jenkins, and REST API experience 
 Excellent written and verbal communication skills 

 For data Science/Machine learning Positions 

Required Skills

 Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT 
 Project work on the technologies needed 
 Highly motivated, self-learner, and technically inquisitive 
 Experience in programming language Java and understanding of the software development life cycle 
 Knowledge of Statistics, SAS, Python, Computer Vision, and data visualization tools 
 Excellent written and verbal communication skills 

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow 

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team and only connect with candidates who are matching client requirements. 

 No phone calls, please.  Shortlisted candidates would be reached out. No third-party or agency candidates or c2c candidates.","Analítica de datos, Lenguajes de programación y Programación, Ciclo de vida de desarrollo de software (SDLC), Comunicación, Desarrollo de software, Java, JavaScript, Plataforma Java y Stack",Solicitar
https://www.linkedin.com/jobs/view/3982686310/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=pzWmi%2BR56JDm4UYlqLliCA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer Sr@Dallas and NJ,"Presencial Media jornada
Coincide con tus preferencias de empleo. El tipo de empleo es Media jornada.
Intermedio",hace 6 días,"Dallas, TX","Acerca del empleo
""ALL our jobs are US based and candidates must be in the US with valid US Work Authorization. Please apply on our website directly."" Greetings from Canopy One Solutions,Hope your day is treating you well!Please glance the requirement & respond me back with your finest consultant Resumes & Contact Details.

Project Details:Role:Data Engineer SrLocation:Dallas and NJDuration:6+ months/longtermType:FTEInterview Criteria: Telephonic + Skype

Job Description

Data Modeling and Design: This is the core skill, encompassing proficiency in database design (relational and NoSQL), SQL development, and data modeling tools.

Data Management and Technologies: Understanding data warehousing, data governance, data security, and big data technologies like Hadoop and Spark.

Programming Languages: Familiarity with languages like Python, Java, and experience with ETL/ELT tools for data extraction, transformation, and loading.

Cloud Computing: Knowledge of cloud platforms like AWS, Azure, and GCP for data storage, processing, and analytics

Knowledge of architecture for GenAI RAG applications will be an added advantage","Almacenamiento de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL), NoSQL y SQL, Bases de datos, Diseño de bases de datos, Extracción de datos, Herramientas de modelización y Modelado de datos",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3934500009/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=YELkCXM1zNzHaum2iAH8RA%3D%3D&trackingId=Ynl8QXpg%2F6jB080M8XuxxQ%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Data Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 2 meses,"Topeka, KS","Acerca del empleo
For more than 12 years Synergisticit has helped Jobseekers stand out from other Jobseekers by providing candidates the requisite skills, experience and technical competence to outperform at interviews and at clients. Here at SynergisticIT We just don't focus on getting you a tech Job we make careers.

 In this market also our candidates get multiple job offers and $100k + salaries.

 please check the below links to see success outcomes of our candidates .

 https://www.synergisticit.com/candidate-outcomes/

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023

All Positions are open for all visas and US citizens

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.

Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

 https://www.youtube.com/watch?v=OFoqPTNORew

 https://www.youtube.com/watch?v=-HkNN1ag6Zk

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

 https://youtu.be/bJJl27D8bh0

 We want Data Science/Machine learning/Data Analyst and Java Full stack candidates

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Project work on the technologies needed

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools

Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow

REQUIRED SKILLS For Java /Full Stack/Software Positions

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT

Highly motivated, self-learner, and technically inquisitive

Experience in programming language Java and understanding of the software development life cycle

Project work on the skills

Knowledge of Core Java , javascript , C++ or software programming

Spring boot, Microservices, Docker, Jenkins and REST API's experience

Excellent written and verbal communication skills

 If you get emails from our Job Placement team and are not interested please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.

 No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Analítica, Analítica de datos, Ciencia de datos, Lenguajes de programación, Visualización y Visualización de datos, Comunicación, Desarrollo de software, Java y Plataforma Java",Solicitar
https://www.linkedin.com/jobs/view/3943543072/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=AFtzXErqtYZBjfZOwSQetg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
About Cypris:

At Cypris, we're building the single ecosystem for global innovation data. Cypris is an AI-powered research tool that centralizes data sources like scientific papers, global patents, market news, and company data into one platform. Cypris arms users with access to unique insights from 500M+ global data points, answering key questions about their market, competitors, core technologies & more to support new product development, commercial strategy & accelerate global innovation.

We're connecting R&D teams to the global innovation landscape like the Bloomberg Terminal did for the finance world & Pitchbook for venture capital. Current users include leading R&D & innovation teams at mid-size to Fortune 100 companies operating in emerging markets like aerospace, genomics, cancer research, autonomous vehicles, and more.

About the Role:

In this role, you will be responsible for designing, building, and maintaining scalable data pipelines and systems to support our data-driven platform. You will work closely with other members of our engineering team to ensure the availability and quality of data required to supply innovation analytics and insights. This is an exciting opportunity to contribute to our data infrastructure and help shape the future of our data capabilities.

In This Role You Will:

Design, develop, and optimize robust data pipelines to process and transform large datasets from various sources
Optimize data stores' performance in terms of index response times and query response times
Implement and maintain ETL processes to ensure data accuracy and integrity
Collaborate with cross-functional teams to understand data requirements and deliver effective data solutions
Develop and maintain data warehouses and data lakes to support business intelligence and analytics
Monitor and troubleshoot data pipeline performance and reliability, implementing improvements as needed
Ensure data security and compliance with relevant regulations and standards
Stay up-to-date with the latest technologies and best practices in data engineering and incorporate them into our processes


Requirements

A Key Candidate Will Have:

7+ years of proven experience as a Data Engineer or in a similar role
Bachelor's or Master's degree in Computer Science, Engineering, or a related field
Proficiency in programming languages like Python, Java, or Scala
Experience with cloud platforms such as GCP (preferred), AWS, Google Cloud, or Azure
Hands-on experience with big data technologies such as Hadoop, Spark, or similar frameworks
Knowledge of data warehousing concepts and experience with tools like Redshift, BigQuery, or Snowflake
Familiarity with ETL tools and processes
Strong problem-solving skills and attention to detail
The desire to contribute and grow at an early-stage startup


Technologies We Use: 

Python
GCP 
Apache Beam
MongoDB
Elasticsearch


Benefits

Through this role, you will get:

A strong base salary, bonus structure + equity
To have your voice & opinion heard
Proper training to be armed with the right knowledge to find success in our market
To help drive the future growth of our platform and team

How do I get in touch?

Please apply & we'll be in touch! If your resume and background appear like a good match, we will reach out for an initial phone screen.

Where are Cypris's offices located?

Cypris has offices in New York, Los Angeles, and Boulder.

What are Cypris' future hiring plans?

We're always looking for the best talent across departments. We will regularly post new job opportunities on the Cypris Careers page, Wellfound, or LinkedIn.

How does Cypris make money?

We sell annual access to our dashboard to R&D, innovation & IP teams looking to better understand the innovation landscape. We also build custom reports for objective-oriented projects. Both can be included in one package.

What does the interview process look like?

The process depends on the role. For all positions, we'll start with an introduction call with our Talent team.

From there, the interview process will vary by role. We typically do a longer call with one of our team members who can appropriately vet your skills & ""speak your language"" - on the engineering side, this might mean some technical questions, while on the sales side, we'll ask you to build a quick presentation about yourself.

Finally, we'll check over a few references. If you're near one of our offices, we may give you the opportunity to meet in person.🚀","Almacenamiento de datos, Analítica de datos, Extraer, transformar y cargar (ETL), Google BigQuery, Ingeniería de datos y Scala, Amazon Redshift, Ciencias de la computación, Resolución de problemas y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3857226270/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=cuU00zUId5Hmu4QSLugr9Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 4 meses,Estados Unidos,"Acerca del empleo
Remote Opportunity

Healthcare Experience Required

Please find below JD: Need Profiles ASAP

Strong experience with HL7 FHIR concepts and terminology
Strong experience with Google Cloud Healthcare Data Engine, Whistle, and BigQuery
Strong experience with common data engineering tools (Spark, Python, shell scripting)
This is the 2nd category – these are engineers with strong experience mapping Hl7 to FHIR and strong google native stacks experience,
This common Ingestion Framework spits out Hl7 messages – this is where 2nd block of work start. HL7 messages need to be converted to FHIR using GCP native stack ( big query + whistle) –
Folks who have hands on experience of mapping hl7 to fhir will be required along with hands on experience with GCP native stack.","Apache Spark, Buena práctica clínica, Ciencia de datos, Extraer, transformar y cargar (ETL), Google BigQuery, Google Cloud y Ingeniería de datos, Fast Healthcare Interoperability Resources (FHIR), HL7 y Stack",Solicitud sencilla
https://www.linkedin.com/jobs/view/3984136360/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=6Arw2h3ppqn0mG4oI20trg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Castle Rock, CO","Acerca del empleo
Role: Data Engineer

Duration: 12+ months

Location: 2x week onsite in Castle Rock, CO (Must be local)

TOP SKILLS: Azure (Data Lake, Data Factory), SSIS, SQL Server, (Must have most if not all skills)","Almacenamiento de datos, Azure Data Factory, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , SQL y SQL Server Integration Services (SSIS), Bases de datos y Lagos de datos",Solicitar
https://www.linkedin.com/jobs/view/3978256583/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=78%2F6lEefPMAONoDEjmhZBQ%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer - (Azure),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Job Description
iTalent Digital is seeking a Data Engineer to join our Retail Technology and Data Practice Groups.

The role is an exciting opportunity into one of our fastest growing clients, who is a global prestigious omni-retail leader. You will also interact closely with our iTalent Data and Retail Practice Leaders, who will partner with you and provide ongoing coaching and mentoring.

Your area of responsibility is to deliver on retail media network (RMN) and product promotion projects on our client's eCommmerce site. This role is part of the eCommerce IT team partnering with internal stakeholders.

A successful consultant will have strong data expertise with ability to jump into a complex environment. You will drive BI centric priorities pulling/extracting/joining various data sources to generate meaningful insights and meet business requirements for decision-making activities.

This position has the potential to become a long-term engagement.

Required qualifications and skills
4+ years of experience building software as a team following Agile methodologies.
Lead development, operations, and support with documentation, testing, stories, and fixes for onshore and offshore teams.
Data Architecture and Design – Develop, implement, and maintain efficient ETL and real-time data load to ensure seamless data flow and consolidate information from multiple sources.
Data Quality and Governance – Establish and enforce strict data quality standards for accuracy, completeness, and consistency.
Optimization and Performance – Optimize data pipelines and storage solutions to uphold performance and efficiency expectations.
Teamwork – Collaborate with multiple stakeholders to understand business objectives and deliver engineering solutions and support. Work with technology teams both on and offshore.
Technology stack: Azure Databricks, Azure Data Factory, Scala, SQL, Kafka and streaming jobs, PySpark, Azure Eco System and Google Big Query.

Preferred qualifications and skills
Experience in Retail
Background in Data Engineering, Data Science, Statistics
Strong production-level Python and SQL programming knowledge. Bonus if you have also worked with BigQuery.
Experience building and scaling batch ETL/ELT data pipelines, especially in Apache Airflow, Kubernetes, and DBT. Understanding of size and performance constraints.
Experience building and scaling streaming data pipelines, especially in Apache Kafka, Spark, or Beam.
Experience architecting and building data products with Cloud Services and Data Warehousing services in GCP or Azure (GCP is preferred).
Knowledge of columnar databases, infrastructure automation tools like Terraform, data modeling techniques, data warehouse design patterns and google analytics is advantageous.
Demonstrated experience generating proactive business insights and recommendations from complex data.
Extensive knowledge of distributed systems, scalability, process flows and procedures to aid analyses and recommendations for solution offerings.

Education
Bachelors Degree

Company description
About iTalent Digital:
A woman- and minority-owned digital consulting company, we celebrate individuals and diversity, cultivating a culture where our people can excel and lead balanced lives. Recruitment at iTalent is guided by an unwavering principle: Only hire the best. Because we have the best people, we have the privilege of working with the best clients, doing the best work, and effecting transformative change at work and in our communities.

Additional info
What you get:
You get the chance to work with some of the best brands and high-performance teams out there! iTalent offers our W2 consultants excellent benefits such as medical, dental, vision, life insurance and 401K + matching. We are growing and we want to see you grow!

Log onto www.iTalentDigital.com to learn more about what working at iTalent can mean for you.","Extraer, transformar y cargar (ETL), Google Cloud , Python y SQL, Conocimientos comerciales, Tecnología para minoristas y Terapia dialéctica conductual",Solicitud sencilla
https://www.linkedin.com/jobs/view/3961914704/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=AskY7cpg3gCuqr2wBmjQOg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 6 días,"Nueva York, NY","Acerca del empleo
You Lead the Way. We’ve Got Your Back.

With the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, you’ll learn and grow as we help you create a career journey that’s unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.

At American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong.

Join Team Amex and let's lead the way together.

As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skill fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.

Let’s share success. 

Global Infrastructure Product Services aims to establish and reinforce a culture of effective metrics, data-driven business processes, architecture simplification, and cost transparency.

A successful Data Analyst knows that delivering on that promise takes foresight, planning and agility. We are seeking Data Analysts who are not only technically adept, but also understand the importance of harnessing the power of data to streamline financial operations, enhance decision-making and improve business outcomes.

This role is focused on developing and maintaining a dynamic technology infrastructure cost allocation model, and requires a blend of technical prowess in SQL and database technologies, understanding of enterprise infrastructure components, and the ability to continuously integrate and improve our financial modeling processes.

Let’s Build On What You Know.

If you are a pioneer in developing robust cost allocation models and infrastructure analysis, you'll find a fit within our Global Infrastructure Cost Transparency team responsible for establishing consistent on-premises workload attribution, unit economics and business value metrics. By leveraging ApptioOne and TBM Studio, you will ensure that our technology spending is transparent, efficient, and properly aligned with our strategic objectives. Here’s just some of what you’ll do:

Architect and refine a robust technology infrastructure cost allocation model using ApptioOne and TBM Studio, with a focus on continuous integration and improvement
Extract, transform and load (ETL) data from diverse sources, including general ledger fields and other systems of record into a coherent, actionable format 
Maintain seamless data links between internal systems and ApptioOne, ensuring high data quality and consistency 
Work closely with stakeholders across Technology, Finance and business unit portfolio leaders to define data and analysts requirements and incorporate cost drivers, allocation methods, and infrastructure nuances 
Regularly review and update the cost model to reflect changes in technology infrastructure, business strategy and financial policies 
Develop SQL queries, scripts and routines to automate data processing and enhance the model’s accuracy and efficiency 
Generate insightful data visualization and reports to aid in decision-making and demonstrate the cost model’s value to stakeholders 
Lead training sessions and create comprehensive documentation to empower end users to leverage the cost model effectively 
Function as an active member of an agile team through consistent development practices (tools, components, and documentation) 
Find opportunities to adopt innovative technologies 
Take part in reviews of individual or team contributions 
Work on prioritized/assigned features for ongoing sprints 
Communicate and work reciprocally with business and product teams to support changes and implementation 

Are you up for the challenge? Here’s what you should have: 

Hands-on expertise with distributed (multi-tiered) systems and relational database design, development, troubleshooting and automated testing 
Demonstrated experience in data visualization with a focus on financial data analysis and cost modeling 
Advanced proficiency in SQL and familiarity with other relational database technologies 
Proficiency with industry recognized ETL methods, processes and standards 
Proficiency with KPI and metric design 
Thorough understanding of enterprise infrastructure technologies such as Compute, Storage, Network, Mainframe to inform model development and adjustments 
Strong analytical, problem-solving and project management skills, coupled with a continuous improvement mindset 
Excellent communication skills, capable of explaining complex data issues in simple terms to a diverse audience 
Strong proficiency in data manipulation and analysis using programming languages such as Python, R, or Scala 
Demonstrable experience with data visualization and reporting tools (e.g.: MS SQL Reporting Services, Tableau, Informatica, Cognos, Microstrategy, PowerBI, matplotlib, etc.) 
Bachelor’s or Master’s degree in Computer Science, Engineering, Finance or a related discipline is a plus 
Deep experience with Apptio One and TBM Studio is highly desirable

Salary Range: $85,000.00 to $150,000.00 annually + bonus + benefits

The above represents the expected salary range for this job requisition. Ultimately, in determining your pay, we’ll consider your location, experience, and other job-related factors.

We back our colleagues and their loved ones with benefits and programs that support their holistic well-being. That means we prioritize their physical, financial, and mental health through each stage of life. Benefits include:

Competitive base salaries 
Bonus incentives 
6% Company Match on retirement savings plan 
Free financial coaching and financial well-being support 
Comprehensive medical, dental, vision, life insurance, and disability benefits 
Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need 
20+ weeks paid parental leave for all parents, regardless of gender, offered for pregnancy, adoption or surrogacy 
Free access to global on-site wellness centers staffed with nurses and doctors (depending on location) 
Free and confidential counseling support through our Healthy Minds program 
Career development and training opportunities

For a full list of Team Amex benefits, visit our Colleague Benefits Site.

American Express is an equal opportunity employer and makes employment decisions without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, disability status, age, or any other status protected by law.

We back our colleagues with the support they need to thrive, professionally and personally. That's why we have Amex Flex, our enterprise working model that provides greater flexibility to colleagues while ensuring we preserve the important aspects of our unique in-person culture. Depending on role and business needs, colleagues will either work onsite, in a hybrid model (combination of in-office and virtual days) or fully virtually.

US Job Seekers/Employees - Click here to view the “Know Your Rights” poster and the Pay Transparency Policy Statement.

If the links do not work, please copy and paste the following URLs in a new browser window: https://www.dol.gov/agencies/ofccp/posters to access the three posters.

Employment eligibility to work with American Express in the U.S. is required as the company will not pursue visa sponsorship for this position.","Analítica, Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Scala y Visualización de datos, Financial Data Analysis, Manipulación de datos, Modelado de datos y Modelos de coste",Solicitar
https://www.linkedin.com/jobs/view/3948071314/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=g2kQylR2n%2FUxmYGUK7LPIg%3D%3D&trk=flagship3_search_srp_jobs,Data Analytics Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 1 mes,"Tysons Corner, VA","Acerca del empleo
Company Description

MicroStrategy transforms organizations into intelligent enterprises through data-driven innovation. We match smart people to dynamic projects and technologies that truly challenge their talents. Curious and creative in outlook, our success is built on the talent and energy of smart and driven people. MicroStrategy (Nasdaq: MSTR) is a worldwide leader in enterprise analytics and mobility software. A pioneer in the BI and analytics space, MicroStrategy delivers innovative software that empowers people to make better decisions and transform the way they do business. We provide our enterprise customers with world-class software and expert services so they can deploy unique intelligence applications.

Job Description

Job Summary:

We are seeking a highly motivated Data Analytics Engineer to join our team. As a Data Analytics Engineer, you will be responsible for designing, developing, and implementing natural language processing (NLP) and AI-based solutions to enhance our products and services. You will work collaboratively with cross-functional teams to identify and solve complex problems using NLP and AI techniques.

Responsibilities

Design, develop, and implement NLP and AI-based solutions to enhance our products and services.
Collaborate with cross-functional teams, including data scientists, software engineers, customer support specialists and product managers, to define and deliver NLP and AI solutions.
Conduct data analysis and exploratory research to identify potential areas for improvement.
Build and maintain large-scale data pipelines and ETL processes to support NLP and AI workflows.
Utilize programming languages such as Python, R, and SQL to develop and deploy NLP and AI models. Leverage and integrate with GPT models.
Implement best practices for NLP and AI model training, validation, and deployment to ensure accuracy, scalability, and maintainability.
Monitor and evaluate the performance of NLP and AI models, and refine them to improve accuracy and efficiency.
Keep up-to-date with the latest NLP and AI research and techniques, and share your knowledge with the team.

Qualifications

Qualifications:

 Bachelor's or Master's degree in Computer Science or a related field.
 Proven experience in developing and deploying NLP and AI-based solutions.
 Strong programming skills in Python, R, SQL, and React.
 Experience with NLP and AI frameworks such as NLTK, Spacy, TensorFlow, or PyTorch.
 Familiarity with data visualization tools such as MicroStrategy preferred.
 Strong analytical and problem-solving skills.

If you are a passionate and driven Data Analytics Engineer who wants to make an impact with cutting-edge technologies, we would love to hear from you!

Additional Information

MicroStrategy is an Equal Employment and Affirmative Action employer F/M/Disability/Vet/Sexual Orientation/Gender Identity. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, national origin, sexual orientation, gender identity, disability, veteran status, sex age, genetic information, or any other legally-protected basis.

MicroStrategy is an Equal Employment /Affirmative Action employer and provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need an accommodation due to a disability, you may contact us about your interest in employment at 703-848-8600.

MicroStrategy is an Equal Employment and Affirmative Action employer F/M/Disability/Vet/Sexual Orientation/Gender Identity.

MicroStrategy is an Equal Employment /Affirmative Action employer and provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need an accommodation due to a disability, you may contact us about your interest in employment at application_accommodations@microstrategy.com.","Analítica, Analítica de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Procesamiento de lenguaje natural, Bases de datos, Ciencias de la computación y Resolución de problemas",Solicitud sencilla
https://www.linkedin.com/jobs/view/3787334858/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=hM3QfAsqtsOzirkMX9%2BBNA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 7 meses,"San Francisco, CA","Acerca del empleo
Why now?

We've always been heavy data users, for product, marketing and growth.

From self-serve solution, we quickly shifted to more advanced systems in which we replicate all interesting data to a warehouse and add BI tools for reporting and querying on top of it.

We're now reaching a limit: we have needs on all front, and can't maintain all our analytics properly. We need dedicated expertise to own this part, manage our data pipeline and tooling, but also coach the team on data to make it use the right way.

We're ready to welcome Slite's first data engineer!

This is a remote position.

💪 What's my mission?

Own the BItoolchain: pick, implement and maintain BI tools and setup.

Help product track the features usage and user behaviours. Help them shape the roadmap and next features accordingly.

Monitor and assess marketing inbound efforts (SEO, natural & paid search, content)

Help growth prioritize and measure the impact of its experiments

Maintain core reports and dashboard for all teams

Coach all team to properly use data in their job

🤗 Who will I be working with?

This is a true core role for the team as it comes in support of all its parts:

@mike VP product of Slite

@adrien growth engineer

@laure marketing

@chris CEO

📖 What do I need?

Why now?

We've always been heavy data users, for product, marketing and growth.

From self-serve solution, we quickly shifted to more advanced systems in which we replicate all interesting data to a warehouse and add BI tools for reporting and querying on top of it.

We're now reaching a limit: we have needs on all front, and can't maintain all our analytics properly. We need dedicated expertise to own this part, manage our data pipeline and tooling, but also coach the team on data to make it use the right way.

We're ready to welcome Slite's first data engineer!

This is a remote position.

💪 What's my mission?

Own the BItoolchain: pick, implement and maintain BI tools and setup.

Help product track the features usage and user behaviours. Help them shape the roadmap and next features accordingly.

Monitor and assess marketing inbound efforts (SEO, natural & paid search, content)

Help growth prioritize and measure the impact of its experiments

Maintain core reports and dashboard for all teams

Coach all team to properly use data in their job

🤗 Who will I be working with?

This is a true core role for the team as it comes in support of all its parts:

@mike VP product of Slite

@adrien growth engineer

@laure marketing

@chris CEO

📖 What do I need?

Strong analytical skills

SQL proficiency

Great communication, especially written communication

Structured mindset

Data tooling experience

Javascript and Git minimal skillset

✨extra little plus

Extra JS and node expertise

AI skills: understanding and ability to work with ML models and foresee the potential it could unlock for Slite would be a huge added value.

Past remote experience

Lever builds modern recruiting software for teams to source, interview, and hire top talent. Our team strives to set a new bar for enterprise software with modern, well-designed, real-time apps. We participated in Y Combinator in summer 2012, and since then have raised $73 million. As the applicant tracking system of choice for Netflix, Eventbrite, ClearSlide, change.org, and thousands more leading companies, Lever means you hire the best by hiring together.

Lever is an equal opportunity employer. We are committed to providing reasonable accommodations and will work with you to meet your needs. If you are a person with a disability and require assistance during the application process, please don’t hesitate to reach out! We celebrate our inclusive work environment and welcome members of all backgrounds and perspectives. Learn more about our team culture and commitment to diversity and inclusion.","Almacenamiento de datos, Analítica de datos, Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Comunicación y Comunicación escrita",Solicitar
https://www.linkedin.com/jobs/view/3979120245/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=euFbrcoT1eI7Fyl%2FDNxmRA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,"Dearborn, MI","Acerca del empleo
Details:

Job Description

Stefanini Group is hiring!

Stefanini is looking for a Data Engineer, Dearborn, MI (Hybrid/Remote) 

For quick apply, please reach out Anmol Tyagi at anmol.tyagi@stefanini.com/ 248-263-8628

We're seeking a Data Engineer who has experience building data products on a cloud analytics platform. You will work on ingesting, transforming, and analyzing large datasets to support the Enterprise in the Data Factory on Google Cloud Platform (GCP). Experience with large scale solution and operationalization of data lakes, data warehouses, and analytics platforms on Google Cloud Platform or other cloud environments is a must. We are looking for candidates who have a broad set of technical skills across these areas.

Key Responsibilities

Work in collaborative environment that leverages paired programming.
Work on a small agile team to deliver curated data products.
Work effectively with fellow data engineers, product owners, data champions and other technical experts.
Demonstrate technical knowledge and communication skills with the ability to advocate for well-designed solutions.
Develop exceptional analytical data products using both streaming and batch ingestion patterns on Google Cloud Platform with solid data warehouse principles.
Be the Subject Matter Expert in Data Engineering with a focus on GCP native services and other well integrated third-party technologies

Job Requirements

Details:

Skills Required

Experience in working in an implementation team from concept to operations, providing deep technical subject matter expertise for successful deployment.
Implement methods for automation of all parts of the pipeline to minimize labor in development and production
Experience in analyzing complex data, organizing raw data, and integrating massive datasets from multiple data sources to build analytical domains and reusable data products
Experience in working with architects to evaluate and productionalize data pipelines for data ingestion, curation, and consumption
Experience in working with stakeholders to formulate business problems as technical data requirements, identify and implement technical solutions while ensuring key business drivers are captured in collaboration with product management

Experience Required

5+ years of SQL development experience
5+ years of analytics/data product development experience required
3+ years of Google cloud experience with solutions designed and implemented at production scale
Experience working in GCP native (or equivalent) services like Big Query, Google Cloud Storage, PubSub, Dataflow, Dataproc, Cloud Build, etc.
Experience migrating Teradata to GCP - Experience working with Airflow for scheduling and orchestration of data pipelines
Experience working with Terraform to provision Infrastructure as Code
2 + years professional development experience in Java or Python

Skills Preferred

Strong drive for results and ability to multi-task and work independently
Self-starter with proven innovation skills
Ability to communicate and work with cross-functional teams and all levels of management 
Demonstrated commitment to quality and project timing
Demonstrated ability to document complex systems

Experience Preferred

Experience converting Microsoft SSRS, SSAS and SSIS to BQ
Experience converting Mainframe JCL, COBOL and CA7
Experience converting SAS code to BQ
Experience converting Web Focus reports in to Qlik Sense
In-depth understanding of Google's product technology (or other cloud platform) and underlying architectures
Experience in working with DBT/Dataform
Experience with DataPlex or other data catalogs is preferred
Experience with development eco-system such as Tekton, Git, Jenkins for CI/CD pipelines
Exceptional problem solving and communication skills
Experience in working with Agile and Lean methodologies
Team player and attention to detail
Experience with performance tuning SQL queries

Education Required

Bachelor's degree in computer science or related scientific field

Education Preferred

GCP Professional Data Engineer Certified
Master's degree in computer science or related field
2+ years mentoring engineers
In-depth software engineering knowledge
Listed salary ranges may vary based on experience, qualifications, and local market. Also, some positions may include bonuses or other incentives***

Stefanini takes pride in hiring top talent and developing relationships with our future employees. Our talent acquisition teams will never make an offer of employment without having a phone conversation with you. Those face-to-face conversations will involve a description of the job for which you have applied. We also speak with you about the process including interviews and job offers.

About Stefanini Group

The Stefanini Group is a global provider of offshore, onshore and near shore outsourcing, IT digital consulting, systems integration, application, and strategic staffing services to Fortune 1000 enterprises around the world. Our presence is in countries like the Americas, Europe, Africa, and Asia, and more than four hundred clients across a broad spectrum of markets, including financial services, manufacturing, telecommunications, chemical services, technology, public sector, and utilities. Stefanini is a CMM level 5, IT consulting company with a global presence. We are CMM Level 5 company.","Almacenamiento de datos, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Google Cloud y Ingeniería de datos, Ajuste de rendimiento, Ciencias de la computación, Comunicación, Datasets y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3982715806/?eBP=BUDGET_EXHAUSTED_JOB&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=fgqcjN1VP4A4MIvjNRerRg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer III,"107,7 US$K/año - 161,5 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 horas,"King of Prussia, PA","Acerca del empleo
About FedEx Dataworks

Born out of FedEx, a pioneer that ships nearly 20 million packages a day and manages endless threads of information, FedEx Dataworks is an organization rooted in connecting the physical and digital sides of our network to meet today's needs and address tomorrow's challenges.

We are creating opportunities for FedEx, our customers, and the world at large by:

Exploring and harnessing data to define and solve true problems
Removing barriers between data sets to create new avenues of insight
Building and iterating on solutions that generate value
Acting as a change agent to advance curiosity and performance
At FedEx Dataworks, we are making supply chains work smarter for everyone.

Summary: The Data Engineer III plays a pivotal role within Dataworks, focused on driving engineering innovation within Dataworks, helping define and build the Dataworks organization and leading the delivery of key business initiatives. He or she acts as a “universal translator” between IT, business, software engineers and data scientists, collaborating with these multi-disciplinary teams. The Data Engineer III will contribute to the adherence of technical standards for data engineering, including the selection and refinements of foundational technical components. Swill work on those aspects of the Dataworks platform that govern the ingestion, transformation, and pipelining of data assets, both to end users within FedEx and into data products and services that may be externally facing. Day-to-day, he or she will be deeply involved in code reviews and large-scale deployments. He or she will also provide mentorship and guidance to junior engineers to support the continued training and up-skilling of the Data Engineering team.

Under limited supervision, supports the design, build, test and maintenance of data pipelines at big data scale. Assists with updating data from multiple data sources. Other functionalities under limited supervision include working on batch processing of collected data and matching its format to the stored data, making sure that the data is ready to be processed and analyzed. Assists with keeping the ecosystem and the pipeline optimized and efficient, troubleshooting standard performance, data related problems and providing L3 support. Implements ETL transformers to reformat and enhance the data. Provides recommendations to complex problems. Provides guidance to those in less senior positions.

Job Description / Responsibilities

Building tools, platforms and pipelines to enable teams to clearly and cleanly analyze data, build models and drive decisions
Scaling up from “laptop-scale” to “cluster scale” problems, in terms of both infrastructure and problem structure and techniques
Delivering tangible value very rapidly, collaborating with complementary teams of varying backgrounds and subject areas
Championing the adherence to best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases
Interacting with senior technologists from the broader enterprise and outside of FedEx (partner ecosystems and customers) to create synergies and ensure smooth deployments to downstream operational systems
Design, implement and optimize ETL processes, data ingestion, and integration workflows on cloud environments
Implement best practices for data management, including data quality and security.
Implement cloud-based data storage solutions, leveraging technologies for performance and cost efficiency
Identify opportunities for automation and process improvement within the data engineering workflow.
Stay updated with emerging technologies and trends in data engineering to enhance our systems and processes.

Skills/Abilities

Technical background in computer science, software engineering, database systems, distributed systems
Fluency with distributed and cloud environments and a strong understanding of how to balance computational considerations with theoretical properties
Detailed knowledge of the Microsoft Azure tooling for large-scale data engineering efforts and deployments is highly preferred
A track record of designing and deploying large scale technical solutions, which deliver tangible, ongoing value:
Direct experience having built and deployed robust, complex production systems that implement modern, data scientific methods at scale
Ability to context-switch, to provide support to dispersed teams which may need an “expert hacker” to unblock an especially challenging technical obstacle, and to work through problems as they are still being defined
Demonstrated ability to deliver technical projects with a team, often working under tight time constraints to deliver value
An ‘engineering’ mindset, willing to make rapid, pragmatic decisions to improve performance, accelerate progress or magnify impacy
Comfort with working with distributed teams on code-based deliverables, using version control systems and code reviews
Ability to conduct data analysis, investigation, and lineage studies to document and enhance data quality and access quickly while adopting new and evolving technologies and apply to projects.
Use of agile and devops practices for project and software management including continuous integration and continuous delivery
Demonstrated expertise working with some of the following common languages and tools:
Spark (Scala and PySpark), HDFS, Kafka and other high volume data tools
SQL and NoSQL storage tools, such as MySQL, Postgres, Cassandra, MongoDB and ElasticSearch
Pandas, Scikit-Learn, Matplotlib, TensorFlow, Jupyter and other Python data tools
Solid understanding of data modeling, data architecture, and data management principles.
Proven track record of solving complex problems and delivering high-quality, scalable solutions.
Technical background in computer science, software engineering, database systems, distributed systems
Fluency with distributed and cloud environments and a strong understanding of how to balance computational considerations with theoretical properties
Detailed knowledge of the Microsoft Azure tooling for large-scale data engineering efforts and deployments is highly preferred
A track record of designing and deploying large scale technical solutions, which deliver tangible, ongoing value
Direct experience having built and deployed robust, complex production systems that implement modern, data scientific methods at scale
Ability to context-switch, to provide support to dispersed teams which may need an “expert hacker” to unblock an especially challenging technical obstacle, and to work through problems as they are still being defined
Demonstrated ability to deliver technical projects with a team, often working under tight time constraints to deliver value
An ‘engineering’ mindset, willing to make rapid, pragmatic decisions to improve performance, accelerate progress or magnify impact
Comfort with working with distributed teams on code-based deliverables, using version control systems and code reviews
Ability to conduct data analysis, investigation, and lineage studies to document and enhance data quality and access quickly while adopting new and evolving technologies and apply to projects.
Use of agile and devops practices for project and software management including continuous integration and continuous delivery
Demonstrated expertise working with some of the following common languages and tools:
Spark (Scala and PySpark), HDFS, Kafka and other high volume data tools
SQL and NoSQL storage tools, such as MySQL, Postgres, Cassandra, MongoDB and ElasticSearch
Pandas, Scikit-Learn, Matplotlib, TensorFlow, Jupyter and other Python data tools
Solid understanding of data modeling, data architecture, and data management principles.
Proven track record of solving complex problems and delivering high-quality, scalable solutions.
Excellent communication and collaboration skills to work effectively in a team environment

Minimum Qualifications

Bachelor’s Degree in Information Systems, Computer Science or a quantitative discipline such as Mathematics or Engineering and/or equivalent formal training or work experience. Three to Four (3 - 4) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Extensive knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Extensive knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience providing leadership in a general planning or consulting setting. Experience as a senior member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.

Pay Transparency: This compensation range is provided as a reasonable estimate of the current starting salary range for this role across all potential locations. If this opportunity includes multiple job levels, the salary information represents the job level minimum and the job level maximum. Actual starting pay would be determined by experience relative to the job, market level, pay at the location for this job and other job-related factors permitted by law. An employee may be eligible for additional pay, premiums, or bonus potential. The Company offers eligible employees health, vision and dental insurance, retirement, and tuition reimbursement.

Pay: Annual Range $107,676 - $161,520

Domicile Information: This position can be domiciled anywhere in the United States. The ability to work remotely within the United States may be available based on business needs.

Application Criteria: Upload a current copy of Resume (Microsoft Word or PDF format only) and answer job screening questionnaire by 5 PM CT, July 31, 2024.

EEO Information

Dataworks does not discriminate against qualified individuals with disabilities in regard to job application procedures, hiring, and other terms and conditions of employment. Further, Dataworks is prepared to make reasonable accommodations for the known physical or mental limitations of an otherwise qualified applicant or employee to enable the applicant or employee to be considered for the desired position, to perform the essential functions of the position in question, or to enjoy equal benefits and privileges of employment as are enjoyed by other similarly situated employees without disabilities, unless the accommodation will impose an undue hardship. If a reasonable accommodation is needed, please contact DataworksTalentAcquisition@corp.ds.fedex.com.

Minimum Education

Bachelor’s Degree in Information Systems, Computer Science or a quantitative discipline such as Mathematics or Engineering and/or equivalent formal training or work experience.

Minimum Experience

Three to Four (3 - 4) years equivalent work experience in measurement and analysis, quantitative business problem solving, simulation development and/or predictive analytics. Extensive knowledge in data engineering and machine learning frameworks including design, development and implementation of highly complex systems and data pipelines. Extensive knowledge in Information Systems including design, development and implementation of large batch or online transaction-based systems. Strong understanding of the transportation industry, competitors, and evolving technologies. Experience providing leadership in a general planning or consulting setting. Experience as a senior member of multi-functional project teams. Strong oral and written communication skills. A related advanced degree may offset the related experience requirements.

Knolwedge, Skills And Abilities

Technical background in computer science, software engineering, database systems, distributed systemsFluency with distributed and cloud environments and a strong understanding of how to balance computational considerations with theoretical propertiesDetailed knowledge of the Microsoft Azure tooling for large-scale data engineering efforts and deployments is highly preferredA track record of designing and deploying large scale technical solutions, which deliver tangible, ongoing valueDirect experience having built and deployed robust, complex production systems that implement modern, data scientific methods at scaleAbility to context-switch, to provide support to dispersed teams which may need an “expert hacker” to unblock an especially challenging technical obstacle, and to work through problems as they are still being definedDemonstrated ability to deliver technical projects with a team, often working under tight time constraints to deliver valueAn ‘engineering’ mindset, willing to make rapid, pragmatic decisions to improve performance, accelerate progress or magnify impactComfort with working with distributed teams on code-based deliverables, using version control systems and code reviewsAbility to conduct data analysis, investigation, and lineage studies to document and enhance data quality and access quickly while adopting new and evolving technologies and apply to projects.Use of agile and devops practices for project and software management including continuous integration and continuous deliveryDemonstrated expertise working with some of the following common languages and tools:Spark (Scala and PySpark), HDFS, Kafka and other high volume data toolsSQL and NoSQL storage tools, such as MySQL, Postgres, Cassandra, MongoDB and ElasticSearchPandas, Scikit-Learn, Matplotlib, TensorFlow, Jupyter and other Python data toolsSolid understanding of data modeling, data architecture, and data management principles.Proven track record of solving complex problems and delivering high-quality, scalable solutions.Excellent communication and collaboration skills to work effectively in a team environment.

Preferred Qualifications

Pay Transparency: 

Pay

Additional Details: 

Born out of FedEx, a pioneer that ships nearly 20 million packages a day and manages endless threads of information, FedEx Dataworks is an organization rooted in connecting the physical and digital sides of our network to meet today's needs and address tomorrow's challenges.

We are creating opportunities for FedEx, our customers, and the world at large by:

Exploring and harnessing data to define and solve true problems
Removing barriers between data sets to create new avenues of insight
Building and iterating on solutions that generate value
Acting as a change agent to advance curiosity and performance

At FedEx Dataworks, we are making supply chains work smarter for everyone.

Dataworks does not discriminate against qualified individuals with disabilities in regard to job application procedures, hiring, and other terms and conditions of employment. Further, Dataworks is prepared to make reasonable accommodations for the known physical or mental limitations of an otherwise qualified applicant or employee to enable the applicant or employee to be considered for the desired position, to perform the essential functions of the position in question, or to enjoy equal benefits and privileges of employment as are enjoyed by other similarly situated employees without disabilities, unless the accommodation will impose an undue hardship. If a reasonable accommodation is needed, please contact DataworksTalentAcquisition@corp.ds.fedex.com.","Analítica de datos, Análisis predictivo, Canalizaciones de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y PySpark, Calidad de datos, Ciencias de la computación, Modelado de datos y Procesamiento por lotes",Solicitar
https://www.linkedin.com/jobs/view/3944912992/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=Vsde2KLmGRikTzr9hbjUqA%3D%3D&trk=flagship3_search_srp_jobs,Claims Data Engineer,"90 US$K/año - 150 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Rockville, MD","Acerca del empleo
Enjoy problem-solving, need a venue to display your creativity, and emerging technologies pique your interest; if so, Barrow Wise Consulting, LLC is for you. As a multi-disciplined leader, you understand the gifts that set you apart from everyone else. Demonstrate innovative solutions to our clients. Join Barrow Wise Consulting, LLC today.

Responsibilities

The Claims Data Engineer will support Barrow Wise's DHS project and perform the following duties:

Design and implement Medicare Part D Claims systems and is also knowledgeable about other Medicaid and Medicare claims systems 
Find trends in datasets and develop workflows and algorithms to make raw data applicable to the enterprise
Design and implement standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)
Ensure quality of technical solutions as data moves across environments
Provide insight into the changing data environment, data processing, data storage, and utilization requirements and offer suggestions for solutions
Map the details within these artifacts to business processes, non-functional characteristics, QA criteria, and technical enablement
Ensure managed analytic assets support strategic goals by creating and verifying data acquisition requirements and strategy
Develop, construct, test, and maintain architectures
Create data and solution architecture with business requirements and use programming language and tools
Identify ways to improve data reliability, efficiency, and quality
Deploy sophisticated analytics programs, machine learning, and statistical methods
Prepare data for predictive and prescriptive modeling and find hidden patterns using data
Use data to discover tasks that can be automated
Create data monitoring capabilities for each business process and work with data consumers on updates
Help maintain the integrity and security of the company data
Work remotely

An Ideal Candidate Has The Following

U.S. Citizenship
Bachelor's degree 
7 years of experience in healthcare claims systems development, implementation, and operations
5 years of experience in database management, automation, and data analytics 
Ability to work independently and meet deadlines

Join the team at Barrow Wise Consulting, LLC for a fulfilling and engaging experience! Our team is dedicated to providing innovative solutions to our clients in an ethical and diverse work environment. We offer competitive compensation packages, excellent benefits, and opportunities for growth and advancement. Barrow Wise is an equal-opportunity, drug-free employer committed to diversity in the workplace. Minority/Female/Disabled/Protected Veteran/LBGT are welcome to apply.

Our employees stand behind Barrow Wise's core values of integrity, quality, innovation, and diversity. We are confident that Barrow Wise's core values, business model, and team focus create positive career paths for our employees. Barrow Wise will continue to lead the industry in delivering new solutions to clients and persevere until the client is overjoyed.

Salary: $90000 - $150000 per year

Job Posted by ApplicantPro","Analítica de datos, Arquitectura de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Administración de bases de datos, Adquisición de datos, Bases de datos, Datasets, Desarrollo de sistemas y Necesidades empresariales",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3982792008/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=c8h88eoj4RVQtb35NCUmVA%3D%3D&trk=flagship3_search_srp_jobs,Big Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 2 días,"Phoenix, AZ","Acerca del empleo
Job Title: Big Data Engineer

Job Location: Phoenix, AZ

Job Type: Contract

Job Description:

iSoftTek Solutions Inc is seeking a talented Big Data Engineer to join our team. In this role, you will be responsible for developing and maintaining big data solutions and frameworks. You will work closely with cross-functional teams to design, implement, and optimize data processing and analytics systems utilizing Hadoop, Spark, and other big data technologies.

Responsibilities:

Design, develop, and implement big data solutions using technologies such as Hadoop and Spark
Develop scalable data pipelines for data ingestion, transformation, and analysis
Collaborate with data scientists and analysts to understand business requirements and design efficient data models and architectures
Optimize and tune big data applications for performance and scalability
Monitor and troubleshoot issues in big data systems and provide timely resolution
Ensure data quality and integrity in all stages of data processing


Requirements:

Bachelor's or Master's degree in Computer Science, Data Science, or a related field
3+ years of experience as a Big Data Engineer
Strong knowledge and hands-on experience with big data technologies such as Hadoop, Spark, Hive, or Kafka
Proficiency in programming languages such as Java, Scala, or Python
Experience with data modeling and data warehousing concepts
Knowledge of SQL and NoSQL databases
Excellent problem-solving and analytical skills
Strong communication and collaboration skills
Ability to work independently and in a team environment


Must Have Skills:

Proficiency in Java or Python
Experience with Spark or PySpark
Strong understanding of SQL and SQL Query
Experience with Shell or Unix Scripting
Working Experience of Hive","Almacenamiento de datos, Apache Kafka, Apache Spark, Hadoop, Hive, PySpark, Scala y Sqoop, Modelado de datos y Modelo de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3980101557/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=6zNGkrU4DxbkKufb2vNgEg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,"Palo Alto, CA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Location : Palo Alto CA

Hire type : Contract.

Job Responsibilities

Build and maintain an ecosystem of services and applications that provide order to result in a multi-product clinical laboratory.

Build data ingestion pipelines to gather data from various data sources. E.g., Salesforce Health Cloud, Lab systems, financial and billing applications.

Develop services to extract, load and transform data to provide purpose-built data stores.

Initiate and lead technical design discussions within and across technical teams.

Create artifacts, such as design and implementation documents, to guide development, implementation, and support.

Code for efficiency, reusability, scalability by following existing frameworks and tools.

Work with DevOps to develop and maintain automated deployment for regular release cadence.

Provide second-tier production support.

Job Qualifications

5+ years of experience developing production quality data pipelines in Python, Scala or Java

3+ Hands-on experience in building custom ETL with focus on design, data modeling, implementation, and maintenance, to cater to the reporting needs of Data Analysts and Data Scientists.

3+ years of experience with developing in AWS and other Cloud environments. AWS S3, AWS Glue/ EMR, Apache Spark, Redshift, Athena

Good understanding of Analytics ready data formats such as Parquet, ORC, JSON, etc. and Open Table formats Apache Iceberg, Apache Hudi, etc.

Experience working in a fast-paced environment leveraging an agile development framework, understanding of test automation and continuous integration.

Bachelor’s degree or higher in software engineering, CS, or any related field

Experience in healthcare industry is highly desired.","Almacenamiento de datos, Apache Spark, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Scala, Amazon Redshift, Diseño técnico y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3948035573/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=5AgSxemURnvGAwev9EeLvg%3D%3D&trk=flagship3_search_srp_jobs,Reporting Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",Publicado de nuevo hace 4 días,"Plano, TX","Acerca del empleo
Job Type

Full-time

Description

We are building a B2B product from the ground up using a modern technology stack that makes a real difference for thousands of patients. Our candidate for this role is a subject matter expert well-versed in azure cloud and engineering practices and principles including planning, building, tuning, and maintaining reporting services, technology, and bi workspaces. The Reporting Engineer is responsible for designing, developing, and maintaining business intelligence solutions using Power BI, Azure services, and Python. This role involves working closely with stakeholders to understand their data requirements, transforming raw data into meaningful insights, and ensuring data accuracy and availability. Hybrid Role – 3 days in office (T-TH)

Power BI Development

Design, develop, and deploy interactive Power BI dashboards and reports.
Create data models and establish data connections.
Implement measures, calculated columns, and DAX queries to support complex data analysis.

Azure Data Services

Utilize Azure Data Factory for ETL (Extract, Transform, Load) processes.
Manage and optimize Azure Data Lake for efficient data storage and retrieval.
Integrate various azure services (such as Azure Synapse Analytics, Microsoft Fabric) to streamline data workflows.

Python Scripting

Develop Python scripts for data cleaning, transformation, and automation tasks.
Create and maintain Python-based APIs and data processing pipelines.
Implement machine learning models and algorithms for advanced data analysis.

Data Management And Governance

Ensure data accuracy, integrity, and security across all reporting solutions.
Develop and maintain documentation for data models, configurations, and processes.
Monitor data quality and troubleshoot issues related to data discrepancies or system errors.

Stakeholder Collaboration

Work closely with business users to gather requirements and translate them into technical specifications.
Provide training and support to end-users on using Power BI reports and dashboards.
Communicate findings and insights effectively to both technical and non-technical stakeholders.

Continuous Improvement

Stay updated with the latest industry trends and best practices in business intelligence and data analytics.
Identify opportunities for process improvement and implement innovative solutions.
Participate in code reviews and knowledge-sharing sessions within the team.

EEO

Requirements

Education:

Bachelor's degree in computer science, information technology, data science, or a related field. 

Experience

Proven experience as a Reporting Engineer or similar role.
Hands-on experience with Power BI, including DAX and Power Query.
Strong knowledge of Azure services, including ADF, Azure SQL DB, MS Fabric and Data Lakes.
Proficiency in Python for data analysis, automation, and machine learning.

Skills

Excellent analytical and problem-solving skills.
Strong understanding of data warehousing concepts and database design.
Ability to manage multiple tasks and projects in a fast-paced environment.
Excellent communication and interpersonal skills.

Certifications (Preferred)

Microsoft Certified: Data Analyst Associate.
Microsoft Certified: Azure Data Engineer Associate.
Relevant Python certifications or coursework.

Work Environment

This position may offer a hybrid work model, with the flexibility to work from home and on-site as required.
Occasional travel might be needed for stakeholder meetings or training sessions.

It Will Be a Plus

Knowledge of HIPAA and SOC2
Azure DevOps and Git experience 
.Net experience and EF (Entity Framework) 

Salary Description

$125,000.00 - $150,000.00","Analítica de datos, Arquitectura de datos y Ciencia de datos, Calidad de datos, Comunicación, Diseño de bases de datos, Especificaciones técnicas, Expresiones de análisis de datos (DAX), Habilidades sociales y Modelado de datos",Solicitar
https://www.linkedin.com/jobs/view/3977833512/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=ZcgXuJjhog6UjBxbn6vWcw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 1 semana,"Greer, SC","Acerca del empleo
Description

We are offering an opportunity for a Data Engineer to join our team in Greer, South Carolina. The successful candidate will be responsible for designing, building, and maintaining efficient, scalable, and reliable data pipelines and systems. This role will require working with large datasets and solving complex data challenges, ensuring data quality and accessibility.

Responsibilities

 Design, develop, and maintain robust data pipelines to support various data processing and analytics needs.
 Collaborate with data scientists, analysts, and other stakeholders to understand data requirements.
 Implement and optimize ETL processes to extract, transform, and load data from various sources into data warehouses and other storage solutions.
 Develop and maintain data models and database schemas that support business objectives.
 Monitor and troubleshoot data pipeline performance, ensuring data integrity and availability.
 Utilize cloud platforms (e.g., AWS, Azure, GCP) to build scalable data infrastructure and leverage cloud services for data processing and storage.
 Write and optimize complex SQL queries for data extraction and analysis.
 Stay up-to-date with the latest trends and technologies in data engineering and recommend improvements to existing systems.
 Ensure compliance with data governance and security policies.

Requirements

 Proficiency in Apache Spark is essential for this role
 Demonstrable experience with Database management and operations
 Knowledge of EO/IR systems is required
 Familiarity with Erwin Data modeling tools
 Experience with HDFS is necessary
 Proficiency in AB Testing methodologies
 Strong background in Analytics
 Knowledge of AWS Technologies is desirable
 Experience in Business Intelligence (BI) tools and methodologies
 Ability to create and interpret Business Requirement Documents
 Bachelor's degree in Computer Science, Data Science, Engineering, or a related field
 Strong communication and interpersonal skills
 Ability to work in a team-oriented environment
 Demonstrated problem-solving skills
 Familiarity with data architecture and data warehouse concepts
 Ability to handle multiple tasks and priorities in a fast-paced environment.

Technology Doesn't Change the World, People Do.®

Robert Half is the world’s first and largest specialized talent solutions firm that connects highly qualified job seekers to opportunities at great companies. We offer contract, temporary and permanent placement solutions for finance and accounting, technology, marketing and creative, legal, and administrative and customer support roles.

Robert Half works to put you in the best position to succeed. We provide access to top jobs, competitive compensation and benefits, and free online training. Stay on top of every opportunity - whenever you choose - even on the go.

All applicants applying for U.S. job openings must be legally authorized to work in the United States. Benefits are available to contract/temporary professionals, including medical, vision, dental, and life and disability insurance. Hired contract/temporary professionals are also eligible to enroll in our company 401(k) plan. Visit

© 2024 Robert Half. An Equal Opportunity Employer. M/F/Disability/Veterans. By clicking “Apply Now,” you’re agreeing to","Apache Spark, Ciencia de datos y Ingeniería de datos, Bases de datos, Ciencias de la computación, Comunicación, Modelado de datos, Modelo de datos, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3984652996/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=Wa%2BulX%2FCBzb5%2BVzKYOgpRQ%3D%3D&trk=flagship3_search_srp_jobs,Sr. Data Management - Big Data/Machine Learning Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,"Richmond, VA","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Location: Must be local to Richmond, VA or McLean, VA

Duration: Up to 1 year

Pay Rate: Up to $75 or $80/hr with $7.50/hr or $8/hr NP Fee

Job Description

We are seeking a highly skilled Senior Data Engineer with expertise in AWS, Scala, and Spark to join our team. This role will support the Sunset Framework, addressing the increased workload and providing additional coverage for Business-as-Usual (BAU) activities. The ideal candidate will be proficient in developing and managing data pipelines using Scala, leveraging the Spark engine, and working with AWS & SQL. Experience with Java and AWS Glue is a plus.

Key Responsibilities

Develop, maintain, and optimize data pipelines using Scala and Spark.

Utilize AWS services to manage and store data efficiently.

Write and execute complex SQL queries for data manipulation and extraction.

Collaborate with cross-functional teams to understand data requirements and implement effective solutions.

Ensure data quality and integrity throughout the data pipeline processes.

Stay updated with emerging technologies and industry trends related to data engineering.

Required Qualifications

Proven experience as a Data Engineer, specifically with Scala and Spark.

Strong expertise in AWS services and SQL.

Ability to manage and optimize large-scale data pipelines.

Excellent problem-solving skills and attention to detail.

Effective communication and collaboration skills.

Preferred Qualifications

Experience with Java programming.

Familiarity with AWS Glue.

Work Arrangement

Hybrid work model with the requirement to be onsite in West Creek or McLean, VA.

Onsite: Tuesday to Thursday

Remote: Monday and Friday

If you are a motivated Senior Data Engineer with the required skills and experience, and you are eligible to work in the United States, we encourage you to apply. This is a fantastic opportunity to work with a leading financial institution and contribute to significant data engineering projects.","Capacidad de análisis y Gestión de datos, Atención al detalle, Bases de datos, Calidad de datos, Comunicación, Gestión de datos maestros, Liderazgo de equipos multidisciplinarios, Manipulación de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3961214465/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=z6DweRAw4k2tDkl9hb49rw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"Maple Grove, MN","Acerca del empleo
Overview:

Build the infrastructure required for optimal extraction, transformation, and loading of data from a variety of data sources.
Maintain optimal data pipeline architecture
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build analytics tools that utilize the data to provide actionable insights into operational efficiency and other key business performance metrics
Work with the stakeholders and business teams to assist with data-related technical issues and support


What You’ll Bring:

3-5 yrs years of relevant and progressive professional experience in data analysis, design and development.
Bachelor’s degree in a related field or equivalent education and/or experience.
Ability to work in a dynamic problem-solving environment and synthesize strategy, plans, and solutions.
Demonstrated ability to deliver in a complex business environment.","Almacenamiento de datos, Analítica de datos, Análisis de datos, Capacidad de análisis, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3984782507/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=03JWGhtmJQVvTE%2B1wYCM0A%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 3 días,"Nueva York, NY","Acerca del empleo
Este anuncio proviene de un tablón de empleos. Más información
Greetings

I hope you are doing well!

Please find the requirement below , If you find yourself comfortable with the requirement please revert with your updated resume and I will get back to you

Job Title Data Engineer

Location DC (Georgetown) or NYC (Manhattan) Hybrid (2 days onsite / week) 

Duration 12 Months

Mandatory Requirement: Active Secret Clearance or higher 

Job Description

As an Engineer - you will help develop and deploy technical solutions to solve our customers' hardest problems, using our platform to integrate data, transform insights, and build first-class applications for operational decisions. You will leverage everything around you: core company products, open source technologies (e.g. GHE), and anything you and your team can build to drive real impact. In this role, you work with customers around the globe, where you gain rare insight into the world's most important industries and institutions. Each mission presents different challenges, from the regulatory environment to the nature of the data to the user population. You will work to accommodate all aspects of an environment to drive real technical outcomes for our customers!

Core Responsibilities

Setup transfers of data feeds from source systems into location accessible to Foundry
Debug issues related to delayed or missing data feeds
Write transformations and derive new datasets using Spark for distributed computation
Monitor build progress and debug build problems
Using Foundry's application development framework to design applications that address operational questions
Rapid development and iteration cycles with SME's including testing and troubleshooting application issues
Executing requests for information (RFI's) surrounding the platform's data footprint

What We Value

Strong engineering background, preferably in fields such as Computer Science, Mathematics, Software Engineering, Physics, or Data Science.
Proficiency with programming languages such as Python (Pyspark, Pandas) SQL, R or similar languages.
Ability to work effectively in teams of technical and non-technical individuals.
Skill and comfort working in a rapidly changing environment with dynamic objectives and iteration with users.
Demonstrated ability to continuously learn, work independently, and make decisions with minimal supervision.
Proven track-record of strong customer communications including feedback gathering, execution updates, and troubleshooting.

Thanks & Regards

Santosh Pal

IT Technical Recruiter

Phone : 201-578-5048

Email: Santosh@stellentit.com

Gtalk: Santosh@stellentit.com","Almacenamiento de datos, Analítica de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Pandas (Software) y PySpark, Bases de datos, Comunicación, Datasets y Resolución de incidencias",Solicitar
https://www.linkedin.com/jobs/view/3984604928/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=EaMIbNJfTYmKriUbcPhcoA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer Databricks,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 días,Estados Unidos,"Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Digital HIE Inc, is seeking the following. Apply via Dice today!

You Have:

3+ years of experience with data engineering, developing and implementing data pipelines 
3 + years of experience developing with Python and SQL in a professional environment 
Familiarity or experience with software delivery lifecycle practices, including continuous integration, configuration management, unit testing, and Agile development 
Experience working with and optimizing large-scale data systems 
Ability to obtain and maintain a Public Trust clearance 
HS diploma or GED 

Nice If You Have:

3+ years of experience working with big data technologies and distributed computing processing 
3+ years of experience working in Azure or AWS cloud in development capacity 
Experience with data ingest and analytics tools, including Data Factory, Databricks, Spark, Airflow, Power BI, Tableau 
Ability to implement ITIL methodologies 
Ability to quickly learn technical concepts 
Ability to communicate with multiple functional groups and leadership 
Ability to display a positive, can-do attitude to solve challenges and collaborate effectively 

Data Engineer Databricks","Airflow, Almacenamiento de datos, Apache Spark, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos , Python y SQL, Comunicación",Ya no se aceptan solicitudes
https://www.linkedin.com/jobs/view/3980674594/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=uzfRiA1NLtu8KgjCzCb9%2Fg%3D%3D&trk=flagship3_search_srp_jobs,Analytics Engineer I,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 6 días,"Phoenix, AZ","Acerca del empleo
About The Company

Republic Services, Inc. (NYSE: RSG) is a leader in the environmental services industry. We provide customers with the most complete set of products and services, including recycling, waste, special waste, hazardous waste and field services. Our industry-leading commitments to advance circularity and support decarbonization are helping deliver on our vision to partner with customers to create a more sustainable world.

In 2023, Republic’s total company revenue was $14.9 billion, and adjusted EBITDA was $4.4 billion. We serve 13 million customers and operate more than 1,000 locations, including collection and transfer stations, recycling and polymer centers, treatment facilities, and landfills.

Although we operate across North America, the collection, recycling, treatment, or disposal of materials is a local business, and the dynamics and opportunities differ in each market we serve. By combining local operational management with standardized business practices, we drive greater operating efficiencies across the company while maintaining day-to-day operational decisions at the local level, closest to the customer.

Our customers, including small businesses, major corporations and municipalities, want a partner with the expertise and capabilities to effectively manage their multiple recycling and waste streams. They choose Republic Services because we are committed to exceeding their expectations and helping them achieve their sustainability goals. Our 41,000 team members understand that it's not just what we do that matters, but how we do it.

Our Company Values Guide Our Daily Actions

Safe: We protect the livelihoods of our colleagues and communities.
Committed to Serve: We go above and beyond to exceed our customers’ expectations.
Environmentally Responsible: We take action to improve our environment.
Driven: We deliver results in the right way.
Human-Centered: We respect the dignity and unique potential of every person.

We are proud of our high employee engagement score of 86. We have an inclusive and diverse culture where every voice counts. In addition, our team positively impacted 4.6 million people in 2023 through the Republic Services Charitable Foundation and local community grants. These projects are designed to meet the specific needs of the communities we serve, with a focus on building sustainable neighborhoods.

STRATEGY

Republic Services’ strategy is designed to generate profitable growth. Through acquisitions and industry advancements, we safely and sustainably manage our customers’ multiple waste streams through a North American footprint of vertically integrated assets.

We focus on three areas of growth to meet the increasing needs of our customers: recycling and waste, environmental solutions and sustainability innovation.

With our integrated approach, strengthening our position in one area advances other areas of our business. For example, as we grow volume in recycling and waste, we collect additional material to bolster our circularity capabilities. And as we expand environmental solutions, we drive additional opportunities to provide these services to our existing recycling and waste customers.

Recycling and Waste

We continue to expand our recycling and waste business footprint throughout North America through organic growth and targeted acquisitions. The 13 million customers we serve and our more than 5 million pick-ups per day provide us with a distinct advantage. We aggregate materials at scale, unlocking new opportunities for advanced recycling. In addition, we are cross-selling new products and services to better meet our customers’ specific needs.

Environmental Solutions

Our comprehensive environmental solutions capabilities help customers safely manage their most technical waste streams. We are expanding both our capabilities and our geographic footprint. We see strong growth opportunities for our offerings, including PFAS remediation, an increasing customer need.

SUSTAINABILITY INNOVATION

Republic’s recent innovations to advance circularity and decarbonization demonstrate our unique ability to leverage sustainability as a platform for growth.

The Republic Services Polymer Center is the nation’s first integrated plastics recycling facility. This innovative site processes rigid plastics from our recycling centers, producing recycled materials that promote true bottle-to-bottle circularity. We also formed Blue Polymers, a joint venture with Ravago, to develop facilities that will further process plastic material from our Polymer Centers to help meet the growing demand for sustainable packaging. We are building a network of Polymer Centers and Blue Polymer facilities across North America.

We continue to advance decarbonization at our landfills. As demand for renewable energy continues to grow, we have 70 landfill gas-to-energy projects in operation and plan to expand our portfolio to 115 projects by 2028.

RECENT RECOGNITION

Barron’s 100 Most Sustainable Companies
CDP Discloser
Dow Jones Sustainability Indices
Ethisphere’s World’s Most Ethical Companies
Fortune World’s Most Admired Companies
Great Place to Work
Sustainability Yearbook S&P Global

POSITION SUMMARY: The Analytics Engineer I assists in the execution, development, maintenance, and continuous refinement of analytics and reporting solutions. This position works on ETL processes, data warehouses, cubes, dashboards, and model deployment. The Analytics Engineer stays up-to-date on the latest technology and maintains system and tool documentation.

Principal Responsibilities

Collaborates with internal customers, analysts, developers, data scientists, and database administrators.
Provides underlying data support for business metrics.
Writes ETL scripts using SQL Server Integration Services (SSIS) or similar tools to consume and process new data into the data warehouse.
Designs and builds automated processes that leverage Excel, SQL and VBA to facilitate data transfer across teams
Designs dashboards and tools to allow users to consume and interact with data.
Implements in-database machine learning solutions using SQL Server and R/Python.
Ensures security and integrity across all data.
Identifies risks and opportunities across the business and drive solutions.
Troubleshoots and debugs issues.
Performs other job-related duties as assigned or apparent.

Knowledge / Skills / Abilities

Proficiency with relational databases and the ability to write SQL queries.
Proficiency with programming in R or Python in data science and/or engineer settings.
Proficiency with Excel and PowerBI.

Qualifications

2 years of demonstrated experience working with relational databases in the context of developing ETL processes and implementing data and analytics solutions.
2 years of experience maintaining databases and tools in a production environment including query performance tuning and proactive maintenance.
Experience with source control (e.g., Github, SVN, Tortoise) in a multi-developer environment.
Experience working with a formal software development methodology.
Strong requirements gathering and documentation skills.

Minimum Qualifications

Bachelor’s Degree in an analytical field (Mathematics, Computer Science, Information Management, Statistics, Engineering, )
2 years of experience with advanced programming in SQL and R or Python, working in a production environment with relational databases.

Rewarding Compensation And Benefits

Eligible employees can elect to participate in:

 Comprehensive medical benefits coverage, dental plans and vision coverage.
 Health care and dependent care spending accounts.
 Short- and long-term disability.
 Life insurance and accidental death & dismemberment insurance.
 Employee and Family Assistance Program (EAP).
 Employee discount programs.
 Retirement plan with a generous company match.
 Employee Stock Purchase Plan (ESPP).

The statements used herein are intended to describe the general nature and level of the work being performed by an employee in this position, and are not intended to be construed as an exhaustive list of responsibilities, duties and skills required by an incumbent so classified. Furthermore, they do not establish a contract for employment and are subject to change at the discretion of the Company.

EEO STATEMENT:Republic Services is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, protected veteran status, relationship or association with a protected veteran (spouses or other family members), genetic information, or any other characteristic protected by applicable law.","Base de datos relacional, Ciencia de datos, Extraer, transformar y cargar (ETL), SQL y SQL Server Integration Services (SSIS), Bases de datos, Ciencias de la computación, Cubes, Documentación y Lenguaje de consulta (query)",Solicitar
https://www.linkedin.com/jobs/view/3978637305/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=KdIzody27n8jWx2peAdlQQ%3D%3D&trk=flagship3_search_srp_jobs,Data & Analytics - Data Engineer - Data Quality - Contract - 24-00215,"Híbrido Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 1 semana,"Belén, PA","Acerca del empleo
Our direct client is seeking a Data Engineer for a 6-month contract opportunity with their Bethlehem, PA location.

Responsibilities:

Develop and maintain datamarts using databricks to support business analytics and reporting use cases. 
Develop & Implement ETL/ELT processes to extract, transform and load data using an existing framework. 
Collaborate with business stakeholders to gather requirements and ensure the datamarts/data assets that are built meets the business needs. 
Create & maintain data dictionary/ETL mapping documents. 
Reverse engineer existing data prep processes and convert them into reusable governed and certified data assets. 
Implement data quality checks and ensure data integrity within the datamarts /data assets. 

Qualifications:

Proven experience as a Data Engineer, ETL developer or in a similar role
Proficient in SQL and Python. 
Knowledge of Databricks. 
Familiarity with data warehousing concepts and best practices. 
Strong understanding of data modeling, and in particular dimension modeling. 
Excellent problem-solving skills and the ability to multitask, work independently and as part of a team. 
Good communication skills and the ability to collaborate effectively within and across the team.","Almacenamiento de datos, Analítica de datos, Analítica empresarial, Extraer, transformar y cargar (ETL), Python y SQL, Azure Databricks, Diccionario de datos, Modelado de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3970231184/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=M%2F%2Fuy9IJVYsUZcTMqEf3uQ%3D%3D&trk=flagship3_search_srp_jobs,Data Warehouse Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 semanas,"Jersey City, NJ","Acerca del empleo
Key Responsibilities:
Design and implement robust, scalable, and efficient data pipelines and architectures.
Create and manage ETL (Extract, Transform, Load) processes to efficiently move data from source systems into the data warehouse.
Integrate data from various sources, ensuring consistency, accuracy, and availability of data for analysis.
Monitor and optimize data warehouse performance, ensuring efficient query execution and data retrieval.
Implement data quality checks and validation procedures to ensure data accuracy and reliability.
Work closely with business analysts, data scientists, and other stakeholders to understand data requirements and deliver solutions that meet business needs.
Maintain comprehensive documentation of data warehouse designs, ETL processes, and data integration workflows.
Identify and resolve data-related issues, ensuring minimal disruption to data warehouse operations.
Ensure data security and compliance with relevant regulations and standards

Qualifications:
Bachelor’s degree in Computer Science, Information Technology, or a related field.
Minimum of 5 years of experience in data warehousing, ETL development, and database management.
Proficiency in SQL, ETL tools (e.g., Informatica, Talend, SSIS), and data modeling. Experience with data warehousing technologies (e.g., Amazon Redshift, Snowflake, Google BigQuery) is a plus.
Strong analytical and problem-solving skills with the ability to identify and implement effective solutions.
Excellent written and verbal communication skills, with the ability to interact effectively with team members and stakeholders.","Almacenamiento de datos y Extraer, transformar y cargar (ETL), Almacenamiento",Solicitud sencilla
https://www.linkedin.com/jobs/view/3983402411/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=0xKItOJ9ZwOYkNvcuE8f7Q%3D%3D&trk=flagship3_search_srp_jobs,Applied Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 5 días,"Meridian, ID","Acerca del empleo
Job#: 2032651

Job Description:

Applied Data Engineer

Meridian, ID

 IT - Hybrid

Description

Our client in Meridian, ID is looking for an Applied Data Engineer to provide innovative modernization and ensure the availability, reliability, and performance of the Data Analytics eco-system.

Assembling large, complex sets of data that meet non-functional and functional business requirements
Identifying, designing, and implementing internal process improvements, including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes 
Building required infrastructure for optimal extraction, transformation, and loading of data from various data sources using AWS and SQL technologies
Building analytical tools to utilize the data pipeline, providing actionable insight into key business performance metrics, including operational efficiency and customer acquisition 
Working with stakeholders including the Executive, Product, Data, and Design teams to support their data infrastructure needs while assisting with data-related technical issues
Working with the architects, including enterprise, application, and data architects, to design, implement, and support data lake
Translating analytical program models, including but not limited to scripting, error handling, and documentation 
Evaluating new technologies for data and technology modernization within data ecosystem
Acting as technical lead for data lead projects and initiatives
Coaching and mentoring to less experienced engineers
Performing all other assigned tasks and requirements as needed

Qualifications:

Graduate and/or bachelor’s degree in Information Systems, Informatics, Statistics, Computer Science or another quantitative field
5 years of data engineering experience 
Ability to build and optimize data sets, data pipelines and architectures 
Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions 
Excellent analytical skills associated with working on unstructured datasets 
Ability to build processes that support data transformation, workload management, data structures, dependency and metadata
Recommends data strategies, ETL processes, and procedures for getting data in and out of the data lake
Experience in data access and delivery technologies, including familiarity with data quality assessment, data organization, metadata, and data profiling
Ability to take complex, ambiguous problems, break them down into smaller parts, and problem solve to come up with a whole, integrated, and strategic solution
Demonstrated understanding and experience using software and tools including relational NoSQL and SQL databases including SQL Server and Postgres
Hands on experience with AWS Services like Cloud Formation, S3, Glue, Lambda, Event Bridge, SNS/SQS, and others
Demonstrated ability to self-learn and lead teams into new technologies and engineering methods
Demonstrated ability to solve technical problems relevant to data engineering using programming languages (SQL and Python)
Demonstrated ability to solution with AWS services and provision infrastructure from code/template
Excellent data manipulation and analysis skills
Excellent skills in SQL, data modeling, data warehousing, data lake, and OLAP
Excellent written and oral communication skills
Experience with Bitbucket and TeamCity, a plus
Experience with Redgate Flyway, a plus
Familiarity with Agile Framework

EEO Employer

Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] or 844-463-6178 .

Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.

Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.

4400 Cox Road

Suite 200

Glen Allen, Virginia 23060

Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] (Do not submit resumes or solicit consultants to this email address). UnitedHealthcare creates and publishes the Transparency in Coverage Machine-Readable Files on behalf of Apex Systems.","Arquitectura de datos, Extraer, transformar y cargar (ETL), Ingeniería de datos y Visualización de datos, Calidad de datos, Establecer prioridades del trabajo, Manipulación de datos, Metadatos, Modelado de datos y Perfiles de datos",Solicitar
https://www.linkedin.com/jobs/view/3981004821/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=I5J8TR%2Fc13MZxLhO%2BLPiBw%3D%3D&trk=flagship3_search_srp_jobs,DENG3 - Data Engineer 3,"64,28 US$/h - 107,14 US$/h Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 días,"Birmingham, AL","Acerca del empleo
Enter Job Description...
The Data Engineer at APC Power Delivery's AMI Analytics team plays a pivotal role in advancing data-driven decision-making and operational efficiency across business units.
Working closely with data scientists and within a cloud environment, this role requires expertise in data engineering, cloud technologies, and collaborative problem-solving.
Key Responsibilities:
Data Pipeline Development:
Assist TO in designing, building, and maintaining scalable data pipelines to ingest, transform, and store large volumes of AMI data from various sources.
Collaborate with data scientists to ensure seamless integration of data pipelines with analytical models and AI processes.
Data Modeling and Architecture:
Develop and implement data models and architectures optimized for AMI data analytics, ensuring efficiency, scalability, and data integrity.
Implement best practices for data storage, partitioning, and indexing to optimize performance and facilitate analysis.
Cloud Environment Management:
Work within cloud environments - Azure to deploy, manage, and optimize data engineering solutions.
Collaborate with cloud architects and administrators to ensure security, compliance, and cost-effectiveness of cloud infrastructure.
Visualization and Reporting:
Develop and maintain Power BI dashboards and reports to visualize AMI data insights and facilitate data-driven decision-making.
Create custom UI/UX applications
Ensure the accuracy, reliability, and usability of visualizations to meet business requirements.
Data Cataloging and Documentation:
Catalog and document AMI data sources, datasets, and metadata to facilitate data discovery, lineage, and governance.
Implement data cataloging best practices to ensure the availability and accessibility of AMI data assets.
Collaboration and Support:
Collaborate with cross-functional teams to understand data requirements and support analytical initiatives.
Provide technical support and troubleshooting for data-related issues, ensuring the reliability and availability of data infrastructure.
Job Requirements:
Education/Experience:
Bachelor’s degree in Computer Science, Information Technology, or related field.
3+ years of experience in data engineering or related roles, preferably in a cloud environment.



EEO Employer
LanceSoft is a certified Minority Business Enterprise (MBE) and an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.
This policy applies to all employment practices within our organization, including hiring, recruiting, promotion, termination, layoff, recall, leave of absence, compensation, benefits, training, and apprenticeship. LanceSoft makes hiring decisions based solely on qualifications, merit, and business needs at the time.
Aptitudes y experiencia deseables
DATA MODELING","Ingeniería de datos y Visualización, Ciencias de la computación, Datasets, Diseño de la interfaz de usuario, Fiabilidad, Modelado de datos, Modelo de datos, Necesidades empresariales y Usabilidad",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982797592/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=3S47PEZ2j8xUXMt7HJZZ%2BA%3D%3D&trk=flagship3_search_srp_jobs,AI agent developer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 días,San Francisco y alrededores,"Acerca del empleo
Role : AI agent developer
Location : San Franscisco CA Onsite relocation is fine.
Duration : 1+ year
Visa : USC, GC, H1, H4 EAD.

Note : Candidate must have availability only one week. Ideally client want to start by next Monday. They can do BGC on the Job (employment verification and criminal check)

Qualifications
• Proficient in Python, React or Next
• Experienced with AI agent frameworks and AI models
• Proven experience in AI agent development
• Strong problem-solving skills and the guts to take on mind-bending challenges
• A team player mindset, ready to collaborate and elevate our small but mighty team
Responsibilities
• Spearhead the development, design, and deployment of our state-of-the-art AI agents
• Collaborate closely with our passionate founding team to craft the future roadmap
• Provide technical mentorship, promoting best practices and setting coding standards
• Ensure seamless integration of our agents into our enterprise customer VPCs
• Dive deep into the challenges of AI-driven codebase maintenance, creating solutions that are both innovative and practical
• Prioritizing actions over words empowers team members to demonstrate their commitment through results, rather than intentions or promises, fostering a culture of accountability and execution","Python, React.js",Solicitud sencilla
https://www.linkedin.com/jobs/view/3982360634/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=X%2BqNG2iKw4n8qWkWZbzXww%3D%3D&trackingId=azhphIoXb0kLUcpr51Y0vQ%3D%3D&trk=flagship3_search_srp_jobs,Technology and Data - Software Engineer 3 - Contingent,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,"Summit, NJ","Acerca del empleo
Job Description: In this contingent resource assignment, you may: Consult on or participate in moderately complex initiatives and deliverables within Software Engineering and contribute to large-scale planning related to Software Engineering deliverables. Review and analyze moderately complex Software Engineering challenges that require an in-depth evaluation of variable factors. Contribute to the resolution of moderately complex issues and consult with others to meet Software Engineering deliverables while leveraging solid understanding of the function, policies, procedures, and compliance requirements. Collaborate with client personnel in Software Engineering.

About The Role

Individual contributor role for Oracle FCCM Mantas developer; Hands on coding role; develop code for multiple projects/requirements simultaneously. Provides technical support to the co-located teams, design and automation solutions to make the development process better and faster. Takes a lead role in driving the usage of latest technology/Frameworks. Motivates the team to use the tools that improve the quality of the product.

Job duties:

 Configuration/threshold parameters for all the models from the FCCM components namely Anti Money Laundering Enterprise edition. 
 Experience using Scenario manager to customize the OOTB scenarios. 
 Good knowledge in MANTAS data models such as Client and FSDM. 
 Conduct data assessment requirements to evaluate data quality and adequacy to run the models. 
 Assist production and non-Production deployments. 
 Coordinate and Collaborate with Oracle organization's resources in ensuring a successful deployment of the FCCM suite of applications. 

Top Skills:

 Mantas/FCCM/OFSAA experience
 Oracle Database/PL/SQL
 Autosys is good to have. 
 Unix shell scripting is good to have. 

Required Qualifications:

 5 years of Software Engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, or education. 
 Oracle Mantas/FCCM (Financial Crime and Compliance)/OFSAA (Oracle Financial Services Analytical Applications) experience good to have. 
 FCCM Configuration
 Unix, PL/SQL
 Mantas scenario development
 Autosys
 Strong knowledge of PL/SQL

Top Skills:

 Mantas/FCCM/OFSAA experience
 Oracle Database/PL/SQL
 Autosys is good to have. 
 Unix shell scripting is good to have. 

Essential Qualifications

 7+ years of IT experience in design and development of projects using Oracle SQL, PL/SQL
 Design and develop load processes for SQL & PL/SQL and queries for very large databases with large transaction volumes. 
 Working with production support teams to define application run books and pre-production checks. 
 Self-starter that can navigate complex requirements and independently execute projects. 
 Strong debugging and problem-solving skills. 
 Should have strong communication and presentation skills. 
 Preferable experience in Agile delivery. 

EEO:

“Mindlance is an Equal Opportunity Employer and does not discriminate in employment on the basis of – Minority/Gender/Disability/Religion/LGBTQI/Age/Veterans.”
Aptitudes y experiencia deseables
MANTAS","Lenguajes de programación, Oracle Database, PL/SQL y SQL, Adobe InDesign, Aptitudes para hacer presentaciones, Data Assessment, Mantas, Oracle Financial Services Analytical Applications (OFSAA) y Presentaciones",Solicitar
https://www.linkedin.com/jobs/view/3956785103/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=msBQc7UaW1jLTS0qTYaVfg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 meses,"Washington, DC","Acerca del empleo
Analytica is seeking a remoteData Engineer (MLOps)to support one or more dynamic, long-term federal government enterprise data programs. The ideal candidate will lead the architecture and implementation of on-premises and cloud big data solutions as part of enterprise data modernization.&#8239;
Analytica has been recognized by&#8239;Inc. Magazine&#8239;as one of the fastest-growing 250 businesses in the US for 3 years. We work with U.S. government clients in health, civilian, and national security missions to build better technology products that impact our day-to-day lives. The company offers competitive compensation with opportunities for bonuses, employer-paid health care, training and development funds, and 401k match.&#8239;&#8239;&#8239;

Responsibilities include (but not limited to):&#8239;
Buildout data pipelines in AWS for data lakes and data warehouses.
implementing data pipelines, via a variety of tools including Amazon S3, AWS Glue, Amazon Textract, Amazon Comprehend, AWS Lambda, SQL and/or Python scripts, in the cloud to an existing data lake and data warehouse.
Implementdata pipelines, for batch and streaming data sources, from external feeds into a cloud-based data lake and eventually into data warehouses; 
Implementdata cataloging to share metadata information for datasets in the data lake;
Use AWS serverless components in the data pipeline architecture;
Use IaC tools to deploy the pipelines within AWS.
&#8239;Basic Qualifications:&#8239;
5+ years of hands-on Data Integration experience creating and maintaining efficient scripts/data pipelines to clean, transform and ingest data from a variety of formats into database tables, data warehouses or data lake repositories.
&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;&#8203;Experience building data pipelines using AWS serverless components; using AWS Glue to build, maintain and monitor ETL jobs; using Python to implement ETL scripts and AWS Secrets Manager to manage credentials.
Experience with AI/ML, NLP, Sentiment Analysis, etc. with one of the leading cloud providers is a plus.
AWSS ML Certification or AWS Data Engineer Certification is desired.
Must be US Citizen
Must be able to obtain and maintain a Public Trust security clearance
About&#8239;Analytica: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. The company is an award-winning SBA certified 8(a) small business&#8239;that has&#8239;been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing&#8239;companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company&#8239;is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.","Almacenamiento de datos, Canalizaciones de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Ciencias de la computación y Datasets",Solicitud sencilla
https://www.linkedin.com/jobs/view/3940976163/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=bXC1S%2B6PaPUyzz7E50oR4g%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 mes,Estados Unidos,"Acerca del empleo
Work is REMOTE w/Video interviews. 

Data Engineer with 7 years experience in software engineering. 
2 years experience with Golang. 
2 years building and maintaining data-intensive APIs using a RESTful approach. 
Experience with stream processing using Apache Kafka. 
Unit Testing and Test Driven Development methodologies. 
Creating and maintaining containerized application deployments with a platform like Docker.","Almacenamiento de datos, Apache Kafka, Big data, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Bases de datos, Go y Representational State Transfer",Solicitud sencilla
https://www.linkedin.com/jobs/view/3943056082/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=IGLZZgd%2FisjUUtB0y3k%2BRg%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",Publicado de nuevo hace 2 semanas,"Troy, MI","Acerca del empleo
About HTC Global Services:
Shaping careers since 1990 - our long tenured employees are a testimony of the work culture. Join our global employee base of 12,000 and help us bring human expertise to tech in order to deliver purposeful solutions that amplify value.

Job Description:

HTC Global Services is seeking a senior Data Engineer experienced with building data movement and data curation processes using modern toolsets including Snowflake, Python, YAML, and Azure Data Factory. This is NOT a traditional data modeling and ETL batch data processing role. This role will primarily be:
Designing, coding, and testing Python scripts to deliver prescribed business outcomes.
Conducting detailed data analysis in response to both trouble tickets and new development.
Providing real-time technical support during U.S. business hours.
The varied and non-traditional mix of technologies and processes at play will require the successful candidate to be a very fast learner and a strong experienced problem-solver.
The core project involves acquiring and transforming data from various source systems and curating that data for consumption by a customer-facing portal. While this is primarily an individual contributor role, the role will also serve as an onshore coordinator for a larger offshore development team.
Our client is a Pennsylvania-based transportation and logistics company. This role is primarily remote though, so candidates may reside anywhere in the United States. Be advised however that occasional travel (10-15%) may be required as the needs of the project dictate.
Required People Skills:
Professional, confident, and cordial demeanor.
Excellent communication skills – both oral and written.
Proven ability to operate in dynamic environments and still deliver quality outcomes on a timely basis.
Self-motivated, proactive, efficient, and highly organized.
Strong desire and proven ability to learn new things quickly.
Ability to work independently, with a high level of performance, with minimal direct supervision.
Ability to smoothly collaborate with client and team resources to ensure results meet the defined business needs and exceed client expectations.
Experience coordinating the efforts of geographically distributed team members.
Ability to travel occasionally (10-15%) as required.
Required Technical Skills:
5+ years of experience implementing, enhancing, or supporting data solutions in support of downstream applications.
Proven ability to write, refine, and troubleshoot highly efficient data manipulation code using Python, PySpark, and YAML.
Proven experience working in technical support role – particularly for web applications and their underlying databases.
Direct experience working with Snowflake databases.
Ability to create and manage pipelines in Azure Data Factory.
Strong problem-solving and troubleshooting skills – especially the ability to identify and resolve processing issues in near real time.
Strong communication skills – particularly in terms of interacting with business users and teammates to quickly analyze and resolve both production issues and novel development challenges.
Value Add Skills: (helpful but not required):
Direct experience working in the Transportation and Logistics industry.
Direct hands-on experience working with the SnowPark toolset for data movement and transformation.
Direct hands-on experience working with Databricks tools and environments.
Advanced programming skills in C#, PowerShell, Python, or other equivalent modern programming languages.
Experience working in the Jira Agile project management toolset.
Relevant certifications in the toolsets of interest.

Benefits:
At HTC Global Services our associates have access to a comprehensive benefits package that includes Health, Dental, Vision, Paid-Time-Off, Paid Holidays, 401K matching, Life and Accidental Death Insurance, Short- & Long-Term Disability Insurance, and a variety of other offerings.
 Diversity & Inclusion 
Our success as a company is built on practicing inclusion and embracing diversity. HTC Global Services is committed to providing a work environment free from discrimination and harassment, where all employees are treated with respect and dignity. Together we work to create and maintain an environment where everyone feels valued, included, and respected. At HTC Global Services, our differences are embraced and celebrated. HTC is an Equal Opportunity Employer. We respect and seek to empower each individual and support the diverse cultures, perspectives, skills, and experiences within our workforce. HTC is proud to be recognized as a National Minority Supplier

EEO/M/F/V/H","PySpark, Manipulación de datos, Snowflake cloud y YAML",Solicitar
https://www.linkedin.com/jobs/view/3984542110/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=VjD7ODdxaxKaqe3wvDKL1A%3D%3D&trk=flagship3_search_srp_jobs,"Data/Backend Engineer II, Content Understanding","Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 4 días,"Nueva York, NY","Acerca del empleo
As a Software Engineer in our Content Understanding teams, you will help define and build ML deployed at scale in support of a broad range of use cases driving value in media and catalog understanding. We are looking for a strong Data / Backend Engineer with experience in ML Ops supporting Machine Learning systems at scale in production. This role will be for a team focused on NLP/LLMs.

What You'll Do

Build large-scale batch and real-time data pipelines with data processing frameworks like Scio, Beam, Dataflow, and Flink, to run inference on ML models on the Google Cloud Platform
Work closely with Machine learning engineers to design, develop, and deploy services in Java that serve ML models with a focus on high availability, robustness, and monitoring
Use best practices in continuous integration and delivery
Help drive optimization, testing, and tooling to improve data and systems quality
Work in multi-functional agile teams to continuously experiment, iterate, and deliver on new product objectives
Take operational responsibility for the services that are owned by your team
Work in an environment that supports your individual growth by providing you ambitious tasks to tackle and the time needed to acquire new skills

Who You Are

Have 3+ years of professional experience working in a product facing environment
Worked with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra
Experience deploying and supporting Machine Learning systems in production at scale
Experienced in writing distributed, high-volume services in Java or Scala
Have an understanding of system design, data structures, and algorithms
Knowledgeable about data modeling, data access, and data storage techniques
Deployed and operated services in a cloud environment such as GCP or AWS
Enjoy close collaboration with Machine Learning engineers and passionate about the software ML systems architecture across the stack
Care about agile software processes, data-driven development, reliability, and responsible experimentation

Where You Will Be

For this role you will be based in New York City, USA 

The United States base range for this position is $122 716 - $175 308, plus equity. The benefits available for this position include health insurance, six month paid parental leave, 401(k) retirement plan, monthly meal allowance, 23 paid days off, 13 paid flexible holidays. These ranges may be modified in the future.

Today, we are the world’s most popular audio streaming subscription service.","Desarrollo web back end, Bases de datos, Ciencias de la computación, Fiabilidad y Optimización",Solicitar
https://www.linkedin.com/jobs/view/3974953625/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=RkMksdwdiLUyG5%2BgfIr8HQ%3D%3D&trk=flagship3_search_srp_jobs,Analytics Data Engineer,"115 US$K/año - 165 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 2 semanas,"South San Francisco, CA","Acerca del empleo
Who We Are

Claritas Rx is a venture-backed digital health startup that brings clarity to the challenges of specialty biopharmaceutical products in the marketplace. In today’s highly complex specialty networks, our mission is to illuminate the patient experience beyond the clinical trial. Claritas Rx leverages a proprietary technology platform and deep manufacturer expertise to automate and integrate channel, commercial, and clinical data and help biopharmaceutical companies generate actionable business insights. Our work uncovers the real-world variables impacting patient access, duration of therapy, and other metrics key to commercial success, making a real impact on patient healthcare.

 The Position

We are a rapidly growing healthcare technology company seeking an Analytics Data Engineer. As part of our Data Science team reporting to the Sr. Director of Analytics, you will be responsible for delivering on many data engineering initiatives towards our Analytics product roadmap. The role partners very closely with the Product Development and Customer Experience team to deeply understand the semantics of data to which we have access and scale our environment in support of AI / machine learning solutions. Our products generate actionable business and clinical insights for biotech and pharmaceutical companies. If you are passionate about revolutionizing healthcare through innovative health data platforms and want to significantly impact a fast-paced growth environment, we would love to hear from you!
 
Responsibilities
Design, build and maintain the technology architecture, solutions, and software to capture, manage, store and utilize structured and unstructured data for the purposes of Data Science solutions.
Design, develop and maintain Claritas Analytics Data Model to support scalable Advanced Analytics and Data Science solutions
Rapidly iterate and evolve Claritas ML feature stores to enable scalable machine learning solutions.
Scale and upgrade Claritas Analytics Data Model, ML feature stores and pipeline to efficiently and reliably process millions of rows of data
Develop and deploy complex business metrics and reports, in collaboration with Data Science, Product and Customer Success team members. 
Reviews business and product requirements for our data model and feature stores and make updates to accommodate ongoing business goals.
Execute data quality checks and validation processes to ensure the accuracy, completeness, and consistency of data across different systems and platforms
Document data pipelines, workflows, and best practices for knowledge sharing and maintainability
 
Who You Are

Requirements:
B.S. in computer science, engineering, or related degree
5+ years of professional data engineering experience, focused on data architecture and business intelligence systems including ELT development, data transformation, and data modeling.
Deep understanding of data modeling concepts (ETL, data structures, metadata and workflow management).
Proficient in Python (Pandas) and SQL, and understand the best practices for when to use which language for analytical reporting
Passionate about understanding the business needs behind reporting requirements, and able to appropriately design, transform raw data to produce required metrics
Experience with reporting tools such as Tableau or Looker
Experience with Agile software development (e.g. scrum, Kanban).
Excellent communication skills to narrate data driven insights and technical approach
Experience in a fast-paced environment with enterprise class software release experience as well as SaaS based short, quick product life cycles.
Ability to prioritize, organize and execute multiple tasks with attention to detail
Ability to work and collaborate proactively in a fast paced, dynamic and team-oriented environment

Preferred:
Experience working in AWS and Snowflake
Deep knowledge of data management in specialty biotech, and/or operations in reimbursement call centers or specialty pharmacies.
Familiarity with HIPAA/SOC/HITRUST/GDPR requirements.
 
Join Us

We are seeking to add new expertise and perspective to our strong team of experienced professionals. We aspire to a culture of accelerated professional development through shared learning and collaboration; a respectful and fun work environment; and employee empowerment through the effective use of technology and tools.

In addition to our great environment, we offer a competitive salary and benefits package and the opportunity to make a significant impact on a first-in-industry digital health solution. Please send a cover letter along with your resume when applying to the position of interest. Claritas Rx embraces diversity, equality, and transparency. We are committed to building a team that comprises a variety of backgrounds, perspectives, and talents. We believe the more inclusive we are, the better we are.
Email careers@claritasrx.com","Amazon Web Services (AWS), Análisis de datos, Análisis de datos estadísticos y Big data, Modelado de datos y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3909464337/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=cjaNaT4AAIXrK1L4H7ceBw%3D%3D&trk=flagship3_search_srp_jobs,Senior Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 3 meses,"Draper, UT","Acerca del empleo
Redo is a fast-growing early-stage SaaS startup helping merchants offer free, easy returns. Our office is in Draper, Utah.

We are hiring our first full-time data engineer to develop and maintain our growing data infrastructure.

Responsibilities
Work with stakeholders to collect requirements for data systems.
Architect and implement data systems.
Produce high-quality reports according to business requirements.
Help identify important business trends.

Required Qualifications
At least 4 years of full-time experience in data engineering.
Excellent SQL skills.
Experience organizing data warehouses and reports.
Strong coding abilities (e.g. Python).
Track record of delivering high-impact outcomes.
Excitement about working at an early-stage startup.
Pride in owning projects and driving them to successful completion.
Ability to translate business requirements into technical implementation.
Strong sense of urgency.
Commitment to collaborative problem solving and quality outcomes.

Technologies Used
Snowflake
SQL
PostgreSQL
Amplitude

 Optional Qualifications
Experience at an early stage startup.
Experience in e-commerce.
Experience with product A/B testing.
Proficiency in statistical methods.","Almacenamiento de datos, Aprendizaje automático, Extraer, transformar y cargar (ETL), Ingeniería de datos y Inteligencia empresarial, Bases de datos y Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3932602272/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=HI8H73XIFqpXQnNcM3eVSA%3D%3D&trk=flagship3_search_srp_jobs,Software & Data Engineer (Remote - Latin America),"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 3 meses,Estados Unidos,"Acerca del empleo
Fractal River is a consulting firm that helps growth-stage startups scale faster. It does so by working hands-on with the startups to build the technical and operational infrastructure they need to scale, while at the same time transferring knowledge and helping them develop their own internal capabilities.

We create and manage analytics infrastructure and cloud-based systems, with technologies such as Python, SQL, Docker/Linux, and serverless capabilities and infrastructure (Snowflake, PostgreSQL, RedShift, Lambda, etc.) hosted within the Amazon or Google clouds. We also configure and set up a variety of systems along the startup’s customer journey including HubSpot, Salesforce, and ZenDesk.

We are seeking a Data Engineer to join our team. 

What You’ll Do

Help design and implement data pipelines and analytic infrastructures with the latest technologies.
Create beautiful dashboards and reports, and work with customers to create self-service exploration data cubes using Looker.
Develop Python & SQL components to integrate and automate a variety of processes and tools.
Leverage APIs from multiple systems to extract and update data, trigger and monitor processes and in general help tie our customers’ infrastructures into cohesive platforms that power their growth.
Maintain and oversee cloud infrastructure to ensure it is running with the reliability and performance our customers expect.
Help create Data Models, best practices, and technical documents for our customers.
Develop our internal best practices, policies, and processes regarding DevOps and DataOps.
Coordinate projects, activities, and tasks to ensure objectives and key results are met.
Help identify opportunities, generate innovative solutions, and improve existing product features.

Who We Are Looking For

Our ideal candidate is someone with 2-5 years of working experience in fast-paced environments, a high tolerance for ambiguity, and a passion for constant learning. We’re looking for motivated, highly adaptive people who enjoy the challenge of working with brilliant people in multiple, innovative startups to put in place technical infrastructures that power their growth. They must be able to interact with customers and perform not only technical work but can understand needs, extract requirements, and design and propose solutions as well.

Candidates Must Ideally Also

Be adept at data analysis and process improvement.
Work collaboratively but are also able to own a project with little guidance
Thrive in an environment with constant and quick iterations.
Seek out creative solutions to challenging problems.
Have strong communication skills in English (CEFR level C1 is a must, see details).
Have strong attention to detail.
Be wizards in Python and/or speak SQL as a second language.
Be very familiar with databases & data warehouses: Snowflake, Redshift, BigQuery, Postgres, MySQL, etc.
Be comfortable creating complex data models and visualizations.
Be familiar with development tools (Terraform, GitHub, VSCode, CircleCI, Docker, dbt).
Be knowledgeable of AWS environments (EC, Lambda, IAM, SQS, RDS, Kinesis, VPC, etc.)
Like to work in a very diverse work environment and be comfortable with non-hierarchical organizations.
Have a bachelor’s degree in computer science or similar.
Google Professional Data Engineer / Cloud Developer certifications are a plus.
AWS associate or professional-level certifications (Solutions Architect, Developer, DevOps Engineer) are a plus.
Knowledge of Google Analytics, Salesforce, HubSpot, and/or ZenDesk is a plus.
Knowledge of Looker or Tableau is a plus.

Besides the benefits that as per law you are entitled to, we offer:

Personal development plan with accelerated career track.
Access to an extensive reference library of books, case studies, and best practices.
Unlimited budget for work-related books.
Support for certifications (time to study, sandboxes, all related costs) including AWS, Google, Microsoft, and other technologies.
Special bonuses for certifications acquired.
Online training (Udemy, nanodegrees, etc.), English language training.
Stretch assignments, pair-programming and code reviews with new technologies.
Yearly performance bonus based on personal performance.
Yearly success bonus based on company performance.
Home office setup including fast internet, large monitor, and your favorite keyboard and mouse. After a few months, gaming chair, espresso maker, standing desk, and speakers (or other items like these).
Monthly wellness budget to cover items such as sodas, tea, snacks, pet care, yoga, or other wellness-related items.
Company card for wellness budget and company expenses.
Private healthcare contribution.
Three floating holidays and unlimited (but reasonable) PTO starting the first year.
Fun company off-sites!

We are proud to be an equal opportunity employer.","Analítica de datos, Análisis de datos y Ingeniería de datos, Ciencias de la computación, Comunicación, Coordinación de proyectos, Cubes, Data Cubes, Inglés y Modelo de datos",Solicitar
https://www.linkedin.com/jobs/view/3967784788/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=ClVIdRldaL%2FxMl6oWoMDhw%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"120 US$K/año - 135 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Atlanta, GA","Acerca del empleo
Data Engineer

Location: Atlanta, GA
Employment Type: Contract to Hire
Compensation: up to $135k DOE

The data engineer is responsible for resolving technical challenges to enhance business decision-making capabilities. Your technical expertise, business understanding, and creativity will be crucial in developing tools for automating reporting and generating business insights.

Responsibilities:

- Develop data strategies in collaboration with business units, design and manage data warehouses.
- Implement BI frontends like Power BI, Tableau, or equivalent.
- Serve as the SQL specialist, utilizing SQL and other platforms for data gathering, cleaning, dashboard creation, KPI development, and trend analysis.
- Maintain efficient data pipeline architectures and aggregate large datasets.

Ideal Candidate:

- Bachelor's degree or equivalent experience.
- 5+ years in similar roles, advanced SQL proficiency, and relational database experience.
- Experience with big data pipelines, DBT, and BI tools.
- Familiarity with cloud technologies (e.g., Google BigQuery, Azure) and automation concepts.
- Strong problem-solving skills, detail-oriented, and committed to accuracy.
- Excellent communication and documentation skills, with a focus on customer service.","Canalizaciones de datos y SQL, Estrategia de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3924412512/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=KfK5rs4LoihteXP4ouSTuA%3D%3D&trk=flagship3_search_srp_jobs,ETL Pentaho Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 2 meses,Estados Unidos,"Acerca del empleo
Job Title: ETL Pentaho Data Engineer

Location: Remote

Jd

Experience with relational databases, Data Warehouses and Data Integration is required. 
Experience with Pentaho highly preferred.
6+ Years’ experience working in a data integration capacity.
Working knowledge of key concepts in data warehousing and data integration
Experience and exposure to unit and user accepting testing.
Demonstrated experience defining data management standards and principles.
Strong working knowledge with SQL, such as SQL Server, or MySQL including physical table design, optimization, and development of stored procedures.
Strong working knowledge with on-prem and Cloud integration/ETL tools like Talend and SSIS, other ETL tools like DataStage and Pentaho.
Strong working knowledge with data modeling including logical and physical data models.
Working knowledge with one or more data modeling tools like DBSchema or Erwin.
Solid experience with creation of technical requirements, user documentation and operations guides.
Knowledge of data virtualization concepts and technologies like Denodo are desirable.
Good working knowledge of business processes as it relates to Data Management initiatives.","Almacenamiento de datos, Base de datos relacional, Extraer, transformar y cargar (ETL), Gestión de datos y Integración de datos, Bases de datos, Documentación de usuario, Modelado de datos, Modelo de datos y Requisitos técnicos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3927281687/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=t0lOFwJxn2SbgnXB%2FpynvA%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"108 US$K/año - 198,5 US$K/año Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,"San José, CA","Acerca del empleo
Our Company

Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.

We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!

Adobe is seeking a Data Engineer who consistently demonstrates talent and motivation to join our exceptional Marketing Operations and Engineering department in San Jose. The Marketing Operations and Engineering team is part of Growth Marketing and Insights (GMI) organization and is responsible for the data architecture, transformation, and engineering needs of the entire organization. This includes managing backend data for various marketing measurement tools and processes, including attribution, product and customer segmentation, lifetime value, and more.

Responsibilities:

Design, develop, and maintain scalable and reliable data pipelines and infrastructure. 
Collaborate with teams from different departments to gather and analyze data requirements. 
Optimize data storage, processing, and retrieval for efficient and flawless operation. 
Implement data models and schemas to support various data-driven applications. 
Ensure data quality, integrity, and security throughout the data lifecycle. 
Perform data analysis and provide insights to drive informed decision-making. 
Keep yourself informed about the latest developments and effective approaches in data engineering. 

Requirements:

Bachelor's degree in Computer Science, Engineering, or a related field, with at least 3 years of data engineering experience. 
Proven experience in designing and developing data pipelines and ETL processes. 
Strong programming skills in Python, Java, or Scala. 
 Experience with Spark, Databricks, Airflow and related Big Data technologies 
Familiarity with cloud-based data platforms (e.g., AWS, Azure, GCP). 
Knowledge of SQL and database systems. 
 Knowledge of UNIX and shell scripting 
Strong problem-solving and analytical abilities. 
Excellent communication and collaboration skills. 
 Experience in marketing domain is a plus 

Join Adobe, a company that values diversity, inclusion, and equal opportunity. We are an equal opportunity employer and do not discriminate based on gender, race, color, ethnicity, national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. At Adobe, we believe that diverse perspectives drive innovation and creativity.

Note: If you have a disability or special need that requires accommodation during the application process, please contact us at accommodations@adobe.com or call (408) 536-3015. We aim to ensure our website and application process are accessible to all users.

We strictly adhere to fair competition practices and maintain a free and open marketplace for all employees. We have policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other's employees.

Our compensation reflects the cost of labor across several  U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $108,000 -- $198,500 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.

Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances.

Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.

Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.

Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.","Airflow, Analítica de datos, Apache Spark, Canalizaciones de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Ciencias de la computación, Comunicación, Modelo de datos y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3958536316/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=bn2IGSnZRNlmuFBSDgLPWQ%3D%3D&trk=flagship3_search_srp_jobs,Snowflake - Sr. Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,"Dallas, TX","Acerca del empleo
About Tredence:
Tredence focuses on last mile delivery of insights into actions by uniting its strengths in business analytics, data science, and software engineering. The largest companies across industries are engaging with Tredence and deploying its prediction and optimization solutions at scale –empowering end users to improve decision making. Headquartered in the San Francisco Bay Area, the company serves clients in the US, Canada, Europe, and SE Asia. Learn more at www.tredence.com 

Hands on Snowflake experience with strong knowledge about below:
Setting up best practice for Role based access for the Snowflake environment
hands on Python and R programming expert for converting some R code base leveraging SnowPark 
Strong hands on Snow SQL expertise
Expereince in writting RegEx based parsing in Snowflake for PDF files
Strong communication to handle customer requirements and delivery expectations.
Co-ordinate work allocation with offshore team

Nice to have:
Snowpark container services
Cortex AI (Document AI)

About you:
You are self-motivated, collaborative, eager to learn, and hands on
You love trying out new apps, and find yourself coming up with ideas to improve them
You stay ahead with all the latest trends and technologies
You are particular about following industry best practices and have high standards regarding quality

Why join Tredence?
There is a reason we are one of the fastest growing private companies in the country! You will have the opportunity to work with some of the smartest, friendliest, hardest working people in the data analytics space. You will work with the latest technologies and interface directly with the key decision stakeholders at our clients, some of the largest and most innovative businesses in the world. We offer a 401k match; full medical, dental and vision benefits, a fun team atmosphere and a work life balance. Our people are our greatest asset and we value every one of them. Come see why we’re so successful in one of the most competitive and fastest growing industries in the world","SQL, Snowflake",Solicitud sencilla
https://www.linkedin.com/jobs/view/3920347527/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=GWkKKEPYBcinGrrqd5E8HA%3D%3D&trk=flagship3_search_srp_jobs,Remote Work - Need Azure Data Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",hace 2 meses,Estados Unidos,"Acerca del empleo
100% Remote

Need valid LinkedIn

Must be W2 or 1099

Must HAVES

Banking/Financial Services Experience

Azure Cloud

Azure Data Factory

Databricks

Azure SQL

Python – or same/similar scripting for automation

NEED

Azure Services/Azure Functions

Azure Purview

Azure Power-Automate

GCP or AWS Cloud atop Azure is nice as well

Moving on Prem Automations via Automic & SSIS to Azure cloud technologies

Purpose

Looking for a team player, with design experience in Azure data tooling. The individual would be someone that preferably has migration experience in any capacity to help in creating the movement of data and processes from on prem solutions to Azure cloud.

Let me know if you have any candidates!","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL), Google Cloud , Ingeniería de datos , SQL y SQL Server Integration Services (SSIS), Automatización, Azure Databricks y Bases de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3941803360/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=wwcjfiAK8vxRPg3w0yxVJg%3D%3D&trk=flagship3_search_srp_jobs,Scala Data Engineer - (U.S. remote),"140 US$K/año - 170 US$K/año En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 1 semana,Estados Unidos,"Acerca del empleo
#Hiringnow We are actively hiring (Data Engineers)

We are seeking several Senior Data Engineers to join a new team to be a strong technical resource on a dynamic and growing team of engineers. Our ideal candidate is passionate about creating well-architecture solutions containing thoroughly tested code. The ability to communicate effectively and create relationships by empathizing with client goals is a highly valued skill within our company culture.

Core Responsibilities:

Develop new and enhance existing application services
Writing tests to maintain code quality
Understand and adapt to our client's evolving business requirements within the television advertising domain.
Participate in detailed technical design sessions to understand client needs and provide productive feedback
Identify new opportunities, tools, and services to enhance the software platform
Support and troubleshoot issues, identify the root cause, and proactively recommend corrective actions


Skills & Experience:

Scala 2.12 + development experience
Passionate about developing clean and maintainable code with little or no side-effects
Experience building data pipelines
Strong hands-on experience with AWS running spark jobs on ephemeral EMR clusters on AWS and S3. 
Experience with relational and non-relational databases
Willingness to learn new technologies and takes pride in keeping up with the latest technologies and practices within the Scala and Spark development community
Excellent oral and written communication skills
Strong analytical and problem-solving skills
Self-directed and can effectively deliver solutions with little oversight
Bachelor's or master's degree in computer science, computer engineering, or other technical disciplines or equivalent work experience is preferred but not required


$140,000 - $170,000 a year

Salary is commensurate with experience. We offer base salary + bonus and competitive benefits, including medical, dental, vision, pet insurance, 401k, and generous Paid Time off (PTO).

We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.","Apache Spark y Scala, Amazon S3, Anuncios, Bases de datos, Comunicación, Comunicación escrita, Diseño técnico, Necesidades empresariales y Resolución de problemas",Solicitar
https://www.linkedin.com/jobs/view/3984134920/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=a2Vc0MkTsi%2B1n3nji93Z7Q%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 4 días,"Irving, TX","Acerca del empleo
POSITION TITLE: Data Engineer

Duration : Through 12/31/2024

OFFICE LOCATION: Phoenix, AZ

CORE TIME ZONE: MST

FULL-TIME WORKING REMOTELY (from home): Yes

REMOTE WORK COMMENTS: Must be available during normal working hours in MST (AZ time)

Must Skills

AWS Lambda, ETL, Snowflake, and SQL

Position Summary

We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives

Principal Responsibilities

 Create and maintain optimal data pipeline architecture,
 Assemble large, complex data sets that meet functional / non-functional business requirements.
 Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
 Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies.
 Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
 Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
 Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
 Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
 Work with data and analytics experts to strive for greater functionality in our data systems.
 Troubleshoots issues with minimal guidance, identifies bottlenecks in existing data workflows and provides solutions for a scalable, defect-free application
 Works with onshore/offshore team to analyze, develop and improve pipeline run times as well as produce accurate defect free code
 Complies with Company policy and practices relating to the System Development Life Cycle.
 Provides Tier 3 support and resolution of IT issues escalated by IT Customer Support.
 Support audit and compliance reporting requests.
 Support the operation of MarkLogic and Snowflake products on a 24/7 basis as needed.
 Supports production environment in the event of emergency
 Participate in on-call support 24x7 weekly rotation of the operation of Informatica.
 Performs other job-related duties as assigned or apparent.

Minimum Qualifications

 2+ years of experience in a Data Engineer role, who has attained a bachelor's degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.
 AWS: 1 year experience
 DevOps Practices: 1 year experience
 2+ years' experience working with data warehousing, ETL development and ETL architecture.
 2+ years' experience combined experience with any of the following database technologies (RDBMS: MSSQL, MySQL Oracle; NoSQL: MarkLogic, Snowflake, DynamoDB, Redis).
 2 years' experience working on large data initiatives (?5 terabytes).
 1 years' experience with JavaScript

Preferred Qualifications

 2+ years' experience working with data warehousing, ETL development and ETL architecture.
 2+ years' experience combined experience with any of the following database technologies (RDBMS: MSSQL, MySQL Oracle; NoSQL: MarkLogic, Snowflake, DynamoDB, Redis).
 2 years' experience working on large data initiatives (?5 terabytes).
 1+ years' experience with JavaScript
 Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
 Experience building and optimizing 'big data' data pipelines, architectures and data sets.
 Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
 Build processes supporting data transformation, data structures, metadata, dependency and workload management.
 Good knowledge and experience of working with OO Javascript, XHTML, CSS, XML, Ajax and one or more JavaScript libraries (e.g. Prototype, JQuery)
 Experience with web services (e.g. RESTful services), including the ability to programmatically interact with data formats that may include XML, JSON and RDF
 Experience with writing software for complex web-based business applications which makes use of client-side data capture, validation and presentation
 Working knowledge of version control systems (e.g. SVN, Git)","Almacenamiento de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Sistema de gestión de bases de datos relacionales, Ciencias de la computación, JavaScript, MarkLogic, Necesidades empresariales, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3976912372/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=Wa7B%2BRLMaYmGxtcFHevSbA%3D%3D&trk=flagship3_search_srp_jobs,Snowflake Data Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Intermedio",hace 1 semana,"Dallas, TX","Acerca del empleo
Title: Snowflake Data Engineer
Location: Dallas, TX
 About the Role:
We are seeking a Data Engineer to join our team and leverage their expertise in Snowflake, dbt, and data transformation tools to build and maintain scalable data pipelines. You will be responsible for extracting, transforming, and loading (ETL) data from Microsoft SQL Server (MSSQL) into our Snowflake data warehouse, ensuring clean, reliable data for analysis and reporting.
Responsibilities:
Design, develop, and maintain data pipelines using Snowflake, dbt, or Coalesce.
Write and optimize SQL queries within dbt for efficient data transformation in Snowflake.
Utilize Coalesce or similar data transformation tools to migrate data from MSSQL to Snowflake.
Cleanse, validate, and transform data to ensure accuracy and consistency.
Design and implement data models in dbt to transform raw data into analytical-ready datasets.
Automate data pipelines and ensure their reliability and scalability.
Collaborate with data analysts, data scientists, and business stakeholders to understand data needs and translate them into technical solutions.
Develop and implement unit tests for data pipelines and data models.
Monitor and troubleshoot data pipelines for any issues.
Document data pipelines and models for future reference.
Qualifications:
Bachelor's degree in Computer Science, Data Science, Statistics, or a related field
Minimum 2+ years of experience as a Data Engineer or similar role.
Proven experience with Snowflake, including writing SQL queries and leveraging its functionalities.
Expertise in dbt, including model development, testing, and deployment.
Experience with data transformation tools like dbt, Coalesce, or Hevo Data.
Strong understanding of data warehousing concepts and best practices.
Experience with ETL/ELT processes.
Excellent SQL programming skills (T-SQL and Snowflake SQL).
Experience with version control systems (e.g., Git).
Familiarity with data quality concepts and techniques.
Excellent communication and collaboration skills.
Bonus Points:
Experience with cloud platforms (e.g., AWS, Azure).
Experience with data orchestration tools (e.g., Airflow, Luigi).
Experience with data governance and security practices.",Git y SQL,Solicitud sencilla
https://www.linkedin.com/jobs/view/3972895429/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=FyjGfFIU8NTVdRcO5rZLJg%3D%3D&trk=flagship3_search_srp_jobs,Informatics Data Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Chula Vista, CA","Acerca del empleo
Position Summary

The majority of the data engineer’s responsibility will be in building, managing and optimizing data pipelines and then moving these data pipelines effectively into production for key data and analytics consumers (like business/data analysts, data scientists or any persona that needs curated data for data and analytics use cases). This individual will also need to guarantee compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines. The newly hired data engineer will be the key interface in operationalizing data and analytics on behalf of the Informatics team and organizational outcomes. This role will require both creative and collaborative working with the Informatics team and the wider IS team.

Compliance With Regulations

Work closely with all departments necessary to ensure that the processes, programs, and services are accomplished in a timely and efficient manner in accordance with CHG policies and procedures and in compliance with applicable state and federal regulations, including the Department of Health Care Services (DHCS) and the Centers for Medicare & Medicaid Services (CMS).

Responsibilities

Build, optimize, automate, and monitor data pipelines, primarily those that feed into the Enterprise Data Warehouse.
Collaborate in designing and optimizing the existing Enterprise Data Warehouse to best support the data science and analytic needs of the Informatics team.
Use proven architectures and techniques to automate the most common and repeatable data preparation and integration tasks.
Participate in ensuing compliance and governance by maintaining metadata and master data via a data catalog.
Train the Informatics team in the data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.
When necessary, assist in producing datasets and simple analytics tools to be used by both technical and non-technical end users.
Define and execute appropriate methods; maintain and develop the technical skills necessary to support the current systems; adhere to standard testing methodology to ensure that new or modified programs have no adverse impact on systems operation.
Enhance professional growth and development by participating in educational programs, reading current literature, attending in-service training and workshops, attending meetings as required, and participating on committees as directed.
Support the team effort by arranging and conducting meetings; assist in the care and maintenance of department facilities, computer equipment, and supplies; maintain departmental policies and procedures, objectives, quality assurance program, safety, environmental and infection control standards; perform other related duties as requested or assigned.
Maintain product and company reputation and contribute to the team effort by conveying professional image.

Education

Bachelor’s degree required.

Experience/Skills

Five years of experience with SQL Server Integration Services (SSIS) and data warehouse development.
Five years of experience in database analysis, advanced SQL programming in SQL Server or other relational database management system.
Experience in developing database specifications, test plans, and system documentation.
Knowledge of SQL Server Reporting Services (SSRS), Microsoft Excel, and Tableau preferred.
Excellent oral and written communication; good analytical, logical and organization skills; excellent time management; ability to work on multiple projects concurrently; adherence to security requirements and confidentiality as required by the company and HIPAA.
Experience in data science/machine learning preferred.
Experience in healthcare preferred.

Physical Requirements

Minimum 30 lb. Lifting.
May be required to work evenings and/or weekends.

**Must have current authorization to work in the USA**

The above statements are intended to describe the general nature and level of work being performed. They are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required of personnel so classified.

Community Health Group is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment based on any protected characteristic as outlined by federal, state, or local laws. This policy applies to all employment practices within our organization, including hiring, recruiting, promotion, termination, layoff, recall, leave of absence, compensation, benefits, and trainings. Community Health Group makes hiring decisions based solely on qualifications, merit, and business needs at the time. For more information, see Personnel Policy 3101 Equal Employment Opportunity/Affirmative Action.

Employment Type: Full Time

Salary: $129,783 - $136,758 Annual

Bonus/Commission: No","Almacenamiento de datos, Análisis de bases de datos, Ciencia de datos y SQL Server Integration Services (SSIS), Administración de bases de datos, Bases de datos, IT Documentation, Logics, Preparación de datos y Sistema de gestión de bases de datos (SGBD)",Solicitud sencilla
https://www.linkedin.com/jobs/view/3971087952/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=bXF3rdOTh0N%2B9lI5d6qmjw%3D%3D&trk=flagship3_search_srp_jobs,Data Aquisition Engineer,"Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 2 semanas,"Auburn Hills, MI","Acerca del empleo
Company Description

Do you want to shape the world of tomorrow and work on innovative projects? Then you're a good fit for us! We are a global engineering group with 15,000 employees and 140 locations worldwide. With our services, we are driving the topics of energy efficiency, climate protection and decarbonization and developing tech industries into new dimensions.

In United States, the world of SEGULA Technologies Experts revolves around industries such as automotive, software, energy and life sciences. As an independent player, we support OEMs and suppliers with personal engineering services. Take advantage of new opportunities and take on exciting tasks and attractive positions with direct customer assignment.

Job Description

The Data Acquisition engineer works closely with the platform engineers, CAE and modeling groups to help define the test and determine the required channels needed to record to support the engineering needs for the project.
The Data Acquisition Engineer is responsible for performing Powertrain and or Body Chassis data acquisitions for all engine / chassis components.
The Data Acquisition engineer is responsible for recording the data, validating the data and performing preliminary analysis of the data.
The Data Acquisition Engineer is responsible for performing the Analysis for Driveline Acquisitions.
The Data Acquisition Engineer is responsible to document and format the data when forwarded to suppliers.
The DAQ Engineer addresses development and field issues in addition to correlation studies supporting CAE activities
Responsible for the development of engine vibration profiles for accelerated vibration fatigue testing.
Perform fatigue analysis of vehicle/engine components
Responsible for data acquisition of vehicle/engine components at the engine-dynos and at Proving Grounds.
Use statistical tools to predict fatigue life and correlate to CAE models.
Work closely with the engine / chassis engineers to solve durability issues
Work with SBU personnel to design and calibrate load transducers using automotive parts
Work with various transducers, accelerometers, proximity probes, etc, to collect engine / chassis data
Use nCode data analysis software to reduce data
Use both eDAQ and MARS data acquisition hardware to collect data
Responsible for analyzing and uploading data to RLDMS
Responsible for holding handoff meeting to release data to the engineering community
Responsible for uploading all DAQ supporting documents, photos and writing final report in the SPMS system

Additional Information

No travel is expected","Analítica de datos, Ciencia de datos, Extraer, transformar y cargar (ETL) y Ingeniería de datos, Adquisición de datos, Análisis de durabilidad, Fatigue Testing, Herramientas estadísticas, Ingeniería asistida por ordenador y nCode",Solicitud sencilla
https://www.linkedin.com/jobs/view/3978196614/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=pt3ksQ4%2FQRdvZM%2Br7uRb5w%3D%3D&trk=flagship3_search_srp_jobs,Data Engineer II,"Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.",hace 1 semana,"Dallas, TX","Acerca del empleo
Description & Requirements

As a Data Engineer II at Forvis Mazars, you will be a member of the Enterprise Data Services (EDS) team who works across Firm Technology Services on different projects and in a variety of settings to build systems that collect, manage, and convert raw data into usable information. The goal is to make data available in a secure and performant manner to support the technical, operational, and business needs of Forvis Mazars. Typically, a Data Engineer II is responsible for building data pipelines and supporting infrastructure to bring together information from different source systems.

We are recognized internally as ""the go-to people"" for the most complex Information Management assignments. We analyze, design, and develop enterprise data and information architecture deliverables, focusing on data as an asset of value to the business.

Working on the Enterprise Data Services team means we solve hard, complicated problems with aggressive timelines. Having and making a conscious effort to support and contribute towards a collegial attitude working together is of paramount importance for the successful functioning of the team. Each team member is expected to actively participate in the development of solutions as called upon and be able to work independently with confidence and knowing you have the support of the team.

How You Will Contribute

Collaborate with a team to understand, in depth, both the business and technical problems EDS seek to solve

Maintain an enterprise-perspective when designing, developing, and implementing solutions and delivering services

Work with technical resources to assist and guidance to other teams (such as Software Development, Robotic Processing & Automation, etc.) in developing conceptual, logical, and physical data models to meet specific application needs, to enable support for Forvis Mazars enterprise data needs, in a way consistent with Forvis Mazars enterprise and data architecture principles and guidelines

Work with the Enterprise Data Services (EDS) Analytics team to establish dashboards and other monitoring tools to monitor the status of data pipelines and data quality in the pipelines

Support updating enterprise systems and processes (example, STAR updates for Practice Structure Changes)

Operationally maintain & troubleshoot existing data integrations

Support the growing data management infrastructure, which may include:

Maintain the Master Data and Master Metadata Repositories.

Assist with the development, implementation, and maintenance of DataSpring repositories

Data Quality

Data Lineage


We are looking for people who have Forward Vision and:

Strong analytical and design skills

Work collegially and collaboratively within the team

Ability to consult successfully with Product Management, Project Management, and business stakeholders



Minimum Qualifications

Bachelor's degree in a related field

3 years or more of experience in a related field

Knowledge of Data and its relationship to Information and the development of domain driven data products

Proficiency in SQL/SSMS/SSIS/SSAS/Data Modeling to make necessary modifications

Experience with data profiling in order to work with business stakeholders and understand their data needs

SQL Proficiency and knowledge of semantic data concepts

Proficiency using ETL tools such as SQL Server Management Studio (SSMS), SQL Server Integration Studio (SSIS), or Azure Data Factory (ADF)



, , , ,","Arquitectura de datos, Extraer, transformar y cargar (ETL), Herramientas ETL, SQL Server Integration Services (SSIS) y Visualización de datos, Calidad de datos, Modelado de datos, Perfiles de datos, SQL Server Analysis Services (SSAS) y SQL Server Management Studio",Solicitar
https://www.linkedin.com/jobs/view/3937678500/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=zvogcTqbrRp%2B0HXYsqaMNQ%3D%3D&trk=flagship3_search_srp_jobs,"Associate Data Engineer, Advanced Analytics (Hybrid)","Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"Farmington Hills, MI","Acerca del empleo
Amerisure creates exceptional value for its partners, policyholders and employees. As a property and casualty insurance company, Amerisure’s promise to our partner agencies and policyholders begins with a comprehensive line of insurance products designed to protect businesses, as well as the health and safety of every employee. Amerisure is an A.M. Best “A” (excellent) rated company and services mid-sized commercial enterprises focused in construction, manufacturing and healthcare. We are ranked as one of the top 100 Property & Casualty companies in the United States, and have nearly $1 Billion of Direct Written Premium and $1.15 billion in surplus.

Amerisure is recruiting for an Associate Data Engineer, Advanced Analytics to join our team in our Farmington Hills, MI. This is a hybrid position working Tuesday’s and Wednesday’s, in the office, and remote the remainder of the week. 

Position Summary:

Provides data management to support advanced analytics and related functions in the organization. This position will plan, analyze, design, develop, implement, and maintain data infrastructures that ensure the timeliness, availability, and integrity of data for department and company initiatives.

Responsibilities:

Collaborate with cross-functional teams to understand data structures and requirements, including analytics team, actuarial, IT, and relevant stakeholders.
Collect and cleanse data including extracting and combining data from multiple sources using languages such as SQL, SAS, and Python.
Design data pipelines for efficient data extraction, transformation, and loading (ETL).
Develop and maintain database schemas, tables, and queries to support analytics projects.
Contribute to the development of data models that align with business needs and facilitate effective data analysis.
Design improvements to existing data capture to assimilate new data elements as needed.
Build use cases and execute test plans that encompass unit testing and analysis of results.
Maintain and promote a high standard for data management industry best practices.
Document new and existing systems, develop workflows, and create other supporting documents in adherence to all departmental and corporate standards.


Requirements:

Bachelor’s degree in computer science, data analytics, or related field.
Experience in data management or related field.
Knowledge of insurance industry processes and data preferred.
Strong understanding of databases, data warehousing, and ETL processes.
Proficiency in SQL
AWS Cloud experience preferred.
Basic programming skills with Python.
Intermediate proficiency with Microsoft Office Suite.
Familiarity with agile methodologies.
Ability to build positive partnerships and work collaboratively with cross-functional business teams.
Excellent problem solving, critical thinking and decision-making skills.
Excellent interpersonal and communication skills with the ability to interact with all levels of the organization.


Just as we are committed to creating exceptional value for our Partners For Success® agencies and policyholders, Amerisure also remains committed to being an employer of choice. We reinforce this commitment by adhering to an Employee Value Proposition that, in part, is provided through a competitive total rewards package. Amerisure offers a Compensation & Benefits Package that includes competitive base pay, performance-based incentive pay, comprehensive health & welfare benefits, 401(k) savings plan and profit sharing. In addition to generous paid time off programs, we allow our employees flexible and remote work arrangements. If you strive for excellence and are committed to personal and professional growth, Amerisure is looking for you.

Amerisure Insurance provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Amerisure Insurance complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Amerisure Insurance expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of Amerisure’s employees to perform their job duties may result in discipline up to and including discharge.","Capacidad de análisis, Computación en la nube, Extraer, transformar y cargar (ETL), Gestión de datos y SQL, Atención al detalle, Colaboración entre equipos, Creación de relaciones, Linaje de datos y Redacción de artículos destacados",Solicitar
https://www.linkedin.com/jobs/view/3956457171/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=JCWLVTZ6Kk19EIJahrgiKQ%3D%3D&trk=flagship3_search_srp_jobs,Data Loss Prevention Engineer,"8.130 US$/mes - 10,9 US$K/mes En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Sin experiencia",Publicado de nuevo hace 2 semanas,"California, Estados Unidos","Acerca del empleo
Job Description And Duties

The authority to establish this position and make appointment is contingent upon Legislative and Governor’s approval and enactment of the 2024/2025 Budget Act.

This advertisement is posted as until filled. The Job Control will remain open, and applications will be reviewed every other Friday, until filled.

Job Summary

Under direction of the IT Manager I as part of the Information Security Office (ISO), the incumbent works both independently and as part of the Information Security team in support of the mission of the department through continuous improvement of the department’s information security program and dedication to protecting the confidentiality, security, and availability of department information resources. Areas of responsibility include cloud and on-premises information security technology systems and services, information security operations and incident response, information security audits and assessments, information security policy and procedure, and information security compliance and reporting.

This is an opportunity to join a newly energized and highly innovative team, eager to embrace new technologies to effect digital transformation at the California Department of Managed Health Care (DMHC). Your work as a team member in the Office of Technology and Innovation (OTI) is to support DMHC’s mission to protect consumers’ health care rights and ensure a stable health care delivery system! Our goal is to be the best and most innovative technology office within the State of California!

The incumbent must have the ability to effectively work from home (Telework), or in our downtown Sacramento office. In order to Telework, the incumbent must have a Cable or Fiber connection to the Internet.

JOIN US ON OUR MISSION!

Your work supports the mission of the DMHC to protect consumers’ health care rights and ensure a stable health care delivery system for more than 29.7 million Californians.

ELIGIBILITY CONSIDERATIONS

Applicants must successfully complete an online examination and be authorized to work in the United States without the need for visa sponsorship now or in the future.

This position is eligible for a hybrid telework schedule under Government Code 14200. Under the Department of Managed Health Care’s (DMHC) current hybrid telework policy, employees may be required to report in-person to the office two times per week and will be required to utilize a shared or “hotel” workspace. Employees may also be required to report in person more frequently as determined by management, which may include working at a DMHC designated headquarters and/or offsite events or locations. Employees must work from a location within California while employed at the DMHC

Please let us know how you heard about this position by taking a brief survey: DMHC Recruitment Survey.

You will find additional information about the job in the  Duty Statement .

Working Conditions

The DMHC utilizes a hybrid telework model to provide all employees with an avenue to telework while ensuring business and operational needs are met.

Remote-Centered employees are expected to maintain a safe and distraction free work environment at the approved alternate work location. Remote-Centered employees agree to adhere to the state telework policy, the DMHC’s telework policy, and conditions cited in the Telework Agreement (STD 200).

Office-Centered employees are expected to maintain a dedicated workstation at a DMHC official worksite. Office-Centered employees are expected to work in a climate-controlled office or cubicle under artificial lighting.

Minimum Requirements You will find the Minimum Requirements in the Class Specification.

 INFORMATION TECHNOLOGY SPECIALIST II 

Additional Documents

 Job Application Package Checklist 
 Duty Statement 

Position Details

Job Code #:

JC-437972

Position #(s):

409-521-1414-XXX

Working Title:

 Data Loss Prevention Engineer 

Classification:

INFORMATION TECHNOLOGY SPECIALIST II

$8,130.00 - $10,893.00 A

# of Positions:

1

Work Location:

Sacramento County

Telework:

Hybrid

Job Type:

Permanent, Full Time

Department Information

The mission of the California Department of Managed Health Care (DMHC) is to protect consumers’ health care rights and ensure a stable health care delivery system. The DMHC accomplishes its mission by ensuring the health care system works for consumers. The Department protects the health care rights of more than 29.7 million Californians by regulating health care service plans, assisting consumers through a consumer Help Center, educating consumers on their rights and responsibilities and preserving the financial stability of the managed health care system.

DMHC values diversity at all levels of the organization and is committed to fostering an environment in which employees from a variety of backgrounds, cultures, and personal experiences are welcomed and can thrive. DMHC believes the diversity of our employees and their unique ideas inspire innovative solutions to further our mission. Join DMHC and help us improve the lives of all Californians.

If you are interested in learning about the Department of Managed Health Care (DMHC) culture from the perspective of someone like yourself, contact our Someone Like Me program. Within five business days of your request to participate in the Someone Like Me program, you will be matched with a DMHC employee with a similar background to discuss the DMHC’s culture. This program is not part of, or in any way affiliated with the application or hiring process. Prospective employees must complete the application process on Cal Careers (e.g., submit your application within the specified timeframes on the job posting) to be considered for hire at the DMHC. None of the information you provide through the Someone Like Me program will be relayed to the Hiring Unit.

Department Website: http://www.dmhc.ca.gov

Special Requirements

 The position(s) require(s) a Background Investigation be cleared prior to being hired. 

Any documents you submit for a job vacancy such as your state application, resume, cover letter, educational transcripts, etc. SHOULD NOT include ANY confidential information. Confidential information that should be excluded or removed from these documents include, but is not limited to, your Social Security Number (SSN), birthday, driver’s license number (unless required), basis of eligibility, examination results, LEAP status, marital status, and age. Confidential information on the first page of applications submitted electronically online, such as Easy ID number, SSN, examination related information, and driver’s license number will automatically be redacted upon submission.

Possession of Minimum Qualifications will be verified prior to interview and/or appointment. If you are using education to meet the minimum qualifications for this position, you must submit a copy of your high school transcripts or college/university transcript(s). Unofficial transcripts may be accepted during the application process; however, submission of official transcripts may be required prior to appointment.

Application Instructions

Dates printed on Mobile Bar Codes, such as the Quick Response (QR) Codes available at the USPS, are not considered Postmark dates for the purpose of determining timely filing of an application.

Final Filing Date: Until Filled

Who May Apply

Individuals who are currently in the classification, eligible for lateral transfer, eligible for reinstatement, have list eligibility, are in the process of obtaining list eligibility, or have SROA and/or Surplus eligibility (please attach your letter, if available). SROA and Surplus candidates are given priority; therefore, individuals with other eligibility may be considered in the event no SROA or Surplus candidates apply.

Applications will be screened and only the most qualified applicants will be selected to move forward in the selection process. Applicants must meet the Minimum Qualifications stated in the Classification Specification(s).

How To Apply

Complete Application Packages (including your Examination/Employment Application (STD 678) and applicable or required documents) must be submitted to apply for this Job Posting. Application Packages may be submitted electronically through your CalCareer Account at www.CalCareers.ca.gov. When submitting your application in hard copy, a completed copy of the Application Package listing must be included. If you choose to not apply electronically, a hard copy application package may be submitted through an alternative method listed below:

Address for Mailing Application Packages

You may submit your application and any applicable or required documents to:

Department of Managed Health Care

Attn: Human Resources - Julian Ortega

980 9th Street, Suite 500

Sacramento , CA 95814

Address for Drop-Off Application Packages

You may drop off your application and any applicable or required documents at:

Department of Managed Health Care

Human Resources - Julian Ortega

980 9th Street, Suite 500

Sacramento , CA 95814

08:00 AM - 05:00 PM

Required Application Package Documents

The following items are required to be submitted with your application. Applicants who do not submit the required items timely may not be considered for this job:

Current version of the State Examination/Employment Application STD Form 678 (when not applying electronically), or the Electronic State Employment Application through your Applicant Account at www.CalCareers.ca.gov. All Experience and Education relating to the Minimum Qualifications listed on the Classification Specification should be included to demonstrate how you meet the Minimum Qualifications for the position. 
Resume is required and must be included. 
Statement of Qualifications - Please see the Statement of Qualifications (SOQ) requirements below 

Applicants requiring reasonable accommodations for the hiring interview process must request the necessary accommodations if scheduled for a hiring interview. The request should be made at the time of contact to schedule the interview. Questions regarding reasonable accommodations may be directed to the EEO contact listed on this job posting.

Desirable Qualifications

In addition to evaluating each candidate's relative ability, as demonstrated by quality and breadth of experience, the following factors will provide the basis for competitively evaluating each candidate:

Experience with writing and/or updating policies/procedures. 
Working knowledge and understanding of State and Federal regulation including: NIST 800-53 and NIST Cybersecurity Framework (CSF), State Information Management Manual (SIMM), and/or State Administrative Manual (SAM). 
Experience with data classification and data loss prevention tools and best practices. 
Experience with email hygiene solutions. 
Knowledge of computer networking concepts, protocols, and network security methodologies. 
Experience and knowledge in the practices, principles, and techniques of information security. 
Demonstrated ability to effectively communicate verbally and in writing to all levels of management and staff. 
Experience with developing or providing training. 
Background in either performing audits or responding to audit requests. 

Benefits

Benefit information can be found on the CalHR website and the CalPERS website.

Contact Information

The Human Resources Contact is available to answer questions regarding the application process. The Hiring Unit Contact is available to answer questions regarding the position.

Department Website: http://www.dmhc.ca.gov

Human Resources Contact:

Julian Ortega

(916) 403-4332

Julian.Ortega@dmhc.ca.gov

Hiring Unit Contact:

Justin Tomek

(916) 323-7908

justin.tomek@dmhc.ca.gov

Please direct requests for Reasonable Accommodations to the interview scheduler at the time the interview is being scheduled. You may direct any additional questions regarding Reasonable Accommodations or Equal Employment Opportunity for this position(s) to the Department's EEO Office.

EEO Contact:

EEO Office

(916) 738-3394

EEO@dmhc.ca.gov

California Relay Service: 1-800-735-2929 (TTY), 1-800-735-2922 (Voice) TTY is a Telecommunications Device for the Deaf, and is reachable only from phones equipped with a TTY Device.

Additional Application Instructions

To be considered for this vacancy, please complete all applicable fields on the application form, including a list or description of previous/current occupational experience in the duties performed section.

Do NOT include your social security number on any submitted documents. If submitting your application via postal mail or in person, please notate RPA: 23-319 and Job Control: JC-437972 on your application.

All Experience and Education relating to the Minimum Qualifications listed on the Classification Specification should be included to clearly demonstrate how you meet the Minimum Qualifications for the position on your State Application (STD Form 678). The application should also clearly demonstrate the candidate’s ability to meet the Desirable Qualifications identified in this job advertisement. The Classification Specification for the Information Technology Specialist II is located at the top of this Job Announcement Posting under Minimum Requirements.

Foreign Degrees or Transcripts – Applicants with foreign degrees or transcripts who wish to apply that coursework toward meeting the minimum qualifications of the classification must provide a transcript evaluation that indicates the number of units to which his/her foreign coursework is equivalent. DMHC accepts foreign transcript evaluations that are completed by one of the agencies approved by the California Commission on Teacher Credentialing.

PLEASE NOTE: If you are mailing your application, it must be postmarked by the final filing date. Hand delivered applications must be submitted no later than 5:00 p.m. on the final filing date. Applications slipped under the door at the Human Resources Office will be time stamped the following business day.

Qualifications

 Statement of Qualifications (SOQ) Instructions 

The Statement of Qualifications (SOQ) is used to evaluate applicants for potential interview and is critical to the hiring process. If your qualifications meet the requirements for the position, you may be invited for an interview. The SOQ consists of questions that must specifically be addressed and submitted with an application using the provided SOQ Template. Resumes and cover letters DO NOT take the place of the SOQ. Failure to submit a completed SOQ Template disqualifies an applicant from hiring consideration. 

Download the SOQ Template

Equal Opportunity Employer

The State of California is an equal opportunity employer to all, regardless of age, ancestry, color, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding and related medical conditions), and sexual orientation.

It is an objective of the State of California to achieve a drug-free work place. Any applicant for state employment will be expected to behave in accordance with this objective because the use of illegal drugs is inconsistent with the law of the State, the rules governing Civil Service, and the special trust placed in public servants.","Extraer, transformar y cargar (ETL) y Ingeniería de datos, Cables, Clasificación de datos, Comunicación, Higiene, Prevención de pérdida de datos (DLP), Specifications, Transcripts y Óptica de fibras",Solicitar
https://www.linkedin.com/jobs/view/3818366561/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=7AxEyKMzB3eDZbxO4YxPCQ%3D%3D&trk=flagship3_search_srp_jobs,Entry Level Developer/Coder/Programmer/Data Scientist/Analyst/Engineer,"Presencial Contrato por obra
Coincide con tus preferencias de empleo. El tipo de empleo es Contrato por obra.
Sin experiencia",hace 5 meses,"Rialto, CA","Acerca del empleo
2024 is finally here and we hope the Job market improves however as per a resume builder survey based on response from more than 900 companies 4 out of 10 companies are planning to have layoffs in 2024 or have a hiring freeze.  Almost 390,000 tech employees have been laid off since 2022 and it's still ongoing. The effect of this has led hundreds of thousands of laid off Tech employees competing with existing Jobseekers.

 AI is replacing many normal jobs which were done by people. As per news reports Google is planning to Client off 30,000 employees in its ad sales who will be replaed by AI ad technology.

 Entry level Job seekers struggle to get responses to their applications, are getting ghosted after interviews.  In such a scenario the Job seekers need  to differentiate themselves by ensuring to obtain exceptional skills and technologies so that they can wear multiple roles at a client as clients now would want to expand roles and responsibilities assigned to a particular job to save costs.

Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.

All Positions are open for all visas and US citizens

We are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs as Clients now post covid can also hire remote workers which increases even more competition for jobseekers.

We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart lab s etc to name a few.

We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.

Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in the Tech Industry.

 We assist in filing for STEM extension and also for H1b and Green card filing to Candidates

We also offer Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If a Jobseeker is qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection are totally based on clients discretion not ours.

 If you applied for a job and got emails from our skill enhancement team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. 

 please check the below links to see success outcomes of our candidates and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers

 https://www.synergisticit.com/candidate-outcomes/

 We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 

https://reg.rf.oracle.com/flow/oracle/cwoh23/OCWExhibitorCatalog/page/OCWexhibitorcatalog

 https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrn

 https://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Q

 https://www.youtube.com/watch?v=NVBU9RYZ6UI

 https://www.youtube.com/watch?v=EmO7NrWHkLM

 https://www.youtube.com/watch?v=OAFOhcGy9Z8

For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/

 We are looking for the right matching candidates for our clients

 Please apply via the job posting

REQUIRED SKILLS For Java /Full Stack/Software Programmer

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Project work on the skills
Knowledge of Core Java , javascript , C+
or software programming
Spring boot, Microservices, Docker, Jenkins and REST API's experience
Excellent written and verbal communication skills

 For data Science/Machine learning Positions

Required Skills

Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT
Project work on the technologies needed
Highly motivated, self-learner, and technically inquisitive
Experience in programming language Java and understanding of the software development life cycle
Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools
Excellent written and verbal communication skills

 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow

 If you get emails from our skill enhancement team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements. 

 No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates","Lenguajes de programación y Programación, Ciencias de la computación, Desarrollo de software, Java, JavaScript, Plataforma Java, Programación en C, Stack y Transferencia de Estado Representacional (REST)",Solicitar
https://www.linkedin.com/jobs/view/3975034703/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=rG7EbLs1STaE5TsGmI7zlA%3D%3D&trk=flagship3_search_srp_jobs,Azure Data Engineer,"110 US$K/año - 120 US$K/año Presencial Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Algo de responsabilidad",hace 1 semana,"Seattle, WA","Acerca del empleo
Role: Azure Data Engineer
Job Type: Full Time
Location: Seattle, WA (Onsite)

Job Description:
Must have skills:

Proficient in Python and PySpark.
Strong knowledge of SQL.
Experience in building and optimizing complex data pipelines in Azure.
Hand on experience on Azure Databricks.
Strong working knowledge on Azure components and services such as Storage Account, Azure Data Factory, Active Directory, Synapse, Key Vault, VMSS, Function apps, web apps, Log analytics workspace , Service principal, managed identity , ACR and container instances etc.
Should possesses knowledge about Azure networking services.
Knowledge on Azure infrastructure creation using ARM templates.
Well versed with GitLab and Azure DevOps.

Roles and Responsibilities:

Build and maintain data pipelines
Build and support required Azure Infrastructure - configurations, updates, etc.
Support Data storage and processing framework
Working with stakeholders including data, design, product
Build processes that support data transformation, workload management, dependency and metadata","PySpark, Azure Databricks",Solicitud sencilla
https://www.linkedin.com/jobs/view/3980465699/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=h%2FeRLBssUCqwWZm2oRJiYw%3D%3D&trk=flagship3_search_srp_jobs,Teamcenter PLM Data Migration Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",hace 1 semana,Estados Unidos,"Acerca del empleo
Technical Skills

Experienced in the Data migration projects in PLM industry in specific to TeamCenter

Proficiency in data migration tools and technologies (e.g., ETL tools, SQL, scripting languages).

In-depth knowledge of Teamcenter PLM architecture, data models, and integration points.

Strong understanding of data mapping, transformation, and validation techniques.

Key Responsibilities

Develop and implement comprehensive data migration strategies and plans for Teamcenter

PLM.

Assess the current data environment and identify potential challenges and risks associated with

migration.

Analyze and map source data to target Teamcenter PLM structures.

Define and document data transformation rules to ensure data integrity and consistency during

migration.

Lead the execution of data migration activities, including data extraction, transformation, and

loading (ETL) processes.

Utilize migration tools and scripts to automate data migration tasks and improve efficiency.

Conduct data validation and reconciliation to ensure the accuracy and completeness of

migrated data.

Identify and resolve data discrepancies and issues during and after migration.

Collaborate with cross-functional teams, including IT, PLM developers, business analysts, and

end-users, to gather requirements and ensure successful migration.

Provide regular updates and reports on migration progress, issues, and resolutions to

stakeholders.

Monitor and support post-migration activities, addressing any issues that arise to ensure a

smooth transition.

Provide training and documentation to end-users and support teams on new data structures and

processes within Teamcenter PLM.

Qualifications

Essential Skills:

Excellent problem-solving and analytical skills.

Strong communication and interpersonal skills.

Ability to work independently and collaboratively in a team environment.""

Strong general IT skills with experience with both the Windows and Linux OS platforms

Experience Performing Teamcenter Systems Data Migration Skills

Strong customer focus, including the ability to manage customer needs and multiple work

priorities.

Needs strong oral and written communication, analytical, and problem-solving skills, as well as

excellent judgment and self-motivation.

Lead the execution of data migration activities, including data extraction, transformation, and

loading (ETL) processes.

Ability to work independently with little to no supervision researching new technologies or

comparing.

At Least years of strong TCE admin experience","Extraer, transformar y cargar (ETL), Asignación de datos, Comunicación, Habilidades sociales, Migración de datos, Modelo de datos, Obtención de requisitos, Resolución de problemas, Teamcenter y Validación de datos",Solicitud sencilla
https://www.linkedin.com/jobs/view/3966430399/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=9U0LZVz7it2zy9uudbE82g%3D%3D&trk=flagship3_search_srp_jobs,"Data Engineer I (Snowflake, AWS and Python)","105,1 US$K/año - 173,4 US$K/año Híbrido Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 semanas,"Hartford, CT","Acerca del empleo
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Travelers, is seeking the following. Apply via Dice today!

Who Are We?

Taking care of our customers, our communities and each other. That's the Travelers Promise. By honoring this commitment, we have maintained our reputation as one of the best property casualty insurers in the industry for over 160 years. Join us to discover a culture that is rooted in innovation and thrives on collaboration. Imagine loving what you do and where you do it.

Compensation Overview

The annual base salary range provided for this position is a nationwide market range and represents a broad range of salaries for this role across the country. The actual salary for this position will be determined by a number of factors, including the scope, complexity and location of the role; the skills, education, training, credentials and experience of the candidate; and other conditions of employment. As part of our comprehensive compensation and benefits program, employees are also eligible for performance-based cash incentive awards.

Salary Range

$105,100.00 - $173,400.00

Target Openings

1

What Is the Opportunity?

Travelers Data Engineering team constructs pipelines that contextualize and provide easy access to data by the entire enterprise. As a Data Engineer, you will play a key role in growing and transforming our analytics landscape. In addition to your strong analytical mind, you will bring your inquisitive attitude and ability to translate the stories found in data. You will leverage your ability to design, build and deploy data solutions that capture, explore, transform, and utilize data to support Artificial Intelligence, Machine Learning and business intelligence/insights.

What Will You Do?

Strong hands-on technical skills in Snowflake cost monitoring, performance optimization and DB administration.
Strong Snowflake SQL tuning skills to optimize long running and very complex queries.
Strong knowledge in overall data analytics environment and be very familiar with ETL and Reporting workloads. Be able to effectively identify performance bottlenecks and tune/optimize the code to improve workload response time.
Have positive energy, creative mindset and can-do attitude. Be able to take on new initiatives, conduct research and proof of concept work. For example, conduct POC on GenAI models in Snowflake and AWS.
Be able to take the lead and ownership to proactively drive the work forward, without day to day manager supervision and task assignments.
Build and operationalize complex data solutions, correct problems, apply transformations, and recommend data cleansing/quality solutions.
Design data solutions.
Analyze sources to determine value and recommend data to include in analytical processes.
Incorporate core data management competencies including data governance, data security and data quality.
Collaborate within and across teams to support delivery and educate end users on data products/analytic environment.
Perform data and system analysis, assessment and resolution for defects and incidents of moderate complexity and correct as appropriate.
Test data movement, transformation code, and data components.
Perform other duties as assigned

What Will Our Ideal Candidate Have?

Bachelor's Degree in STEM related field or equivalent
Six years of related experience
Proficient use of tools, techniques, and manipulation including Cloud platforms, programming languages, and an understanding of software engineering practices. 
The ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases, and contribute to tradeoff and negotiation discussions.
Demonstrated track record of domain expertise including the ability to understand technical concepts and possess in-depth knowledge of immediate systems worked on.
Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems.
Strong verbal and written communication skills with the ability to interact with team members and business partners.
Leadership - Intermediate leadership skills with a proven track record of self-motivation in identifying personal growth opportunities.

What is a Must Have?

Bachelor's degree or equivalent training with data tools, techniques, and manipulation.
Four years of data engineering or equivalent experience.

What Is in It for You?

Health Insurance: Employees and their eligible family members - including spouses, domestic partners, and children - are eligible for coverage from the first day of employment.
Retirement: Travelers matches your 401(k) contributions dollar-for-dollar up to your first 5% of eligible pay, subject to an annual maximum. If you have student loan debt, you can enroll in the Paying it Forward Savings Program. When you make a payment toward your student loan, Travelers will make an annual contribution into your 401(k) account. You are also eligible for a Pension Plan that is 100% funded by Travelers.
Paid Time Off: Start your career at Travelers with a minimum of 20 days Paid Time Off annually, plus nine paid company Holidays.
Wellness Program: The Travelers wellness program is comprised of tools, discounts and resources that empower you to achieve your wellness goals and caregiving needs. In addition, our mental health program provides access to free professional counseling services, health coaching and other resources to support your daily life needs.
Volunteer Encouragement: We have a deep commitment to the communities we serve and encourage our employees to get involved. Travelers has a Matching Gift and Volunteer Rewards program that enables you to give back to the charity of your choice.

Employment Practices

Travelers is an equal opportunity employer. We believe that we can deliver the very best products and services when our workforce reflects the diverse customers and communities we serve. We are committed to recruiting, retaining and developing the diverse talent of all of our employees and fostering an inclusive workplace, where we celebrate differences, promote belonging, and work together to deliver extraordinary results.

In accordance with local law, candidates seeking employment in Colorado are not required to disclose dates of attendance at or graduation from educational institutions.

If you are a candidate and have specific questions regarding the physical requirements of this role, please send us an so we may assist you.

Travelers reserves the right to fill this position at a level above or below the level included in this posting.

To learn more about our comprehensive benefit programs please visit . Data Engineer I (Snowflake, AWS and Python)","Extraer, transformar y cargar (ETL), Gobierno de datos y Ingeniería de datos, Calidad de datos, Depuración de programas, Limpieza de datos, Resolución de problemas, SQL Tuning, Snowflake y Snowflake cloud",Solicitar
https://www.linkedin.com/jobs/view/3873538106/?eBP=NOT_ELIGIBLE_FOR_CHARGING&refId=aeKGUOsCh%2B1xBv8QKfgSew%3D%3D&trackingId=fhDVZQziIsRtny1GwSMNgA%3D%3D&trk=flagship3_search_srp_jobs,MuleSoft/Data Integration Engineer,"En remoto
Coincide con tus preferencias de empleo. La modalidad laboral es En remoto.
Jornada completa
Coincide con tus preferencias de empleo. El tipo de empleo es Jornada completa.
Intermedio",Publicado de nuevo hace 2 meses,Estados Unidos,"Acerca del empleo
Description

About Our Team

 

Our employees thrive in a culture that's fast-paced and ego-free, where innovation and collaboration are encouraged at every turn. We are an organization that provides federal agencies instant access to experienced and talented professionals who understand their unique challenges and know the most efficient ways to address them. We are continually investing in resources and talent, so we stay prepared with specialized teams in place who are experts in creating tailored technologies. Our solutions empower Federal organizations to grow, modernize, and succeed in a rapidly evolving landscape.

We value all voices and want to attract talent from all backgrounds. We're on the lookout for individuals who are passionate about technology and thrive in environments where problem-solving is approached with creativity and enthusiasm. If you're someone who enjoys continuously expanding your skill set while tackling real-world business problems, you'll feel right at home with us. Veterans and military spouses are especially encouraged to bring your unique and valuable experience to our team.

About The Role

 

We are seeking a senior MuleSoft/Data Integration Engineer. As a MuleSoft Engineer, you will be responsible for designing, developing, and deploying scalable integration solutions that enable seamless connectivity between disparate systems, applications, and data sources. You will work closely with clients and cross-functional teams to understand business requirements, architect integration solutions, and implement best-in-class APIs, connectors, and workflows using MuleSoft technologies. We're seeking team members to help modernize and enhance enterprise system that connects providers directly with millions of US veterans seeking care and services.

Responsibilities

Our team member will work in an Agile-Scrum environment on a collaborative team to:

Collaborate with stakeholders and architects to gather and analyze business requirements for integration projects.
Design and architect end-to-end integration solutions using MuleSoft's Anypoint Platform, including API-led connectivity, message routing, mediation, and data transformation.
Develop custom APIs, connectors, and data mappings using MuleSoft's Anypoint Studio and Anypoint Runtime.
Configure and deploy MuleSoft applications to on-premises or cloud-based environments, ensuring scalability, reliability, and performance.
Implement security measures, such as OAuth, JWT, or SSL, to protect APIs and ensure compliance with security standards.
Conduct unit testing, integration testing, and performance testing of MuleSoft applications to ensure quality and reliability.
Provide technical expertise and support to project teams throughout the software development lifecycle, including requirements gathering, design, development, testing, and deployment.
Collaborate with cross-functional teams, including developers, architects, testers, and business analysts, to ensure successful delivery of integration projects.
Stay updated with the latest MuleSoft features, tools, and best practices, and proactively recommend enhancements and improvements to existing solutions.
Develop and maintain API documentation, including specifications, guidelines, and best practices. 

Qualifications

Bachelor's degree in Computer Science, Information Technology, or related field.
Have extensive hands-on experience with MuleSoft Any point Platform, including Mule ESB, Any point studio, Any point manager, Any point connectors 
MuleSoft certification(s), such as MuleSoft Certified Developer - Level 1 and 2 or MuleSoft Certified Platform/Integration Architect, are highly preferred.
Proven experience in designing and implementing integration solutions using MuleSoft's Anypoint Platform.
Strong understanding of integration patterns, RESTful APIs, SOAP web services, and messaging protocols (such as HTTP, JMS, or MQTT).
Proficiency in MuleSoft development tools, including Anypoint Studio, Anypoint Runtime, DataWeave, and MuleSoft Connectors.
Experience with enterprise integration patterns (EIPs), message queues, and event-driven architectures.
Excellent communication and interpersonal skills, with the ability to collaborate effectively with clients and cross-functional teams.
Strong analytical and problem-solving skills, with a keen attention to detail and accuracy.
Experience with Java based integrations is highly preferred.
At least three (3) years of demonstrated expertise designing, implementing, and supporting Enterprise-grade technical solutions meeting complex business requirements. 
Ability to acquire a Public Trust clearance.","Bus de servicios de empresa, Ciencias de la computación, Comunicación, Integración de empresas, Mule ESB, MuleSoft Anypoint Platform, Necesidades empresariales, Resolución de problemas, SOAP y Transferencia de Estado Representacional (REST)",Solicitar
